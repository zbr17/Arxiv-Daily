# ArXiv 每日推荐 - 2025-10-31

> 更新于北京时间：2025-10-31 12:25:16
> 已自动阅读了 245 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：119128

## Autonomous driving and large models

### [Score: 9.0/10] Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving
- **Authors:** Lin Liu, Guanyi Yu, Ziying Song, Junqiao Li, Caiyan Jia, Feiyang Jia, Peiliang Wu, Yandan Luo
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26292](https://arxiv.org/abs/2510.26292)
- **Reason:** Proposes CATG, a constrained flow matching framework for autonomous driving planning that mitigates mode collapse and incorporates safety/kinematic constraints, achieving 2nd place in NavSim v2 challenge.
Score: 9
Field: Autonomous driving and large models

## 多模态大模型

### [Score: 9.0/10] Emu3.5: Native Multimodal Models are World Learners
- **Authors:** Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26583](https://arxiv.org/abs/2510.26583)
- **Reason:** 提出大规模多模态世界模型Emu3.5，支持视觉-语言交互生成，创新Discrete Diffusion Adaptation提升推理效率20倍，开源且由BAAI等知名机构研发，是多模态大模型的最新重要进展。
Score: 9
Field: 多模态大模型

### [Score: 8.0/10] SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models
- **Authors:** Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26769](https://arxiv.org/abs/2510.26769)
- **Reason:** 提出轻量级激活引导模块SteerVLM，解决VLMs的输出对齐问题，引入VNIA多模态数据集，方法高效且对多模态大模型的可控性有重要提升。
Score: 8
Field: 多模态大模型

### [Score: 8.0/10] MemEIC: A Step Toward Continual and Compositional Knowledge Editing
- **Authors:** Jin Seong, Jiyun Park, Wencke Liermann, Hongseok Choi, Yoonji Nam, Hyun Kim, Soojong Lim, Namhoon Lee
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25798](https://arxiv.org/abs/2510.25798)
- **Reason:** 提出MemEIC方法，解决多模态大模型的持续知识编辑问题，结合外部记忆和LoRA实现跨模态组合编辑，实证提升复杂问题性能。
Score: 8
Field: 多模态大模型

### [Score: 8.0/10] Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start
- **Authors:** Kun Chen, Peng Shi, Haibo Qiu, Zhixiong Zeng, Siqi Yang, Wenji Mao, Lin Ma
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25801](https://arxiv.org/abs/2510.25801)
- **Reason:** 提出偏好-based冷启动方法，解耦多模态学习中的任务和格式，提升模型泛化，实证在MEGA-Bench等基准上有显著提升。
Score: 8
Field: 多模态大模型

### [Score: 8.0/10] π_RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models
- **Authors:** Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang, Chao Yu
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25889](https://arxiv.org/abs/2510.25889)
- **Reason:** 提出π_RL框架，用在线RL微调基于流的VLA模型，提升机器人任务性能（如LIBERO和ManiSkill基准），是多模态大模型在Vision-Language-Action方向的重要应用。
Score: 8
Field: 多模态大模型

### [Score: 8.0/10] Modular Linear Tokenization (MLT)
- **Authors:** Tcharlies Schmitz
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25952](https://arxiv.org/abs/2510.25952)
- **Reason:** 提出MLT方法，用模运算和线性变换实现高基数分类ID的可逆tokenization，比监督嵌入更高效，是多模态大模型中tokenizer的创新工作。
Score: 8
Field: 多模态大模型

### [Score: 8.0/10] GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks
- **Authors:** Chenrui Shi, Zedong Yu, Zhi Gao, Ruining Feng, Enqi Liu, Yuwei Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, Qing Li
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26098](https://arxiv.org/abs/2510.26098)
- **Reason:** 论文针对VLM在GUI任务中的失败模式，提炼了GUI知识的三个维度（界面感知、交互预测、指令理解）并构建评估基准，直接对应多模态大模型中的GUI Grounding和GUI Agent研究，揭示了VLM在GUI任务中的知识缺陷，对提升多模态大模型的GUI能力有重要价值。
Score: 8
Field: 多模态大模型

### [Score: 8.0/10] Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis
- **Authors:** Xinhan Zheng, Huyu Wu, Xueting Wang, Haiyun Jiang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26721](https://arxiv.org/abs/2510.26721)
- **Reason:** 聚焦多模态大模型的文本偏置问题，通过注意力键空间分析揭示偏置源于视觉与文本键空间的内在错位，为改进多模态模型的模态对齐提供了关键机制 insights
Score: 8
Field: 多模态大模型

### [Score: 7.0/10] ChartAB: A Benchmark for Chart Grounding & Dense Alignment
- **Authors:** Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26781](https://arxiv.org/abs/2510.26781)
- **Reason:** 针对VLMs的图表理解问题，提出ChartAB基准，评估图表grounding和对齐能力，揭示现有模型弱点，推动多模态大模型的图表理解研究。
Score: 7
Field: 多模态大模型

### [Score: 7.0/10] ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion
- **Authors:** Sungho Koh, SeungJu Cha, Hyunwoo Oh, Kwanyoung Lee, Dong-Jin Kim
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25818](https://arxiv.org/abs/2510.25818)
- **Reason:** 提出ScaleDiff框架，通过Neighborhood Patch Attention和Latent Frequency Mixing提升扩散模型的高分辨率生成效率，兼容多种架构，是多模态大模型中图像生成的实用改进。
Score: 7
Field: 多模态大模型

### [Score: 7.0/10] Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens
- **Authors:** Ziliang Chen, Tianang Xiao, Jusheng Zhang, Yongsen Zheng, Xipeng Chen
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26302](https://arxiv.org/abs/2510.26302)
- **Reason:** 采用token级因果框架分析CLIP的组合性缺陷，提出composition nonidentifiability解释其对概念操作的不敏感性，对多模态大模型的组合性推理改进有指导意义。
Score: 7
Field: 多模态大模型

## Multi-modal large models

### [Score: 8.0/10] MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency
- **Authors:** Nicolas Dufour, Lucas Degeorge, Arijit Ghosh, Vicky Kalogeiton, David Picard
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25897](https://arxiv.org/abs/2510.25897)
- **Reason:** Proposes a multi-reward conditioned pretraining method for text-to-image (T2I) models that enhances visual quality and training efficiency, addressing single-reward optimization limitations in T2I alignment.
Score: 8
Field: Multi-modal large models

### [Score: 8.0/10] SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing
- **Authors:** Sung-Hoon Yoon, Minghan Li, Gaspard Beaudouin, Congcong Wen, Muhammad Rafay Azhar, Mengyu Wang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25970](https://arxiv.org/abs/2510.25970)
- **Reason:** Introduces a flow decomposition-and-aggregation framework for inversion-free text-to-image editing, resolving critical issues of semantic fidelity and attribute disentanglement in rectified flow models.
Score: 8
Field: Multi-modal large models

### [Score: 8.0/10] EgoExo-Con: Exploring View-Invariant Video Temporal Understanding
- **Authors:** Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26113](https://arxiv.org/abs/2510.26113)
- **Reason:** Introduces EgoExo-Con, a benchmark for view-invariant video temporal understanding in Video-LLMs, and proposes View-GRPO to improve cross-view consistency, addressing multi-view video reasoning limitations.
Score: 8
Field: Multi-modal large models

### [Score: 8.0/10] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark
- **Authors:** Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil Iyer, Haidar Khan, Lingkun Kong, Roy Luo, Tiffany Ma, Zhen Qiao, David Tran, Wenfang Xu, Skyler Yeatman, Chen Zhou, Gunveer Gujral, Yinglong Xia, Shane Moon, Nicolas Scheffer, Nirav Shah, Eun Chang, Yue Liu, Florian Metze, Tammy Stark, Zhaleh Feizollahi, Andrea Jessee, Mangesh Pujari, Ahmed Aly, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Wen-tau Yih, Xin Luna Dong
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26160](https://arxiv.org/abs/2510.26160)
- **Reason:** Introduces CRAG-MM, a comprehensive benchmark for multi-modal multi-turn RAG in wearable scenarios, addressing gaps in evaluating MM-RAG for real-world applications like smart glasses.
Score: 8
Field: Multi-modal large models

### [Score: 8.0/10] LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation
- **Authors:** Xiangqing Zheng, Chengyue Wu, Kehai Chen, Min Zhang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26412](https://arxiv.org/abs/2510.26412)
- **Reason:** Introduces LoCoT2V-Bench, a benchmark for long-form complex text-to-video generation with multi-dimensional evaluation (event alignment, temporal consistency, narrative flow), addressing limitations of existing T2V benchmarks.
Score: 8
Field: Multi-modal large models

### [Score: 8.0/10] Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing
- **Authors:** Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26474](https://arxiv.org/abs/2510.26474)
- **Reason:** Addresses the Matthew effect (bias towards simple queries) in LVLMs self-improvement using distribution-reshaping and trajectory-resampling strategies, improving visual reasoning for complex tasks.
Score: 8
Field: Multi-modal large models

### [Score: 7.0/10] Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders
- **Authors:** Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26027](https://arxiv.org/abs/2510.26027)
- **Reason:** Proposes a Video-LLM architecture with stacked temporal attention in the vision encoder to boost temporal reasoning and action recognition, addressing gaps in video understanding for Video-LLMs.
Score: 7
Field: Multi-modal large models

### [Score: 7.0/10] Security Risk of Misalignment between Text and Image in Multi-modal Model
- **Authors:** Xiaosen Wang, Zhijin Ge, Shaokang Wang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26105](https://arxiv.org/abs/2510.26105)
- **Reason:** Identifies a novel security risk (PReMA attack) in multi-modal diffusion models due to text-image misalignment, highlighting vulnerabilities in fixed-prompt image-editing applications.
Score: 7
Field: Multi-modal large models

### [Score: 7.0/10] OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research
- **Authors:** Caoshuo Li, Zengmao Ding, Xiaobin Hu, Bang Li, Donghao Luo, Xu Peng, Taisong Jin, Yongge Liu, Shengwei Han, Jing Yang, Xiaoping He, Feng Gao, AndyPian Wu, SevenShu, Chaoyang Wang, Chengjie Wang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26114](https://arxiv.org/abs/2510.26114)
- **Reason:** Presents OracleAgent, the first multimodal reasoning agent for Oracle Bone Script (OBS) research, integrating LLM-powered tools and a domain-specific multimodal knowledge base to assist OBS analysis.
Score: 7
Field: Multi-modal large models

### [Score: 7.0/10] Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition
- **Authors:** Pei Peng, MingKun Xie, Hang Hao, Tong Jin, ShengJun Huang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26466](https://arxiv.org/abs/2510.26466)
- **Reason:** Proposes a representation-level counterfactual approach to debias zero-shot recognition in VLMs, subtracting background-only activation to mitigate object-context shortcuts, improving reliability.
Score: 7
Field: Multi-modal large models

### [Score: 7.0/10] CATCH: A Modular Cross-domain Adaptive Template with Hook
- **Authors:** Xinjin Li, Yulie Lu, Jinghan Cao, Yu Ma, Zhenglin Li, Yeyang Zhou
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26582](https://arxiv.org/abs/2510.26582)
- **Reason:** Proposes CATCH, a plug-and-play framework for cross-domain VQA adaptation using domain classifiers and dual adapters, improving generalization of VQA models to out-of-domain scenarios (e.g., medical, remote sensing).
Score: 7
Field: Multi-modal large models

## 自动驾驶与大模型

### [Score: 8.0/10] All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles
- **Authors:** Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Ahmad Sarlak, Mahlagha Fazeli, Abolfazl Razi
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26641](https://arxiv.org/abs/2510.26641)
- **Reason:** 针对自动驾驶目标检测的综述，系统梳理传感器融合、VLMs/LLMs等最新技术，覆盖多模态大模型在自动驾驶中的应用，对该方向有全面指导价值。
Score: 8
Field: 自动驾驶与大模型

### [Score: 7.0/10] Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization
- **Authors:** Zhipeng Bao, Qianwen Li
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26023](https://arxiv.org/abs/2510.26023)
- **Reason:** 论文提出基于LLM的自动驾驶车辆 immobilization恢复框架StuckSolver，通过LLM的自推理或乘客引导生成恢复指令，属于自动驾驶与大模型的交叉研究，解决了自动驾驶中的实际场景问题，符合用户的研究方向。
Score: 7
Field: 自动驾驶与大模型

## 深度学习理论

### [Score: 8.0/10] A Practitioner's Guide to Kolmogorov-Arnold Networks
- **Authors:** Amir Noorizadegan, Sifan Wang, Leevan Ling
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25781](https://arxiv.org/abs/2510.25781)
- **Reason:** 系统综述Kolmogorov-Arnold Networks（KANs）的理论、架构和实践，覆盖基函数选择等关键问题，对深度学习理论中的网络架构研究有全面参考价值。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization
- **Authors:** Di Zhang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26068](https://arxiv.org/abs/2510.26068)
- **Reason:** 提出通过度量优化构建自适应流形模型的框架，结合离散微分几何和自动微分，对深度学习理论中的动态模型和几何学习有深远理论贡献。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime
- **Authors:** Beomhan Baek, Minhak Song, Chulhee Yun
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26303](https://arxiv.org/abs/2510.26303)
- **Reason:** 研究全批量与增量Adam在可分数据上的隐式偏见差异，发现增量Adam收敛到ℓ2-最大间隔分类器，填补了Adam在不同批量设置下的理论空白，对optimizer的理论理解有重要价值。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] LLMs Process Lists With General Filter Heads
- **Authors:** Arnab Sen Sharma, Giordano Rogers, Natalie Shapira, David Bau
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26784](https://arxiv.org/abs/2510.26784)
- **Reason:** 发现大语言模型中存在通用过滤头（filter heads）编码抽象过滤谓词，属于深度学习理论中模型结构与计算机制的研究，对理解transformer的泛化能力有重要价值
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?
- **Authors:** Zihan Pengmei, Costas Mavromatis, Zhengyuan Shen, Yunyi Zhang, Vassilis N. Ioannidis, Huzefa Rangwala
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25791](https://arxiv.org/abs/2510.25791)
- **Reason:** 研究Chain-of-Thought（CoT）对Transformer学习动力学的影响，通过grokking现象分析CoT作用机制，对深度学习理论中的大模型推理学习有重要启示。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks
- **Authors:** Jialong Sun, Xinpeng Ling, Jiaxuan Zou, Jiawen Kang, Kejia Zhang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25800](https://arxiv.org/abs/2510.25800)
- **Reason:** 揭示神经网络在时间序列任务中的低频spectral bias现象，提出FreLE损失函数优化模型泛化，对深度学习理论中的模型偏差研究有实用价值。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training
- **Authors:** Hong Wang, Haiyang Xin, Jie Wang, Xuanze Yang, Fei Zha, Huanshuo Dong, Yan Jiang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25803](https://arxiv.org/abs/2510.25803)
- **Reason:** 提出MoE-POT架构，用混合专家网络解决PDE预训练的异质性问题，提升零样本性能，对深度学习理论中的神经算子和预训练研究有创新贡献。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Robust GNN Watermarking via Implicit Perception of Topological Invariants
- **Authors:** Jipeng Li, Yannning Shen
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25934](https://arxiv.org/abs/2510.25934)
- **Reason:** 提出InvGNN-WM方法，基于拓扑不变量实现GNN的鲁棒水印，解决模型知识产权问题，对深度学习理论中的GNN安全研究有理论和实证贡献。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Contrastive Predictive Coding Done Right for Mutual Information Estimation
- **Authors:** J. Jon Ryu, Pavan Yeddanapudi, Xiangxiang Xu, Gregory W. Wornell
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25983](https://arxiv.org/abs/2510.25983)
- **Reason:** 改进InfoNCE提出InfoNCE-anchor，提升互信息估计准确性，揭示自监督学习中密度比的重要性，对深度学习理论中的自监督学习研究有理论贡献。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations
- **Authors:** Darius Masoum Zadeh-Jousdani, Elvin Hajizada, Eyke Hüllermeier
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25993](https://arxiv.org/abs/2510.25993)
- **Reason:** 提出PCN-TA方法，利用时间相关性提升预测编码网络的在线学习效率，有生物 plausibility，对深度学习理论中的在线学习和神经模型研究有实用价值。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods
- **Authors:** Jiali Cheng, Chirag Agarwal, Hadi Amiri
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26038](https://arxiv.org/abs/2510.26038)
- **Reason:** 研究知识蒸馏对模型去偏的影响，揭示蒸馏后去偏能力的变化，分析内部机制，对深度学习理论中的模型蒸馏和偏差研究有重要发现。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Towards Scaling Laws for Symbolic Regression
- **Authors:** David Otte, Jörg K. H. Franke, Frank Hutter
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26064](https://arxiv.org/abs/2510.26064)
- **Reason:** 研究符号回归的缩放规律，用transformer pipeline验证计算与性能的幂律关系，对深度学习理论中的符号回归和模型缩放研究有开创性贡献。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Angular Steering: Behavior Control via Rotation in Activation Space
- **Authors:** Hieu M. Vu, Tan M. Nguyen
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26243](https://arxiv.org/abs/2510.26243)
- **Reason:** 提出通过激活空间旋转实现LLM行为的精细控制，统一现有加法和正交化技术，解决参数敏感和无关特征影响问题，对大模型可靠行为调控有理论和实践价值。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics
- **Authors:** Zhiyang Xun, Shivam Gupta, Eric Price
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26324](https://arxiv.org/abs/2510.26324)
- **Reason:** 结合扩散模型与退火Langevin动力学解决后验采样中的分数估计误差问题，证明仅需L4分数误差界即可多项式时间采样，对生成模型后验推理理论有贡献。
Score: 7
Field: 深度学习理论

## 深度学习可解释性

### [Score: 8.0/10] Exploring Human-AI Conceptual Alignment through the Prism of Chess
- **Authors:** Semyon Lomaso, Judah Goldfeder, Mehmet Hamza Erol, Matthew So, Yao Yan, Addison Howard, Nathan Kutz, Ravid Shwartz Ziv
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26025](https://arxiv.org/abs/2510.26025)
- **Reason:** 通过国际象棋任务分析AI与人类概念的对齐，发现深层表示偏离人类，对深度学习可解释性中的人类-AI概念对齐研究有重要实证洞见。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Towards Piece-by-Piece Explanations for Chess Positions with SHAP
- **Authors:** Francesco Spinnato
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.25775](https://arxiv.org/abs/2510.25775)
- **Reason:** 论文将SHAP（属于用户关注的Shapley value方法）应用于国际象棋局面评估的可解释性分析，直接对应深度学习可解释性方向，探索了如何通过特征消融计算每枚棋子对引擎评估结果的贡献，符合用户对white-box explanation的研究兴趣。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] On Measuring Localization of Shortcuts in Deep Networks
- **Authors:** Nikita Tsoy, Nikola Konstantinov
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26560](https://arxiv.org/abs/2510.26560)
- **Reason:** 提出量化shortcut在深度网络中的层定位方法，通过反事实训练分析不同层对shortcut的贡献，揭示shortcut学习的分布式特性，对深度学习可解释性和shortcut mitigation有帮助。
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] Faithful and Fast Influence Function via Advanced Sampling
- **Authors:** Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26776](https://arxiv.org/abs/2510.26776)
- **Reason:** 论文针对Influence Function（IF）计算中的高方差问题，提出基于特征和logits的先进采样技术，提升了IF估计的准确性和效率，属于深度学习可解释性的重要改进，解决了IF实际应用中的关键瓶颈，符合用户对可解释性方法的研究兴趣。
Score: 7
Field: 深度学习可解释性

## Deep learning explainability

### [Score: 7.0/10] Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances
- **Authors:** Fernando Alonso-Fernandez, Kevin Hernandez Diaz, Jose M. Buades, Kiran Raja, Josef Bigun
- **Published:** 2025-10-31
- **Link:** [https://arxiv.org/abs/2510.26282](https://arxiv.org/abs/2510.26282)
- **Reason:** Uses LIME heatmaps and Jensen-Shannon divergence to analyze attention patterns and complementarity of CNN architectures (SqueezeNet, MobileNetv2, ResNet50) for periocular verification, contributing to deep learning explainability.
Score: 7
Field: Deep learning explainability

