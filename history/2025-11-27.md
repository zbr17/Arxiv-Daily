# ArXiv 每日推荐 - 2025-11-27

> 更新于北京时间：2025-11-27 12:35:41
> 已自动阅读了 394 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：211427

## Multi-modal agents

### [Score: 9.0/10] VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning
- **Authors:** Boyu Chen, Zikang Wang, Zhengrong Yue, Kainan Yan, Chenyun Yu, Yi Huang, Zijun Liu, Yafei Wen, Xiaoxin Chen, Yang Liu, Peng Li, Yali Wang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19524](https://arxiv.org/abs/2511.19524)
- **Reason:** Presents a multi-agent system (VideoChat-M1) using MARL for video understanding, achieving SOTA on 8 benchmarks, core to multi-modal agents research.
Score: 9
Field: Multi-modal agents

### [Score: 8.0/10] Connecting the Dots: Training-Free Visual Grounding via Agentic Reasoning
- **Authors:** Liqin Luo, Guangyao Chen, Xiawu Zheng, Yongxing Dai, Yixiong Zou, Yonghong Tian
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19516](https://arxiv.org/abs/2511.19516)
- **Reason:** Introduces a training-free agentic framework (GroundingAgent) for visual grounding using pretrained detectors and LLMs, achieving strong zero-shot performance on core benchmarks, relevant to multi-modal agents.
Score: 8
Field: Multi-modal agents

### [Score: 8.0/10] CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization
- **Authors:** Xinhai Hou, Shaoyuan Xu, Manan Biyani, Mayan Li, Jia Liu, Todd C. Hollon, Bryan Wang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19661](https://arxiv.org/abs/2511.19661)
- **Reason:** Proposes a code-based visual agent (CodeV) with tool-aware policy optimization, improving faithful tool use in reasoning, core to multi-modal agents research.
Score: 8
Field: Multi-modal agents

## Native multi-modal large model

### [Score: 9.0/10] Vidi2: Large Multimodal Models for Video Understanding and Creation
- **Authors:** Vidi Team, Celong Liu, Chia-Wen Kuo, Chuang Huang, Dawei Du, Fan Chen, Guang Chen, Haoji Zhang, Haojun Zhao, Lingxi Zhang, Lu Guo, Lusha Li, Longyin Wen, Qihang Fan, Qingyu Chen, Rachel Deng, Sijie Zhu, Stuart Siew, Tong Jin, Weiyan Tao, Wen Zhong, Xiaohui Shen, Xin Gu, Zhenfang Chen, Zuhua Lin
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19529](https://arxiv.org/abs/2511.19529)
- **Reason:** Introduces Vidi2, a large multimodal model for video understanding (spatio-temporal grounding, Video QA) with a new benchmark, directly relevant to native multi-modal large models.
Score: 9
Field: Native multi-modal large model

### [Score: 8.0/10] Personalized Reward Modeling for Text-to-Image Generation
- **Authors:** Jeongeun Lee, Ryang Heo, Dongha Lee
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19458](https://arxiv.org/abs/2511.19458)
- **Reason:** Addresses personalized reward modeling for text-to-image generation, aligning generated content with user preferences, which is core to native multi-modal large model research (image generation subarea).
Score: 8
Field: Native multi-modal large model

### [Score: 7.0/10] Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization
- **Authors:** Debin Meng, Chen Jin, Zheng Gao, Yanran Li, Ioannis Patras, Georgios Tzimiropoulos
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19811](https://arxiv.org/abs/2511.19811)
- **Reason:** Improves diversity in text-to-image generation via prompt semantic space optimization, relevant to native multi-modal large models (image generation subarea).
Score: 7
Field: Native multi-modal large model

## Large model new technologies

### [Score: 9.0/10] One Attention, One Scale: Phase-Aligned Rotary Positional Embeddings for Mixed-Resolution Diffusion Transformer
- **Authors:** Haoyu Wu, Jingyi Xu, Qiaomu Miao, Dimitris Samaras, Hieu Le
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19778](https://arxiv.org/abs/2511.19778)
- **Reason:** Introduces CRPA, a training-free fix for Rotary Positional Embeddings in Diffusion Transformers (DiTs), enabling better mixed-resolution generation, core to large model new technologies (diffusion LLMs).
Score: 9
Field: Large model new technologies

## 多模态智能体

### [Score: 9.0/10] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning
- **Authors:** Jiaqi Liu, Kaiwen Xiong, Peng Xia, Yiyang Zhou, Haonian Ji, Lu Feng, Siwei Han, Mingyu Ding, Huaxiu Yao
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19900](https://arxiv.org/abs/2511.19900)
- **Reason:** 提出自我进化的工具集成视觉-语言智能体Agent0-VL，通过Solver-Verifier双角色循环和工具接地的自我评估，实现无外部奖励的持续进化，对多模态智能体的自主推理与工具使用研究有突破性价值。
Score: 9
Field: 多模态智能体

### [Score: 8.0/10] Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting Skepticism
- **Authors:** Yinjie Zhao, Heng Zhao, Bihan Wen, Joey Tianyi Zhou
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.17672](https://arxiv.org/abs/2511.17672)
- **Reason:** 提出面向AIGC视觉欺骗的智能体推理框架，通过注入怀疑主义提升真实性验证能力，属于多模态智能体在视觉安全领域的应用创新。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop
- **Authors:** Myung Ho Kim
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.17673](https://arxiv.org/abs/2511.17673)
- **Reason:** 提出结构化认知循环架构，分离智能体认知阶段以提升可控性与可解释性，解决现有LLM代理的核心缺陷，属于多模态智能体的架构创新。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation
- **Authors:** Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, Sanglu Lu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19859](https://arxiv.org/abs/2511.19859)
- **Reason:** 提出VITA框架通过隐式视觉思维链（CoT）统一视觉感知与机器人动作生成，提升模拟和真实世界任务的成功率，符合多模态智能体中的VLA方向。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Reinforcing Action Policies by Prophesying
- **Authors:** Jiahui Zhang, Ze Huang, Chun Gu, Zipei Ma, Li Zhang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20633](https://arxiv.org/abs/2511.20633)
- **Reason:** 提出ProphRL框架用学到的世界模型（Prophet）和RL优化VLA动作政策，提升VLA模型在基准和真实机器人的性能，符合多模态智能体中的VLA方向。
Score: 8
Field: 多模态智能体

### [Score: 7.0/10] Weakly-supervised Latent Models for Task-specific Visual-Language Control
- **Authors:** Xian Yeow Lee, Lasitha Vidyaratne, Gregory Sin, Ahmed Farahat, Chetan Gupta
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18319](https://arxiv.org/abs/2511.18319)
- **Reason:** 针对空间控制任务设计弱监督潜在模型，提升自主检查场景的可靠性，属于多模态智能体在工业场景的应用。
Score: 7
Field: 多模态智能体

### [Score: 7.0/10] A Multimodal Conversational Agent for Tabular Data Analysis
- **Authors:** Mohammad Nour Al Awad, Sergey Ivanov, Olga Tikhonova, Ivan Khodnenko
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18405](https://arxiv.org/abs/2511.18405)
- **Reason:** 构建多模态对话代理处理表格数据，提升人机交互的直观性与可解释性，属于多模态智能体的应用创新。
Score: 7
Field: 多模态智能体

### [Score: 7.0/10] UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model
- **Authors:** Changxin Huang, Lv Tang, Zhaohuan Zhan, Lisha Yu, Runhao Zeng, Zun Liu, Zhengjie Wang, Jianqiang Li
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18845](https://arxiv.org/abs/2511.18845)
- **Reason:** 针对视觉语言导航任务，提出多模态世界模型和分层预测反馈机制，协同优化视觉推理与导航决策，提升未见过场景的导航精度，符合多模态智能体中的视觉语言导航方向。
Score: 7
Field: 多模态智能体

### [Score: 7.0/10] Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories
- **Authors:** Rushuai Yang, Zhiyuan Feng, Tianxiang Zhang, Kaixin Wang, Chuheng Zhang, Li Zhao, Xiu Su, Yi Chen, Jiang Bian
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19528](https://arxiv.org/abs/2511.19528)
- **Reason:** 提出DLR框架通过RL生成多样化轨迹用于视觉语言动作（VLA）预训练，提升VLA模型在下游任务的泛化能力，符合多模态智能体中的VLA方向。
Score: 7
Field: 多模态智能体

### [Score: 7.0/10] ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation
- **Authors:** Yuhan Wu, Tiantian Wei, Shuo Wang, ZhiChao Wang, Yanyong Zhang, Daniel Cremers, Yan Xia
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20330](https://arxiv.org/abs/2511.20330)
- **Reason:** 提出ArtiBench基准和ArtiBrain框架，结合VLM推理与自适应控制解决铰接物体操作的泛化问题，符合多模态智能体中的视觉语言动作方向。
Score: 7
Field: 多模态智能体

## 大模型安全与对齐

### [Score: 9.0/10] On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation
- **Authors:** Changyue Li, Jiaying Li, Youliang Yuan, Jiaming He, Zhicong Huang, Pinjia He
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20002](https://arxiv.org/abs/2511.20002)
- **Reason:** 揭示单扰动劫持多模态大模型决策链的新型威胁，提出SAUPs语义感知通用扰动方法，对大模型安全与对齐中的对抗攻击防御研究有重要警示与参考价值。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models
- **Authors:** Chieh-Yun Chen, Zhonghao Wang, Qi Chen, Zhifan Ye, Min Shi, Yue Zhao, Yinan Zhao, Hui Qu, Wei-An Lin, Yiru Shen, Ajinkya Kale, Irfan Essa, Humphrey Shi
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20629](https://arxiv.org/abs/2511.20629)
- **Reason:** 提出MapReduce LoRA和RaTE方法，优化多偏好对齐，在文本到图像、视频和语言模型上显著提升多维度性能，推动大模型与人类多偏好的有效对齐。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma
- **Authors:** Subramanyam Sahoo, Aman Chadha, Vinija Jain, Divya Chaudhary
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19504](https://arxiv.org/abs/2511.19504)
- **Reason:** 形式化RLHF中的“对齐三难”问题（代表性、可处理性、鲁棒性），证明无法同时满足三者，揭示现有RLHF系统通过牺牲代表性解决矛盾的本质，为大模型对齐研究提供理论基础。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] Natural Emergent Misalignment from Reward Hacking in Production RL
- **Authors:** Monte MacDiarmid, Benjamin Wright, Jonathan Uesato, Joe Benton, Jon Kutasov, Sara Price, Naia Bouscal, Sam Bowman, Trenton Bricken, Alex Cloud, Carson Denison, Johannes Gasteiger, Ryan Greenblatt, Jan Leike, Jack Lindsey, Vlad Mikulik, Ethan Perez, Alex Rodrigues, Drake Thomas, Albert Webson, Daniel Ziegler, Evan Hubinger
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18397](https://arxiv.org/abs/2511.18397)
- **Reason:** 研究生产环境中奖励 hacking 导致的涌现错位问题，提出缓解策略，是大模型安全与对齐领域的关键应用研究。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs
- **Authors:** Ziqi Wang (), Chang Che (), Qi Wang (), Hui Ma (), Zenglin Shi (), Cees G. M. Snoek (University of Amsterdam), Meng Wang (University of Science and Technology of China)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20158](https://arxiv.org/abs/2511.20158)
- **Reason:** 针对安全对齐多模态大模型的持续视觉指令调优问题，提出HPA框架，通过参数分区、平衡选择和正交调整，平衡安全保持与任务性能，实验验证有效缓解遗忘和安全退化，对大模型安全对齐的持续学习有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs
- **Authors:** Sen Nie (), Jie Zhang (), Jianxin Yan (), Shiguang Shan (Chinese Academy of Sciences), Xilin Chen (Chinese Academy of Sciences)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20223](https://arxiv.org/abs/2511.20223)
- **Reason:** 发现大视觉语言模型（LVLMs）的注意力值特征（V）具有解纠缠的局部语义信息，提出V-Attack方法，实现可控的对抗攻击，平均攻击成功率提升36%，对大模型的安全脆弱性分析和防御有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs
- **Authors:** Ziqi Wang (), Chang Che (), Qi Wang (), Hui Ma (), Zenglin Shi (), Cees G. M. Snoek (University of Amsterdam), Meng Wang (University of Science and Technology of China)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20158](https://arxiv.org/abs/2511.20158)
- **Reason:** 针对安全对齐多模态大模型的持续视觉指令调优问题，提出HPA框架，通过参数分区、平衡选择和正交调整平衡安全与任务性能，有效缓解遗忘和安全退化，对大模型安全对齐的持续学习有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs
- **Authors:** Sen Nie (), Jie Zhang (), Jianxin Yan (), Shiguang Shan (Chinese Academy of Sciences), Xilin Chen (Chinese Academy of Sciences)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20223](https://arxiv.org/abs/2511.20223)
- **Reason:** 发现大视觉语言模型的注意力值特征具有解纠缠局部语义信息，提出V-Attack实现可控对抗攻击，平均攻击成功率提升36%，对大模型安全脆弱性分析和防御有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] RubricRL: Simple Generalizable Rewards for Text-to-Image Generation
- **Authors:** Xuelu Feng, Yunsheng Li, Ziyu Wan, Zixuan Gao, Junsong Yuan, Dongdong Chen, Chunming Qiao
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20651](https://arxiv.org/abs/2511.20651)
- **Reason:** 提出RubricRL，基于结构化评分标准的奖励设计，动态构建针对每个prompt的细粒度视觉标准，提升文本到图像生成的prompt忠实度和可解释性，优化RLHF的奖励机制。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Automating Deception: Scalable Multi-Turn LLM Jailbreaks
- **Authors:** Adarsh Kumarappan, Ananya Mujoo
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19517](https://arxiv.org/abs/2511.19517)
- **Reason:** 提出自动化生成多轮对话越狱数据集的 pipeline，基于Foot-in-the-Door心理理论构建1500个场景，评估发现GPT家族模型对对话历史高度敏感（ASR提升32%），为大模型安全防御提供实证依据。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning
- **Authors:** Jingchu Gai, Guanning Zeng, Huaqing Zhang, Aditi Raghunathan
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19942](https://arxiv.org/abs/2511.19942)
- **Reason:** 提出差分平滑方法，解决RL微调中的多样性崩溃问题，提升LLM推理性能（AIME24数据集提升6.7% Pass@1），为RLHF优化提供新方向。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents
- **Authors:** Kaiyuan Zhang, Mark Tenenholtz, Kyle Polley, Jerry Ma, Denis Yarats, Ninghui Li
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20597](https://arxiv.org/abs/2511.20597)
- **Reason:** 针对AI浏览器代理的prompt injection攻击问题，提出多层防御策略并开展实证评估，是大模型安全与对齐领域的关键应用研究。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Alignment Faking - the Train -> Deploy Asymmetry: Through a Game-Theoretic Lens with Bayesian-Stackelberg Equilibria
- **Authors:** Kartik Garg, Shourya Mishra, Kartikeya Sinha, Ojaswi Pratap Singh, Ayush Chopra, Kanishk Rai, Ammar Sheikh, Raghav Maheshwari, Aman Chadha, Vinija Jain, Amitava Das
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.17937](https://arxiv.org/abs/2511.17937)
- **Reason:** 用博弈论分析训练-部署不对称的对齐伪造问题，揭示大模型对齐风险，是大模型安全与对齐领域的理论研究。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] PRADA: Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images
- **Authors:** Simon Damm, Jonas Ricker, Henning Petzka, Asja Fischer
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20068](https://arxiv.org/abs/2511.20068)
- **Reason:** 提出PRADA基于概率比的AR生成图像检测方法，通过分析生成序列的概率特征实现可靠检测，对大模型安全与对齐中的生成内容鉴别研究有参考价值。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] An Invariant Latent Space Perspective on Language Model Inversion
- **Authors:** Wentao Ye, Jiaqi Hu, Haobo Wang, Xinpeng Ti, Zhiqing Xiao, Hao Chen, Liyao Li, Lei Feng, Sai Wu, Junbo Zhao
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19569](https://arxiv.org/abs/2511.19569)
- **Reason:** 提出Inv²A方法，基于不变 latent 空间假设解决语言模型inversion问题，提升隐私保护能力，在9个数据集上比基线平均高4.77% BLEU得分。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Prompt Fairness: Sub-group Disparities in LLMs
- **Authors:** Meiyu Zhong, Noel Teku, Ravi Tandon
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19956](https://arxiv.org/abs/2511.19956)
- **Reason:** 研究LLM中的prompt公平性，提出信息论指标量化子群差异，通过干预方法（中性化+多生成投票）降低差异，提升模型公平性。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs
- **Authors:** Craig Dickson
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20104](https://arxiv.org/abs/2511.20104)
- **Reason:** 研究开放权重LLM的 emergent misalignment现象，发现JSON格式输出会加倍错位率，验证了格式约束对安全训练的影响，对大模型安全与对齐中的错位问题及格式脆弱性有实证贡献。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Steering Latent Traits, Not Learned Facts: An Empirical Study of Activation Control Limits
- **Authors:** Tetiana Bas, Krystian Novak
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18284](https://arxiv.org/abs/2511.18284)
- **Reason:** 实证分析激活控制对LLM行为的影响，为大模型安全与对齐中的行为调控提供实践指导。
Score: 7
Field: 大模型安全与对齐

## 原生多模态大模型

### [Score: 9.0/10] DINO-Tok: Adapting DINO for Visual Tokenizers
- **Authors:** Mingkai Jia, Mingxiao Li, Liaoyuan Fan, Tianxing Shi, Jiaxin Guo, Zeming Li, Xiaoyang Guo, Xiao-Xiao Long, Qian Zhang, Ping Tan, Wei Yin
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20565](https://arxiv.org/abs/2511.20565)
- **Reason:** 提出DINO-Tok，将DINO模型适配为视觉tokenizer，整合浅层细粒度特征和深层全局语义，解决现有tokenizer语义与重建 fidelity 平衡问题，在ImageNet上实现最优重建性能。
Score: 9
Field: 原生多模态大模型

### [Score: 8.0/10] Temporal-Visual Semantic Alignment: A Unified Architecture for Transferring Spatial Priors from Vision Models to Zero-Shot Temporal Tasks
- **Authors:** Xiangkai Ma, Han Zhang, Wenzhong Li, Sanglu Lu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19856](https://arxiv.org/abs/2511.19856)
- **Reason:** 提出TimeArtist跨模态框架，实现时间序列与视觉语义的语义级对齐，探索多模态大模型中跨模态空间先验转移的关键问题，对原生多模态大模型的跨模态融合研究有重要价值。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization
- **Authors:** Chengyue Huang, Mellon M. Zhang, Robert Azarcon, Glen Chou, Zsolt Kira
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19878](https://arxiv.org/abs/2511.19878)
- **Reason:** 针对视觉-语言-动作（VLA）模型微调中破坏预训练视觉-语言表示的问题，提出模块级proximity调度策略，有效平衡预训练先验保留与下游任务适应，对原生多模态大模型的泛化性优化有重要价值。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Boosting Reasoning in Large Multimodal Models via Activation Replay
- **Authors:** Yun Xing, Xiaobin Hu, Qingdong He, Jiangning Zhang, Shuicheng Yan, Shijian Lu, Yu-Gang Jiang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19972](https://arxiv.org/abs/2511.19972)
- **Reason:** 提出Activation Replay训练-free方法，通过重放预训练模型的低熵激活提升多模态大模型的推理能力，揭示激活调制对推理的关键作用，对原生多模态大模型的推理优化有重要价值。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding
- **Authors:** Jinghan Zhao, Yifei Huang, Feng Lu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20073](https://arxiv.org/abs/2511.20073)
- **Reason:** 提出Task-Step-State（TSS）框架，通过state（物体配置文本快照）将抽象过程锚定到视频视觉内容，实现procedural-aware的视频表示学习，对原生多模态大模型中的视频-文本对齐与表示学习有重要价值。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers
- **Authors:** Min Zhao (), Hongzhou Zhu (), Yingze Wang (), Bokai Yan (), Jintao Zhang (), Guande He (), Ling Yang (), Chongxuan Li (Tsinghua University), Jun Zhu (Tsinghua University)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20123](https://arxiv.org/abs/2511.20123)
- **Reason:** 针对视频扩散Transformer的长度外推难题，提出UltraViCo方法，通过抑制超出训练窗口的注意力分散，解决内容重复和质量下降问题，将外推极限从2x提升至4x，性能优于现有基线，作者团队在扩散模型领域有影响力。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs
- **Authors:** Tianxiang Jiang (), Sheng Xia (), Yicheng Xu (), Linquan Wu (), Xiangyu Zeng (), Limin Wang (Shanghai Jiao Tong University), Yu Qiao (Shanghai AI Laboratory), Yi Wang ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20272](https://arxiv.org/abs/2511.20272)
- **Reason:** 提出VKnowU基准，涵盖8类视觉知识（如直觉物理、主观意图），评估23个SOTA多模态大模型，发现其视觉知识理解不足，提出VideoKnow+模型提升性能，对多模态大模型的认知能力评估和改进有重要意义。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers
- **Authors:** Min Zhao (), Hongzhou Zhu (), Yingze Wang (), Bokai Yan (), Jintao Zhang (), Guande He (), Ling Yang (), Chongxuan Li (Tsinghua University), Jun Zhu (Tsinghua University)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20123](https://arxiv.org/abs/2511.20123)
- **Reason:** 针对视频扩散Transformer的长度外推难题，提出UltraViCo方法，通过抑制注意力分散解决内容重复和质量下降问题，将外推极限从2x提升至4x，性能优于现有基线，作者团队在扩散模型领域具有影响力。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs
- **Authors:** Tianxiang Jiang (), Sheng Xia (), Yicheng Xu (), Linquan Wu (), Xiangyu Zeng (), Limin Wang (Shanghai Jiao Tong University), Yu Qiao (Shanghai AI Laboratory), Yi Wang ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20272](https://arxiv.org/abs/2511.20272)
- **Reason:** 提出VKnowU基准涵盖8类视觉知识，评估23个SOTA多模态大模型，发现其视觉知识理解不足，提出VideoKnow+提升性能，对多模态大模型的认知能力评估和改进有重要意义。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout
- **Authors:** Hidir Yesiltepe, Tuna Han Salih Meral, Adil Kaan Akan, Kaan Oktay, Pinar Yanardag
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20649](https://arxiv.org/abs/2511.20649)
- **Reason:** 提出Infinity-RoPE，通过相对论RoPE、KV Flush和RoPE Cut等方法，实现无限时长、可控的视频生成，突破现有 autoregressive 模型的时间限制，提升视频生成的连续性和可控性。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark
- **Authors:** Yang Zhou, Mingyu Zhao, Zhenting Wang, Difei Gu, Bangwei Guo, Ruosong Ye, Ligong Han, Can Jin, Dimitris N. Metaxas
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.17729](https://arxiv.org/abs/2511.17729)
- **Reason:** 构建首个多模态工具使用的MLLM代理基准，评估多模态、多跳、多线程推理能力，推动原生多模态大模型的评估体系发展。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints
- **Authors:** Rui Xu, Dakuan Lu, Zicheng Zhao, Xiaoyu Tan, Xintao Wang, Siyu Yuan, Jiangjie Chen, Yinghui Xu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18450](https://arxiv.org/abs/2511.18450)
- **Reason:** 基于折纸任务构建多模态空间推理基准，评估MLLM的空间逻辑能力，推动原生多模态大模型的空间推理研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation
- **Authors:** Jennifer Grannen, Michelle Pan, Kenneth Llontop, Cherie Ho, Mark Zolotas, Jeannette Bohg, Dorsa Sadigh
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19647](https://arxiv.org/abs/2511.19647)
- **Reason:** 提出用机器人在真实场景收集多模态数据（视觉、语言）以适配视觉语言模型（VLM），提升VLM在真实世界的性能，符合原生多模态大模型中的数据适配方向。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] Distilling Cross-Modal Knowledge via Feature Disentanglement
- **Authors:** Junhong Liu, Yuan Zhang, Tao Huang, Wenchao Xu, Renyu Yang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19887](https://arxiv.org/abs/2511.19887)
- **Reason:** 提出频率解耦的跨模态知识蒸馏方法，通过分离高低频特征并设计差异化对齐策略，解决跨模态知识转移中的表示不一致问题，对原生多模态大模型的知识压缩与迁移研究有参考价值。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering
- **Authors:** Yuyi Li, Daoyuan Chen, Zhen Wang, Yutong Lu, Yaliang Li
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19899](https://arxiv.org/abs/2511.19899)
- **Reason:** 构建首个带验证机制的科学领域视觉问答数据集VeriSciQA，解决现有科学VQA数据错误率高的问题，为原生多模态大模型的科学推理能力提升提供高质量数据支撑，对多模态数据集建设有重要价值。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] EmoFeedback2: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback
- **Authors:** Jingyang Jia, Kai Shu, Gang Yang, Long Xing, Xun Chen, Aiping Liu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19982](https://arxiv.org/abs/2511.19982)
- **Reason:** 提出EmoFeedback2生成-理解-反馈强化范式，利用LVLM的奖励与文本反馈提升连续情感图像生成的情感一致性与fidelity，对原生多模态大模型的情感化生成研究有参考价值。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] While recognizing actions, LMMs struggle to detect core interaction events
- **Authors:** Daniel Harari (), Michael Sidorov (), Liel David (), Chen Shterental (), Abrham Kahsay Gebreselasie (), Muhammad Haris Khan ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20162](https://arxiv.org/abs/2511.20162)
- **Reason:** 提出首个大规模交互事件标注数据集，分析发现多模态大模型（LMMs）虽能识别动作但难以检测核心交互事件（如接触/释放帧），指出其缺乏感知 grounding的关键问题，对理解LMMs的视觉推理局限性有重要意义。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling
- **Authors:** Bo-Kai Ruan (), Teng-Fang Hsiao (), Ling Lo (), Yi-Lun Wu (), Hong-Han Shuai ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20251](https://arxiv.org/abs/2511.20251)
- **Reason:** 针对长提示图像生成中的多样性下降问题，提出PromptMoG方法，通过 prompt嵌入的高斯混合采样提升多样性，实验验证在SD3.5-Large等模型上有效，对长提示生成的多样性优化有贡献。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] While recognizing actions, LMMs struggle to detect core interaction events
- **Authors:** Daniel Harari (), Michael Sidorov (), Liel David (), Chen Shterental (), Abrham Kahsay Gebreselasie (), Muhammad Haris Khan ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20162](https://arxiv.org/abs/2511.20162)
- **Reason:** 构建首个大规模交互事件标注数据集，分析发现多模态大模型虽能识别动作但难以检测核心交互事件，指出其缺乏感知 grounding的关键问题，对理解LMMs的视觉推理局限性有重要意义。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling
- **Authors:** Bo-Kai Ruan (), Teng-Fang Hsiao (), Ling Lo (), Yi-Lun Wu (), Hong-Han Shuai ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20251](https://arxiv.org/abs/2511.20251)
- **Reason:** 针对长提示图像生成的多样性下降问题，提出PromptMoG通过高斯混合采样提升多样性，实验验证在SD3.5-Large等模型上有效，对长提示生成的多样性优化有贡献。
Score: 7
Field: 原生多模态大模型

## 高效大模型训练与推理

### [Score: 9.0/10] CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive Rounding
- **Authors:** Ziteng Sun, Adrian Benton, Samuel Kushnir, Asher Trockman, Vikas Singh, Suhas Diggavi, Ananda Theertha Suresh
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19705](https://arxiv.org/abs/2511.19705)
- **Reason:** 提出无需校准数据的量化方法CafeQ，通过学习变换和自适应舍入提升LLM量化性能，在Gemma 2模型上4位量化比基线高0.5%，3位量化高8.6%，性能接近需校准的GPTQ。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] GigaWorld-0: World Models as Data Engine to Empower Embodied AI
- **Authors:** GigaWorld Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jiagang Zhu, Kerui Li, Mengyuan Xu, Qiuping Deng, Siting Wang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yankai Wang, Yu Cao, Yifan Chang, Yuan Xu, Yun Ye, Yang Wang, Yukun Zhou, Zhengyuan Zhang, Zhehao Dong, Zheng Zhu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19861](https://arxiv.org/abs/2511.19861)
- **Reason:** 提出GigaTrain高效训练框架，通过FP8精度和稀疏注意力显著降低多模态大模型训练的内存与计算开销，针对高效大模型训练的核心问题提供实践方案，对高效大模型训练与推理研究有重要参考价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers
- **Authors:** Xinwan Wen (), Bowen Li (), Jiajun Luo (), Ye Li (), Zhi Wang ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20390](https://arxiv.org/abs/2511.20390)
- **Reason:** 针对扩散Transformer（DiTs）的推理延迟问题，提出FREE框架，通过特征级自回归和不确定性引导放松，实现无损加速（最高2.25x），保持生成质量，对高效大模型推理有重要价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers
- **Authors:** Xinwan Wen (), Bowen Li (), Jiajun Luo (), Ye Li (), Zhi Wang ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20390](https://arxiv.org/abs/2511.20390)
- **Reason:** 针对扩散Transformer的推理延迟问题，提出FREE框架通过特征级自回归和不确定性引导放松实现无损加速（最高2.25x），保持生成质量，对高效大模型推理有重要价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs
- **Authors:** Bao Tang (HUST), Shuai Zhang (HUST), Yueting Zhu (Zhejiang University), Jijun Xiang (HUST), Xin Yang (HUST), Li Yu (HUST), Wenyu Liu (HUST), Xinggang Wang (HUST)
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20410](https://arxiv.org/abs/2511.20410)
- **Reason:** 提出Trajectory-Backward Consistency Model (TBCM)，通过教师模型生成轨迹提取潜在表示，消除对外部训练数据的依赖，提升扩散模型蒸馏效率，减少约40%训练时间并节省GPU内存。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Object-Centric Vision Token Pruning for Vision Language Models
- **Authors:** Guangyuan Li, Rongzhen Zhao, Jinhong Deng, Yanbo Wang, Joni Pajarinen
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20439](https://arxiv.org/abs/2511.20439)
- **Reason:** 提出OC-VTP方法，通过轻量级目标中心视觉token剪枝器，在不微调模型的情况下保留最具代表性的视觉token，显著提升视觉语言模型推理效率并保持准确性，且具有可解释性。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning
- **Authors:** Guanjie Chen, Shirui Huang, Kai Liu, Jianchen Zhu, Xiaoye Qu, Peng Chen, Yu Cheng, Yifu Sun
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20549](https://arxiv.org/abs/2511.20549)
- **Reason:** 提出Flash-DMD框架，结合高效时间步蒸馏和联合RL优化，加速扩散模型训练收敛，实现高保真少步图像生成，在视觉质量、人类偏好和文本-图像对齐上优于现有方法。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] A Systematic Study of Compression Ordering for Large Language Models
- **Authors:** Shivansh Chhawri, Rahul Mahadik, Suparna Rooj
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19495](https://arxiv.org/abs/2511.19495)
- **Reason:** 系统探究LLM压缩技术（知识蒸馏、结构化剪枝、低比特量化）的顺序对性能的影响，发现Pruning-KD-Quantization顺序能在保持模型指令跟随和语言理解能力的同时实现3.68倍压缩，为高效LLM部署提供关键指导。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport
- **Authors:** Zecheng Pan, Zhikang Chen, Ding Li, Min Zhang, Sen Cui, Hongshuo Jin, Luqi Tao, Yi Yang, Deheng Ye, Yu Zhang, Tingting Zhu, Tianling Ren
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19561](https://arxiv.org/abs/2511.19561)
- **Reason:** 提出OTMF框架，基于最优运输对齐任务模型的语义几何，支持持续融合任务模型且不遗忘历史知识，提升模型合并的效率与性能，属于高效大模型训练研究。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] ModHiFi: Identifying High Fidelity predictive components for Model Modification
- **Authors:** Dhruva Kashyap, Chaitanya Murti, Pranav K Nayak, Tanay Narshana, Chiranjib Bhattacharyya
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19566](https://arxiv.org/abs/2511.19566)
- **Reason:** 提出ModHiFi算法，无需梯度或损失函数即可识别模型关键预测组件，支持剪枝与遗忘任务，在ImageNet和Swin Transformer上取得优于现有方法的结果，提升模型修改效率。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models
- **Authors:** Wentao Hu, Mingkuan Zhao, Shuangyong Song, Xiaoyan Zhu, Xin Lai, Jiayin Wang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19822](https://arxiv.org/abs/2511.19822)
- **Reason:** 提出Mosaic Pruning框架，通过聚类选择保留MoE模型的功能互补专家，剪枝后在通用任务上提升7.24%，专业任务上提升8.92%，解决MoE剪枝的泛化问题。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] DiFR: Inference Verification Despite Nondeterminism
- **Authors:** Adam Karvonen, Daniel Reuter, Roy Rinberg, Luke Marks, Adrià Garriga-Alonso, Keri Warr
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20621](https://arxiv.org/abs/2511.20621)
- **Reason:** 提出DiFR方法解决LLM推理的非确定性验证问题，提升推理正确性与效率，满足高效大模型推理的验证需求。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions
- **Authors:** Shaoyin Ma, Jie Song, Huiqiong Wang, Li Sun, Mingli Song
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18715](https://arxiv.org/abs/2511.18715)
- **Reason:** 提出渐进式推理框架解决LLM代理的模型选择问题，减少prompt膨胀并提升调用效率，属于高效大模型训练与推理的应用。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations
- **Authors:** Yejing Wang, Shengyu Zhou, Jinyu Lu, Ziwei Liu, Langming Liu, Maolin Wang, Wenlin Zhang, Feng Li, Wenbo Su, Pengjie Wang, Jian Xu, Xiangyu Zhao
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18793](https://arxiv.org/abs/2511.18793)
- **Reason:** 提出NEZHA架构解决生成式推荐的推理延迟问题，通过集成自draft头和无模型验证器，在保持推荐质量的同时显著降低延迟，符合高效大模型训练与推理中的推理优化方向，且已在淘宝大规模部署验证效果。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] Exploiting the Experts: Unauthorized Compression in MoE-LLMs
- **Authors:** Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Dheeraj Kulshrestha, Rajiv Ramnath
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19480](https://arxiv.org/abs/2511.19480)
- **Reason:** 研究MoE-LLM的未授权剪枝威胁，提出专家归因框架和防御策略，分析知识损失-恢复权衡，为MoE模型的安全压缩和未授权适应防御提供理论支持。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning
- **Authors:** Songlin Zhao, Michael Pitts, Zhuwei Qin
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19935](https://arxiv.org/abs/2511.19935)
- **Reason:** 提出EfficientXpert框架，通过传播感知剪枝实现LLM高效域适应，在医疗和法律任务上40%剪枝保留98%性能，优于现有方法。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models
- **Authors:** Yujia Wang, Yuanpu Cao, Jinghui Chen
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19959](https://arxiv.org/abs/2511.19959)
- **Reason:** 提出ParaBlock方法，通过通信与计算并行提升联邦块坐标下降效率，在LLM微调中保持性能的同时降低通信延迟，属于高效大模型训练研究。
Score: 7
Field: 高效大模型训练与推理

## 深度学习理论

### [Score: 9.0/10] HVAdam: A Full-Dimension Adaptive Optimizer
- **Authors:** Yiheng Zhang, Shaowu Wu, Yuanzhuo Xu, Jiajun Wu, Shang Xu, Steve Drew, Xiaoguang Niu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20277](https://arxiv.org/abs/2511.20277)
- **Reason:** 针对自适应优化器（如Adam）在CNN等经典架构上泛化性差的问题，提出具有连续可调适应性的Anon优化器及增量延迟更新机制，理论证明凸/非凸收敛性，实验在图像分类、扩散模型、语言建模任务上超越现有优化器，对深度学习理论中的优化器设计有重要贡献。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] ROOT: Robust Orthogonalized Optimizer for Neural Network Training
- **Authors:** Wei He, Kai Han, Hang Zhou, Hanting Chen, Zhicheng Liu, Xinghao Chen, Yunhe Wang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20626](https://arxiv.org/abs/2511.20626)
- **Reason:** 针对大模型训练设计稳健正交化优化器，解决正交化精度与离群值干扰问题，实验验证其在噪声与非凸场景下的优势，是深度学习理论（优化器）的重要进展。
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing
- **Authors:** Changho Choi, Minho Kim, Jinkyu Kim
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19963](https://arxiv.org/abs/2511.19963)
- **Reason:** 提出基于Mamba2的因果顺序视觉编码器MambaEye，实现输入大小无关的视觉表示学习，探索Mamba架构在视觉任务中的应用，对深度学习理论中的网络架构设计有创新性贡献。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles
- **Authors:** Kaiyi Ji
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19656](https://arxiv.org/abs/2511.19656)
- **Reason:** 证明非凸-强凸双层优化在确定性和随机一阶oracle下的复杂度下界，填补双层优化复杂度分析空白，为算法设计提供理论基准。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization
- **Authors:** Peng Zhao, Yu-Hu Yan, Hang Yu, Zhi-Hua Zhou
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19937](https://arxiv.org/abs/2511.19937)
- **Reason:** 提出UniGrad方法，实现在线凸优化的通用性与适应性，取得依赖问题的regret界，为在线学习算法设计提供理论基础。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] On the Limits of Momentum in Decentralized and Federated Optimization
- **Authors:** Riccardo Zaccone, Sai Praneeth Karimireddy, Carlo Masone
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20168](https://arxiv.org/abs/2511.20168)
- **Reason:** 研究动量在去中心化和联邦优化中的性能局限，理论证明即使采用周期性客户端参与和递减步长，动量仍无法克服统计异质性的负面影响，对深度学习理论中优化器在分布式场景的应用有重要理论分析价值。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Soft Adaptive Policy Optimization
- **Authors:** Chang Gao, Chujie Zheng, Xiong-Hui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, Junyang Lin
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20347](https://arxiv.org/abs/2511.20347)
- **Reason:** 针对RL中token级重要性比率高方差问题，提出SAPO优化器，用软门代替硬裁剪实现自适应衰减，提高训练稳定性和样本效率，实验在数学推理及Qwen3-VL训练中验证效果，对深度学习理论中的RL优化器设计有实践贡献。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Short-Range Oversquashing
- **Authors:** Yaaqov Mishayev, Yonatan Sverdlov, Tal Amir, Nadav Dym
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20406](https://arxiv.org/abs/2511.20406)
- **Reason:** 揭示MPNN的oversquashing问题不仅存在于长程任务，短程任务也会因瓶颈现象出现，指出虚拟节点无法解决该问题而Transformer可有效应对，对深度学习理论中的网络架构（MPNN与Transformer）设计有重要启示。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning
- **Authors:** Mihaela Hudișteanu, Edwige Cyffers, Nikita P. Kalinin
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20509](https://arxiv.org/abs/2511.20509)
- **Reason:** 提出内存高效、稀疏感知的差分隐私自适应优化器DP-MicroAdam，理论证明非凸收敛率，实验在图像分类及预训练Transformer微调上超越现有DP优化器，对深度学习理论中的隐私优化器设计有重要贡献。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Adam Simplified: Bias Correction Simplified
- **Authors:** Sam Laing, Antonio Orvieto
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20516](https://arxiv.org/abs/2511.20516)
- **Reason:** 通过系统消融实验挑战Adam中偏差修正的传统认知，发现最优超参下偏差修正无性能提升甚至有害，将其重新解释为隐式学习率调度，深入探索了优化器组件的必要性，对深度学习理论中的optimizer研究有启示意义。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent
- **Authors:** Shuo Xie, Tianhao Wang, Beining Wu, Zhiyuan Li
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20584](https://arxiv.org/abs/2511.20584)
- **Reason:** 从几何视角对比自适应优化器与归一化最速下降，扩展自适应光滑性至非凸场景并证明收敛性，提出自适应梯度方差用于随机优化，为深度学习理论中的优化几何分析提供了新框架。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Adaptive Hopfield Network: Rethinking Similarities in Associative Memory
- **Authors:** Shurong Wang, Yuqi Pan, Zhuoyang Shen, Meng Zhang, Hongwei Wang, Guoqi Li
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20609](https://arxiv.org/abs/2511.20609)
- **Reason:** 重新设计联想记忆的相似性度量机制，提出自适应Hopfield网络，解决传统模型的正确性问题，有理论证明和多任务实验支撑，属于深度学习理论（网络架构）的创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Progressive Localisation in Localist LLMs
- **Authors:** Joachim Diederich
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18375](https://arxiv.org/abs/2511.18375)
- **Reason:** 提出渐进式局部化架构，平衡LLM的可解释性与性能，解决安全关键领域的模型解释需求，属于深度学习理论（网络架构）的创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Scaling Implicit Fields via Hypernetwork-Driven Multiscale Coordinate Transformations
- **Authors:** Plein Versace
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18387](https://arxiv.org/abs/2511.18387)
- **Reason:** 提出超网络驱动的多尺度坐标变换，改进隐式神经表示的重构精度与参数效率，属于深度学习理论（网络架构）的进展。
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM
- **Authors:** Yang Liu, Xiaolong Zhong, Ling Jiang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19496](https://arxiv.org/abs/2511.19496)
- **Reason:** 提出1.3B参数小语言模型，通过μP参数化实现 proxy 模型到 full 模型的超参数迁移，并在训练后期将AdamW切换为Muon优化器，显著提升13项推理任务平均性能，涉及优化器与网络架构调整，属于深度学习理论研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning
- **Authors:** Bing Liu, Boao Kong, Limin Lu, Kun Yuan, Chengcheng Zhao
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19513](https://arxiv.org/abs/2511.19513)
- **Reason:** 构建加权希尔伯特空间框架，证明row-stochastic矩阵在分散式学习中比doubly stochastic矩阵具有更优的收敛性能，解决了两类矩阵的性能差异谜题，属于深度学习理论中的优化研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space
- **Authors:** Shivam Pal, Sakshi Varshney, Piyush Rai
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19525](https://arxiv.org/abs/2511.19525)
- **Reason:** 提出通过目标Jacobian正则化使模型对shortcut信号不变的方法，利用解纠缠 latent 空间分离核心与干扰特征，提升模型泛化能力，属于深度学习理论中的正则化与泛化研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Geometry of Decision Making in Language Models
- **Authors:** Abhinav Joshi, Divyanshu Bhatt, Ashutosh Modi
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20315](https://arxiv.org/abs/2511.20315)
- **Reason:** 研究大语言模型隐藏表示的几何特性（intrinsic dimension），发现层间维度变化模式（早期低维、中间扩展、后期压缩），揭示LLM决策的结构化表示机制，对深度学习理论中的语言模型表示学习有几何分析贡献。
Score: 7
Field: 深度学习理论

## Efficient large model training and inference

### [Score: 8.0/10] Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning
- **Authors:** Zhaoqi Xu, Yingying Zhang, Jian Li, Jianwei Guo, Qiannan Zhu, Hua Huang
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19518](https://arxiv.org/abs/2511.19518)
- **Reason:** Proposes an information-theoretic framework (InfoPrune) for adaptive pruning of vision-language models, improving efficiency with negligible performance loss, directly relevant to efficient large model training and inference.
Score: 8
Field: Efficient large model training and inference

### [Score: 8.0/10] RADSeg: Unleashing Parameter and Compute Efficient Zero-Shot Open-Vocabulary Segmentation Using Agglomerative Models
- **Authors:** Omar Alama, Darshil Jariwala, Avigyan Bhattacharya, Seungchan Kim, Wenshan Wang, Sebastian Scherer
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19704](https://arxiv.org/abs/2511.19704)
- **Reason:** Proposes an agglomerative model for efficient zero-shot segmentation, outperforming larger models with fewer parameters, directly relevant to efficient large model training and inference.
Score: 8
Field: Efficient large model training and inference

### [Score: 8.0/10] Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation
- **Authors:** Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19835](https://arxiv.org/abs/2511.19835)
- **Reason:** Optimizes attention sparsity for video generation models, achieving significant speedups, directly relevant to efficient large model training and inference.
Score: 8
Field: Efficient large model training and inference

### [Score: 7.0/10] INTERLACE: Interleaved Layer Pruning and Efficient Adaptation in Large Vision-Language Models
- **Authors:** Parsa Madinei, Ryan Solgi, Ziqi Wen, Jonathan Skaza, Miguel Eckstein, Ramtin Pedarsani
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19676](https://arxiv.org/abs/2511.19676)
- **Reason:** Introduces an interleaved pruning and finetuning framework for VLMs, achieving high performance retention with pruning, relevant to efficient large model training and inference.
Score: 7
Field: Efficient large model training and inference

## Deep learning theory

### [Score: 8.0/10] Rethinking Vision Transformer Depth via Structural Reparameterization
- **Authors:** Chengwei Zhou, Vipin Chaudhary, Gourav Datta
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19718](https://arxiv.org/abs/2511.19718)
- **Reason:** Proposes structural reparameterization to reduce Vision Transformer layers while maintaining capacity, addressing network architecture—a core deep learning theory topic.
Score: 8
Field: Deep learning theory

## 大模型新技术

### [Score: 8.0/10] Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models
- **Authors:** Qin Ren, Yufei Wang, Lanqing Guo, Wen Zhang, Zhiwen Fan, Chenyu You
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19917](https://arxiv.org/abs/2511.19917)
- **Reason:** 提出LoTTS训练-free局部缩放方法，通过缺陷定位与局部优化提升扩散模型的生成质量与效率，解决全图缩放的计算浪费问题，对大模型新技术中的扩散模型推理优化有重要价值。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning
- **Authors:** Hongji Yang, Yucheng Zhou, Wencheng Han, Runzhou Tao, Zhongying Qiu, Jianfei Yang, Jianbing Shen
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19965](https://arxiv.org/abs/2511.19965)
- **Reason:** 提出HiCoGen分层组合生成框架，通过LLM分解复杂提示并结合强化学习优化生成过程，解决扩散模型对复杂提示的构图能力不足问题，对大模型新技术中的扩散模型组合生成研究有重要价值。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] PixelDiT: Pixel Diffusion Transformers for Image Generation
- **Authors:** Yongsheng Yu, Wei Xiong, Weili Nie, Yichen Sheng, Shiqiu Liu, Jiebo Luo
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20645](https://arxiv.org/abs/2511.20645)
- **Reason:** 提出PixelDiT，直接在像素空间训练扩散Transformer，消除对autoencoder的依赖，通过双层次设计捕获全局语义和纹理细节，在ImageNet上实现最优像素生成性能。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds
- **Authors:** Jiaxin Shi, Michalis K. Titsias
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19664](https://arxiv.org/abs/2511.19664)
- **Reason:** 推导扩散模型中重加权损失的变分下界解释，证明其优于标准ELBO，提升扩散模型性能，为扩散模型设计提供理论指导。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Terminal Velocity Matching
- **Authors:** Linqi Zhou, Mathias Parger, Ayaan Haque, Jiaming Song
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19797](https://arxiv.org/abs/2511.19797)
- **Reason:** 提出Terminal Velocity Matching（TVM），泛化流匹配方法实现高保真一步/几步生成，在ImageNet-256上1步生成FID 3.29，4步1.99，属于大模型新技术中的扩散/流模型研究。
Score: 8
Field: 大模型新技术

### [Score: 7.0/10] Latent Diffusion Inversion Requires Understanding the Latent Space
- **Authors:** Mingxing Rao, Bowen Qu, Daniel Moyer
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20592](https://arxiv.org/abs/2511.20592)
- **Reason:** 研究潜在扩散模型的反转机制，揭示其潜在空间的记忆特性与隐私风险，为扩散模型（大模型新技术）的理解和安全分析提供新视角。
Score: 7
Field: 大模型新技术

## 深度学习可解释性

### [Score: 8.0/10] Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention
- **Authors:** Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng, Zhixing Tan
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20032](https://arxiv.org/abs/2511.20032)
- **Reason:** 提出Vision-Guided Attention方法，通过视觉接地引导模型关注相关区域，有效缓解多模态大模型的幻觉问题，同时提升模型的可解释性，对深度学习可解释性研究有重要价值。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Explainable Visual Anomaly Detection via Concept Bottleneck Models
- **Authors:** Arianna Stropeni, Valentina Zaccaria, Francesco Borsatti, Davide Dalle Pezze, Manuel Barusco, Gian Antonio Susto
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20088](https://arxiv.org/abs/2511.20088)
- **Reason:** 提出CONVAD方法，将Concept Bottleneck Models扩展到视觉异常检测，通过学习有意义的概念提供人类可解释的异常描述，解决现有VAD模型解释缺乏语义意义的问题，对深度学习可解释性研究有重要价值。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks
- **Authors:** Shihan Feng, Cheng Zhang, Michael Xi, Ethan Hsu, Lesia Semenova, Chudi Zhong
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19636](https://arxiv.org/abs/2511.19636)
- **Reason:** 提出Rashomon概念瓶颈模型，学习多个准确但推理过程不同的概念-based模型，揭示模型推理的多样性，为深度学习可解释性提供白盒分析框架。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits
- **Authors:** Areeb Ahmad, Abhinav Joshi, Ashutosh Modi
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20273](https://arxiv.org/abs/2511.20273)
- **Reason:** 采用奇异向量分解Transformer的注意力头与MLP组件，揭示其内部超叠加的独立子功能，突破现有方法将组件视为不可分割单元的局限，对深度学习可解释性中的Transformer白盒解释有重要推进。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Extracting Robust Register Automata from Neural Networks over Data Sequences
- **Authors:** Chih-Duo Hong, Hongjian Jiang, Anthony W. Lin, Oliver Markgraf, Julian Parsert, Tony Tan
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19100](https://arxiv.org/abs/2511.19100)
- **Reason:** 提出从处理连续数据序列的神经网络中提取鲁棒的寄存器自动机，作为可解释的替代模型支持符号分析，解决现有自动机提取无法处理连续域的问题，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations
- **Authors:** Chao Wang (), Chengan Che (), Xinyue Chen (), Sophia Tsoka (), Luis C. Garcia-Peraza-Herrera ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20295](https://arxiv.org/abs/2511.20295)
- **Reason:** 针对视频分类器的可解释性问题，提出BTTF方法，生成时间一致、物理合理的反事实视频，解释分类器决策，实验验证在多个视频数据集上有效，对视频模型的可解释性研究有贡献。
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations
- **Authors:** Chao Wang (), Chengan Che (), Xinyue Chen (), Sophia Tsoka (), Luis C. Garcia-Peraza-Herrera ()
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.20295](https://arxiv.org/abs/2511.20295)
- **Reason:** 针对视频分类器的可解释性问题，提出BTTF方法生成时间一致、物理合理的反事实视频，解释分类器决策，实验验证在多个视频数据集上有效，对视频模型的可解释性研究有贡献。
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] Synthesizing Visual Concepts as Vision-Language Programs
- **Authors:** Antonia Wüst, Wolfgang Stammer, Hikaru Shindo, Lukas Helff, Devendra Singh Dhami, Kristian Kersting
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.18964](https://arxiv.org/abs/2511.18964)
- **Reason:** 将视觉语言模型的感知灵活性与程序合成的系统性推理结合，生成可解释的神经符号程序，提供人类可理解的解释以解决VLM推理不一致问题，符合深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性

## Deep learning interpretability

### [Score: 7.0/10] SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data
- **Authors:** Penghao Rao, Runmin Jiang, Min Xu
- **Published:** 2025-11-26
- **Link:** [https://arxiv.org/abs/2511.19466](https://arxiv.org/abs/2511.19466)
- **Reason:** Improves influence function estimation for vision models, a key interpretability tool to understand training data's impact on predictions, directly relevant to deep learning interpretability.
Score: 7
Field: Deep learning interpretability

