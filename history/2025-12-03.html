<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-03</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>大模型安全与对齐</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型新技术</a>
<a href='#Deep-learning-theory' >Deep learning theory</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >多模态智能体</a>
<a href='#Large-model-security--alignment' >Large model security & alignment</a>
<a href='#Efficient-large-model-training--inference' >Efficient large model training & inference</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-03</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-03 12:51:34</p>
<p>已自动阅读了 572 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：305619</p>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation</h3>
<p><strong>Authors:</strong> Jun Jia, Hongyi Miao, Yingjie Zhou, Wangqiu Zhou, Jianbo Zhang, Linhan Cao, Dandan Zhu, Hua Yang, Xiongkuo Min, Wei Sun, Guangtao Zhai</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对扩散模型零-shot图像生成提出认证框架，防止未授权使用，解决大模型安全与对齐中的知识产权保护问题，有实际应用价值。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00075' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models</h3>
<p><strong>Authors:</strong> Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出视觉语言预训练模型（VLP）的模型级后门检测方法，涉及大模型安全与对齐中的后门攻击检测，属于核心方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00343' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation</h3>
<p><strong>Authors:</strong> Chengzhi Yu, Yifan Xu, Yifan Chen, Wenyi Zhang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 论文针对大视觉语言模型（LVLMs）的幻觉问题，分析数据生成过程并确认on-policy数据的优势，提出训练幻觉分类器保证干净样本，设计鲁棒迭代DPO算法，显著降低幻觉率（LLaVA-1.5-7B在MMHalBench上降50.8%），甚至让LLaVA-1.5-13B超越GPT-4V性能，直接关联大模型安全与对齐中的幻觉缓解和模型对齐方向，具有强实践价值。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00706' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Benchmarking Overton Pluralism in LLMs</h3>
<p><strong>Authors:</strong> Elinor Poole-Dayan, Jiayi Wu, Taylor Sorensen, Jiaxin Pei, Michiel A. Bakker</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出OvertonScore衡量LLM多元性，开发自动化基准，系统评估模型观点多样性，对大模型对齐意义重大。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01351' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning</h3>
<p><strong>Authors:</strong> Mohammad M Maheri, Xavier Cadet, Peter Chin, Hamed Haddadi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对近似机器遗忘的隐私风险，提出WARP teleportation防御方法，通过神经对称减少遗忘数据泄露，提升隐私保护效果，属于大模型安全与对齐的技术创新。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00272' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking</h3>
<p><strong>Authors:</strong> Lingling Fu ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出升级和合并MoE奖励模型的方法，有效缓解奖励黑客问题，对大模型安全与对齐（RLHF中的奖励模型 robustness）有重要实践价值
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00724' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism</h3>
<p><strong>Authors:</strong> Sandro Andric</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 测量LLM的隐式关联、自我报告与行为利他主义的差距，提出Calibration Gap作为对齐 metric，对大模型安全与对齐具有重要参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01568' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment</h3>
<p><strong>Authors:</strong> Valentin Noël, Elimane Yassine Seidou, Charly Ken Capo-Chichi, Ghanem Amari</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出HalluGraph框架，通过知识图对齐检测法律RAG系统的幻觉，提升大模型输出的可审计性与可靠性，属于大模型安全与对齐的实用方案。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01659' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dual Randomized Smoothing: Beyond Global Noise Variance</h3>
<p><strong>Authors:</strong> Chenhao Sun, Yuhao Mao, Martin Vechev (ETH Zurich)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出双随机平滑框架，解决全局噪声方差的限制，提升不同半径下的鲁棒性认证能力，属于大模型安全与对齐中的鲁棒性研究，实验结果显著。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01782' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Provably Safe Model Updates</h3>
<p><strong>Authors:</strong> Leo Elmecker-Plakolm, Pierre Fasterling, Philip Sosnin, Calvin Tsay, Matthew Wicker</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出可证明安全的模型更新框架，通过最大局部不变域保证更新后性能，属于大模型安全与对齐中的安全认证研究，理论严谨。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01899' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Sitong Fang, Shiyi Hou, Kaile Wang, Boyuan Chen, Donghai Hong, Jiayi Zhou, Josef Dai, Yaodong Yang, Jiaming Ji</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出多智能体辩论框架，通过视觉证据 grounding提升多模态欺骗检测能力，构建MM-DeceptionBench基准，属于大模型安全与对齐中的欺骗行为检测研究，创新性强。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00349' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF</h3>
<p><strong>Authors:</strong> Yifan Xu, Xichen Ye, Yifan Chen, Qiaosheng Zhang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出翻转感知的DPO算法，通过实例依赖翻转概率提升RLHF鲁棒性，属于大模型安全与对齐中的对齐研究，针对偏好翻转问题有针对性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00709' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BioPro: On Difference-Aware Gender Fairness for Vision-Language Models</h3>
<p><strong>Authors:</strong> Yujie Lin, Jiayao Ma, Qingguo Hu, Derek F. Wong, Jinsong Su</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出BioPro框架，通过反事实嵌入与正交投影缓解VLMs性别偏见，保留显式场景性别区分，属于大模型安全与对齐中的公平性研究，平衡公平与性能。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00807' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces</h3>
<p><strong>Authors:</strong> Yandong Sun, Qiang Huang, Ziwei Xu, Yiqun Sun, Yixuan Tang, Anthony K. H. Tung</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出语义场subspace与SAFARI算法，分析嵌入空间语义结构，提升分类与偏倚检测性能，属于深度学习可解释性中的嵌入空间分析研究，通用性强。
Score: 8
[PAPER_START]
Title: Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models
Authors: Cen Lu, Yung-Chen Tang, Andrea Cavallaro
Published: 2025-12-02
Link: https://arxiv.org/abs/2512.00918
Reason: 研究大视觉语言模型语言核心的神经元脆弱性，发现少量神经元消融会引发灾难性崩溃，揭示模型鲁棒性关键弱点，对大模型安全研究意义重大。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00852' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems</h3>
<p><strong>Authors:</strong> Xiaochuan Li, Ke Wang, Girija Gouda, Shubham Choudhary, Yaqun Wang, Linwei Hu, Joel Vaughan, Freddy Lecue</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出动态LLM陪审团系统，通过可靠性预测选择最优评委，提升LLM评估可信度，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01786' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection</h3>
<p><strong>Authors:</strong> Mengxue Hu, Yunfeng Diao, Changtao Miao, Jianshu Li, Zhe Li, Joey Tianyi Zhou</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 构建多模态视频音频AIGC检测数据集，涉及大模型安全与对齐中的AI生成内容检测，属于大模型安全与对齐研究方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.00336' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models</h3>
<p><strong>Authors:</strong> Maya Varma, Jean-Benoit Delbrouck, Sophie Ostmeier, Akshay Chaudhari, Curtis Langlotz</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出自动化方法TRoVe发现VLM的静态特征偏差（如依赖背景而非动态变化），构建评估框架量化偏差影响，为大模型安全与对齐中的偏差修正提供关键工具
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01048' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models</h3>
<p><strong>Authors:</strong> Hamza Tahboub, Weiyan Shi, Gang Hua, Huaizu Jiang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出SocialFusion框架解决VLM的“社会退化”问题（预训练后社会感知能力下降），通过轻量级连接增强社会任务的正迁移，提升多模态智能体的社会交互能力，属于大模型安全与对齐中的能力保持
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01148' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models</h3>
<p><strong>Authors:</strong> Stella Etuk, Ashraf Matrawy</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出基于PII敏感度的自适应差分隐私方法，平衡LLM的隐私保护与效用，属于大模型安全与对齐的实用改进。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01748' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mitigating Gender Bias in Depression Detection via Counterfactual Inference</h3>
<p><strong>Authors:</strong> Mingxuan Hu, Hongbo Ma, Xinlan Wu, Ziqi Liu, Jiaqi Liu, Yangbin Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出因果反事实去偏框架，缓解音频抑郁检测中的性别偏见，提升公平性与性能，属于大模型安全与对齐中的公平性研究，有应用价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01834' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems</h3>
<p><strong>Authors:</strong> Zhiyuan Wang, Aniri, Tianlong Chen, Yue Zhang, Heng Tao Shen, Xiaoshuang Shi, Kaidi Xu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出LEC框架控制LLM错误发现率，提升选择性预测与路由系统可靠性，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.01556' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</h3>
<p><strong>Authors:</strong> Chenting Wang, Yuhan Zhu, Yicheng Xu, Jiange Yang, Ziang Yan, Yali Wang (Shanghai Jiao Tong University), Yi Wang, Limin Wang (Shanghai Jiao Tong University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出Encoder-Predictor-Decoder框架与两阶段预训练策略，无需视频文本监督即可学习视频表示，解决传统MVM的收敛与语义冲突问题，在多基准上取得SOTA，属于原生多模态大模型的基础研究。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01342' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</h3>
<p><strong>Authors:</strong> Zhiheng Liu, Weiming Ren, Haozhe Liu, Zijian Zhou, Shoufa Chen, Haonan Qiu, Xiaoke Huang, Zhaochong An, Fanny Yang, Aditya Patel, Viktar Atliha, Tony Ng, Xiao Han, Chuyan Zhu, Chenyang Zhang, Ding Liu, Juan-Manuel Perez-Rua, Sen He, Jürgen Schmidhuber, Wenhu Chen, Ping Luo, Wei Liu, Tao Xiang, Jonas Schult, Yuren Cong</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对原生多模态大模型的统一视觉表示问题，提出TUNA框架构建统一表示空间，解决了decoupled表示的格式不匹配问题，在图像/视频理解与生成任务上取得SOTA结果，是原生多模态的核心改进。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02014' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions</h3>
<p><strong>Authors:</strong> Egemen Sert, \c{S}eyda Ertekin</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 聚焦视觉语言模型（VLM）的数据-centric微调，构建多模态数据集并优化推理语法，提升VLM多模态推理性能，属于原生多模态大模型核心研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00042' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI</h3>
<p><strong>Authors:</strong> Zag ElSayed, Grace Westerkamp, Gavin Gammoh, Yanchen Liu, Peyton Siekierski, Craig Erickson, Ernest Pedapati</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 使用多模态LLM（GPT-4 Vision）解释EEG的ICA成分，涉及MLLM视觉推理与解释，属于原生多模态大模型研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00194' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction</h3>
<p><strong>Authors:</strong> Jiazhen Liu, Mingkuan Feng, Long Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 解决多模态LLM（MLLM）分割的三难问题，提出同时文本掩码预测方法，属于原生多模态大模型研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00395' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints</h3>
<p><strong>Authors:</strong> Xisheng Feng</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出三阶段框架解决VLM的“模态差距”问题，通过自生成知识提示激活模型参数中的专家知识，减少推理驱动幻觉，在AgroBench基准上显著提升VLM性能（如Weed Identification准确率提升23.6%）
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00882' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</h3>
<p><strong>Authors:</strong> Jaewoo Song, Jooyoung Choi, Kanghyun Baek, Sangyub Lee, Daemin Park, Sungroh Yoon (Seoul National University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对多模态视觉文本生成中长文本注意力稀释问题，提出分治策略与调度注意力掩码，解决长文本生成的准确性与效率权衡，实验验证在单/多句基准上优于现有方法，有代码支持。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01302' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval</h3>
<p><strong>Authors:</strong> Ning Han, Yawen Zeng, Shaohua Long, Chengqing Li (Harbin Institute of Technology), Sijie Yang, Dun Tan, Jianfeng Dong, Jingjing Chen (Institute of Automation, Chinese Academy of Sciences)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出多轮对话式视频检索新任务与IVCR-200K基准，基于MLLMs构建多模态交互框架，解决传统单轮检索的局限性，实验验证数据集与框架的有效性。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01312' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation</h3>
<p><strong>Authors:</strong> Yexin Liu, Wen-Jie Shu, Zile Huang, Haoze Zheng, Yueze Wang, Manyuan Zhang, Ser-Nam Lim (NVIDIA), Harry Yang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对文本引导图像到视频生成的语义忽略问题，提出训练-free的注意力缩放与引导调度策略，解决语义 adherence与视觉质量的权衡，构建OmitI2V基准验证效果，属于原生多模态大模型的关键改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01334' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ViRectify: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Xusen Hei, Jiali Chen, Jinyu Yang, Mengchen Zhao, Yi Cai</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 构建ViRectify基准评估MLLMs的视频推理校正能力，涵盖动态感知、科学推理等任务，提出轨迹证据驱动框架提升校正效果，属于原生多模态大模型的评估与改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01424' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge</h3>
<p><strong>Authors:</strong> Yumeng He, Ying Jiang, Jiayin Lu, Yin Yang, Chenfanfu Jiang (University of California, Los Angeles)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 结合VLM知识提取URDF参数，生成物理一致的铰接物体3D模型，解决传统方法的 labor-intensive 问题，实验验证在多类别上的效果，属于原生多模态大模型的应用创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01629' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval</h3>
<p><strong>Authors:</strong> Xin Wang, Haipeng Zhang, Mang Li, Zhaohui Xia, Yueguo Chen, Yu Zhang, Chunyu Wei</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 在联合视觉-语言空间进行生成编辑，解决零-shot组合图像检索的模态 gap 问题，通过Control-Adapter提升数据效率，实验验证在多基准上的SOTA性能，属于原生多模态大模型的关键改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01636' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models</h3>
<p><strong>Authors:</strong> Patrick Kwon, Chen Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 基于视频扩散模型提出布局引导的漫画故事可视化框架，通过RegionalRoPE与masked条件损失提升角色与风格一致性，实验验证在多指标上的提升，属于原生多模态大模型的应用创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01686' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chunking Strategies for Multimodal AI Systems</h3>
<p><strong>Authors:</strong> Shashanka B R, Mohith Charan R, Seema Banu F</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 系统分析多模态数据分块策略，提出taxonomy与跨模态方法，属于原生多模态大模型中的多模态数据处理研究，覆盖模态全面。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00185' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning</h3>
<p><strong>Authors:</strong> Zhengzhuo Xu, SiNan Du, Yiyan Qi, Siwen Lu, Chengjin Xu, Chun Yuan, Jian Guo</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出ChartPoint框架，通过反射交互结合CoT与视觉 grounding，构建ChartPoint-SFT-62k数据集，提升MLLM图表推理能力，属于原生多模态大模型中的图表理解研究，实用性强。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages</h3>
<p><strong>Authors:</strong> Divendar Murtadak, Yoon Kim, Trilokya Akula</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究多模态LLM（MLLM）的诊断prompting方法，优化视觉复杂度评估性能，涉及MLLM prompt工程，属于原生多模态大模型研究方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00082' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation</h3>
<p><strong>Authors:</strong> Zirui Wang, Tao Zhang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 构建3D场景多模态标注数据集，涉及MLLM场景理解与问题生成，属于原生多模态大模型研究方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00226' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer</h3>
<p><strong>Authors:</strong> Dong In Lee, Hyungjun Doh, Seunggeun Chi, Runlin Duan, Sangpil Kim, Karthik Ramani</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 论文提出基于多模态扩散Transformer（MM-DiT）和4D高斯splatting（4DGS）的训练无关文本驱动4D场景编辑框架，解决现有方法的运动失真、几何漂移问题，通过时空子网格注意力和上下文token传播实现全局一致编辑，在DyNeRF数据集上表现更优，属于原生多模态大模型中的多模态扩散模型应用，具有创新价值。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00677' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards</h3>
<p><strong>Authors:</strong> Qiang Lyu, Zicong Chen, Chongxiao Wang, Haolin Shi, Shibo Gao, Ran Piao, Youwei Zeng, Jianlou Si, Fei Ding, Jing Li, Chun Pong Lau, Weiqiang Wang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 论文针对文本到图像（T2I）模型的Group Relative Policy Optimization（GRPO）方法的共享信用分配和奖励混合问题，提出Multi-GRPO框架，用树状轨迹提升早期步骤信用分配精度，用奖励分组解决多目标优化冲突，在PickScore-25k和OCR-Color-10基准上提升稳定性和对齐性能，属于原生多模态大模型中的T2I生成方向，技术创新显著。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00743' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding</h3>
<p><strong>Authors:</strong> Keita Otani, Tatsuya Harada</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 将场景图接地转化为Markov Random Field的MAP推理，解决复杂视觉查询的结构归纳偏差问题，显著提升VLM对多对象关系的接地性能，属于原生多模态大模型的关键能力优化
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00936' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency</h3>
<p><strong>Authors:</strong> Zhongbin Guo, Jiahe Liu, Wenyu Gao, Yushan Li, Chengzhi Li, Ping Jian</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 将语言-图像分割扩展至3D，利用多视图一致性约束提升语言到3D的准确性（较单视图基线提升15.6点），数据高效且支持零样本部署，属于原生多模态大模型的3D能力延伸
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01008' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards</h3>
<p><strong>Authors:</strong> Shulei Wang, Longhui Wei, Xin He, Jianbo Ouyang, Hui Lu, Zhou Zhao, Qi Tian</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出多主体个性化生成框架PSR，通过 pairwise 主体一致性奖励解决多主体生成中的一致性与文本遵循问题，提升原生多模态大模型的个性化生成能力
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01236' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation</h3>
<p><strong>Authors:</strong> Sheng Liu, Tianyu Luan, Phani Nuney, Xuelu Feng, Junsong Yuan (University at Buffalo)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出基于LLM的3D形状真实感评估 metric，将3D形状编码到语言空间，构建RealismGrading数据集，解决无参考3D形状评估问题，属于原生多模态大模型（3D-语言）的应用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01373' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries</h3>
<p><strong>Authors:</strong> Tushar Pranav, Eshan Pandey, Austria Lyka Diane Bala, Aman Chadha, Indriyati Atmosukarto, Donny Soh Cheng Lock (Nanyang Technological University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 构建RICE-VL基准评估VLM的跨ASEAN文化理解能力，涵盖VQA与视觉 grounding任务，揭示现有模型的西方中心偏差，属于原生多模态大模型的公平性研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01419' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Language-Guided Open-World Anomaly Segmentation</h3>
<p><strong>Authors:</strong> Klara Reichard, Nikolas Brasch, Nassir Navab (Technical University of Munich), Federico Tombari (Technical University of Munich)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出Clipomaly框架，基于CLIP的多模态嵌入实现零-shot开放世界异常分割，解决传统方法无法语义标注未知区域的问题，属于原生多模态大模型的应用创新。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01427' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation</h3>
<p><strong>Authors:</strong> Haodong Yan, Hang Yu, Zhide Zhong, Weilin Yuan, Xin Gong, Zehang Luo, Chengxi Heyu, Junfeng Li, Wenxuan Song, Shunbo Zhou, Haoang Li</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出结构与接触感知的表示，解决手-物体交互视频生成的物理约束问题，实验验证在多数据集上的真实感与一致性，属于原生多模态大模型的应用创新。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01677' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</h3>
<p><strong>Authors:</strong> Xiuli Bi, Die Xiao, Junchao Fan, Bin Xiao</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出语义与空间校正策略，解决CLIP-based弱监督分割的非目标激活问题，通过跨模态原型对齐与超像素引导校正提升性能，实验验证在PASCAL VOC与MS COCO上的SOTA效果，属于原生多模态大模型的改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.01701' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering</h3>
<p><strong>Authors:</strong> Xinyue Wang (), Yuheng Jia (), Hui Liu (), Junhui Hou ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 利用多模态大模型（MLLM）的隐藏状态进行嵌入选择，实现可解释多聚类，对原生多模态大模型的应用（用户驱动聚类）有实践价值
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00725' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals</h3>
<p><strong>Authors:</strong> Yuzheng Gao, Yuxing Long, Lei Kang, Yuchong Guo, Ziyan Yu, Shangqing Mao, Jiyao Zhang, Ruihai Wu, Dongjiang Li, Hui Shen, Hao Dong</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 构建RealAppliance数据集（100个高保真家电资产）和RealAppliance-Bench基准，用于评估多模态大语言模型和具身操纵规划模型，属于原生多模态大模型的评估资源，对该方向研究有重要支撑作用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.00287' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ViT³: Unlocking Test-Time Training in Vision</h3>
<p><strong>Authors:</strong> Dongchen Han, Yining Li, Tianyu Li, Zixuan Cao, Ziming Wang, Jun Song, Yu Cheng, Bo Zheng, Gao Huang (Tsinghua University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 系统研究视觉任务的测试时训练（TTT）设计，提出ViT³架构实现线性复杂度与并行计算，在图像分类、生成等多任务上验证效果，属于深度学习理论中训练策略与模型架构的创新。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01643' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Mean-Field Dynamics of Transformers</h3>
<p><strong>Authors:</strong> Philippe Rigollet (MIT)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 建立Transformer的平均场动力学框架，分析注意力聚类与表示坍塌机制，作者为理论领域权威，贡献重大，属于深度学习理论中的Transformer架构研究。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01868' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DL-CapsNet: A Deep and Light Capsule Network</h3>
<p><strong>Authors:</strong> Pouya Shiri, Amirali Baniasadi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 改进Capsule Network架构，提出深度且轻量的变体，涉及网络架构设计，属于深度学习理论核心方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00061' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation</h3>
<p><strong>Authors:</strong> Xiao Cui, Yulei Qin, Wengang Zhou, Hongsheng Li, Houqiang Li</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 用最优传输优化生成式数据集蒸馏的分布对齐，涉及深度学习分布匹配与数据集蒸馏，属于深度学习理论研究方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00308' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction</h3>
<p><strong>Authors:</strong> Anantha Padmanaban Krishna Kumar (Boston University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究ViT的参数减少策略，通过权重共享（GroupedMLP）与宽度压缩（ShallowMLP）提升性能与训练稳定性（峰值-最终精度退化从0.47%降至0.03%-0.06%），挑战“更大模型更优”的 scaling laws，属于深度学习理论中的网络架构优化
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01059' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Constructing Efficient Fact-Storing MLPs for Transformers</h3>
<p><strong>Authors:</strong> Owen Dugan, Roberto Garcia, Ronny Junkins, Jerry Liu, Dylan Zinsley, Sabri Eyuboglu, Atri Rudra, Chris Ré</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究Transformer中MLP的事实存储机制，提出新构造框架解决之前的局限性，实现渐近最优参数效率，属于深度学习理论中的网络结构与存储机制研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00207' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics</h3>
<p><strong>Authors:</strong> Deep Patel (), Emmanouil-Vasileios Vlatakis-Gkaragkounis ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究了神经网络零和博弈的收敛性，涉及架构、初始化和动态等深度学习理论核心问题，对理解对抗训练、AI对齐等应用的理论基础有价值
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00389' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks</h3>
<p><strong>Authors:</strong> Anish Lakkapragada ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 利用物理启发的奇异学习理论研究神经网路的相变（如grokking），对理解深度学习的泛化和相变现象有理论价值
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00686' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Preventing Model Collapse via Contraction-Conditioned Neural Filters</h3>
<p><strong>Authors:</strong> Zongjian Han (), Yiran Liang (), Ruiwen Wang (), Yiwei Luo (), Yilin Huang (), Xiaotong Song (), Dongqing Wei ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出基于收缩算子的神经滤波器，解决生成模型递归训练中的模型崩溃问题，提供理论收敛保证，对深度学习理论（生成模型训练稳定性）有重要贡献
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00757' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking</h3>
<p><strong>Authors:</strong> Alper Yıldırım, İbrahim Yücedağ</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对Transformer存在的"语义对齐税"和"灾难性刚性"问题展开研究，提出PRISM模型通过谐波表示解决可塑性-稳定性困境，对Transformer架构改进具有重要理论与实践价值。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01208' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</h3>
<p><strong>Authors:</strong> Chujie Zheng, Kai Dang, Bowen Yu, Mingze Li, Huiqiang Jiang, Junrong Lin, Yuqiong Liu, An Yang, Jingren Zhou, Junyang Lin</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对RL与LLM结合的稳定性问题，分析policy gradient的理论与实践 gap，提出Routing Replay等方法提升训练稳定性，属于深度学习理论中RL与LLM融合的关键研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01374' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Unreasonable Effectiveness of Last-layer Retraining</h3>
<p><strong>Authors:</strong> John C. Hill, Tyler LaBonte, Xinchen Zhang, Vidya Muthukumar</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 分析最后一层重训练改善鲁棒性的原因，发现效果来自hold-out集的群体平衡，属于深度学习理论中的模型训练机制研究，结论对鲁棒性优化有指导意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01766' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion</h3>
<p><strong>Authors:</strong> Ahmed Nebli</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出ThermoLion优化器，统一梯度符号与幅度，根据SNR动态调整更新策略，提升视觉模型性能，属于深度学习理论中的优化器研究，实验验证充分。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01881' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SVRG and Beyond via Posterior Correction</h3>
<p><strong>Authors:</strong> Nico Daheim, Thomas Möllenhoff, Ming Liang Ang, Mohammad Emtiyaz Khan</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 将SVRG与贝叶斯后验校正结合，提出牛顿式与Adam式变体，提升Transformer预训练与微调性能，属于深度学习理论中的优化器研究，创新点明确。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01930' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning</h3>
<p><strong>Authors:</strong> Sitao Cheng, Xunjian Yin, Ruiwen Zhou, Yuxuan Li, Xinyi Wang, Liangming Pan, William Yang Wang, Victor Zhong</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究强化学习对复合推理的泛化能力，发现RL可合成原子技能为复杂推理策略，属于深度学习理论的RL与推理结合方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01970' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning</h3>
<p><strong>Authors:</strong> Qiwei Liang, Boyang Cai, Minghao Lai, Sitong Zhuang, Tao Lin, Yan Qin, Yixuan Ye, Jiaming Liang, Renjing Xu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出AFRO自监督框架，通过扩散过程建模状态预测并学习动态感知的3D表示，结合扩散政策提升机器人操纵性能，属于深度学习理论中的表示学习创新，实验验证其在模拟和真实任务中的有效性。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00074' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition</h3>
<p><strong>Authors:</strong> Fadi Dornaika, Danyang Sun</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出基于超像素的数据增强方法改进Cutmix，涉及深度学习数据增强策略，属于深度学习理论研究方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00130' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Towards aligned body representations in vision models</h3>
<p><strong>Authors:</strong> Andrey Gizdov, Andrea Procopio, Yichen Li, Daniel Harari, Tomer Ullman</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究视觉模型的身体表征对齐，涉及模型表示学习，属于深度学习理论研究方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00365' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering</h3>
<p><strong>Authors:</strong> Jian Zhu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出多视图聚类的对比学习框架，涉及对比学习与多视图融合，属于深度学习理论研究方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.00368' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching</h3>
<p><strong>Authors:</strong> Mudit Gaur, Prashant Trivedi, Shuchin Aeron, Amrit Singh Bedi, George K. Atia, Vaneet Aggarwal</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 首次分析Flow Matching生成模型的样本复杂度，填补了连续流生成模型的理论空白，对深度学习理论中的生成模型分析具有重要意义。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01286' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Does Flatness imply Generalization for Logistic Loss in Univariate Two-Layer ReLU Network?</h3>
<p><strong>Authors:</strong> Dan Qiao, Yu-Xiang Wang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究logistic损失下两层ReLU网络的flatness与泛化能力关系，填补了深度学习理论中泛化分析的空白，具有重要理论价值。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01473' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method</h3>
<p><strong>Authors:</strong> Yan Huang, Jinming Xu, Jiming Chen, Karl Henrik Johansson</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出ST-GT统一时空梯度跟踪方法，解决分布式优化中的数据异质性与梯度噪声问题，属于深度学习理论中优化器方向的创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01732' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Active and Noise-Tolerant Strategic Perceptron</h3>
<p><strong>Authors:</strong> Maria-Florina Blacan, Hedyeh Beyhaghi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出主动学习的战略感知器算法，解决战略分类中的噪声问题，属于深度学习理论中的主动学习和算法设计，理论贡献明确。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01783' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning</h3>
<p><strong>Authors:</strong> Lama Alssum, Hasan Abed Al Kader Hammoud, Motasem Alfarra, Juan C Leon Alcazar, Bernard Ghanem (KAUST)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出信息最大化正则化策略，改善持续学习中的灾难性遗忘，计算开销小且适用于多模态数据，属于深度学习理论中的正则化研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01818' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models</h3>
<p><strong>Authors:</strong> Yudi Wu, Wenhao Zhao, Dianbo Liu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 用信息瓶颈理论分析离散潜在生成模型的多样性，分解路径与执行多样性，属于深度学习理论中的生成模型与信息理论研究，视角新颖。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01831' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees</h3>
<p><strong>Authors:</strong> Alessandro Breccia, Federica Gerace, Marco Lippi, Gabriele Sicuro, Pierluigi Contucci</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究Transformer对算术树根序列的学习能力，验证模型对结构化数据的学习能力，属于深度学习理论的网络架构研究方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.01870' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics</h3>
<p><strong>Authors:</strong> Giovanni Conforti (), Alain Durmus (), Le-Tuyet-Nhi Pham ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 首次给出离散扩散模型（包括掩码和随机游走动态）的非渐近收敛保证，对大模型新技术（扩散模型）的理论基础有重要贡献
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00580' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ReactionMamba: Generating Short &Long Human Reaction Sequences</h3>
<p><strong>Authors:</strong> Hajra Anwar Beg, Baptiste Chopin, Hao Tang, Mohamed Daoudi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 采用Mamba模型生成人体反应序列，涉及大模型新技术中的Mamba架构，属于大模型新技术研究方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00208' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction</h3>
<p><strong>Authors:</strong> Junqiao Fan, Pengfei Liu, Haocong Rao</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 用Spatial Mamba改进扩散模型的人体运动预测，涉及大模型新技术中的Mamba与扩散模型结合，属于大模型新技术研究方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00355' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models</h3>
<p><strong>Authors:</strong> Wenshuo Chen, Haosen Li, Shaofeng Liang, Lei Wang, Haozhe Jia, Kaishen Yuan, Jieming Wu, Bowen Tian, Yutao Yue</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 改进扩散模型反演过程，提出投影正交最小二乘法提升鲁棒性与适应性，属于大模型新技术中的扩散模型优化方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00369' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution</h3>
<p><strong>Authors:</strong> Seungho Choi, Jeahun Sung, Jihyong Oh (Seoul National University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 基于扩散先验提出频率对齐自蒸馏策略，解决真实图像超分辨率的低频偏差问题，在U-Net与DiT骨干上验证效果，属于大模型新技术（diffusion LLM）的关键改进。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01390' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MDiff4STR: Mask Diffusion Model for Scene Text Recognition</h3>
<p><strong>Authors:</strong> Yongkun Du, Miaomiao Zhao, Songlin Fan, Zhineng Chen, Caiyan Jia, Yu-Gang Jiang (Shanghai Jiao Tong University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 首次将掩码扩散模型引入场景文本识别，提出噪声策略与token替换机制解决训练推理错位与过自信问题，在多基准上超过SOTA的自回归模型，属于大模型新技术（diffusion LLM）的新应用。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01422' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers</h3>
<p><strong>Authors:</strong> Yiyang Ma, Feng Zhou, Xuedan Yin, Pu Cao, Yonghao Dang, Jianqin Yin</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出训练-free的分辨率缩放方法，通过位置嵌入校正与局部增强机制解决Diffusion Transformers的高分辨率生成问题，实验验证在多下游任务上的有效性，属于大模型新技术（diffusion LLM）的关键改进。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01426' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe</h3>
<p><strong>Authors:</strong> Yahui Liu, Yang Yue, Jingyuan Zhang, Chenxi Sun, Yang Zhou, Wencong Zeng, Ruiming Tang, Guorui Zhou</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 聚焦Diffusion MoE模型的架构配置（如DeepSeek-style专家模块、中间宽度等）及高效训练方法，提供了实用的训练指南，属于大模型新技术（diffusion LLM方向）。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01252' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Diffusion Model Framework for Maximum Entropy Reinforcement Learning</h3>
<p><strong>Authors:</strong> Sebastian Sanokowski, Kaustubh Patil, Alois Knoll</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 将扩散模型与MaxEntRL结合，提出DiffSAC等方法，提升连续控制任务性能与样本效率，属于大模型新技术中的扩散模型应用研究，实验验证充分。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02019' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness</h3>
<p><strong>Authors:</strong> Zhongjie Jiang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出PMCSF框架模拟人类认知缺陷生成合成数据，解决模型崩溃问题，属于大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01354' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Low-Bitrate Video Compression through Semantic-Conditioned Diffusion</h3>
<p><strong>Authors:</strong> Lingdong Wang, Guan-Ming Su, Divya Kothandaraman, Tsung-Wei Huang, Mohammad Hajiesmaili, Ramesh K. Sitaraman</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 用语义条件扩散模型进行低比特率视频压缩，涉及大模型新技术中的扩散模型应用，属于大模型新技术研究方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00408' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model</h3>
<p><strong>Authors:</strong> Alireza Javanmardi, Pragati Jaiswal, Tewodros Amberbir Habtegebrial, Christen Millerdurai, Shaoxiang Wang, Alain Pagani, Didier Stricker</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出反馈驱动的扩散框架TalkingPose，解决长视频人脸与手势动画的时空一致性问题，支持无限时长生成且无额外计算开销，属于大模型新技术中的diffusion扩展应用
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00909' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</h3>
<p><strong>Authors:</strong> Jing He, Haodong Li, Mingzhi Sheng, Ying-Cong Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出两阶段确定性框架，利用生成模型的世界先验提升几何密集预测（如单目深度估计）性能，仅用59K样本即超越大规模数据训练的判别模型，属于大模型新技术中的生成先验应用
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01030' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling</h3>
<p><strong>Authors:</strong> Han-Jin Lee, Han-Ju Lee, Jin-Seong Kim, Seok-Hwan Choi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出DPAC方法，通过将对抗梯度投影至生成得分的切空间，解决扩散模型对抗引导中的分布漂移问题，在保持攻击成功率的同时降低FID（提升生成质量），属于大模型新技术中的diffusion优化
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01153' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance</h3>
<p><strong>Authors:</strong> Pei Yang, Yepeng Liu, Kelly Peng, Yuan Gao, Yiren Song</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 基于Diffusion Transformer提出token化条件重建框架，解决水印去除与内容一致性的权衡问题，实验验证在感知质量与一致性上优于现有方法，属于大模型新技术（diffusion LLM）的应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01314' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reversible Inversion for Training-Free Exemplar-guided Image Editing</h3>
<p><strong>Authors:</strong> Yuke Li, Lianli Gao (University of Electronic Science and Technology of China), Ji Zhang, Pengpeng Zeng, Lichuan Xiang, Hongkai Wen, Heng Tao Shen (University of Electronic Science and Technology of China), Jingkuan Song</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出训练-free的可逆反转框架，通过两阶段去噪与掩码引导选择性去噪解决Exemplar-guided编辑的质量与效率问题，实验验证优于现有方法，属于大模型新技术（diffusion LLM）的创新应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01382' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling</h3>
<p><strong>Authors:</strong> Qisen Wang, Yifan Zhao, Peisen Shen, Jialu Li, Jia Li</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出Hyperspace扩散采样方法，生成3D一致的时间同步多视图视频，解决传统方法的泛化与 scalability问题，属于大模型新技术（diffusion LLM）的创新应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01481' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling</h3>
<p><strong>Authors:</strong> Hailong Yang, Te Zhang, Kup-sze Choi (The Hong Kong Polytechnic University), Zhaohong Deng</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出模糊规则引导的 latent 多路径扩散模型，解决异构图像特征的捕捉问题，通过规则链推理与 latent 压缩提升效率，实验验证在多数据集上的稳定性与准确性，属于大模型新技术（diffusion LLM）的框架创新。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01533' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Consistency Flow Model Achieves One-step Denoising Error Correction Codes</h3>
<p><strong>Authors:</strong> Haoyu Lei, Chin Wa Lau, Kaiwen Zhou, Nian Guo, Farzan Farnia</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出ECCFM框架，通过PF-ODE和差分时间正则化实现扩散模型的一步解码，显著提升推理效率，属于大模型新技术（diffusion方向）的重要进展。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01389' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> How Does RL Post-training Induce Skill Composition? A Case Study on Countdown</h3>
<p><strong>Authors:</strong> Simon Park (Princeton University), Simran Kaur (Princeton University), Sanjeev Arora (Princeton University)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究RL后训练对大模型技能组合的影响，通过Countdown任务分析组合泛化能力，属于大模型新技术中的RL应用研究，作者团队有理论影响力。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.01775' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients</h3>
<p><strong>Authors:</strong> He-Yen Hsieh, Hong Wang, H. T. Kung</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出扩散LLM早期推理终止方法，根据训练梯度动态性检测收敛，提升推理效率，属于大模型新技术中的扩散模型推理优化研究， overhead低。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.00670' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='Deep-learning-theory' class='field-section'>
<h2>Deep learning theory</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Provable Benefit of Sign Descent: A Minimal Model Under Heavy-Tailed Class Imbalance</h3>
<p><strong>Authors:</strong> Robin Yadav, Shuo Xie, Tianhao Wang, Zhiyuan Li</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Provides theoretical analysis of sign descent (an adaptive optimizer) compared to gradient descent under heavy-tailed class imbalance, addressing a key gap in understanding optimizer performance for language modeling tasks.
Score: 9
Field: Deep learning theory</p>
<p><a href='https://arxiv.org/abs/2512.00763' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning</h3>
<p><strong>Authors:</strong> Dongyue Li, Zhenshuo Zhang, Minxuan Duan, Edgar Dobriban, Hongyang R. Zhang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Introduces branching neural networks and an efficient algorithm (AutoBRANE) for multitask algorithmic reasoning, addressing negative interference in multitask learning.
Score: 8
Field: Deep learning theory</p>
<p><a href='https://arxiv.org/abs/2512.01113' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement</h3>
<p><strong>Authors:</strong> Harshil Vejendla</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Introduces a counter-example-driven curriculum framework for Transformers, improving robustness and extrapolation by focusing on model failures, addressing generalization in deep learning.
Score: 8
Field: Deep learning theory</p>
<p><a href='https://arxiv.org/abs/2512.01187' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Estimating the Effective Rank of Vision Transformers via Low-Rank Factorization</h3>
<p><strong>Authors:</strong> Liyu Zerihun</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Proposes a framework to estimate the effective rank (intrinsic dimensionality) of Vision Transformers, addressing overparameterization and providing insights into model complexity.
Score: 7
Field: Deep learning theory</p>
<p><a href='https://arxiv.org/abs/2512.00792' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fiber Bundle Networks: A Geometric Machine Learning Paradigm</h3>
<p><strong>Authors:</strong> Dong Liu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Proposes Fiber Bundle Networks, a geometric framework for machine learning that reformulates classification as interpretable geometric optimization, advancing network architecture theory.
Score: 7
Field: Deep learning theory</p>
<p><a href='https://arxiv.org/abs/2512.01151' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding</h3>
<p><strong>Authors:</strong> Yilong Zhao, Jiaming Tang, Kan Zhu, Zihao Ye, Chi-Chih Chang, Chaofan Lin, Jongseok Park, Guangxuan Xiao, Mohamed S. Abdelfattah, Mingyu Gao, Baris Kasikci, Song Han, Ion Stoica</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对大模型推理时的KV-Cache内存带宽压力问题，提出SparseSpec稀疏自推测解码框架，通过PillarAttn稀疏注意力和系统优化实现推理加速，属于高效大模型推理的关键改进。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01278' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs</h3>
<p><strong>Authors:</strong> Davide Nadalini, Manuele Rusci, Elia Cereda, Luca Benini, Francesco Conti, Daniele Palossi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出低功耗MCU上的多模态设备端学习框架，优化内存与训练效率，属于高效大模型训练与推理中的设备端高效训练方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00086' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Accelerating Streaming Video Large Language Models via Hierarchical Token Compression</h3>
<p><strong>Authors:</strong> Yiyu Wang, Xuyang Liu, Xiyan Gui, Xinying Lin, Boxue Yang, Chenfei Liao, Tailai Chen, Linfeng Zhang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出分层token压缩框架STC，通过缓存相似帧特征与剪枝显著token，在保持99%精度的同时，将ViT编码延迟降低24.5%、LLM预填充延迟降低45.3%，有效解决视频LLM的推理效率瓶颈
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00891' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention</h3>
<p><strong>Authors:</strong> Zipeng Wang, Dan Xu (The Hong Kong University of Science and Technology)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出压缩描述符注意力机制，将Visual Geometry Transformers的复杂度从 quadratic 降为 linear，解决长序列3D重建的效率问题，实验验证在多数据集上的准确性与 scalability，属于高效大模型训练与推理的关键改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01540' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Zhongyu Yang, Dannong Xu, Wei Pang, Yingfang Yuan</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出针对多模态大模型的token pruning方法，通过图结构剪枝去除视觉冗余、查询条件语义剪枝保留相关信息，提升推理效率（如LLaVA-NeXT-7B上6.8x prefill加速）同时保持性能，适用于高效大模型推理场景。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01949' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Coefficients to Directions: Rethinking Model Merging with Directional Alignment</h3>
<p><strong>Authors:</strong> Zhikang Chen (), Sen Cui (), Deheng Ye (), Min Zhang (), Gang Niu (), Yu Zhang (), Masashi Sugiyama (), Tingting Zhu ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对模型合并中的方向不一致问题，提出方向对齐框架，提升模型合并的性能和结构一致性，对高效大模型训练（模型合并是高效手段）有重要贡献
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00391' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models</h3>
<p><strong>Authors:</strong> Shashank Landge, Abhishek Patil, Tejas kamble, Bhushan Buddhivant, Priyanka Joshi</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出基于SVD的权重保存方法，解决LLM混合精度量化中无校准数据或隐私场景下的重要权重保留问题，属于大模型压缩（高效训练推理）方向的实用改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01343' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation</h3>
<p><strong>Authors:</strong> Rohin Manvi, Joey Hong, Tim Seyde, Maxime Labonne, Mathias Lechner, Sergey Levine</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出零开销的推理时奖励与成本预测方法，优化大模型自适应推理的效用与效率，属于高效大模型推理的创新方案。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01457' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Morphling: Fast, Fused, and Flexible GNN Training at Scale</h3>
<p><strong>Authors:</strong> Anubhab, Rupesh Nasre</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出Morphling编译器，将GNN编译为优化的后端实现，显著提升大规模GNN训练的吞吐量与内存效率，属于高效大模型训练的重要改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01678' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference</h3>
<p><strong>Authors:</strong> Sai Gokhale, Devleena Das, Rajeev Patwari, Ashish Sirasao, Elliott Delaye</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出KV缓存与模型压缩的系统优化框架，识别Pareto最优配置，提升长上下文LLM推理效率，属于高效大模型训练与推理中的推理优化研究，实用价值高。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01953' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Low-Rank Prehab: Preparing Neural Networks for SVD Compression</h3>
<p><strong>Authors:</strong> Haoran Qin, Shansita Sharma, Ali Abbasi, Chayne Thrash, Soheil Kolouri</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出预压缩微调阶段，鼓励权重低秩结构，提升SVD压缩后模型性能，优于现有方法，属于高效大模型训练与推理中的模型压缩研究，针对性强。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01980' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs</h3>
<p><strong>Authors:</strong> Jiaming Xu, Jiayi Pan, Hanzhen Wang, Yongkang Zhou, Jiancai Ye, Yu Wang, Guohao Dai</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出SpeContext框架，用蒸馏模型作为检索算法，异步预取数据，提升长上下文LLM推理吞吐量，属于高效大模型训练与推理中的长上下文优化研究，性能提升显著。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00722' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference</h3>
<p><strong>Authors:</strong> Jiaming Tang, Yufei Sun, Yilong Zhao, Shang Yang, Yujun Lin, Zhuoyang Zhang, James Hou, Yao Lu, Zhijian Liu, Song Han</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出VLASH框架，通过未来状态感知的异步推理提升Vision-Language-Action模型的实时性，属于高效大模型训练与推理中的推理优化，实验验证其速度提升和准确性保持。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01031' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving</h3>
<p><strong>Authors:</strong> Abdolazim Rezaei, Mehdi Sookhak</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出基于LoRA和adapter的参数高效微调框架，解决多模态3D目标检测的传感器dropout问题，属于高效大模型训练与推理中的参数高效方法。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation</h3>
<p><strong>Authors:</strong> Louis Geist, Loic Landrieu, Damien Robert</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出轻量级3D分割框架，优化速度与参数，属于高效大模型训练与推理中的轻量级模型设计方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00385' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding</h3>
<p><strong>Authors:</strong> Pengfei Hu, Meng Cao, Yingyao Wang, Yi Wang, Jiahua Dong, Jun Song, Yu Cheng, Bo Zheng, Xiaodan Liang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 论文提出SpecTemp框架，通过强化学习的推测性时间推理，用轻量草稿MLLM快速探索salient帧，强大目标MLLM聚焦推理验证，构建SpecTemp-80K数据集，在多个视频理解基准上保持精度同时显著加速推理，属于高效大模型训练与推理中的推理效率优化方向，具有实用价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00805' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning</h3>
<p><strong>Authors:</strong> Haijian Shao, Wei Liu, Xing Deng</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出量子启发的神经算子几何框架，证明谱-功能等价定理，解决多模态特征异质性与硬件算子冗余问题，对高效大模型训练推理中的结构化剪枝和硬件适配有重要理论与实践价值
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00880' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Accelerating Inference of Masked Image Generators via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Pranav Subbaraman, Shufan Li, Siyan Zhao, Aditya Grover</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出RL框架Speed-RL，通过质量-速度联合奖励加速掩码生成模型推理，在保持生成质量的同时实现3倍加速，属于高效大模型训练推理中的生成模型优化
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01094' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Weight Space Representation Learning with Neural Fields</h3>
<p><strong>Authors:</strong> Zhuoqian Yang (École Polytechnique Fédérale de Lausanne), Mathieu Salzmann (École Polytechnique Fédérale de Lausanne), Sabine Süsstrunk (École Polytechnique Fédérale de Lausanne)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出通过预训练模型和LoRA约束优化空间，将权重作为有效表示，结合扩散模型提升生成质量，涉及高效大模型训练中的低秩适应技术，有实际应用价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01759' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning</h3>
<p><strong>Authors:</strong> Eunjeong Jeong, Giovanni Perin, Howard H. Yang, Nikolaos Pappas</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出语义感知的联邦学习调度框架，用特征代理估计模型冗余，提升能量收集场景下的性能，属于高效大模型训练与推理中的联邦学习研究，场景贴合实际。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01983' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems</h3>
<p><strong>Authors:</strong> Daria Smirnova, Hamid Nasiri, Marta Adamska, Zhengxin Yu, Peter Garraghan</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出GUIDE框架优化LLM编排系统的模型选择，提升能效与准确性，属于高效大模型训练与推理的基础设施优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.01099' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression</h3>
<p><strong>Authors:</strong> Zhengxin Chen, Xiaohai He, Tingrong Zhang, Shuhua Xiong, Chao Ren</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 论文针对图像压缩的非线性变换编码问题，提出MGTPCN框架，结合多尺度门控Transformer（MGT）提取全局多尺度特征和先验引导卷积（PGConv）增强局部特征，在图像压缩任务上超越SOTA算法，属于高效大模型训练与推理中的high compression方向，具有技术创新。
Score: 6
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.00744' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Label Forensics: Interpreting Hard Labels in Black-Box Text Classifier</h3>
<p><strong>Authors:</strong> Mengyao Du, Gang Yang, Han Fang, Quanjun Yin, Ee-chien Chang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出Label Forensics框架，通过语义邻域采样和迭代优化解释黑盒文本分类器的标签语义，属于深度学习可解释性的关键应用。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.01514' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons</h3>
<p><strong>Authors:</strong> Cheng Gao, Huimin Chen, Chaojun Xiao, Zhiyi Chen, Zhiyuan Liu, Maosong Sun</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 系统研究LLM幻觉相关神经元的存在、影响与起源，发现少量神经元可预测幻觉，揭示其因果作用与预训练起源，为模型可解释性与可靠性提供关键 insights。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.01797' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features</h3>
<p><strong>Authors:</strong> Mohammad Zare</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 将文本转换为语义图像表示，通过多任务学习实现特征解耦，提升可解释性，属于深度学习可解释性研究方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.00088' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Faster Verified Explanations for Neural Networks</h3>
<p><strong>Authors:</strong> Alessandro De Palma, Greta Dolcetti, Caterina Urban</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出FaVeX算法解决验证性解释的 scalability问题，通过动态处理与信息复用提升计算效率，能处理几十万非线性激活的大模型，是可解释性的方法创新。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.00164' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AlignSAE: Concept-Aligned Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Minglai Yang, Xinyu Guo, Mihai Surdeanu, Liangming Pan</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出概念对齐的稀疏自编码器，通过预训练+后训练绑定SAE特征与人类概念，实现LLM隐藏特征的解释与控制，属于深度学习可解释性中的白盒解释研究，效果显著。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02004' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Probing the "Psyche" of Large Reasoning Models: Understanding Through a Human Lens</h3>
<p><strong>Authors:</strong> Yuxiang Chen, Zuohan Wu, Ziwei Wang, Xiangning Yu, Xujia Li, Linyi Yang, Mengyue Yang, Jun Wang, Lei Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出人类认知视角的taxonomy，分析LRM推理步骤，构建CAPO自动标注框架，属于深度学习可解释性中的人类认知分析研究，工具链完整。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.00729' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unsupervised decoding of encoded reasoning using language model interpretability</h3>
<p><strong>Authors:</strong> Ching Fang, Samuel Marks</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 用logit lens等可解释性技术解码模型隐藏推理，验证现有技术对编码推理的鲁棒性，为模型可解释性研究提供新视角。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.01222' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI</h3>
<p><strong>Authors:</strong> Kamal Basha S, Athira Nambiar</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 结合监督对比机器遗忘与可解释AI框架，减少声纳图像分类的背景偏差，提升模型鲁棒性与可解释性（如LIME生成更精准的归因），属于深度学习可解释性的实践应用
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.01291' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification</h3>
<p><strong>Authors:</strong> Saeed AlMarri, Mathieu Ravaut, Kristof Juhasz, Gautier Marti, Hamdan Al Ahbabi, Ibrahim Elfadel</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究LLM在金融表格分类中的SHAP解释忠实性，发现自解释与SHAP值的差异，为金融高风险领域的可解释性部署提供了实际 insights，属于可解释性的应用研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.00163' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine</h3>
<p><strong>Authors:</strong> Isara Liyanage (), Uthayasanker Thayasivam ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对可解释提升机（EBM）提出三种增强方法，提升其准确性、公平性和冷启动性能，对深度学习可解释性（玻璃箱模型）的研究有实践价值
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.00528' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?</h3>
<p><strong>Authors:</strong> Matt MacDermott, Qiyao Wei, Rada Djoneva, Francis Rhys Ward</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 研究训练激励对CoT可监控性的影响，发现对抗优化降低可监控性，属于深度学习可解释性中的模型推理可监控性研究，结论对训练策略有指导意义。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.00218' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback</h3>
<p><strong>Authors:</strong> Aiden Yiliu Li, Bizhi Yu, Daoan Lei, Tianhe Ren, Shilong Liu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出Chain-of-Ground框架通过迭代推理与参考反馈提升GUI接地准确性，在基准与真实工业数据集上显著提升，属于多模态智能体的GUI grounding方向。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01979' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR</h3>
<p><strong>Authors:</strong> Lixing Guo, Tobias H\"ollerer</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出AR中语言引导空间检索代理，涉及多模态智能体的语言与空间grounding，属于多模态智能体研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00294' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent</h3>
<p><strong>Authors:</strong> Neeraj Anand, Rishabh Jain, Sohan Patnaik, Balaji Krishnamurthy, Mausoom Sarkar</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 论文针对GUI自动化的空间信息不足和模型尺寸问题，提出基于instruct-BLIP的多模态架构AFRAgent，用自适应特征重归一化增强图像嵌入，在Meta-GUI和AITW基准上达到SOTA，且模型尺寸仅为竞品的1/4，直接关联多模态智能体中的GUI Agent方向，具有实际应用价值。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00846' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MM-ACT: Learn from Multimodal Parallel Generation to Act</h3>
<p><strong>Authors:</strong> Haotian Liang, Xinyi Chen, Bin Wang, Mingkang Chen, Yitian Liu, Yuhao Zhang, Zanxin Chen, Tianshuo Yang, Yilun Chen, Jiangmiao Pang, Dong Liu, Xiaokang Yang, Yao Mu, Wenqi Shao, Ping Luo</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 统一VLA模型整合text、image、action生成，通过多模态并行学习提升机器人任务性能，在LIBERO模拟（96.3%成功率）与真实机器人（72.0%成功率）上取得优异结果，属于多模态智能体的核心进展
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00975' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</h3>
<p><strong>Authors:</strong> Daeun Lee, Subhojyoti Mukherjee, Branislav Kveton (Google), Ryan A. Rossi (Adobe), Viet Dac Lai, Seunghyun Yoon, Trung Bui, Franck Dernoncourt (Google), Mohit Bansal (University of North Carolina at Chapel Hill)</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 构建StreamGaze基准评估MLLMs的gaze引导流媒体视频理解能力，涵盖过去、现在与 proactive 任务，揭示现有模型的推理与意图预测缺陷，属于多模态智能体的关键评估与改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HiconAgent: History Context-aware Policy Optimization for GUI Agents</h3>
<p><strong>Authors:</strong> Xurui Zhou, Gongwei Chen, Yuquan Xie, Zaijing Li, Kaiwen Zhou, Shuai Wang, Shuo Yang, Zhuotao Tian, Rui Shao</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对GUI智能体的历史上下文利用问题，提出HCPO策略优化方法，通过动态上下文采样和锚点引导压缩提升了GUI接地准确率与步骤成功率，同时降低计算开销，实验验证其在GUI-Odyssey等基准上优于现有模型。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01763' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories</h3>
<p><strong>Authors:</strong> Jacob Thompson (), Emiliano Garcia-Lopez (), Yonatan Bisk ()</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出REM基准评估多模态大模型的具身空间推理能力，对多模态智能体（具身应用）的发展有推动作用
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00736' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents</h3>
<p><strong>Authors:</strong> Ruihan Chen, Qiming Li, Xiaocheng Feng, Xiaoliang Yang, Weihong Zhong, Yuxuan Gu, Zekun Zhou, Bing Qin</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出MPR-GUI-Bench基准，用GUI-XLI方法提升多语言GUI智能体的感知与推理能力，属于多模态智能体中的GUI Agent研究，贴合用户高优先级方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00756' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ChartAnchor: Chart Grounding with Structural-Semantic Fidelity</h3>
<p><strong>Authors:</strong> Xinhang Li, Jingbo Zhou, Pengfei Luo, Yixiong Xiao, Tong Xu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 针对图表视觉-语义接地问题，提出基准与方法解决现有局限，提升图表结构与内容对齐准确性，属于多模态智能体的GUI grounding方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01017' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL</h3>
<p><strong>Authors:</strong> Shinji Mai, Yunpeng Zhai, Ziqian Chen, Cheng Chen, Anni Zou, Shuchang Tao, Zhaoyang Liu, Bolin Ding</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 解决agentic RL任务稀缺问题，通过好奇心驱动的任务合成提升智能体学习能力，属于多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01311' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration</h3>
<p><strong>Authors:</strong> Nan Sun, Bo Mao, Yongchang Li, Chenxu Wang, Di Guo, Huaping Liu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出InteractGen框架，将单一大模型分解为多智能体架构（感知、规划、决策等），用于人机协作，属于多模态智能体的架构创新，实际部署验证其提升任务成功率和协作效果。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00797' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</h3>
<p><strong>Authors:</strong> Chaojun Ni, Cheng Chen, Xiaofeng Wang, Zheng Zhu, Wenzhao Zheng, Boyuan Wang, Tianrun Chen, Guosheng Zhao, Haoyun Li, Zhehao Dong, Qiang Zhang, Yun Ye, Yang Wang, Guan Huang, Wenjun Mei</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出轻量级Vision-Language-Action (VLA)模型SwiftVLA，通过4D视觉几何Transformer与融合token增强时空推理，在边缘设备上实现18倍加速与12倍内存 reduction，性能接近7倍大模型，适用于多模态智能体场景
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00903' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance</h3>
<p><strong>Authors:</strong> Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出S²-MLLM框架，通过隐式结构引导增强MLLM的3D空间推理能力，无需依赖点云重建即可提升视觉接地性能（较基线提升显著），适用于多模态智能体的3D场景理解
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01223' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</h3>
<p><strong>Authors:</strong> Yanjia Huang, Xianshun Jiang, Xiangbo Gao, Mingyang Wu, Zhengzhong Tu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出VISTAv2框架，通过动作感知的条件扩散Transformer预测未来视图，融合语言指令生成价值地图以指导视觉-语言导航，属于多模态智能体的导航策略创新，实验验证其在MP3D和RoboTHOR数据集上的性能提升。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.00041' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FOM-Nav: Frontier-Object Maps for Object Goal Navigation</h3>
<p><strong>Authors:</strong> Thomas Chabal, Shizhe Chen, Jean Ponce, Cordelia Schmid</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> 提出FOM-Nav框架，通过Frontier-Object Maps融合视觉语言模型进行目标导航，提升导航效率，属于多模态智能体的导航策略创新，实验验证其在MP3D和HM3D基准上的SOTA性能。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.01009' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='Large-model-security--alignment' class='field-section'>
<h2>Large model security & alignment</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> What Is Preference Optimization Doing, How and Why?</h3>
<p><strong>Authors:</strong> Yue Wang, Qizhou Wang, Zizhuo Zhang, Ang Li, Gang Niu, Bo Han, Masashi Sugiyama</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Analyzes the dynamics of preference optimization methods (DPO, PPO) for LLM alignment, uncovering key differences in their optimization targets and components critical for improving preference-aligned LLMs.
Score: 8
Field: Large model security & alignment</p>
<p><a href='https://arxiv.org/abs/2512.00778' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</h3>
<p><strong>Authors:</strong> MohammadParsa Dini, Human Jafari, Sajjad Amini, MohammadMahdi Mojahedian</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Proposes an adaptive-lambda framework for machine unlearning in diffusion models and VAEs, enabling better removal of forgotten data while preserving generation quality for large model safety.
Score: 7
Field: Large model security & alignment</p>
<p><a href='https://arxiv.org/abs/2512.01054' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='Efficient-large-model-training--inference' class='field-section'>
<h2>Efficient large model training & inference</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</h3>
<p><strong>Authors:</strong> Ningning Chen, Weicai Ye, Ying Jiang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Introduces a wavelet-enhanced 1-bit quantization method for LLMs, improving quantization fidelity while maintaining efficiency for large model inference.
Score: 8
Field: Efficient large model training & inference</p>
<p><a href='https://arxiv.org/abs/2512.00862' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> WUSH: Near-Optimal Adaptive Transforms for LLM Quantization</h3>
<p><strong>Authors:</strong> Jiale Chen, Vage Egiazarian, Torsten Hoefler, Dan Alistarh</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Derives near-optimal adaptive linear transforms for LLM quantization, addressing dynamic range challenges in low-bitwidth quantization and improving upon fixed transforms.
Score: 7
Field: Efficient large model training & inference</p>
<p><a href='https://arxiv.org/abs/2512.00956' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Projection-Free CNN Pruning via Frank-Wolfe with Momentum: Sparser Models with Less Pretraining</h3>
<p><strong>Authors:</strong> Hamza ElMokhtar Shili, Natasha Patnaik, Isabelle Ruble, Kathryn Jarjoura, Daniel Suarez Aguirre</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Reason:</strong> Applies Frank-Wolfe with momentum to CNN pruning, achieving sparser models with less pretraining for efficient model compression.
Score: 7
Field: Efficient large model training & inference</p>
<p><a href='https://arxiv.org/abs/2512.01147' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>