<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-04</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>多模态智能体</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >大模型新技术</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-04</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-04 12:31:12</p>
<p>已自动阅读了 250 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：120542</p>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 10.0/10]</span> GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning</h3>
<p><strong>Authors:</strong> Haolong Yan, Yeqing Shen, Xin Huang, Jia Wang, Kaijun Tan, Zhixuan Liang, Hongxin Li, Zheng Ge, Osamu Yoshie, Si Li, Xiangyu Zhang, Daxin Jiang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 针对GUI智能体的复杂屏幕导航问题，提出GUI模拟环境引擎，通过多轮强化学习提升代理的探索策略，验证了监督微调作为基础、单轮RL增强泛化、多轮RL优化探索的有效性，直接对应用户高优先级的GUI智能体研究方向。
Score: 10
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.02423' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch</h3>
<p><strong>Authors:</strong> Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出30B参数的多模态智能体模型，统一多模态规划、主动图像操作和深度搜索，通过交错推理动态切换视觉操作与知识检索，仅用监督微调在MMSearch和FVQA上超越Gemini 2.5 Flash，实现复杂多步任务的工具调用，是多模态智能体的重要进展。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.02395' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PPTArena: A Benchmark for Agentic PowerPoint Editing</h3>
<p><strong>Authors:</strong> Michael Ofengenden, Yunze Man, Ziqi Pang, Yu-Xiong Wang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出PPTArena基准用于评估PowerPoint编辑智能体，针对GUI Agent的精确编辑问题设计PPTPilot结构感知智能体，解决跨幻灯片、布局敏感等编辑任务，对多模态智能体中的GUI Agent研究有重要意义
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.03042' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</h3>
<p><strong>Authors:</strong> Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出多模态记忆代理，构建 episodic、semantic、visual 三类记忆，通过自适应检索代理迭代选择相关记忆源，解决长视频推理中的上下文限制和视觉细节丢失问题，在5个长视频QA基准上平均超过SOTA 8.4%，属于多模态智能体的长视频推理优化。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.02425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization</h3>
<p><strong>Authors:</strong> Zhengcheng Wang, Zichuan Lin, Yijun Yang, Haobo Fu, Deheng Ye</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出视觉语言导航代理框架，通过视觉提示和步骤级政策优化提升导航性能，属于多模态智能体的重要改进。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.02631' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Understanding and Harnessing Sparsity in Unified Multimodal Models</h3>
<p><strong>Authors:</strong> Shwai He, Chaorui Deng, Ang Li, Shen Yan</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 系统分析统一多模态模型的稀疏性特征（理解组件可压缩、生成组件对压缩敏感），提出MoE适应方法动态激活生成模块的专家，在不损失性能的前提下仅激活约50%参数，显著提升多模态模型的推理效率，属于高效大模型训练与推理的关键优化。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02351' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Glance: Accelerating Diffusion Models with 1 Sample</h3>
<p><strong>Authors:</strong> Zhuobai Dong, Rui Zhao, Songjie Wu, Junchao Yi, Linjie Li, Zhengyuan Yang, Lijuan Wang, Alex Jinpeng Wang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出相位感知的扩散模型加速策略，通过Slow-LoRA（早期语义阶段）与Fast-LoRA（后期冗余阶段）轻量级适配器，仅用1样本训练即可实现5倍推理加速，同时保持生成质量，解决了扩散模型“重计算、慢推理”的核心痛点，泛化性与效率优势显著。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02899' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</h3>
<p><strong>Authors:</strong> Zhenkai Wu, Xiaowen Ma, Zhenliang Ni, Dengming Zhang, Han Shu, Xin Jiang, Xinghao Chen</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出训练-free的视觉语言模型（VLM）token剪枝算法，通过离心式剪枝范式与空间稀疏性缓冲策略，平衡冗余消除与目标区域覆盖，在88.9%剪枝率下保持性能并提升推理速度，直接解决VLM部署的计算成本问题，实验验证了在5个主流VLM上的有效性。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02700' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification</h3>
<p><strong>Authors:</strong> Zhendong Tan, Xingjun Zhang, Chaoyi Hu, Junjie Peng, Kun Xia</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 针对长上下文生成中验证的瓶颈，提出用部分KV状态快速验证的自投机解码方法，显著提升大模型推理速度，属于高效大模型推理加速的重要改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02337' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning</h3>
<p><strong>Authors:</strong> Songqiao Su, Xiaofei Sun, Xiaoya Li, Albert Wang, Jiwei Li, Chris Shum</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 用强化学习优化HGEMM CUDA内核，显著超越cuBLAS等主流库的性能，矩阵乘法是大模型的核心运算，属于高效大模型训练与推理的关键优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02551' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization</h3>
<p><strong>Authors:</strong> Feiyu Wang, Xinyu Tan, Bokai Huang, Yihao Zhang, Guoan Wang, Peizhuang Cong, Tong Yang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出极低比特量化框架，将实值Transformer转换为复值形式，提升量化效率，属于高效大模型训练与推理的量化优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Model Recovery at the Edge under Resource Constraints for Physical AI</h3>
<p><strong>Authors:</strong> Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出FPGA加速的边缘设备模型恢复框架，解决资源约束下的高效部署问题，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity</h3>
<p><strong>Authors:</strong> Hongxiang Liu, Zhifang Deng, Tong Pu, Shengli Lu</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出基于局部相似性的稀疏加速框架，实现Transformer全组件的端到端稀疏加速，有效降低计算量并提升能效，属于高效大模型硬件加速的关键研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02403' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs</h3>
<p><strong>Authors:</strong> Vishnu Sarukkai, Asanshay Gupta, James Hong, Michaël Gharbi, Kayvon Fatahalian</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出上下文蒸馏结合自一致性cascade，无需训练即可降低LLM代理的推理成本，属于高效大模型推理成本优化的实用方法。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.02543' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning</h3>
<p><strong>Authors:</strong> Boyu Zhu, Xiaofei Wen, Wenjie Jacky Mo, Tinghui Zhu, Yanan Xie, Peng Qi, Muhao Chen</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 针对全模态LLM设计统一安全护栏，通过多模态数据集和推理机制提升安全性，直接关联大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.02306' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities</h3>
<p><strong>Authors:</strong> Yuan Xiong, Ziqi Miao, Lijun Li, Chen Qian, Jie Li, Jing Shao</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 研究视觉上下文对多模态大模型安全漏洞的影响，提出Contextual Image Attack方法，针对GPT-4o和Qwen2.5-VL等模型的安全对齐问题，揭示多模态安全脆弱性，对大模型安全与对齐研究有重要价值
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.02973' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration</h3>
<p><strong>Authors:</strong> Zhongyu Yang, Yingfang Yuan, Xuanming Jiang, Baoyi An, Wei Pang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 针对多模态大模型的幻觉问题，提出InEx框架，结合内部自省推理与跨模态多智能体协作，有效缓解幻觉并提升可靠性，对大模型安全与对齐中的输出可靠性研究有价值
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.02981' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models</h3>
<p><strong>Authors:</strong> Naveen George, Naoki Murata, Yuhta Takida, Konda Reddy Mopuri, Yuki Mitsufuji</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出基于生成蒸馏的持续遗忘框架，解决扩散模型的持续数据删除问题，符合GDPR等法规要求，属于大模型安全与对齐的重要研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.02657' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FiMMIA: scaling semantic perturbation-based membership inference across modalities</h3>
<p><strong>Authors:</strong> Anton Emelyanov, Sergei Kudriashov, Alena Fenogenova</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出多模态成员推理框架，针对多模态大模型的隐私安全问题，属于大模型安全与对齐的关键研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.02786' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses</h3>
<p><strong>Authors:</strong> Han Luo, Guy Laban</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 针对敏感场景下LLM响应的心理社会安全问题，设计多智能体评估框架，直接关联大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.02282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Invasive Context Engineering to Control Large Language Models</h3>
<p><strong>Authors:</strong> Thomas Rivasseau</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出侵入式上下文工程控制LLM，提升长上下文场景的安全性，符合大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.03001' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks</h3>
<p><strong>Authors:</strong> Zhiyuan He</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 研究LLM的深度泛化问题，揭示Transformer架构的局限并提出解决方法，属于深度学习理论方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.02677' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework</h3>
<p><strong>Authors:</strong> Haojin Deng, Yimin Yang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出上下文增强的对比损失函数，解决对比学习中增强样本的信息扭曲问题，通过双收敛目标提升学习效率和泛化性，在8个大型基准数据集上超过16种SOTA对比学习方法，对深度学习理论中的对比学习优化有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.02152' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles</h3>
<p><strong>Authors:</strong> Yizhou Zhang, Lun Du</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 用谱动力学理论分析数据curation对模型训练的影响，揭示静态剪枝的局限性和动态curation的加速潜力，属于深度学习理论的重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.02409' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Synthetic Error Injection Fails to Elicit Self-Correction In Language Models</h3>
<p><strong>Authors:</strong> David X. Wu, Shreyas Kapur, Anant Sahai, Stuart Russell</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 研究合成错误注入对LLM自校正的影响，揭示模型学习机制的局限，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.02389' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> A Framework for Causal Concept-based Model Explanations</h3>
<p><strong>Authors:</strong> Anna Rodum Bjørru, Jacob Lysnæs-Larsen, Oskar Jørgensen, Inga Strümke, Helge Langseth</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出因果概念模型解释框架，通过概念干预生成可解释结果，符合深度学习可解释性方向。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> See, Think, Learn: A Self-Taught Multimodal Reasoner</h3>
<p><strong>Authors:</strong> Sourabh Sharma, Sonam Gupta, Sadbhawna</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出自训练框架STL，通过结构化推理模板（先感知视觉属性再推理）和负向 rationale 增强VLMs的推理与感知能力，无需额外标注或 proprietary模型，在多领域任务中提升可解释性和推理性能，属于深度学习可解释性的重要方法。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02456' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models</h3>
<p><strong>Authors:</strong> Joshua Wolff Anderson, Shyam Visweswaran</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 研究公平性约束对Shapley-based特征重要性排名的影响，分析模型公平性与可解释性的关系，揭示公平性改进对解释稳定性的作用，对深度学习可解释性中的公平性与解释一致性研究有重要意义
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02265' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration</h3>
<p><strong>Authors:</strong> Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出多智能体协作的多模态可解释内容安全框架，提升安全评估的可解释性，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02530' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs</h3>
<p><strong>Authors:</strong> Theodoros Aivalis, Iraklis A. Klampanos, Antonis Troumpoukis, Joemon M. Jose</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 用知识图谱实现图像生成的训练数据归因，解决生成模型的透明度问题，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02713' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning</h3>
<p><strong>Authors:</strong> Zhonghao He, Tianyi Qiu, Hirokazu Shirado, Maarten Sap</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出无监督度量评估LLM推理的贝叶斯合理性，关联深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02914' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Enforcing Orderedness to Improve Feature Consistency</h3>
<p><strong>Authors:</strong> Sophie L. Wang, Alex Quach, Nithin Parsan, John J. Yang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 针对稀疏自动编码器（SAE）的特征一致性问题，提出Ordered Sparse Autoencoders（OSAE），解决排列不识别问题，提升可解释性特征的稳定性，对深度学习可解释性中的SAE改进有价值
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.02194' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges</h3>
<p><strong>Authors:</strong> Kevin David Hayes, Micah Goldblum, Vikash Sehwag, Gowthami Somepalli, Ashwinee Panda, Tom Goldstein</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出结构化方法评估文本到图像模型的27种故障模式，用VLM作为法官识别生成图像中的错误，构建多模型生成的数据集，揭示属性保真度和对象表示的系统误差，对原生多模态大模型的可靠性和可解释性研究有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02161' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Le Thien Phuc Nguyen, Zhuoran Yu, Samuel Low Yu Hang, Subin An, Jeongik Lee, Yohan Ban, SeungEun Chung, Thanh-Huy Nguyen, JuWan Maeng, Soochahn Lee, Yong Jae Lee</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 构建AV-SpeakerBench基准，聚焦多模态大模型的视听语音推理能力，设计 speaker-centered 问题和跨模态依赖的提问，评估主流MLLM的性能，为原生多模态大模型的细粒度推理评估提供基础。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02231' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding</h3>
<p><strong>Authors:</strong> Yerim Jeon, Miso Lee, WonJun Moon, Jae-Pil Heo</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出3D空间语言指令掩码策略（3D-SLIM），通过几何自适应掩码和指令感知掩码解决LLM在3D场景理解中的顺序偏见和注意力限制，无需修改架构即可提升空间推理能力，在多个3D场景语言任务中显著超越基线，属于原生多模态大模型的空间推理优化。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02487' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens</h3>
<p><strong>Authors:</strong> Jian Yang, Dacheng Yin, Xiaoxuan He, Yong Li, Fengyun Rao, Jing Lyu, Wei Zhai, Yang Cao, Zheng-Jun Zha</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出噪声查询token机制，通过端到端优化桥接预训练视觉语言模型（VLM）与扩散模型，解决固定查询token导致的任务泛化崩溃问题，实现多任务稳定持续学习，属于原生多模态大模型的融合与优化，实验验证了泛化能力提升。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02536' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective</h3>
<p><strong>Authors:</strong> Qiyao Xue, Weichen Liu, Shiqi Wang, Haoming Wang, Yuyang Wu, Wei Gao</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 构建认知接地的多视图空间推理基准，分析VLMs的推理路径，关联原生多模态大模型的空间理解研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02340' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding</h3>
<p><strong>Authors:</strong> Hyeongseop Rha, Jeong Hun Yeo, Junil Won, Se Jin Park, Yong Man Ro</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出模态重要性引导的多模态情感理解框架，提升多模态LLM的可靠性，关联原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02699' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation</h3>
<p><strong>Authors:</strong> Jianzong Wu, Hao Lian, Dachao Hao, Ye Tian, Qingyu Shi, Biaolong Chen, Hao Jiang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 研究音频-视频联合去噪训练对视频生成的影响，发现音频作为特权信号可正则化视频动态（如碰撞与声音的因果关系），在大运动和物体接触场景中提升视频质量，为原生多模态大模型的跨模态协同训练提供新视角。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.02457' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Taming Camera-Controlled Video Generation with Verifiable Geometry Reward</h3>
<p><strong>Authors:</strong> Zhaoqing Wang, Xiaobo Xia, Zhuolin Bie, Jinlin Liu, Dongdong Yu, Jia-Wang Bian, Changhu Wang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 引入在线强化学习（RL）后训练框架，通过可验证几何奖励（段级相机轨迹对齐）解决扩散模型的相机控制精度问题，实现更准确的几何一致性与视觉质量，属于大模型新技术中的扩散模型RL优化，实验验证优于监督微调（SFT）基线。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02870' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Guided Self-Evolving LLMs with Minimal Human Supervision</h3>
<p><strong>Authors:</strong> Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出少量监督下的LLM自进化框架，解决无引导自进化的漂移问题，属于大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02472' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LumiX: Structured and Coherent Text-to-Intrinsic Generation</h3>
<p><strong>Authors:</strong> Xu Han, Biao Zhang, Xiangjun Tang, Xianzhi Li, Peter Wonka</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出结构化扩散框架，通过Query-Broadcast Attention（跨图结构一致性）与Tensor LoRA（跨图关系建模），实现文本驱动的多内在属性（反照率、辐照度等）联合生成，属于大模型新技术中的扩散模型创新，实验显示比现有方法高23%的对齐度。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02781' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation</h3>
<p><strong>Authors:</strong> Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang, Hubery Yin, Chen Li, Chenyang Si</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 分析bitwise自回归模型的多样性限制（二进制预测空间窄、logits分布过锐），提出自适应logits分布缩放与能量基生成路径搜索，在不损失质量的前提下提升样本多样性，属于大模型新技术中的自回归生成创新，实验验证多样性显著提升。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02931' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models</h3>
<p><strong>Authors:</strong> Xinyue Ai, Yutong He, Albert Gu, Ruslan Salakhutdinov, J Zico Kolter, Nicholas Matthew Boffi, Max Simchowitz</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 提出联合蒸馏框架，加速流模型的似然计算和采样，流模型是大模型新技术中的生成模型方向，属于大模型新技术的研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02636' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</h3>
<p><strong>Authors:</strong> Haoming Liu, Jinnuo Liu, Yanhao Li, Liuyang Bai, Yunkai Ji, Yuanhe Guo, Shenji Wan, Hongyi Wen</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 揭示流基扩散模型的两阶段特性，属于大模型新技术中的扩散模型研究方向，为扩散模型的优化提供理论 insights。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02826' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Improving AI Agents through Self-Play</h3>
<p><strong>Authors:</strong> Przemyslaw Chojecki</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Reason:</strong> 用自玩机制实现AI智能体自改进，提出GVU算子和稳定性条件，属于大模型新技术方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.02731' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>