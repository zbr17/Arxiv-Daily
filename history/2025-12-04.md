# ArXiv 每日推荐 - 2025-12-04

> 更新于北京时间：2025-12-04 12:31:12
> 已自动阅读了 250 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：120542

## 多模态智能体

### [Score: 10.0/10] GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning
- **Authors:** Haolong Yan, Yeqing Shen, Xin Huang, Jia Wang, Kaijun Tan, Zhixuan Liang, Hongxin Li, Zheng Ge, Osamu Yoshie, Si Li, Xiangyu Zhang, Daxin Jiang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02423](https://arxiv.org/abs/2512.02423)
- **Reason:** 针对GUI智能体的复杂屏幕导航问题，提出GUI模拟环境引擎，通过多轮强化学习提升代理的探索策略，验证了监督微调作为基础、单轮RL增强泛化、多轮RL优化探索的有效性，直接对应用户高优先级的GUI智能体研究方向。
Score: 10
Field: 多模态智能体

### [Score: 9.0/10] Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch
- **Authors:** Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02395](https://arxiv.org/abs/2512.02395)
- **Reason:** 提出30B参数的多模态智能体模型，统一多模态规划、主动图像操作和深度搜索，通过交错推理动态切换视觉操作与知识检索，仅用监督微调在MMSearch和FVQA上超越Gemini 2.5 Flash，实现复杂多步任务的工具调用，是多模态智能体的重要进展。
Score: 9
Field: 多模态智能体

### [Score: 9.0/10] PPTArena: A Benchmark for Agentic PowerPoint Editing
- **Authors:** Michael Ofengenden, Yunze Man, Ziqi Pang, Yu-Xiong Wang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.03042](https://arxiv.org/abs/2512.03042)
- **Reason:** 提出PPTArena基准用于评估PowerPoint编辑智能体，针对GUI Agent的精确编辑问题设计PPTPilot结构感知智能体，解决跨幻灯片、布局敏感等编辑任务，对多模态智能体中的GUI Agent研究有重要意义
Score: 9
Field: 多模态智能体

### [Score: 8.0/10] WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning
- **Authors:** Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02425](https://arxiv.org/abs/2512.02425)
- **Reason:** 提出多模态记忆代理，构建 episodic、semantic、visual 三类记忆，通过自适应检索代理迭代选择相关记忆源，解决长视频推理中的上下文限制和视觉细节丢失问题，在5个长视频QA基准上平均超过SOTA 8.4%，属于多模态智能体的长视频推理优化。
Score: 8
Field: 多模态智能体

### [Score: 7.0/10] SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization
- **Authors:** Zhengcheng Wang, Zichuan Lin, Yijun Yang, Haobo Fu, Deheng Ye
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02631](https://arxiv.org/abs/2512.02631)
- **Reason:** 提出视觉语言导航代理框架，通过视觉提示和步骤级政策优化提升导航性能，属于多模态智能体的重要改进。
Score: 7
Field: 多模态智能体

## 高效大模型训练与推理

### [Score: 9.0/10] Understanding and Harnessing Sparsity in Unified Multimodal Models
- **Authors:** Shwai He, Chaorui Deng, Ang Li, Shen Yan
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02351](https://arxiv.org/abs/2512.02351)
- **Reason:** 系统分析统一多模态模型的稀疏性特征（理解组件可压缩、生成组件对压缩敏感），提出MoE适应方法动态激活生成模块的专家，在不损失性能的前提下仅激活约50%参数，显著提升多模态模型的推理效率，属于高效大模型训练与推理的关键优化。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Glance: Accelerating Diffusion Models with 1 Sample
- **Authors:** Zhuobai Dong, Rui Zhao, Songjie Wu, Junchao Yi, Linjie Li, Zhengyuan Yang, Lijuan Wang, Alex Jinpeng Wang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02899](https://arxiv.org/abs/2512.02899)
- **Reason:** 提出相位感知的扩散模型加速策略，通过Slow-LoRA（早期语义阶段）与Fast-LoRA（后期冗余阶段）轻量级适配器，仅用1样本训练即可实现5倍推理加速，同时保持生成质量，解决了扩散模型“重计算、慢推理”的核心痛点，泛化性与效率优势显著。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm
- **Authors:** Zhenkai Wu, Xiaowen Ma, Zhenliang Ni, Dengming Zhang, Han Shu, Xin Jiang, Xinghao Chen
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02700](https://arxiv.org/abs/2512.02700)
- **Reason:** 提出训练-free的视觉语言模型（VLM）token剪枝算法，通过离心式剪枝范式与空间稀疏性缓冲策略，平衡冗余消除与目标区域覆盖，在88.9%剪枝率下保持性能并提升推理速度，直接解决VLM部署的计算成本问题，实验验证了在5个主流VLM上的有效性。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification
- **Authors:** Zhendong Tan, Xingjun Zhang, Chaoyi Hu, Junjie Peng, Kun Xia
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02337](https://arxiv.org/abs/2512.02337)
- **Reason:** 针对长上下文生成中验证的瓶颈，提出用部分KV状态快速验证的自投机解码方法，显著提升大模型推理速度，属于高效大模型推理加速的重要改进。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning
- **Authors:** Songqiao Su, Xiaofei Sun, Xiaoya Li, Albert Wang, Jiwei Li, Chris Shum
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02551](https://arxiv.org/abs/2512.02551)
- **Reason:** 用强化学习优化HGEMM CUDA内核，显著超越cuBLAS等主流库的性能，矩阵乘法是大模型的核心运算，属于高效大模型训练与推理的关键优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization
- **Authors:** Feiyu Wang, Xinyu Tan, Bokai Huang, Yihao Zhang, Guoan Wang, Peizhuang Cong, Tong Yang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02901](https://arxiv.org/abs/2512.02901)
- **Reason:** 提出极低比特量化框架，将实值Transformer转换为复值形式，提升量化效率，属于高效大模型训练与推理的量化优化方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Model Recovery at the Edge under Resource Constraints for Physical AI
- **Authors:** Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02283](https://arxiv.org/abs/2512.02283)
- **Reason:** 提出FPGA加速的边缘设备模型恢复框架，解决资源约束下的高效部署问题，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity
- **Authors:** Hongxiang Liu, Zhifang Deng, Tong Pu, Shengli Lu
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02403](https://arxiv.org/abs/2512.02403)
- **Reason:** 提出基于局部相似性的稀疏加速框架，实现Transformer全组件的端到端稀疏加速，有效降低计算量并提升能效，属于高效大模型硬件加速的关键研究。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs
- **Authors:** Vishnu Sarukkai, Asanshay Gupta, James Hong, Michaël Gharbi, Kayvon Fatahalian
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02543](https://arxiv.org/abs/2512.02543)
- **Reason:** 提出上下文蒸馏结合自一致性cascade，无需训练即可降低LLM代理的推理成本，属于高效大模型推理成本优化的实用方法。
Score: 7
Field: 高效大模型训练与推理

## 大模型安全与对齐

### [Score: 9.0/10] OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning
- **Authors:** Boyu Zhu, Xiaofei Wen, Wenjie Jacky Mo, Tinghui Zhu, Yanan Xie, Peng Qi, Muhao Chen
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02306](https://arxiv.org/abs/2512.02306)
- **Reason:** 针对全模态LLM设计统一安全护栏，通过多模态数据集和推理机制提升安全性，直接关联大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities
- **Authors:** Yuan Xiong, Ziqi Miao, Lijun Li, Chen Qian, Jie Li, Jing Shao
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02973](https://arxiv.org/abs/2512.02973)
- **Reason:** 研究视觉上下文对多模态大模型安全漏洞的影响，提出Contextual Image Attack方法，针对GPT-4o和Qwen2.5-VL等模型的安全对齐问题，揭示多模态安全脆弱性，对大模型安全与对齐研究有重要价值
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration
- **Authors:** Zhongyu Yang, Yingfang Yuan, Xuanming Jiang, Baoyi An, Wei Pang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02981](https://arxiv.org/abs/2512.02981)
- **Reason:** 针对多模态大模型的幻觉问题，提出InEx框架，结合内部自省推理与跨模态多智能体协作，有效缓解幻觉并提升可靠性，对大模型安全与对齐中的输出可靠性研究有价值
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models
- **Authors:** Naveen George, Naoki Murata, Yuhta Takida, Konda Reddy Mopuri, Yuki Mitsufuji
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02657](https://arxiv.org/abs/2512.02657)
- **Reason:** 提出基于生成蒸馏的持续遗忘框架，解决扩散模型的持续数据删除问题，符合GDPR等法规要求，属于大模型安全与对齐的重要研究。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] FiMMIA: scaling semantic perturbation-based membership inference across modalities
- **Authors:** Anton Emelyanov, Sergei Kudriashov, Alena Fenogenova
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02786](https://arxiv.org/abs/2512.02786)
- **Reason:** 提出多模态成员推理框架，针对多模态大模型的隐私安全问题，属于大模型安全与对齐的关键研究。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses
- **Authors:** Han Luo, Guy Laban
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02282](https://arxiv.org/abs/2512.02282)
- **Reason:** 针对敏感场景下LLM响应的心理社会安全问题，设计多智能体评估框架，直接关联大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Invasive Context Engineering to Control Large Language Models
- **Authors:** Thomas Rivasseau
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.03001](https://arxiv.org/abs/2512.03001)
- **Reason:** 提出侵入式上下文工程控制LLM，提升长上下文场景的安全性，符合大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐

## 深度学习理论

### [Score: 9.0/10] Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks
- **Authors:** Zhiyuan He
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02677](https://arxiv.org/abs/2512.02677)
- **Reason:** 研究LLM的深度泛化问题，揭示Transformer架构的局限并提出解决方法，属于深度学习理论方向。
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework
- **Authors:** Haojin Deng, Yimin Yang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02152](https://arxiv.org/abs/2512.02152)
- **Reason:** 提出上下文增强的对比损失函数，解决对比学习中增强样本的信息扭曲问题，通过双收敛目标提升学习效率和泛化性，在8个大型基准数据集上超过16种SOTA对比学习方法，对深度学习理论中的对比学习优化有重要贡献。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles
- **Authors:** Yizhou Zhang, Lun Du
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02409](https://arxiv.org/abs/2512.02409)
- **Reason:** 用谱动力学理论分析数据curation对模型训练的影响，揭示静态剪枝的局限性和动态curation的加速潜力，属于深度学习理论的重要贡献。
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] Synthetic Error Injection Fails to Elicit Self-Correction In Language Models
- **Authors:** David X. Wu, Shreyas Kapur, Anant Sahai, Stuart Russell
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02389](https://arxiv.org/abs/2512.02389)
- **Reason:** 研究合成错误注入对LLM自校正的影响，揭示模型学习机制的局限，属于深度学习理论方向。
Score: 7
Field: 深度学习理论

## 深度学习可解释性

### [Score: 9.0/10] A Framework for Causal Concept-based Model Explanations
- **Authors:** Anna Rodum Bjørru, Jacob Lysnæs-Larsen, Oskar Jørgensen, Inga Strümke, Helge Langseth
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02735](https://arxiv.org/abs/2512.02735)
- **Reason:** 提出因果概念模型解释框架，通过概念干预生成可解释结果，符合深度学习可解释性方向。
Score: 9
Field: 深度学习可解释性

### [Score: 8.0/10] See, Think, Learn: A Self-Taught Multimodal Reasoner
- **Authors:** Sourabh Sharma, Sonam Gupta, Sadbhawna
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02456](https://arxiv.org/abs/2512.02456)
- **Reason:** 提出自训练框架STL，通过结构化推理模板（先感知视觉属性再推理）和负向 rationale 增强VLMs的推理与感知能力，无需额外标注或 proprietary模型，在多领域任务中提升可解释性和推理性能，属于深度学习可解释性的重要方法。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models
- **Authors:** Joshua Wolff Anderson, Shyam Visweswaran
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02265](https://arxiv.org/abs/2512.02265)
- **Reason:** 研究公平性约束对Shapley-based特征重要性排名的影响，分析模型公平性与可解释性的关系，揭示公平性改进对解释稳定性的作用，对深度学习可解释性中的公平性与解释一致性研究有重要意义
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration
- **Authors:** Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02530](https://arxiv.org/abs/2512.02530)
- **Reason:** 提出多智能体协作的多模态可解释内容安全框架，提升安全评估的可解释性，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs
- **Authors:** Theodoros Aivalis, Iraklis A. Klampanos, Antonis Troumpoukis, Joemon M. Jose
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02713](https://arxiv.org/abs/2512.02713)
- **Reason:** 用知识图谱实现图像生成的训练数据归因，解决生成模型的透明度问题，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning
- **Authors:** Zhonghao He, Tianyi Qiu, Hirokazu Shirado, Maarten Sap
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02914](https://arxiv.org/abs/2512.02914)
- **Reason:** 提出无监督度量评估LLM推理的贝叶斯合理性，关联深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] Enforcing Orderedness to Improve Feature Consistency
- **Authors:** Sophie L. Wang, Alex Quach, Nithin Parsan, John J. Yang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02194](https://arxiv.org/abs/2512.02194)
- **Reason:** 针对稀疏自动编码器（SAE）的特征一致性问题，提出Ordered Sparse Autoencoders（OSAE），解决排列不识别问题，提升可解释性特征的稳定性，对深度学习可解释性中的SAE改进有价值
Score: 7
Field: 深度学习可解释性

## 原生多模态大模型

### [Score: 8.0/10] FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges
- **Authors:** Kevin David Hayes, Micah Goldblum, Vikash Sehwag, Gowthami Somepalli, Ashwinee Panda, Tom Goldstein
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02161](https://arxiv.org/abs/2512.02161)
- **Reason:** 提出结构化方法评估文本到图像模型的27种故障模式，用VLM作为法官识别生成图像中的错误，构建多模型生成的数据集，揭示属性保真度和对象表示的系统误差，对原生多模态大模型的可靠性和可解释性研究有重要价值。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models
- **Authors:** Le Thien Phuc Nguyen, Zhuoran Yu, Samuel Low Yu Hang, Subin An, Jeongik Lee, Yohan Ban, SeungEun Chung, Thanh-Huy Nguyen, JuWan Maeng, Soochahn Lee, Yong Jae Lee
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02231](https://arxiv.org/abs/2512.02231)
- **Reason:** 构建AV-SpeakerBench基准，聚焦多模态大模型的视听语音推理能力，设计 speaker-centered 问题和跨模态依赖的提问，评估主流MLLM的性能，为原生多模态大模型的细粒度推理评估提供基础。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding
- **Authors:** Yerim Jeon, Miso Lee, WonJun Moon, Jae-Pil Heo
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02487](https://arxiv.org/abs/2512.02487)
- **Reason:** 提出3D空间语言指令掩码策略（3D-SLIM），通过几何自适应掩码和指令感知掩码解决LLM在3D场景理解中的顺序偏见和注意力限制，无需修改架构即可提升空间推理能力，在多个3D场景语言任务中显著超越基线，属于原生多模态大模型的空间推理优化。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens
- **Authors:** Jian Yang, Dacheng Yin, Xiaoxuan He, Yong Li, Fengyun Rao, Jing Lyu, Wei Zhai, Yang Cao, Zheng-Jun Zha
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02536](https://arxiv.org/abs/2512.02536)
- **Reason:** 提出噪声查询token机制，通过端到端优化桥接预训练视觉语言模型（VLM）与扩散模型，解决固定查询token导致的任务泛化崩溃问题，实现多任务稳定持续学习，属于原生多模态大模型的融合与优化，实验验证了泛化能力提升。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective
- **Authors:** Qiyao Xue, Weichen Liu, Shiqi Wang, Haoming Wang, Yuyang Wu, Wei Gao
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02340](https://arxiv.org/abs/2512.02340)
- **Reason:** 构建认知接地的多视图空间推理基准，分析VLMs的推理路径，关联原生多模态大模型的空间理解研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding
- **Authors:** Hyeongseop Rha, Jeong Hun Yeo, Junil Won, Se Jin Park, Yong Man Ro
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02699](https://arxiv.org/abs/2512.02699)
- **Reason:** 提出模态重要性引导的多模态情感理解框架，提升多模态LLM的可靠性，关联原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation
- **Authors:** Jianzong Wu, Hao Lian, Dachao Hao, Ye Tian, Qingyu Shi, Biaolong Chen, Hao Jiang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02457](https://arxiv.org/abs/2512.02457)
- **Reason:** 研究音频-视频联合去噪训练对视频生成的影响，发现音频作为特权信号可正则化视频动态（如碰撞与声音的因果关系），在大运动和物体接触场景中提升视频质量，为原生多模态大模型的跨模态协同训练提供新视角。
Score: 7
Field: 原生多模态大模型

## 大模型新技术

### [Score: 8.0/10] Taming Camera-Controlled Video Generation with Verifiable Geometry Reward
- **Authors:** Zhaoqing Wang, Xiaobo Xia, Zhuolin Bie, Jinlin Liu, Dongdong Yu, Jia-Wang Bian, Changhu Wang
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02870](https://arxiv.org/abs/2512.02870)
- **Reason:** 引入在线强化学习（RL）后训练框架，通过可验证几何奖励（段级相机轨迹对齐）解决扩散模型的相机控制精度问题，实现更准确的几何一致性与视觉质量，属于大模型新技术中的扩散模型RL优化，实验验证优于监督微调（SFT）基线。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Guided Self-Evolving LLMs with Minimal Human Supervision
- **Authors:** Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02472](https://arxiv.org/abs/2512.02472)
- **Reason:** 提出少量监督下的LLM自进化框架，解决无引导自进化的漂移问题，属于大模型新技术方向。
Score: 8
Field: 大模型新技术

### [Score: 7.0/10] LumiX: Structured and Coherent Text-to-Intrinsic Generation
- **Authors:** Xu Han, Biao Zhang, Xiangjun Tang, Xianzhi Li, Peter Wonka
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02781](https://arxiv.org/abs/2512.02781)
- **Reason:** 提出结构化扩散框架，通过Query-Broadcast Attention（跨图结构一致性）与Tensor LoRA（跨图关系建模），实现文本驱动的多内在属性（反照率、辐照度等）联合生成，属于大模型新技术中的扩散模型创新，实验显示比现有方法高23%的对齐度。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation
- **Authors:** Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang, Hubery Yin, Chen Li, Chenyang Si
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02931](https://arxiv.org/abs/2512.02931)
- **Reason:** 分析bitwise自回归模型的多样性限制（二进制预测空间窄、logits分布过锐），提出自适应logits分布缩放与能量基生成路径搜索，在不损失质量的前提下提升样本多样性，属于大模型新技术中的自回归生成创新，实验验证多样性显著提升。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models
- **Authors:** Xinyue Ai, Yutong He, Albert Gu, Ruslan Salakhutdinov, J Zico Kolter, Nicholas Matthew Boffi, Max Simchowitz
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02636](https://arxiv.org/abs/2512.02636)
- **Reason:** 提出联合蒸馏框架，加速流模型的似然计算和采样，流模型是大模型新技术中的生成模型方向，属于大模型新技术的研究。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity
- **Authors:** Haoming Liu, Jinnuo Liu, Yanhao Li, Liuyang Bai, Yunkai Ji, Yuanhe Guo, Shenji Wan, Hongyi Wen
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02826](https://arxiv.org/abs/2512.02826)
- **Reason:** 揭示流基扩散模型的两阶段特性，属于大模型新技术中的扩散模型研究方向，为扩散模型的优化提供理论 insights。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Self-Improving AI Agents through Self-Play
- **Authors:** Przemyslaw Chojecki
- **Published:** 2025-12-03
- **Link:** [https://arxiv.org/abs/2512.02731](https://arxiv.org/abs/2512.02731)
- **Reason:** 用自玩机制实现AI智能体自改进，提出GVU算子和稳定性条件，属于大模型新技术方向。
Score: 7
Field: 大模型新技术

