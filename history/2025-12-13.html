<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-13</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>大模型安全与对齐</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >多模态智能体</a>
<a href='#' >高效大模型训练与推理</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-13</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-13 12:32:54</p>
<p>已自动阅读了 255 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：134430</p>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Multi-dimensional Preference Alignment by Conditioning Reward Itself</h3>
<p><strong>Authors:</strong> Jiho Jang, Jinyoung Kim, Kyungjune Baek, Nojun Kwak</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对扩散模型的多维度奖励冲突问题，提出MCDPO框架，通过解耦的Bradley-Terry目标和维度奖励 dropout实现平衡优化，显著提升对齐效果，属于大模型安全与对齐方向的关键创新。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10237' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</h3>
<p><strong>Authors:</strong> Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Tianyu Du, Jinbao Li, Jianhai Chen, Shouling Ji</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对后门攻击的理论基础薄弱问题，通过分析稀疏决策边界的模型操纵机制，提出Eminence框架。该框架利用特征边界模糊性，以极低中毒率（<0.1%）实现高攻击成功率（>90%），并提供了边际样本参数偏移量化等理论保证，对大模型的后门攻击与防御研究具有重要参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10402' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Multi-Objective Reward and Preference Optimization: Theory and Algorithms</h3>
<p><strong>Authors:</strong> Akhil Agnihotri</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 将约束RL统一到平均成本、episodic和偏好驱动范式，提出warmPref-PS（整合异质评分者偏好数据）、PSPL（从轨迹比较中学习）、MOPO（LLM多目标对齐）等算法。这些方法有理论保证（如遗憾界）和实证验证，对大模型的安全对齐研究具有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10601' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research</h3>
<p><strong>Authors:</strong> Dani Roytburg, Beck Miller</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 分析AI安全与伦理研究的分歧，通过 bibliometric 分析揭示 institutional 分割，并提出整合路径，对大模型安全与对齐的跨领域合作有指导意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10058' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Targeted Data Protection for Diffusion Model by Matching Training Trajectory</h3>
<p><strong>Authors:</strong> Hojun Lee, Mijin Koo, Yeji Song, Nojun Kwak</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出TAFAP方法通过匹配训练轨迹实现扩散模型的目标数据保护，解决扩散模型的隐私与未授权使用问题，属于大模型安全与对齐的核心内容。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10433' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</h3>
<p><strong>Authors:</strong> Tong Zhang, Carlos Hinojosa, Bernard Ghanem</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出CAPTAIN框架通过语义特征注入缓解扩散模型的记忆问题，解决扩散模型的隐私与版权问题，属于大模型安全与对齐中的记忆 mitigation。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10655' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</h3>
<p><strong>Authors:</strong> Skyler Wu, Aymen Echarghaoui</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对MRPO中参考模型权重设置的非统计问题，提出四种加权策略（离线验证、在线滑动窗口、Thompson采样等），实验证明其性能优于现有方法。同时发现单参考DPO的表现可能更优，对LLM的人类偏好对齐方法改进具有研究价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10040' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Robust AI Security and Alignment: A Sisyphean Endeavor?</h3>
<p><strong>Authors:</strong> Apostol Vassilev</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 从信息论角度分析AI安全与对齐的局限性，扩展哥德尔不完备定理到AI，对大模型安全与对齐的理论边界有深刻洞察。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10100' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance</h3>
<p><strong>Authors:</strong> Byeong Ho Kang, Wenli Yang, Muhammad Bilal Amin</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出可信任编排AI的十大标准及控制平面治理框架，聚焦AI系统的透明性、可验证性与人类控制，直接关联大模型安全与对齐的核心目标。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10304' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</h3>
<p><strong>Authors:</strong> Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Jahnvi Singh, Vinay Chamola, Yash Sinha, Murari Mandal, Dhruv Kumar</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 研究LLM作为科学评审者时对间接提示注入的脆弱性，量化其将拒绝转为接受的风险，揭示LLM在高风险应用中的安全隐患，属于大模型安全与对齐中的鲁棒性问题。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Challenges of Evaluating LLM Safety for User Welfare</h3>
<p><strong>Authors:</strong> Manon Kempermann, Sai Suresh Macharla Vasu, Mahalakshmi Raveenthiran, Theo Farrell, Ingmar Weber</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 探讨LLM在用户福利安全评估中的挑战，强调上下文依赖的风险，提出上下文感知的评估方法，属于大模型安全与对齐中的用户安全问题。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.10687' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding</h3>
<p><strong>Authors:</strong> Yuchen Feng (), Zhenyu Zhang (), Naibin Gu (), Yilong Chen (), Peng Fu (), Zheng Lin (), Shuohuan Wang (), Yu Sun (), Hua Wu (), Weiping Wang (), Haifeng Wang ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 模仿人类动态视觉感知过程，提出Blink动态视觉token分辨率框架，通过注意力引导的token超分和动态丢弃增强多模态大模型的视觉感知能力，有效提升多模态理解性能。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='Dynamic Visual Token Resolution for Enhanced Multimodal Understanding' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Grounding Everything in Tokens for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Xiangxuan Ren (), Zhongdao Wang (), Liping Hou (), Pin Tang (), Guoqing Wang (), Chao Ma ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出GETok方法，将2D空间关系直接嵌入多模态大模型的token中，通过网格token和偏移token实现精确的物体grounding，解决MLLMs的空间定位瓶颈，显著提升空间推理能力。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10554' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</h3>
<p><strong>Authors:</strong> Delong Chen (), Mustafa Shukor (), Theo Moutakanni (), Willy Chung (), Jade Yu (), Tejaswi Kasarla (), Allen Bolourchi (), Yann LeCun (), Pascale Fung ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出VL-JEPA视觉语言模型，基于JEPA架构预测文本嵌入，提升性能同时减少参数，支持开放词汇分类、检索等任务，属于原生多模态大模型的架构创新，对VLM的设计有重要参考。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10942' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diffusion Is Your Friend in Show, Suggest and Tell</h3>
<p><strong>Authors:</strong> Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 结合扩散模型与自回归生成范式，解决离散域生成任务中的性能瓶颈，在COCO图像caption任务上取得SOTA结果，属于原生多模态大模型方向的重要改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10038' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation</h3>
<p><strong>Authors:</strong> Yuan-Ming Li (), Qize Yang (), Nan Lei (), Shenghao Fu (), Ling-An Zeng (), Jian-Fang Hu (), Xihan Wei (), Wei-Shi Zheng ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出IRMoGen范式，将运动生成、评估和优化迭代交织，通过三阶段训练和自动数据引擎提升文本到运动的对齐性能，解决现有模型理解与生成分离的问题，推进多模态运动生成研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10730' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos</h3>
<p><strong>Authors:</strong> Yulu Gan (), Ligeng Zhu (), Dandan Shan (), Baifeng Shi (), Hongxu Yin (), Boris Ivanovic (), Song Han (), Trevor Darrell (), Jitendra Malik (), Marco Pavone (), Boyi Li ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出全自动的FoundationMotion数据 pipeline，生成大规模运动数据集，微调模型提升运动理解和空间推理性能，超过闭源模型，为多模态大模型的运动感知提供 scalable 解决方案。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10927' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mull-Tokens: Modality-Agnostic Latent Thinking</h3>
<p><strong>Authors:</strong> Arijit Ray (), Ahmed Abdelkader (), Chengzhi Mao (), Bryan A. Plummer (), Kate Saenko (), Ranjay Krishna (), Leonidas Guibas (), Wen-Sheng Chu ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出模态无关的Mull-Tokens，让模型在多模态任务中自由使用 latent 思考，提升空间推理等复杂任务的性能，解决现有模型多模态推理的脆性问题，对多模态大模型的推理机制有创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10941' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules</h3>
<p><strong>Authors:</strong> Yanbei Jiang, Xueqi Ma, Shu Liu, Sarah Monazam Erfani, Tongliang Liu, James Bailey, Jey Han Lau, Krista A. Ehinger</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 系统分析VLM中注意力头的功能角色，揭示推理模块的存在及其层级组织，对原生多模态大模型的内部机制理解和优化有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10300' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Woojun Jung, Jaehoon Go, Mingyu Jeon, Sunjae Yoon, Junyeong Kim</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对MLLM的视觉上下文盲问题，提出分层上下文组合策略，通过注意力熵动态调整裁剪尺寸，有效融合局部细节与全局上下文，提升细粒度视觉感知能力，属于原生多模态大模型方向的针对性优化。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.10362' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration</h3>
<p><strong>Authors:</strong> Sicheng Mo (), Thao Nguyen (), Richard Zhang (), Nick Kolkin (), Siddharth Srinivasan Iyer (), Eli Shechtman (), Krishna Kumar Singh (), Yong Jae Lee (), Bolei Zhou (), Yuheng Li ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出Group Diffusion，让样本在生成时共享注意力，实现跨样本协作，提升扩散模型的生成质量，发现跨样本推理的新机制，对扩散模型的推理策略有重要创新。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10954' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering</h3>
<p><strong>Authors:</strong> Ziseok Lee, Minyeong Hwang, Sanghyun Jo, Wooyeol Lee, Jihyung Ko, Young Bin Park, Jae-Mun Choi, Eunho Yang, Kyungsu Kim</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对扩散模型推理时的生成路径崩溃问题，提出路径存在性准则及ACE方法，解决扩散模型可控生成的关键问题，属于大模型新技术中的扩散模型方向。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10339' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> What matters for Representation Alignment: Global Information or Spatial Structure?</h3>
<p><strong>Authors:</strong> Jaskirat Singh (), Xingjian Leng (), Zongze Wu (), Liang Zheng (), Richard Zhang (), Eli Shechtman (), Saining Xie ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 系统研究生成模型中表示对齐的关键因素，发现空间结构而非全局语义主导生成性能，提出iREPA方法提升表示对齐的收敛速度和效果，对生成模型的表示学习机制有深入洞察。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10794' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</h3>
<p><strong>Authors:</strong> Yiwen Tang (), Zoey Guo (), Kaixin Zhu (), Ray Zhang (), Qizhi Chen (), Dongzhi Jiang (), Junli Liu (), Bohan Zeng (), Haoming Song (), Delin Qu (), Tianyi Bai (), Dan Xu (), Wentao Zhang (), Bin Zhao ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 首次系统研究RL在文本到3D生成中的应用，评估奖励设计、算法和基准，提出Hi-GRPO和MME-3DR基准，推动RL在3D生成中的发展，属于大模型新技术的探索。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10949' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</h3>
<p><strong>Authors:</strong> Qitao Zhao (), Hao Tan (), Qianqian Wang (), Sai Bi (), Kai Zhang (), Kalyan Sunkavalli (), Shubham Tulsiani (), Hanwen Jiang ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出E-RayZer自监督3D重建模型，通过显式几何重建学习3D-aware表示，提升3D下游任务性能，建立新的3D视觉预训练范式，属于大模型新技术的创新。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10950' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scaling Behavior of Discrete Diffusion Language Models</h3>
<p><strong>Authors:</strong> Dimitri von Rütte, Janis Fluri, Omead Pooladzandi, Bernhard Schölkopf, Thomas Hofmann, Antonio Orvieto</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 系统研究离散扩散语言模型的缩放规律，对比不同噪声类型的性能，为扩散LLM的高效训练提供了关键参考，对大模型新技术中的扩散模型优化有重要价值。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10858' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Guided Transfer Learning for Discrete Diffusion Models</h3>
<p><strong>Authors:</strong> Julian Kleutgens, Claudio Battiloro, Lingkai Kong, Benjamin Grewe, Francesca Dominici, Mauricio Tec</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出离散扩散模型的引导迁移学习方法，无需修改预训练模型即可适配目标分布，并通过高效采样策略降低计算成本，对大模型新技术中的扩散模型迁移有实用价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.10877' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning</h3>
<p><strong>Authors:</strong> Yizhou Zhang</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 从算子理论出发推导深度学习训练动态，得到光谱传输-耗散PDE，统一解释了神经缩放律、双下降、NTK训练与特征学习等关键现象。该框架连接了算子几何、优化动态与缩放行为，为深度学习理论的基础研究提供了统一视角，具有重要学术价值。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10427' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Neural Collapse in Test-Time Adaptation</h3>
<p><strong>Authors:</strong> Xiao Chen, Zhongjing Du, Jiazhen Huang, Xu Jiang, Li Lu, Jingyan Jiang, Zhi Wang</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 扩展神经崩溃现象到样本级，提出NC3+理论解释测试时自适应的性能下降原因，进而提出NCTTA方法提升模型鲁棒性，属于深度学习理论方向的重要理论贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10421' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation</h3>
<p><strong>Authors:</strong> Sarwan Ali</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对Nesterov加速梯度法在非凸或病态问题上的发散缺陷，提出HB-SGE方法，结合重球动量与预测梯度外推，提供鲁棒的一阶优化方案。该方法有强凸函数的收敛性证明，且在病态二次函数和Rosenbrock函数上的实证表现优于SGD和NAG，对深度学习优化器的鲁棒性研究具有重要参考价值。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10033' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Token Sample Complexity of Attention</h3>
<p><strong>Authors:</strong> Léa Bohbot, Cyril Letrouit, Gabriel Peyrė, François-Xavier Vialard</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 分析注意力机制在长序列下的token样本复杂度，建立收敛边界，深化了对注意力行为的理论理解，对深度学习理论中注意力架构的 scalability研究有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10656' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning by Analogy: A Causal Framework for Composition Generalization</h3>
<p><strong>Authors:</strong> Lingjing Kong, Shaoan Xie, Yang Jiao, Yetian Chen, Yanhui Guo, Simone Shao, Yan Gao, Guangyi Chen, Kun Zhang</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出基于因果模块化的组合泛化框架，通过类比学习解释模型的组合能力，对深度学习理论中的泛化问题提供了新的因果视角。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10669' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stronger Normalization-Free Transformers</h3>
<p><strong>Authors:</strong> Mingzhi Chen, Taiming Lu, Jiachen Zhu, Mingjie Sun, Zhuang Liu</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出Derf函数替代归一化层，在多个领域（视觉、语音、DNA）超越现有归一化-free方法，对深度学习理论中的网络架构（归一化-free）有重要创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10938' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Optimal transport unlocks end-to-end learning for single-molecule localization</h3>
<p><strong>Authors:</strong> Romain Seailles (DI-ENS), Jean-Baptiste Masson (IP, CNRS, UPCit\'e), Jean Ponce (DI-ENS, CDS), Julien Mairal (LJK)</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 将单分子定位的训练目标重新表述为最优传输的集合匹配问题，提出可微分的最优传输损失，消除NMS的需求，实现端到端学习，提升高密发射下的性能，属于深度学习理论中最优传输与端到端学习的结合。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10683' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Ensemble Post Learning for Noisy Domain Generalization</h3>
<p><strong>Authors:</strong> Wang Lu (), Jindong Wang ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出SEPL方法，通过特征探测训练和预测集成解决噪声域泛化中的伪特征放大问题，利用模型内部的 latent 特征多样化提升鲁棒性，属于深度学习理论中域泛化与噪声鲁棒性的结合。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10818' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning</h3>
<p><strong>Authors:</strong> Yi Huang, Qingyun Sun, Yisen Gao, Haonan Yuan, Xingcheng Fu, Jianxin Li</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对信息瓶颈（IB）对标签噪声的脆弱性，提出LaT-IB方法，通过“最小充分清洁”准则和噪声感知 latent 解纠缠，分离干净标签与噪声信息。该方法有理论互信息界证明，实验验证了在标签噪声下的鲁棒性，对信息瓶颈的鲁棒性扩展研究具有贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10573' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation</h3>
<p><strong>Authors:</strong> Lin Du, Lu Bai, Jincheng Li, Lixin Cui, Hangyuan Du, Lichi Zhang, Yuting Chen, Zhao Li</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出基于线图聚合的GNN架构LGAN，提升了GNN的表达能力（超过2-WL测试）并降低计算复杂度，对深度学习理论中的图神经网络架构设计有贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Neuronal Attention Circuit (NAC) for Representation Learning</h3>
<p><strong>Authors:</strong> Waleed Razzaq, Izis Kankaraway, Yun-Bo Zhao</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出生物启发的连续时间注意力机制NAC，结合线虫神经回路的稀疏性，实现高效自适应动态，对深度学习理论中的注意力架构有创新贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search</h3>
<p><strong>Authors:</strong> Oscar Robben, Saeed Khalilian, Nirvana Meratnia</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出硬件感知的神经架构搜索AEBNAS，优化早退网络的出口分支，属于深度学习理论中的network architecture与hardware-aware设计。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values</h3>
<p><strong>Authors:</strong> Brian P. Powell, Jordan A. Caraballo-Vega, Mark L. Carroll, Thomas Maxwell, Andrew Ptak, Greg Olmschenk, Jorge Martinez-Palomera</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 研究二进制编码对神经网络外推周期性函数的作用，揭示了内部bit-phase表示的机制，对深度学习理论中的模型外推能力研究有贡献。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.10817' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality</h3>
<p><strong>Authors:</strong> Lingjing Kong, Shaoan Xie, Guangyi Chen, Yuewen Sun, Xiangchen Song, Eric P. Xing, Kun Zhang</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 利用因果最小性原则赋予生成模型可解释的潜表示和可控性，理论上保证了表示的因果一致性，对深度学习可解释性中的生成模型解释有重要理论和实践价值。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10720' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection</h3>
<p><strong>Authors:</strong> Jian-Yu Jiang-Lin (), Kang-Yang Huang (), Ling Zou (), Ling Lo (), Sheng-Ping Yang (), Yu-Wen Tseng (), Kun-Hsiang Lin (), Chia-Ling Chen (), Yu-Ting Ta (), Yan-Tsung Wang (), Po-Ching Chen (), Hongxia Xie (), Hong-Han Shuai (), Wen-Huang Cheng ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出TriDF基准，从感知、检测和幻觉三个维度评估DeepFake检测模型的可解释性，揭示模型解释的可靠性与检测性能的关联，为可解释DeepFake检测提供统一框架。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10652' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Interpretable and Steerable Concept Bottleneck Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Akshay Kulkarni, Tsui-Wei Weng, Vivek Narayanaswamy, Shusen Liu, Wesam A. Sakla, Kowshik Thopalli</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出概念瓶颈稀疏自动编码器（CB-SAE），通过剪枝低效用神经元和引入概念瓶颈，显著提升了SAE的可解释性和可操控性，对深度学习可解释性中的稀疏自动编码器应用有重要改进。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10805' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DCFO Additional Material</h3>
<p><strong>Authors:</strong> Tommaso Amico, Pernille Matthews, Lena Krieger, Arthur Zimek, Ira Assent</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对LOF异常检测算法提出反事实解释方法DCFO，解决了现有方法缺乏可解释性的问题，对深度学习可解释性中的异常检测解释有应用价值。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10659' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Linear socio-demographic representations emerge in Large Language Models from indirect cues</h3>
<p><strong>Authors:</strong> Paul Bouchaud, Pedro Ramaciotti</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 研究LLM中社会人口统计特征的线性表示，揭示隐式偏差的来源，对深度学习可解释性中的模型偏差分析有重要价值。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10065' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</h3>
<p><strong>Authors:</strong> Nick Jiang, Xiaoqing Sun, Lisa Dunlap, Lewis Smith, Neel Nanda</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 利用稀疏自动编码器生成可解释嵌入，用于分析非结构化数据（如模型响应、文档），对深度学习可解释性中的嵌入解释提供了实用工具。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10092' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes</h3>
<p><strong>Authors:</strong> Xuan Zhao, Zhuo Cao, Arya Bangun, Hanno Scharr, Ira Assent</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 结合反事实样本和Wasserstein原型改进模型重建，缓解了决策边界偏移问题，提升了代理模型与目标模型的一致性，对深度学习可解释性中的模型重建有贡献。
Score: 6
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10878' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</h3>
<p><strong>Authors:</strong> Kristina Korotkova, Aleksandr Katrutsa</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 评估Frank-Wolfe方法在白盒对抗攻击中的性能，对比传统方法的优势，对白盒可解释性中的对抗攻击构建有参考价值。
Score: 6
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10936' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Reverse Thinking Enhances Missing Information Detection in Large Language Models</h3>
<p><strong>Authors:</strong> Yuxin Liu, Chaojie Gu, Yihang Zhang, Bin Qian, Shibo He</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出反向思维框架，将缺失信息检测转化为反向推理问题，显著提升LLM的推理完整性，对深度学习可解释性中的推理 robustness有改进价值。
Score: 6
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.10273' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management</h3>
<p><strong>Authors:</strong> Shizuo Tian, Hao Wen, Yuxuan Chen, Jiacheng Liu, Shanhui Zhao, Guohong Liu, Ju Ren, Yunxin Liu, Yuanchun Li</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出程序引导的上下文管理框架AgentProg，解决长 horizons GUI Agents的上下文稀释问题，直接对应多模态智能体中的GUI Agent子方向，实验验证在AndroidWorld上的SOTA性能。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.10371' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</h3>
<p><strong>Authors:</strong> Yongqiang Yu, Xuhui Li, Hazza Mahmood, Jinxing Zhou, Haodong Hong, Longtao Jiang, Zhiqiang Xu, Qi Wu, Xiaojun Chang</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对视觉-语言导航（VLN）任务提出用户反馈驱动的持续适应框架，解决真实场景中VLN的适应性问题，属于多模态智能体的导航子方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.10322' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution</h3>
<p><strong>Authors:</strong> Zouying Cao, Jiaji Deng, Li Yu, Weikang Zhou, Zhaoyang Liu, Bolin Ding, Hai Zhao</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出动态过程记忆框架ReMe，解决LLM Agents的经验驱动进化问题，属于多模态智能体中的Agent记忆与进化方向，实验验证在BFCL-V3和AppWorld上的SOTA性能。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.10696' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</h3>
<p><strong>Authors:</strong> Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 构建多模态机器人导航的高真实感仿真平台，评估VLM在城市环境中的导航和协作能力，为多模态智能体的训练和评估提供了重要工具。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.10046' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs</h3>
<p><strong>Authors:</strong> Chao Gong, Depeng Wang, Zhipeng Wei, Ya Guo, Huijia Zhu, Jingjing Chen</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出跨模态语义筛和同步增强RoPE，自适应减少音视频token数量，在保持性能的同时实现2-3倍加速和内存 reduction，属于高效大模型训练与推理方向的实用改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10324' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA</h3>
<p><strong>Authors:</strong> Pasquale De Marinis (), Gennaro Vessio (), Giovanna Castellano ()</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出TaP方法，利用LoRA实现少样本语义分割的高效编码器自适应，解决编码器对新类泛化的瓶颈，在保持计算效率的同时提升分割性能，属于参数高效微调的重要应用。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10521' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization</h3>
<p><strong>Authors:</strong> Kesheng Chen, Wenjian Luo, Zhenqian Zhu, Yamin Hu, Yiya Xi</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对现有LLM能力-效率权衡方法的粗细粒度矛盾，提出BAMBO框架，通过贝叶斯自适应多目标块优化构建帕累托集。该方法解决了传统模型级或层级方法的局限性，实验证明能发现更优的帕累托前沿，对高效LLM的模型选择与优化具有实践意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.09972' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Qingsen Ma, Dianyun Wang, Jiaming Lyu, Yaoye Wang, Lechen Ning, Sujie Zhu, Zhenbo Xu, Liuyu Xiang, Huining Li, Huijia Wu, Zhaofeng He</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出STA-Attention框架，通过Top-K稀疏自动编码器分解LLM的KV缓存为可解释的语义原子，揭示了Key-Value的不对称性（Key为稀疏路由器、Value为稠密内容载体），并提出双预算策略优化缓存。实验证明该方法在保持模型性能的同时提升了缓存效率，对解决LLM推理的内存瓶颈问题具有实际价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10547' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</h3>
<p><strong>Authors:</strong> Max Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出高效的剪枝掩码优化算法SparseSwaps，通过1-swap策略提升LLM剪枝后的性能，且计算 tractable，对高效大模型训练与推理中的模型剪枝技术有重要改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10922' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification</h3>
<p><strong>Authors:</strong> Hendrik Borras, Yong Wu, Bernhard Klein, Holger Fröning</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 针对贝叶斯神经网络提出多级别量化框架，在保留不确定性量化的同时减少计算和内存开销，对高效大模型训练与推理中的模型压缩技术有实际价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10602' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Asynchronous Reasoning: Training-Free Interactive Thinking LLMs</h3>
<p><strong>Authors:</strong> George Yakushev, Nataliia Babina, Masoud Vahid Dastgerdi, Vyacheslav Zhdanovskiy, Alina Shutova, Denis Kuznedelev</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出无需训练的异步推理方法，利用旋转位置编码实现同时思考、倾听和生成，显著降低实时交互延迟，对高效大模型训练与推理中的推理效率优化有贡献。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10931' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning</h3>
<p><strong>Authors:</strong> Logan Robbins</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出模型内部并行解码的Transformer架构，通过Note Conditioning实现推测一致性，提升生成效率，对高效大模型训练与推理中的解码优化有创新。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10054' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Multi-Granular Node Pruning for Circuit Discovery</h3>
<p><strong>Authors:</strong> Muhammad Umair Haider, Hammad Rizwan, Hassan Sajjad, A. B. Siddique</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Reason:</strong> 提出多粒度节点剪枝框架用于LLM的电路发现，属于高效大模型训练与推理中的high compression方向，解决现有剪枝方法的粒度与效率问题。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.10903' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>