<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-17</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>原生多模态大模型</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型新技术</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习理论</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-17</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-17 12:52:35</p>
<p>已自动阅读了 485 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：263596</p>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</h3>
<p><strong>Authors:</strong> Chenrui Fan, Yijun Liang, Shweta Bhardwaj, Kwesi Cobbina, Ming Li, Tianyi Zhou</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 构建V-REX基准，将多步探索性视觉推理转化为“问题链（CoQ）”，评估VLMs的规划与执行能力，属于原生多模态大模型中的推理能力评估，对提升模型处理复杂视觉任务的能力有重要价值。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.11995' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</h3>
<p><strong>Authors:</strong> Siyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, Jing Fang, Zetao Fang, Chengjian Feng, Han Feng, Mingyuan Gao, Yu Gao, Qiushan Guo, Boyang Hao, Qingkai Hao, Bibo He, Qian He, Tuyen Hoang, Ruoqing Hu, Xi Hu, Weilin Huang, Zhaoyang Huang, Zhongyi Huang, Siqi Jiang, Wei Jiang, Yunpu Jiang, Zhuo Jiang, Ashley Kim, Jianan Kong, Zhichao Lai, Shanshan Lao, Ai Li, Feiya Li, Gen Li, Huixia Li, JiaShi Li, Liang Li, Ming Li, Tao Li, Xian Li, Xiaojie Li, Xiaoyang Li, Xingxing Li, Yameng Li, Yifu Li, Yiying Li, Chao Liang, Ying Liang, Zhiqiang Liang, Wang Liao, Yalin Liao, Heng Lin, Kengyu Lin, Shanchuan Lin, Xi Lin, Zhijie Lin, Feng Ling, Fangfang Liu, Gaohong Liu, Jiawei Liu, Jie Liu, Shouda Liu, Shu Liu, Sichao Liu, Songwei Liu, Xin Liu, Xue Liu, Yibo Liu, Zikun Liu, Zuxi Liu, Junlin Lyu, Lecheng Lyu, Qian Lyu, Han Mu, Xiaonan Nie, Jingzhe Ning, Xitong Pan, Yanghua Peng, Lianke Qin, Xueqiong Qu, Yuxi Ren, Yuchen Shen, Guang Shi, Lei Shi, Yan Song, Yinglong Song, Fan Sun, Li Sun, Renfei Sun, Zeyu Sun, Wenjing Tang, Zirui Tao, Feng Wang, Furui Wang, Jinran Wang, Junkai Wang, Ke Wang, Kexin Wang, Qingyi Wang, Rui Wang, Sen Wang, Shuai Wang, Tingru Wang, Weichen Wang, Xin Wang, Yanhui Wang, Yue Wang, Yuping Wang, Yuxuan Wang, Ziyu Wang, Guoqiang Wei, Wanru Wei, Di Wu, Guohong Wu, Hanjie Wu, Jian Wu, Jie Wu, Ruolan Wu, Xinglong Wu, Yonghui Wu, Ruiqi Xia, Liang Xiang, Fei Xiao, XueFeng Xiao, Pan Xie, Shuangyi Xie, Shuang Xu, Jinlan Xue, Bangbang Yang, Ceyuan Yang, Jiaqi Yang, Runkai Yang, Tao Yang, Yang Yang, Yihang Yang, ZhiXian Yang, Ziyan Yang, Yifan Yao, Zilyu Ye, Bowen Yu, Chujie Yuan, Linxiao Yuan, Sichun Zeng, Weihong Zeng, Xuejiao Zeng, Yan Zeng, Chuntao Zhang, Heng Zhang, Jingjie Zhang, Kuo Zhang, Liang Zhang, Liying Zhang, Manlin Zhang, Ting Zhang, Weida Zhang, Xiaohe Zhang, Xinyan Zhang, Yan Zhang, Yuan Zhang, Zixiang Zhang, Fengxuan Zhao, Huating Zhao, Yang Zhao, Hao Zheng, Jianbin Zheng, Xiaozheng Zheng, Yangyang Zheng, Yijie Zheng, Jiexin Zhou, Kuan Zhu, Shenhan Zhu, Wenjia Zhu, Benhui Zou, Feilong Zuo</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出原生音视频联合生成基础模型，通过双分支Diffusion Transformer实现音视频同步生成，属于原生多模态大模型的native multi-modal设计。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13507' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Towards Scalable Pre-training of Visual Tokenizers for Generation</h3>
<p><strong>Authors:</strong> Jingfeng Yao (), Yuda Song (), Yucong Zhou (), Xinggang Wang ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对视觉tokenizer预训练的“缩放问题”（更好的像素重建不带来更好的生成），提出VTP框架联合优化图像-文本对比、自监督和重建损失，显著提升生成性能和收敛速度，属于原生多模态大模型中的tokenizer设计与生成优化方向，解决了关键瓶颈问题。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13687' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities</h3>
<p><strong>Authors:</strong> Santosh Patapati</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出CLARGA通用多模态融合架构，通过图注意力网络学习任意模态的表示，支持亚二次复杂度的高效融合，属于原生多模态大模型中的模态融合技术，对多模态任务的通用表示学习有意义。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.11901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer</h3>
<p><strong>Authors:</strong> Guanfang Dong, Luke Schultz, Negar Hassanpour, Chao Gao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出RePack框架，将视觉基础模型（VFM）的高维特征打包为紧凑表示，注入扩散Transformer以提升性能与收敛速度（DiT-XL/2在64 epoch达到FID 3.66，比SOTA快35%），属于原生多模态大模型中的特征融合优化，对利用基础模型增强扩散生成有意义。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12083' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation</h3>
<p><strong>Authors:</strong> Xuancheng Xu, Yaning Li, Sisi You, Bing-Kun Bao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出SMRABooth框架，通过自监督编码器与光流编码器对齐主题与运动表示，结合LoRA微调生成定制视频，解决现有方法难以同时保持主题一致性与运动连贯性的问题，属于原生多模态大模型中的可控视频生成，对个性化内容创作有价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12193' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KlingAvatar 2.0 Technical Report</h3>
<p><strong>Authors:</strong> Kling Team, Jialu Chen, Yikang Ding, Zhixue Fang, Kun Gai, Yuan Gao, Kang He, Jingyun Hua, Boyuan Jiang, Mingming Lao, Xiaohan Li, Hui Liu, Jiwen Liu, Xiaoqiang Liu, Yuan Liu, Shun Lu, Yongsen Mao, Yingchao Shao, Huafeng Shi, Xiaoyu Shi, Peiqin Sun, Songlin Tang, Pengfei Wan, Chao Wang, Xuebo Wang, Haoxian Zhang, Yuanxing Zhang, Yan Zhou</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出时空级联框架生成高分辨率、长时一致的Avatar视频，改进现有方法的时间漂移问题，属于原生多模态大模型的视频生成。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13313' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RecTok: Reconstruction Distillation along Rectified Flow</h3>
<p><strong>Authors:</strong> Qingyu Shi, Size Wu, Jinbin Bai, Kaidong Yu, Yujing Wang, Yunhai Tong, Xiangtai Li, Xuelong Li</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 通过整流流重构蒸馏改进视觉tokenizer，提升latent空间语义表达与生成质量，属于原生多模态大模型的tokenizer设计。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13421' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence</h3>
<p><strong>Authors:</strong> Ruiyan Wang, Teng Hu, Kaihui Huang, Zihan Su, Ran Yi, Lizhuang Ma</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出通用姿态引导视频生成框架，支持人类与非人类姿态输入，提升视频生成的姿态一致性与泛化性，属于原生多模态大模型的视频生成。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13465' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation</h3>
<p><strong>Authors:</strong> Jiangning Zhang, Junwei Zhu, Zhenye Gan, Donghao Luo, Chuming Lin, Feifan Xu, Xu Peng, Jianlong Hu, Yuansen Liu, Yijia Hong, Weijian Cao, Han Feng, Xu Chen, Chencan Fu, Keke He, Xiaobin Hu, Chengjie Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出多模态驱动的高保真数字人长期动画框架，支持文本、音频与单帧图像输入，属于原生多模态大模型的数字人生成。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13495' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MMhops-R1: Multimodal Multi-hop Reasoning</h3>
<p><strong>Authors:</strong> Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Bing Li, Chunfeng Yuan, Guangting Wang, Fengyun Rao, Ying Shan, Weiming Hu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出多模态检索增强生成框架用于多跳推理，通过强化学习优化推理路径规划，属于原生多模态大模型的多模态推理。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13573' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</h3>
<p><strong>Authors:</strong> Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, Kai Han</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出统一框架实现视频音频联合生成，通过
[PAPER_START]
Title: Recurrent Video Masked Autoencoders
Authors: Daniel Zoran (), Nikhil Parthasarathy (), Yi Yang (), Drew A Hudson (), Joao Carreira (), Andrew Zisserman ()
Published: 2025-12-16
Link: https://arxiv.org/abs/2512.13684
Reason: 提出递归视频掩码自编码器（RVM），通过Transformer递归网络聚合时序特征，高效捕获视频时空结构，在小模型 regime 无需知识蒸馏即可取得 competitive 性能，与原生多模态大模型中的视频理解方向高度相关。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13677' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning</h3>
<p><strong>Authors:</strong> Yongcan Yu, Lingxiao He, Shuo Lu, Lijun Sheng, Yinuo Xu, Yanbo Wang, Kuangpu Guo, Jianjie Cheng, Meng Wang, Qianlong Xie, Xingxing Wang, Dapeng Hu, Jian Liang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 重新评估SFT在VLM推理中的作用，发现其在弱模型、数据效率与跨模态迁移上的优势，为原生多模态大模型的推理优化提供关键实证依据。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12690' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Directional Textual Inversion for Personalized Text-to-Image Generation</h3>
<p><strong>Authors:</strong> Kunhee Kim, NaHyeon Park, Kibeom Hong, Hyunjung Shim</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Addresses embedding norm inflation in textual inversion for text-to-image generation, improving personalization and prompt fidelity, directly relevant to native multi-modal large models.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13672' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs</h3>
<p><strong>Authors:</strong> Mingrui Ye, Chanjin Zheng, Zengyi Yu, Chenyu Xiang, Zhixue Zhao, Zheng Yuan, Helen Yannakoudakis</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Introduces a benchmark for evaluating children's art using attribute-aware multi-modal LLMs, advancing multi-modal large model capabilities in artistic evaluation.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12503' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> CineLOG: A Training Free Approach for Cinematic Long Video Generation</h3>
<p><strong>Authors:</strong> Zahra Dehghanian, Morteza Abolghasemi, Hamid Beigy, Hamid R. Rabiee</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出CineLOG数据集与管道，将文本到长视频生成任务分解为四个阶段，利用电影 taxonomy 指导相机与类型控制，属于原生多模态大模型中的长视频生成，对复杂电影级内容创作有帮助。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12209' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Animus3D: Text-driven 3D Animation via Motion Score Distillation</h3>
<p><strong>Authors:</strong> Qi Sun, Can Wang, Jiaxiang Shang, Wensen Feng, Jing Liao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出文本驱动3D动画框架，用Motion Score Distillation解决现有方法运动幅度小或抖动的问题，实验生成的动画运动更显著且保持视觉完整性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12534' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space</h3>
<p><strong>Authors:</strong> Chengzhi Liu, Yuzhe Yang, Yue Fan, Qingyue Wei, Sheng Liu, Xin Eric Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出DMLR框架，在潜在空间动态交织多模态推理，通过置信度引导的潜政策梯度优化和动态视觉注入策略提升MLLM的推理效率和效果，实验在7个基准上表现优于现有方法。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.12623' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Few-Step Distillation for Text-to-Image Generation: A Practical Guide</h3>
<p><strong>Authors:</strong> Yifan Pu, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Fan Wang, Bohan Zhuang, Gao Huang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 系统研究扩散蒸馏技术在文本到图像生成中的应用，解决了从离散类标签到自由文本prompt的适配问题，给出实用训练指南并开源模型，有效提升文本到图像生成的效率与落地性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13006' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</h3>
<p><strong>Authors:</strong> Jiaqi Wang, Weijia Wu, Yi Zhan, Rui Zhao, Ming Hu, James Cheng, Wei Liu, Philip Torr, Kevin Qinghong Lin</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 构建ASMR视频基准评估AI生成视频的真实度与音视频一致性，涉及原生多模态大模型的生成质量评估，揭示现有VLMs的感知局限性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13281' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</h3>
<p><strong>Authors:</strong> Zhihang Liu, Xiaoyi Bao, Pandeng Li, Junjie Zhou, Zhaohe Liao, Yefei He, Kaixun Jiang, Chen-Wei Xie, Yun Zheng, Hongtao Xie</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 结合MLLM与扩散模型实现表格创造性可视化生成，涉及多模态推理与生成，属于原生多模态大模型应用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13303' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</h3>
<p><strong>Authors:</strong> Anran Qi, Changjian Li, Adrien Bousseau, Niloy J. Mitra</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 结合Proxy Dynamic Graph与扩散模型实现图像到视频的可控编辑，支持遮挡区域用户定制，属于原生多模态大模型的图像/视频生成。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13392' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding</h3>
<p><strong>Authors:</strong> Piyush Bagad, Andrew Zisserman</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 通过MLLM适应实现时间感知的视频文本嵌入，提升视频检索的时间一致性，属于原生多模态大模型的视频文本检索。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13511' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</h3>
<p><strong>Authors:</strong> Shweta Mahajan, Shreya Kadambi, Hoang Le, Munawar Hayat, Fatih Porikli</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 评估VLM对物理动作的生成与反转能力，构建可逆动作数据集，属于原生多模态大模型的物理推理。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13609' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</h3>
<p><strong>Authors:</strong> Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出RoboTracer这一3D感知的视觉-语言模型（VLM），通过通用空间编码器与回归监督解码器实现3D空间指称与测量，结合强化微调提升多步 metric-grounded推理能力，构建了大规模TraceSpatial数据集及基准，针对多模态大模型的空间推理能力增强有创新，属于原生多模态大模型研究范畴。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.13660' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering</h3>
<p><strong>Authors:</strong> Zihu Wang, Boxun Xu, Yuxuan Xia, Peng Li</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对VLMs的幻觉问题，提出VEGAS方法，整合视觉编码器的注意力图引导语言模型中间层，自适应修正未聚焦关键物体的token，显著降低幻觉率，属于大模型安全与对齐中的对齐优化，对提升模型输出的视觉忠实性有关键作用。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12089' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space</h3>
<p><strong>Authors:</strong> Samarth Raina (), Saksham Aggarwal (), Aman Chadha (), Vinija Jain (), Amitava Das ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 分析DPO的内部机制，发现其并非修改模型信念，而是作为低秩steering机制扰动激活空间，通过提取引导向量验证了这一结论，对大模型对齐的机制理解有重要贡献，属于大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.11838' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</h3>
<p><strong>Authors:</strong> Erik Larsen ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 研究LLM安全拒绝行为的不稳定性，发现随机种子和温度显著影响拒绝决策，指出单样本安全评估的不足，提出多样本评估建议，属于大模型安全与对齐中的安全评估方向，实验扎实且有实际指导意义。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12066' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models</h3>
<p><strong>Authors:</strong> Futa Waseda, Shojiro Yamabe, Daiki Shiono, Kento Sasaki, Tsubasa Takahashi</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLMs）易受排版攻击的问题，构建RIO-Bench基准，评估模型在“读取或忽略文本”任务中的鲁棒性，属于大模型安全与对齐中的对抗鲁棒性研究，对提升模型对文本攻击的抵抗力有帮助。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.11899' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SPDMark: Selective Parameter Displacement for Robust Video Watermarking</h3>
<p><strong>Authors:</strong> Samar Fares, Nurbek Tastan, Karthik Nandakumar</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出SPDMark框架，通过选择性参数位移在视频扩散模型中嵌入鲁棒水印，支持帧级篡改检测与顺序恢复，属于大模型安全与对齐中的版权保护与溯源，对生成式视频的安全应用有重要意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12090' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</h3>
<p><strong>Authors:</strong> Bo Liu, Qiao Qin, Qinghui He</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 利用因果推断分离生成图像的因果与非因果特征，提升生成图像检测的泛化性，属于大模型安全与对齐中的生成内容检测。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13285' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MineTheGap: Automatic Mining of Biases in Text-to-Image Models</h3>
<p><strong>Authors:</strong> Noa Cohen, Nurit Spingarn-Eliezer, Inbar Huberman-Spiegelglas, Tomer Michaeli</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 利用遗传算法自动挖掘文本到图像模型的偏见，属于大模型安全与对齐中的公平性问题，揭示模型在模糊指令下的偏见行为。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13427' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors</h3>
<p><strong>Authors:</strong> Max McGuinness (), Alex Serrano (), Luke Bailey (), Scott Emmons ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 展示LLM可以通过微调学习逃避未见过的激活监测，验证了这种逃避行为的普遍性和选择性，对大模型安全中的监测鲁棒性研究有重要启示，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.11949' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Extract Context for Context-Aware LLM Inference</h3>
<p><strong>Authors:</strong> Minseon Kim (), Lucas Caccia (), Zhengyan Shi (), Matheus Pereira (), Marc-Alexandre C\^ot\'e (), Xingdi Yuan (), Alessandro Sordoni ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出上下文提取框架，通过强化学习生成上下文信号引导LLM推理，提升安全响应的准确性（减少有害输出和不必要拒绝），属于大模型安全与对齐中的安全推理方向，实验验证有效。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.11986' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment</h3>
<p><strong>Authors:</strong> Yawen Shao, Jie Xiao, Kai Zhu, Yu Liu, Wei Zhai, Yang Cao, Zheng-Jun Zha</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出VGPO框架解决flow matching图像生成中GRPO的时间信用分配与优化停滞问题，提升图像质量与任务准确性，属于大模型对齐的重要改进。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12387' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs</h3>
<p><strong>Authors:</strong> Si Qi Goh, Yongsen Zheng, Ziyao Liu, Sami Hormi, Kwok-Yan Lam</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出FROC框架解决LLM机器遗忘中的风险控制问题，平衡遗忘充分性与效用损失，实验验证风险控制的有效性，对大模型安全与对齐有价值
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13337' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Async Control: Stress-testing Asynchronous Control Measures for LLM Agents</h3>
<p><strong>Authors:</strong> Asa Cooper Stickland, Jan Michelfeit, Arathi Mani, Charlie Griffin, Ollie Matthews, Tomek Korbak, Rogan Inglis, Oliver Makins, Alan Cooney</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Investigates asynchronous monitoring of LLM agents to mitigate misalignment, focusing on practical security measures for large model deployment, directly addressing large model safety and alignment.
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13526' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline</h3>
<p><strong>Authors:</strong> Akhmadillo Mamirov, Faiaz Azmain, Hanyu Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Develops a weighted transparency framework and automated pipeline to evaluate safety-critical disclosures in AI models, directly addressing large model security and alignment through transparency.
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12443' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</h3>
<p><strong>Authors:</strong> Anika Sharma, Malavika Mampally, Chidaksh Ravuru, Kandyce Brennan, Neil Gaikwad</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 从认知、人际、结构多层面评估LLM对污名化议题的对齐能力，揭示当前LLM在复杂心理建构上的理解缺陷，为大模型安全与对齐提供实证依据。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13142' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</h3>
<p><strong>Authors:</strong> Zihui Zhao, Zechang Li</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对DPO的弱学习信号问题，提出Hint引导的反射机制，构建更具对比度的偏好对，提升对齐效率与幻觉抑制能力，属于大模型安全与对齐的核心算法改进。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13240' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models</h3>
<p><strong>Authors:</strong> Hao Chen, Yiwei Wang, Songze Li</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对现有扩散模型概念移除的单向策略缺陷，提出双向框架同时抑制有害语义与增强安全替代，通过mask过滤避免无关内容干扰，平衡了概念移除效果与生成视觉保真度，实验优于现有基线。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13039' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Face Identity Unlearning for Retrieval via Embedding Dispersion</h3>
<p><strong>Authors:</strong> Mikhail Zakharov</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 通过分散嵌入实现人脸身份不可检索，解决人脸检索中的身份泄露问题，属于大模型安全与对齐中的隐私保护。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13317' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning to Generate Cross-Task Unexploitable Examples</h3>
<p><strong>Authors:</strong> Haoxuan Qu, Qiuchi Xiang, Yujun Cai, Yirui Wu, Majid Mirmehdi, Hossein Rahmani, Jun Liu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 生成跨任务不可利用示例，保护个人数据不被未授权利用，属于大模型安全与对齐中的数据隐私保护。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13416' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</h3>
<p><strong>Authors:</strong> Wenhan Chen, Sezer Karaoglu, Theo Gevers</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 通过3D几何时间一致性检测AI生成视频，提升生成视频检测泛化性，属于大模型安全与对齐中的生成内容检测。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.13665' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play</h3>
<p><strong>Authors:</strong> Vince Trencsenyi</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Uses hypergame theory and answer-set programming to address agent misalignment in strategic play, providing a formal framework for large model alignment.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.11942' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation</h3>
<p><strong>Authors:</strong> Dang Phuong Nam, Nguyen Kieu, Pham Thanh Hieu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Embeds ethical safeguards into text-to-image generation via a fine-tuned classifier and optimized diffusion model, addressing safety and bias in generative AI.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12501' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Value-Aware Multiagent Systems</h3>
<p><strong>Authors:</strong> Nardine Osman</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Introduces a framework for value-aware AI systems, focusing on value learning, alignment, and explainability, directly contributing to large model security and alignment.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12652' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</h3>
<p><strong>Authors:</strong> Saad Alqithami</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出生成式Agent的记忆管理框架与隐私保护遗忘策略，解决长期交互中的性能与隐私瓶颈，同时构建基准测试集，属于大模型安全与对齐（隐私）和多模态智能体的交叉方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12856' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution</h3>
<p><strong>Authors:</strong> Boyang Yan</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对AI编码Agent的安全风险，提出事务性沙箱框架解决破坏性命令与状态不一致问题，提升自主执行安全性，属于大模型安全与对齐的重要实践。
Score: 6
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.12806' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</h3>
<p><strong>Authors:</strong> Rheeya Uppaal, Phu Mon Htut, Min Bai, Nikolaos Pappas, Zheng Qi</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 研究VLMs推理链的视觉忠实性，提出训练与参考无关的评估框架，通过自反思修正不忠实的感知步骤，提升推理的透明性与可靠性，属于深度学习可解释性中的推理链解释，对模型的可解释性与可信度有重要价值。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12218' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</h3>
<p><strong>Authors:</strong> Miriam Horovicz</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Proposes the first Shapley value-based framework for explaining tool importance in LLM agents, directly aligning with deep learning explainability via Shapley values.
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12597' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse Concept Anchoring for Interpretable and Controllable Neural Representations</h3>
<p><strong>Authors:</strong> Sandy Fraser, Patryk Wielopolski</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出稀疏概念锚定方法，用少量监督实现 latent space 的可解释与可控，支持可逆行为引导与永久概念移除，属于可解释性研究的重要进展。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12469' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Accuracy of Newton Step and Influence Function Data Attributions</h3>
<p><strong>Authors:</strong> Ittai Rubinstein, Samuel B. Hopkins</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 分析Newton Step与影响函数的数据归因准确性，证明NS比IF更准确且误差 scaling 更优，为可解释性中的数据归因方法提供理论验证。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12572' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Khawla Elhadri, Jörg Schlötterer, Christin Seifert</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Proposes an interpretable neural network architecture for tabular data using sparse autoencoders, addressing deep learning explainability by decomposing features into monosemantic concepts.
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.13442' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rough Sets for Explainability of Spectral Graph Clustering</h3>
<p><strong>Authors:</strong> Bartłomiej Starosta, Sławomir T. Wierzchoń, Piotr Borkowski, Dariusz Czerski, Marcin Sydow, Eryk Laskowski, Mieczysław A. Kłopotek</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 用粗糙集理论增强谱聚类的可解释性，解决谱空间难以关联数据内容的问题，属于white-box explanation的可解释性研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12436' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Causal inference and model explainability tools for retail</h3>
<p><strong>Authors:</strong> Pranav Gupta, Nithin Surendran</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 应用因果推断与SHAP值解释零售模型，发现固有可解释模型的SHAP值方差更低，为可解释性在实际场景中的应用提供实证支持。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12605' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Feeling the Strength but Not the Source: Partial Introspection in LLMs</h3>
<p><strong>Authors:</strong> Ely Hahami, Lavik Jain, Ishaan Sinha</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Investigates LLMs' ability to introspect their own activations, revealing partial introspection capabilities and prompt sensitivity, contributing to deep learning explainability.
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.12411' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection</h3>
<p><strong>Authors:</strong> Francesca Da Ros, Luca Di Gaspero, Kevin Roitero</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 探索LLM对组合优化问题的内部表示，通过直接查询与探针分析揭示LLM对问题结构的学习，为大模型在优化任务中的可解释性提供实证，属于深度学习可解释性的重要研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.13374' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</h3>
<p><strong>Authors:</strong> Shu Yu, Chaochao Lu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出自适应干预框架改进扩散模型的物理对齐与OOD泛化，通过因果干预增强扩散模型推理能力，属于diffusion LLM新技术。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.13290' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> On the Design of One-step Diffusion via Shortcutting Flow Paths</h3>
<p><strong>Authors:</strong> Haitao Lin (), Peiyan Hu (), Minsi Ren (), Zhifeng Gao (), Zhi-Ming Ma (), Guolin ke (), Tailin Wu (), Stan Z. Li ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出单步扩散模型的通用设计框架，解耦组件选择，实现无需预训练、蒸馏或curriculum learning的SOTA性能（ImageNet-256x256 FID50k 2.85），属于大模型新技术中的扩散模型高效设计方向，理论和实验贡献显著。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.11831' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Exploring the Design Space of Transition Matching</h3>
<p><strong>Authors:</strong> Uriel Singer, Yaron Lipman</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 系统研究Transition Matching的head设计、训练与采样策略，发现MLP head与高频率采样的最优组合，为生成模型的大模型新技术提供关键设计指引。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12465' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity</h3>
<p><strong>Authors:</strong> Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 建立prompt engineering的理论框架，将prompt视为外部程序分析Transformer计算机制，为大模型新技术中的prompt设计提供理论基础。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12688' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos</h3>
<p><strong>Authors:</strong> Tejas Panambur, Ishan Rajendrakumar Dave, Chongjian Ge, Ersin Yumer, Xue Bai</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出CreativeVR框架，利用扩散先验引导视频的结构与运动修复，解决AIGC和真实视频的严重 artifacts问题，属于大模型新技术中的扩散模型应用创新，对视频修复任务有重要突破。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models</h3>
<p><strong>Authors:</strong> Ryan Po, Eric Ryan Chan, Changan Chen, Gordon Wetzstein</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对自回归视频扩散模型的暴露偏差与漂移问题，提出BAgger自监督方案，通过模型自身 rollout 构建纠正轨迹，缓解长期生成的质量下降，属于大模型新技术中的扩散模型改进，对视频生成的稳定性有帮助。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12080' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Speedrunning ImageNet Diffusion</h3>
<p><strong>Authors:</strong> Swayam Bhanded</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 整合token routing、架构改进和训练优化，提出SR-DiT框架，在140M参数模型上达到与685M参数模型相当的ImageNet生成效果，是该模型大小下的state-of-the-art。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12386' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On Approaches to Building Surrogate ODE Models for Diffusion Bridges</h3>
<p><strong>Authors:</strong> Maria Khilchuk, Vladimir Latypov, Pavel Kleshchev, Alexander Hvatov</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出SINDy-FM与DSBM-NeuralODE代理模型，提升扩散桥的效率与可解释性，属于大模型新技术中的扩散模型改进。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation</h3>
<p><strong>Authors:</strong> Yunhong Min, Juil Koo, Seungwoo Yoo, Minhyuk Sung</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 用Bézier函数参数化stochastic interpolant调度器，提升diffusion/flow模型的few-step生成性能，训练轻量（15分钟），实验覆盖多种模型，对diffusion模型的高效生成有帮助
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.13255' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unified Control for Inference-Time Guidance of Denoising Diffusion Models</h3>
<p><strong>Authors:</strong> Maurya Goyal, Anuj Singh, Hadi Jamali-Rad</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出统一的扩散模型推理引导框架UniCoDe，结合采样和梯度引导的优势，解决现有方法效率低或生成结果偏差大的问题，实验在多个任务上表现出竞争力。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12339' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits</h3>
<p><strong>Authors:</strong> Foivos Paraperas Papantoniou, Stathis Galanakis, Rolandos Alexandros Potamias, Bernhard Kainz, Stefanos Zafeiriou</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出身份和视角感知的对话肖像视频扩散模型，改进现有方法的身份漂移与运动多样性问题，属于diffusion相关的大模型新技术。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.13247' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</h3>
<p><strong>Authors:</strong> Susung Hong (), Chongjian Ge (), Zhifei Zhang (), Jui-Hsien Wang ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出DiffusionBrowser框架，通过多分支解码器实现扩散模型生成过程的交互式预览（RGB和场景本征），支持中间步骤的引导，同时揭示生成过程的内部机制，属于大模型新技术中的扩散模型交互与可解释方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.13690' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Path Integral Diffusion: AdaPID</h3>
<p><strong>Authors:</strong> Michael Chertkov (University of Arizona), Hamidreza Behjoo (University of Arizona)</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出AdaPID框架，通过分段常数参数化和分层refinement优化扩散模型的路径调度，提升采样质量和效率，属于大模型新技术中的扩散模型路径设计方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.11858' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion</h3>
<p><strong>Authors:</strong> Michael Chertkov (University of Arizona)</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出GH-PID框架，结合随机最优传输与引导扩散，实现线性可解的引导生成，支持多专家融合等场景，属于大模型新技术中的扩散模型引导与最优传输方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.11859' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the continuity of flows</h3>
<p><strong>Authors:</strong> Congzhou M Sha</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 分析flow matching框架中的拓扑约束与速度场不连续性问题，通过双峰高斯混合的理论分析验证，对diffusion模型的理论理解有帮助
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.12821' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Image Diffusion Preview with Consistency Solver</h3>
<p><strong>Authors:</strong> Fu-Yun Wang, Hao Zhou, Liangzhe Yuan, Sanghyun Woo, Boqing Gong, Bohyung Han, Ming-Hsuan Yang, Han Zhang, Yukun Zhu, Ting Liu, Long Zhao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Introduces a trainable high-order solver for diffusion models to enable efficient preview-and-refine workflows, advancing diffusion model technology relevant to large model new techniques.
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.13592' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Socratic Students: Teaching Language Models to Learn by Asking Questions</h3>
<p><strong>Authors:</strong> Rajeev Bhatt Ambati, Tianyi Niu, Aashu Singh, Shlok Mishra, Shashank Srivastava, Snigdha Chaturvedi</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出学生主导的主动查询策略，通过DPO训练提升LLM在数学与编码任务中的学习效率，探索LLM主动学习的新范式，属于大模型新技术与高效训练的重要方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.13102' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10×</h3>
<p><strong>Authors:</strong> Jiangning Zhang, Junwei Zhu, Teng Hu, Yabiao Wang, Donghao Luo, Weijian Cao, Zhenye Gan, Xiaobin Hu, Zhucun Xue, Chengjie Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 通过Transformer重构策略优化预训练模型前向逻辑，加速原生4K视频生成，属于高效大模型训练与推理中的推理加速。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.13492' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models</h3>
<p><strong>Authors:</strong> Zhengyang Wang (), Ziyue Liu (), Ruijie Zhang (), Avinash Maurya (), Paul Hovland (), Bogdan Nicolae (), Franck Cappello (), Zheng Zhang ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对低秩瓶颈LLM的训练缩放问题，提出BOOST框架，结合瓶颈感知的张量并行、在线RMSNorm等优化，实现显著的训练速度提升（1.46-1.91x快于全秩模型），属于高效大模型训练与推理中的训练优化方向，解决了低秩模型的并行问题。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12131' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision</h3>
<p><strong>Authors:</strong> Yuseon Choi, Sangjin Kim, Jungjun Oh, Byeongcheol Kim, Hoi-Jun Yoo</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出SeVeDo加速器，结合SVD分离高敏感组件与分层组量化，显著提升Transformer低比特推理的能效，实验验证ViT和Llama2的性能，对高效大模型推理有实际贡献
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12930' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder</h3>
<p><strong>Authors:</strong> Tianyu Zhang, Dong Liu, Chang Wen Chen</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出AEIC框架，利用浅编码器与一步扩散解码器实现超低比特率（<0.05 bpp）图像压缩，在1080P输入上达到35.8 FPS编码效率，兼顾质量与速度，属于高效大模型训练与推理中的压缩技术，对边缘设备的图像传输有意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12229' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models</h3>
<p><strong>Authors:</strong> Yuqing Lei, Yingjun Du, Yawen Huang, Xiantong Zhen, Ling Shao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出MetaTPT元学习框架，通过自监督辅助任务指导测试时prompt tuning，提升VLMs的域适应能力，在多个基准上超越现有方法，属于高效大模型训练与推理中的测试时适应，对模型泛化有帮助。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12268' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching</h3>
<p><strong>Authors:</strong> Tingyan Wen, Haoyu Li, Yihuang Chen, Xing Zhou, Lifei Zhu, Xueqian Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出X-Slim缓存框架，统一利用扩散模型的时间、结构和空间冗余，通过双阈值控制器实现“推- polish”缓存策略，在保持生成质量的同时大幅提升速度，实验在FLUX.1-dev等模型上达到4.97x加速。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12604' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference</h3>
<p><strong>Authors:</strong> Shengling Qin, Hao Yu, Chenxin Wu, Zheng Li, Yizhong Cao, Zhengyang Zhuge, Yuxin Zhou, Wentao Yao, Yi Zhang, Zhengheng Wang, Shuai Bai, Jianwei Zhang, Junyang Lin</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对视觉语言推理中的重复计算痛点，提出缓存复用框架，通过动态层感知重计算策略平衡效率与精度，实验验证仅需计算2-5%的token即可保持全量计算的精度，推理速度提升1.2x-16x，具备强实际部署价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12977' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs</h3>
<p><strong>Authors:</strong> Prashant Pandey ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出KV缓存复用方法，通过句子嵌入匹配缓存的前缀prompt，复用KV状态以扩展小LLM的上下文容量，同时保持输出 fidelity，属于高效大模型训练与推理中的推理优化方向，提升低参数模型的实用性。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.11851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search</h3>
<p><strong>Authors:</strong> Ao Zhou (), Jianlei Yang (), Tong Qiao (), Yingjie Qi (), Zhi Yang (), Weisheng Zhao (), Chunming Hu ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出GCoDE框架，联合优化GNN的架构设计与设备-边缘映射方案，通过系统感知的搜索策略找到最优解，实现显著的速度提升和能耗降低，属于高效大模型训练与推理中的推理优化方向，针对GNN的co-inference问题有创新性。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.11856' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training</h3>
<p><strong>Authors:</strong> Yuting Tang (), Weibang Jiang (), Shanglin Li (), Yong Li (), Chenyu Liu (), Xinliang Zhou (), Yi Ding (), Cuntai Guan ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出EEG-DLite数据蒸馏框架，通过自监督autoencoder将EEG数据编码为latent表示，筛选出小而informative的子集，仅用5%的2500小时数据即可达到全量数据的训练性能，属于高效大模型训练与推理中的数据高效训练方向，解决了EEG foundation model的训练资源问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12210' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining</h3>
<p><strong>Authors:</strong> Jesse Ponnock</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 探索金融领域持续预训练的缩放定律，发现金融语言高规则性与数据高效性，为domain-adaptive pretraining的高效大模型训练提供实践指导。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12384' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain</h3>
<p><strong>Authors:</strong> Animesh Mishra</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 用随机矩阵理论与Frequent Directions sketching实现拜占庭鲁棒的去中心化联邦学习，支持1.5B参数模型，属于高效大模型训练中的鲁棒性优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12617' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization</h3>
<p><strong>Authors:</strong> Li Xia</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 结合随机投影NTK、阶段蒸馏与Nesterov动量，实现通信量减少98.7%的去中心化学习，属于高效大模型训练中的通信优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12737' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improving Recursive Transformers with Mixture of LoRAs</h3>
<p><strong>Authors:</strong> Mohammadmahdi Nouriborji, Morteza Rohanian, Omid Rohanian</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对递归Transformer参数共享导致的层表达能力下降问题，提出MoL（Mixture of LoRAs）增强表达能力，预训练模型性能超过全参数基线，对高效大模型训练有价值
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12880' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks</h3>
<p><strong>Authors:</strong> Jonathan Wenshøj, Tong Chen, Bob Pepin, Raghavendra Selvan</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出端到端联合剪枝与量化的CoDeQ方法，用死区量化器诱导稀疏性，无需辅助步骤，在ResNet-18上保持精度同时降低计算量，对模型压缩有价值
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12981' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving</h3>
<p><strong>Authors:</strong> Dong Liu, Yanxuan Yu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Proposes a disaggregated KV-cache architecture using CXL and FPGA to improve LLM serving throughput and reduce memory costs, directly contributing to efficient LLM inference.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.11920' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description</h3>
<p><strong>Authors:</strong> Mahathir Monjur, Shahriar Nirjon</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出mmWeaver框架，利用隐式神经表示（INR）和超网络实现mmWave信号的高效合成，实现49倍压缩并比传统方法快6-35倍，属于高效大模型训练与推理中的模型压缩与效率优化，对带宽受限场景的信号生成有价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.11894' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</h3>
<p><strong>Authors:</strong> Xinqi Jin, Hanxun Yu, Bohan Yu, Kebin Liu, Jian Liu, Keda Tao, Yixuan Pei, Huan Wang, Fan Dang, Jiangchuan Liu, Weiqiang Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出视觉token剪枝方法，通过计算空间相邻视频token的最大相似度减少冗余token，提升MLLM在在线视频理解的效率，实验在多个基准上提升4% accuracy且剪枝延迟可忽略。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12560' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Efficient Vision-Language Reasoning via Adaptive Token Pruning</h3>
<p><strong>Authors:</strong> Xue Li, Xiaonan Song, Henry Hu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出自适应token剪枝方法，结合ViT CLS注意力和CLIP文本-图像相似性动态保留重要视觉token，减少VLM推理的计算量（40% FLOPs减少），同时保持精度，实验效果显著。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12701' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Effective Fine-Tuning with Eigenvector Centrality Based Pruning</h3>
<p><strong>Authors:</strong> Shaif Chowdhury, Soham Biren Katlariwala, Devleena Kashyap</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 用图论特征向量中心性剪枝神经网络，在提升fine-tuning准确性的同时降低模型复杂度，属于高效大模型训练中的剪枝优化。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12543' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models</h3>
<p><strong>Authors:</strong> Haotian Xu, Tian Gao, Tsui-Wei Weng, Tengfei Ma</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出输入稀疏化方法，用自发神经元补偿性能损失，提升LLM效率，属于高效大模型训练中的输入优化。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12744' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Federated Learning with Feedback Alignment</h3>
<p><strong>Authors:</strong> Incheol Baek, Hyungbin Kim, Minseo Kim, Yon Dohn Chung</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 用反馈对齐缓解联邦学习的局部漂移，提升收敛性，属于高效大模型训练中的联邦学习优化。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.12762' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Element-wise Modulation of Random Matrices for Efficient Neural Layers</h3>
<p><strong>Authors:</strong> Maksymilian Szorc</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Introduces a parametrized random projection layer that reduces trainable parameters while maintaining accuracy, contributing to efficient neural network design for resource-limited settings.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.13480' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Error-Driven Prompt Optimization for Arithmetic Reasoning</h3>
<p><strong>Authors:</strong> Árpád Pándy, Róbert Lakatos, András Hajdu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对小模型（SLM）的算术推理缺陷，提出错误驱动的提示优化框架，通过聚类错误迭代优化提示规则，在隐私合规下超越大模型性能，属于高效大模型训练与推理（小模型优化）的重要实践。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.13323' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Phase transitions reveal hierarchical structure in deep neural networks</h3>
<p><strong>Authors:</strong> Ibrahim Talha Ersoy (), Andr\'es Fernando Cardozo Licha (), Karoline Wiesner ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 通过相变分析揭示DNN损失景观中的鞍点与层级结构，连接模式连通性等现象，建立训练现象的几何解释框架，属于深度学习理论中的模型训练动态与损失景观方向，理论贡献显著。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.11866' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Optimal Mistake Bounds for Transductive Online Learning</h3>
<p><strong>Authors:</strong> Zachary Chase, Steve Hanneke, Shay Moran, Jonathan Shafer</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 解决30年open问题，证明transductive在线学习的最优错误边界为Ω(√d)，并给出匹配上界，是深度学习理论中的在线学习理论突破。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12567' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection</h3>
<p><strong>Authors:</strong> Jiahao Zhao</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出LLM驱动的目标检测架构合成框架，从数据集内在特征（如目标尺度分布、场景密度）生成网络配置，解决传统手动设计耗时和NAS计算量大的问题，实验证明生成的架构性能优于基线模型且具有更好的性能-参数权衡。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12281' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</h3>
<p><strong>Authors:</strong> Hyunju Lee, Youngmin Oh, Jeimin Jeon, Donghyeon Baek, Bumsub Ham</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出渐进式ViT架构搜索框架，从小子网训练扩展到大多子网，减少超网训练中大小子网的干扰问题，实验在ImageNet和多个迁移学习基准上效果优于现有Transformer架构搜索方法。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12296' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LitePT: Lighter Yet Stronger Point Transformer</h3>
<p><strong>Authors:</strong> Yuanwen Yue (), Damien Robert (), Jianyuan Wang (), Sunghwan Hong (), Jan Dirk Wegner (), Christian Rupprecht (), Konrad Schindler ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 分析3D点云网络中卷积与注意力的角色，发现早期层用卷积提取低-level几何、深层用注意力捕获高-level语义更高效，提出LitePT架构，结合两者并引入PointROPE位置编码，在参数更少的情况下性能优于现有模型，属于深度学习理论中的网络架构设计方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13689' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency</h3>
<p><strong>Authors:</strong> Koffi O. Ayena ()</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 分析SiLU激活函数的网络逼近能力，证明其对光滑函数的指数级逼近率和深度效率，优于ReLU网络的构造，属于深度学习理论中的激活函数与模型表达能力方向，理论贡献显著。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12132' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimized Architectures for Kolmogorov-Arnold Networks</h3>
<p><strong>Authors:</strong> James Bagrow, Josh Bongard</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出过参数化结合稀疏化的KAN架构，在保持可解释性的同时提升模型表达能力，属于深度学习理论中的网络架构优化。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12448' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics</h3>
<p><strong>Authors:</strong> Jingdi Lei, Di Zhang, Soujanya Poria</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出无误差线性注意力EFLA，通过连续时间动力学的精确解实现线性时间复杂度，为深度学习理论中的注意力机制优化提供新路径。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12602' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks</h3>
<p><strong>Authors:</strong> Shivansh Sahni, Wenzhi Zhang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出Liquid Reasoning Transformer，用自适应深度与迭代修正提升结构化推理能力，属于深度学习理论中的Transformer架构优化。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12792' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</h3>
<p><strong>Authors:</strong> Anastasiia Alokhina, Pan Li</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 研究Transformer处理可变长度输入的size generalization问题，提出理论框架分析几何数据的误差界，对深度学习理论中的泛化分析有重要贡献
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12805' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource</h3>
<p><strong>Authors:</strong> Sofiya Zaichyk</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Develops a statistical framework for learning under distributional drift using reproducibility budgets, advancing theoretical understanding of generalization in deep learning.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13506' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</h3>
<p><strong>Authors:</strong> Leonard Bereska, Zoe Tzifa-Kratira, Reza Samavi, Efstratios Gavves</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Proposes an information-theoretic framework to measure superposition in neural networks using sparse autoencoders, advancing theoretical understanding of neural representation organization.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13568' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Entropy Collapse: A Universal Failure Mode of Intelligent Systems</h3>
<p><strong>Authors:</strong> Truong Xuan Khanh, Truong Quynh Hoa</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Identifies entropy collapse as a universal failure mode in intelligent systems (including AI), providing a theoretical framework for understanding degradation in deep learning models.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12381' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization</h3>
<p><strong>Authors:</strong> Bizhe Bai, Hongming Wu, Peng Ye, Tao Chen</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对LLM自监督RL的“策略崩溃”问题，提出动量锚定的Policy Optimization框架，结合IQR过滤保持策略多样性，显著提升训练稳定性与推理性能，属于深度学习理论（优化器）与高效大模型训练的核心改进。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13070' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data</h3>
<p><strong>Authors:</strong> Shubhada Agrawal, Aaditya Ramdas</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 研究sub-Gaussian mixture的路径级regret界，证明几乎必然的$\ln\ln T$ regret，桥接对抗性在线学习与博弈论统计，为深度学习理论中的在线学习提供重要理论支撑。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12325' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</h3>
<p><strong>Authors:</strong> Gelesh G Omathil, Sreeja CS</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出PerNodeDrop正则化，按样本-节点扰动平衡模型专业化与正则化，提升泛化能力，属于深度学习理论中的正则化方法优化。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12663' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Solving a Machine Learning Regression Problem Based on the Theory of Random Functions</h3>
<p><strong>Authors:</strong> Yuriy N. Bakhvalov</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 用随机函数理论推导回归方法，得到广义多调和样条核，为深度学习理论中的回归方法提供坚实理论基础。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12731' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization</h3>
<p><strong>Authors:</strong> Xiaoyu He, Yu Cai, Jin Jia, Canxi Huang, Wenqing Chen, Zibin Zheng</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出Alada自适应动量方法，用秩一分解估计二阶矩，实现内存高效的大规模矩阵优化，理论性能与Adam对齐，实验验证NLP任务的有效性，属于optimizer的改进
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13034' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks</h3>
<p><strong>Authors:</strong> Vítor M. Hanriot, Luiz C. B. Torres, Antônio P. Braga</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Unifies support vector machines and neural networks using Gabriel graphs, proposing a new neural network architecture and training strategies, directly contributing to deep learning theory.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13410' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective</h3>
<p><strong>Authors:</strong> Haoyang Chen, Richong Zhang, Junfan Chen</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Reinterprets in-context learning from a transductive label propagation perspective, advancing theoretical understanding of ICL mechanisms relevant to deep learning theory.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.12175' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Differentiable Evolutionary Reinforcement Learning</h3>
<p><strong>Authors:</strong> Sitao Cheng, Tianle Li, Xuhan Huang, Xunjian Yin, Difan Zou</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出可微分进化RL框架，通过元优化器进化奖励函数，解决复杂任务中的奖励设计难题，提升Agent对齐与泛化能力，属于深度学习理论（进化RL）与大模型新技术的交叉方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13399' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing</h3>
<p><strong>Authors:</strong> Yan Li, Lin Liu, Xiaopeng Zhang, Wei Xue, Wenhan Luo, Yike Guo, Qi Tian</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出密集梯度流优化框架用于细粒度图像编辑，涉及优化器改进，提升图像编辑的精度与可控性。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.13276' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation</h3>
<p><strong>Authors:</strong> Zihan Wang, Seungjun Lee, Guangzhao Dai, Gim Hee Lee</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出动态3D视觉语言规划模型，统一embodied agent的规划、接地和导航，用3D Chain-of-Thought和协同学习从碎片化监督中学习，实验在多个导航基准上达到state-of-the-art。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.12622' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Motus: A Unified Latent Action World Model</h3>
<p><strong>Authors:</strong> Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, Hongyan Zhao, Hanyu Liu, Zhizhong Su, Lei Ma, Hang Su, Jun Zhu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出统一的潜在动作世界模型，整合多模态理解、视频生成与动作控制，利用光流学习像素级“ delta动作”，通过三阶段训练与数据金字塔提升性能，实验在模拟与真实场景中显著优于X-VLA等基线，推进多模态智能体的统一建模。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.13030' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</h3>
<p><strong>Authors:</strong> Tong Wei, Yijun Yang, Changhao Zhang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 解决多模态智能体训练中昂贵教师模型的依赖问题，利用训练过程中的合并checkpoint作为“免费教师”，通过监督微调或软logit蒸馏引导RL训练，提升agentic VLM精度10-30%，同时减少50%训练时间与60%计算成本。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.13043' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Interactive Intelligence for Digital Humans</h3>
<p><strong>Authors:</strong> Yiyi Cai, Xuangeng Chu, Xiwei Gao, Sitong Gong, Yifei Huang, Caixin Kang, Kunhang Li, Haiyang Liu, Ruicong Liu, Yun Liu, Dianwen Ng, Zixiong Su, Erwin Wu, Yuhan Wu, Dingkun Yan, Tianyu Yan, Chang Zeng, Bo Zheng, You Zhou</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出交互式数字人框架，支持个性对齐表达、自适应交互与自我进化，属于多模态智能体。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.13674' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents</h3>
<p><strong>Authors:</strong> Youngmin Im, Byeongung Jo, Jaeyoung Wi, Seungwoo Baek, Tae Hoon Min, Joo Hyung Lee, Sangeun Oh, Insik Shin, Sunjae Lee</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> Develops a modular benchmarking framework for mobile GUI agents, addressing evaluation challenges and advancing multi-modal agent capabilities relevant to GUI navigation.
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.12634' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</h3>
<p><strong>Authors:</strong> Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 提出工具增强的单智能体框架用于工业异常检测，通过多阶段视觉检查提升检测精度，属于多模态智能体。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.13671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</h3>
<p><strong>Authors:</strong> Mahir Labib Dihan, Tanzima Hashem, Mohammed Eunus Ali, Md Rizwan Parvez</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对Web环境下LLM智能体的短视问题，提出树搜索框架优化Agent的长期决策与安全回溯，提升Web任务成功率，属于多模态智能体（GUI Agent）的关键改进。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.12692' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations</h3>
<p><strong>Authors:</strong> Emre Can Acikgoz, Jinoh Oh, Joo Hyuk Jeon, Jie Hao, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur, Xiang Li, Chengyuan Ma, Xing Fan</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 针对多轮对话中的歧义问题，提出多Agent协同澄清框架，提升任务成功率并减少对话轮次，属于多模态智能体（对话Agent）的关键改进。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.13154' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning</h3>
<p><strong>Authors:</strong> Emre Can Acikgoz, Jinoh Oh, Jie Hao, Joo Hyuk Jeon, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur, Xiang Li, Chengyuan Ma, Xing Fan</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Reason:</strong> 利用RL增强LLM的主动对话能力，通过奖励设计平衡询问与行动，提升任务完成率，属于多模态智能体（对话Agent）与大模型新技术的交叉方向。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.13159' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>