# ArXiv 每日推荐 - 2025-12-18

> 更新于北京时间：2025-12-18 12:36:24
> 已自动阅读了 254 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：130243

## 高效大模型训练与推理

### [Score: 10.0/10] OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration
- **Authors:** Ruitong Sun, Tianze Yang, Wei Niu, Jin Sun
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14096](https://arxiv.org/abs/2512.14096)
- **Reason:** 通过进化算法优化引导调度和自适应缓存加速DiT，实现53%计算节省且质量提升，支持多个主流扩散模型，是高效大模型推理的SOTA方法。
Score: 10
Field: 高效大模型训练与推理

### [Score: 9.0/10] Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models
- **Authors:** Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14008](https://arxiv.org/abs/2512.14008)
- **Reason:** 通过动态截断掩码token和寄存器token加速离散扩散模型推理，在文本到图像生成等任务上实现2x速度提升且保持质量，属于高效大模型推理的关键改进。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices
- **Authors:** HyperAI Team, Yuchen Liu, Kaiyang Han, Zhiqiang Xia, Yuhang Dong, Chen Song, Kangyu Tang, Jiaming Xu, Xiushi Feng, WenXuan Yu, Li Peng, Mingyang Wang, Kai Wang, Changpeng Yang, Yang Li, Haoyu Lu, Hao Wang, Bingna Xu, Guangyao Liu, Long Huang, Kaibin Guo, Jinyang Wu, Dan Wu, Hongzhen Wang, Peng Zhou, Shuai Nie, Shande Wang, Runyu Shi, Ying Huang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14052](https://arxiv.org/abs/2512.14052)
- **Reason:** 针对边缘设备设计高效多模态大模型，通过图像分块、动态分辨率压缩和双一致性学习实现低延迟高性能，是高效大模型在边缘场景的重要突破。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Low-Rank Compression of Language Models via Differentiable Rank Selection
- **Authors:** Sidhant Sundrani, Francesco Tudisco, Pasquale Minervini
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13733](https://arxiv.org/abs/2512.13733)
- **Reason:** 提出LLRC方法通过可微分秩选择实现LLM的低秩压缩，在不微调的情况下提升压缩率和下游任务性能，超过现有启发式方法。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction
- **Authors:** Mohammad Mozaffari, Samuel Kushnir, Maryam Mehri Dehnavi, Amir Yazdanbakhsh
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13886](https://arxiv.org/abs/2512.13886)
- **Reason:** 提出OPTIMA方法将LLM剪枝的权重重建转化为QP问题，在单加速器上实现one-shot剪枝，提升精度和效率。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations
- **Authors:** Wentao Guo, Mayank Mishra, Xinle Cheng, Ion Stoica (University of California, Berkeley), Tri Dao (Princeton University)
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14080](https://arxiv.org/abs/2512.14080)
- **Reason:** 针对MoE模型面临的IO开销大、计算效率低等问题，提出内存高效算法、GPU kernel优化及token rounding方法，实现激活内存减少45%、计算吞吐量提升1.86倍，且开源代码，对高效训练大模型MoE架构有重要实践价值。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
- **Authors:** Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13996](https://arxiv.org/abs/2512.13996)
- **Reason:** 提出动态Top-p MoE路由机制，精确控制稀疏度，实验优于Top-k和固定Top-p基线，显著提升预训练效率和性能。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] CurvaDion: Curvature-Adaptive Distributed Orthonormalization
- **Authors:** Bhavesh Kumar, Roger Jin, Jeffrey Quesnelle
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13728](https://arxiv.org/abs/2512.13728)
- **Reason:** 提出曲率自适应的分布式正交化方法，利用RMMC检测高曲率区域减少同步次数，降低LLM分布式训练的通信瓶颈。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] MIDUS: Memory-Infused Depth Up-Scaling
- **Authors:** Taero Kim, Hoyoon Byun, Youngjun Choi, Sungrae Park, Kyungwoo Song
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13751](https://arxiv.org/abs/2512.13751)
- **Reason:** 用head-wise memory替换FFN实现LLM的depth up-scaling，提升模型容量的同时保持效率，优化大模型scaling策略。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs
- **Authors:** Rachit Bansal, Aston Zhang, Rishabh Tiwari, Lovish Madaan, Sai Surya Duvvuri, Devvrit Khatri, David Brandfonbrener, David Alvarez-Melis, Prajjwal Bhargava, Mihir Sanjay Kale, Samy Jelassi
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13898](https://arxiv.org/abs/2512.13898)
- **Reason:** 提出context-specific的测试时训练改进长上下文LLM的性能，克服静态自注意力的分数稀释问题，比现有推理策略更有效。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Arithmetic-Intensity-Aware Quantization
- **Authors:** Taig Singh, Shreshth Rajan, Nikhil Iyer
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14090](https://arxiv.org/abs/2512.14090)
- **Reason:** 提出混合精度量化框架AIQ，通过最大化算术强度同时最小化精度损失，解决内存绑定模型的推理吞吐量瓶颈，实验验证在ResNet-20/CIFAR-10和MobileNetV2上显著提升性能，对大模型高效推理有实用价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] ReflCtrl: Controlling LLM Reflection via Representation Engineering
- **Authors:** Ge Yan (Lily), Chung-En Sun (Lily), Tsui-Wei (Lily), Weng
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13979](https://arxiv.org/abs/2512.13979)
- **Reason:** 通过表征工程控制LLM反射行为，在保持性能的同时节省33.6%推理token，对提升推理效率有实际贡献。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees
- **Authors:** Junjie Ma, Jinlong Li
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14069](https://arxiv.org/abs/2512.14069)
- **Reason:** 提出RL-based动态草稿树投机采样方法，实现3.17x-4.82x推理加速，有效提升LLM推理效率。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models
- **Authors:** Hanning Chen, Keyu Man, Kevin Zhu, Chenguang Zhu, Haonan Li, Tongbo Luo, Xizhou Feng, Wei Sun, Sreen Tallam, Mohsen Imani, Partha Kanuparthy
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14141](https://arxiv.org/abs/2512.14141)
- **Reason:** 构建检测计算机视觉模型性能反模式的基准，提出轻量模型+LLM的迭代方法，帮助优化模型训练与推理效率，属于高效大模型训练的辅助工具。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy
- **Authors:** Steve Nwaiwu, Nipat Jongsawat, Anucha Tungkasthan
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13725](https://arxiv.org/abs/2512.13725)
- **Reason:** 系统评估量化对因果推理的影响，提出GraphRAG缓解压缩退化，为高效因果AI系统部署提供指导。
Score: 7
Field: 高效大模型训练与推理

## 原生多模态大模型

### [Score: 9.0/10] STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning
- **Authors:** Jie Qin, Jiancheng Huang, Limeng Qiao, Lin Ma
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13752](https://arxiv.org/abs/2512.13752)
- **Reason:** 提出堆叠自回归方案统一多模态理解与生成，通过VQ增强图像表示和隐式推理提升生成质量，在GenEval等基准上取得SOTA，解决多模态任务冲突问题。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding
- **Authors:** Shuang Cheng, Yuhua Jiang, Zineng Zhou, Dawei Liu, Wang Tao, Linfeng Zhang, Biqing Qi, Bowen Zhou
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14068](https://arxiv.org/abs/2512.14068)
- **Reason:** 首次系统将块式离散扩散应用于视觉语言理解，提出高效稳定训练框架，结果超过AR和全局扩散基线，推动原生多模态大模型的架构创新。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image
- **Authors:** Sicheng Xu, Guojun Chen, Jiaolong Yang, Yizhong Zhang, Yu Deng, Steve Lin, Baining Guo
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14677](https://arxiv.org/abs/2512.14677)
- **Reason:** 将VASA-1的2D音频驱动能力迁移至3D，实现高效3D头像生成，支持高帧率自由视角视频，推动原生多模态大模型的3D交互能力。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
- **Authors:** Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14698](https://arxiv.org/abs/2512.14698)
- **Reason:** 系统研究MLLMs的视频 temporal grounding，构建高质量TimeLens-Bench和TimeLens-100K数据集，提出有效训练策略，性能超过GPT-5等 proprietary模型。
Score: 9
Field: 原生多模态大模型

### [Score: 8.0/10] Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline
- **Authors:** Weikang Bai, Yongkun Du, Yuchen Su, Yazhen Xie, Zhineng Chen
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13731](https://arxiv.org/abs/2512.13731)
- **Reason:** 提出新型表达式tokenizer和结构化数学语言，评估多模态大模型在复杂数学表达式识别的性能，为原生多模态大模型在符号-视觉任务的应用提供基准和基线。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding
- **Authors:** Zongyao Li, Kengo Ishida, Satoshi Yamazaki, Xiaotong Ji, Jianquan Liu
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14017](https://arxiv.org/abs/2512.14017)
- **Reason:** 构建首个长视频QA的关键帧采样基准，分析采样策略对MLLMs性能的影响，提出自适应平衡采样方法，推动原生多模态大模型在长视频任务的高效应用。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models
- **Authors:** Ruishu Zhu, Zhihao Huang, Jiacheng Sun, Ping Luo, Hongyuan Zhang, Xuelong Li
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14099](https://arxiv.org/abs/2512.14099)
- **Reason:** 用离散扩散模型解决多视图一致图像生成，通过掩码token预测和自注意力保持跨视图一致性，无需复杂3D几何约束，属于原生多模态大模型的生成任务改进。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] SuperCLIP: CLIP with Simple Classification Supervision
- **Authors:** Weiheng Zhao, Zilong Huang, Jiashi Feng, Xinggang Wang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14480](https://arxiv.org/abs/2512.14480)
- **Reason:** 针对CLIP仅优化全局图像-文本相似性的不足，加入分类监督提升token-level细粒度对齐，改进多模态模型的视觉-文本对齐性能，属于原生多模态大模型方向的研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking
- **Authors:** Lihong Wang, Liangqi Li, Weiwei Feng, Jiamin Wu, Changtao Miao, Tieru Wu, Rui Ma, Bo Zhang, Zhe Li
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14654](https://arxiv.org/abs/2512.14654)
- **Reason:** 针对多模态数学推理任务提出Reason Chunking机制，模拟人类专家问题解决模式，构建CRUX数据集并采用渐进式训练策略，显著提升模型在数学基准上的性能，属于原生多模态大模型的重要探索。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Native and Compact Structured Latents for 3D Generation
- **Authors:** Jianfeng Xiang, Xiaoxue Chen, Sicheng Xu, Ruicheng Wang, Zelong Lv, Yu Deng, Hongyuan Zhu, Yue Dong, Hao Zhao, Nicholas Jing Yuan, Jiaolong Yang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14692](https://arxiv.org/abs/2512.14692)
- **Reason:** 提出O-Voxel稀疏体素结构学习3D数据的原生结构化潜变量，解决现有3D生成表示难以捕捉复杂拓扑和细节的问题，提升3D生成质量。
Score: 8
Field: 原生多模态大模型

## 大模型安全与对齐

### [Score: 9.0/10] Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach
- **Authors:** Ashish Mishra, Gyanaranjan Nayak, Tarun Kumar, Arpit Shah, Suparna Bhattacharya, Martin Foltin
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14113](https://arxiv.org/abs/2512.14113)
- **Reason:** 提出无训练无数据的CLIP选择性遗忘方法，利用零空间投影擦除目标类信息，解决大模型安全与对齐中的隐私和模型净化问题。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World
- **Authors:** Shuxin Zhao, Bo Lang, Nan Xiao, Yilang Zhang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14158](https://arxiv.org/abs/2512.14158)
- **Reason:** 提出基于连续交互空间的目标检测后门攻击，通过几何关系实现鲁棒攻击，突破现有单触发单目标的限制，为大模型安全与对齐中的后门防御提供新挑战与 insights。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation
- **Authors:** Dawid Malarz, Artur Kasymov, Filip Manjak, Maciej Zi\k{e}ba, Przemys{\l}aw Spurek
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13953](https://arxiv.org/abs/2512.13953)
- **Reason:** 提出unbranding任务解决文本到图像生成中的商标安全问题，构建基准并提出VLM-based评估，为大模型安全与对齐中的内容安全提供新方向。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models
- **Authors:** Ashish Mishra, Tarun Kumar, Gyanaranjan Nayak, Arpit Shah, Suparna Bhattacharya, Martin Foltin
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14137](https://arxiv.org/abs/2512.14137)
- **Reason:** 用闭包形式的零空间投影实现CLIP的无数据无训练零样本遗忘，保持模型整体性能的同时擦除目标类记忆，属于大模型安全与对齐的高效unlearning方法。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models
- **Authors:** Md. Hasib Ur Rahman
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13741](https://arxiv.org/abs/2512.13741)
- **Reason:** 提出语义湍流metric检测LLM的jailbreak攻击，基于模型潜在空间的轨迹变化，轻量且实时，为大模型安全提供新方法。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces
- **Authors:** Subramanyam Sahoo, Jared Junkin
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13821](https://arxiv.org/abs/2512.13821)
- **Reason:** 提出CTVP协议通过执行轨迹的语义轨道分析检测代码生成模型的恶意行为，理论上保证不可gamify，提升大模型安全能力。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] A First-Order Logic-Based Alternative to Reward Models in RLHF
- **Authors:** Chunjin Jian, Xinhua Zhu
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14100](https://arxiv.org/abs/2512.14100)
- **Reason:** 提出基于逻辑相似度的奖励机制替代传统RLHF的奖励模型，引入S-GRPO框架优化对齐训练，实验表明优于SFT和现有偏好学习方法，为大模型安全对齐提供了更灵活的方法创新。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making
- **Authors:** Yitong Luo, Ziang Chen, Hou Hei Lam, Jiayu zhan, Junqi Wang, Zhenliang Zhang, Xue Feng
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13716](https://arxiv.org/abs/2512.13716)
- **Reason:** 提出价值驱动的个性化决策框架，通过两阶段方法提升AI与用户价值偏好的对齐性，实验超过多个强基线，对构建可解释的个性化AI agent有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos
- **Authors:** Zhaolun Li, Jichang Li, Yinqi Cai, Junye Chen, Xiaonan Luo, Guanbin Li, Rushi Lan
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14601](https://arxiv.org/abs/2512.14601)
- **Reason:** 提出利用大模型（如CLIP）探测特征空间分布差异的深度伪造检测框架，解决跨域泛化问题，属于大模型安全与对齐中的深度伪造检测研究。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models
- **Authors:** TK Lee
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13762](https://arxiv.org/abs/2512.13762)
- **Reason:** 研究RLHF对齐模型的拒绝行为和习得无能，提出交互级审计框架，对理解对齐副作用有参考价值。
Score: 7
Field: 大模型安全与对齐

## 深度学习可解释性

### [Score: 9.0/10] Enhancing Interpretability for Vision Models via Shapley Value Optimization
- **Authors:** Kanglong Fan, Yunqiao Yang, Chen Ma
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14354](https://arxiv.org/abs/2512.14354)
- **Reason:** 提出将Shapley值估计作为辅助任务的自解释框架，解决post-hoc方法不忠实和自解释模型性能兼容差的问题，直接对应研究方向中的Shapley value与可解释性需求。
Score: 9
Field: 深度学习可解释性

### [Score: 9.0/10] Explainable reinforcement learning from human feedback to improve alignment
- **Authors:** Shicheng Liu, Siyuan Xu, Wenjie Qiu, Hangfan Zhang, Minghui Zhu
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13837](https://arxiv.org/abs/2512.13837)
- **Reason:** 提出解释RLHF生成不满意响应的方法，通过识别相关训练数据并unlearning改进，同时保留满意响应，结合可解释性和对齐优化。
Score: 9
Field: 深度学习可解释性

### [Score: 8.0/10] Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes
- **Authors:** Joseph Hoche, Andrei Bursuc, David Brellmann, Gilles Louppe, Pavel Izmailov, Angela Yao, Gianni Franchi
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14177](https://arxiv.org/abs/2512.14177)
- **Reason:** 针对LVLM语义不确定性问题，提出基于语义高斯过程的贝叶斯框架SGPU，避免传统聚类方法的脆弱性，支持黑盒与白盒设置，属于深度学习可解释性中的不确定性量化研究。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models
- **Authors:** Siegfried Ludwig, Stylianos Bakas, Konstantinos Barmpas, Georgios Zoumpourlis, Dimitrios A. Adamos, Nikolaos Laskaris, Yannis Panagakis, Stefanos Zafeiriou
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13806](https://arxiv.org/abs/2512.13806)
- **Reason:** 提出D3方法分离EEG信号的潜在成分，避免隐藏过拟合，提升模型的可解释性和泛化性，为深度学习模型的可解释性研究提供新视角。
Score: 7
Field: 深度学习可解释性

## 深度学习理论

### [Score: 9.0/10] Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean
- **Authors:** Chuan He
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14686](https://arxiv.org/abs/2512.14686)
- **Reason:** 针对随机一阶方法的偏差-方差权衡，分析从有界方差到无限均值的重尾噪声场景，提出Clipped SFOM的统一复杂度保证，理论贡献显著，属于深度学习理论中优化方法的核心研究。
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] Optimizing Rank for High-Fidelity Implicit Neural Representations
- **Authors:** Julian McGinnis, Florian A. Hölzl, Suprosanna Shit, Florentin Bieder, Paul Friedrich, Mark Mühlau, Björn Menze, Daniel Rueckert, Benedikt Wiestler
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14366](https://arxiv.org/abs/2512.14366)
- **Reason:** 挑战vanilla MLP无法表示高频内容的传统观点，指出训练中秩退化是核心问题，通过优化网络秩提升INR保真度，属于深度学习理论中模型训练与架构的关键研究。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Scaling and Transferability of Annealing Strategies in Large Language Model Training
- **Authors:** Siqi Wang, Zhengyu Chen, Teng Xiao, Zheqi Lv, Jinluan Yang, Xunliang Cai, Jingang Wang, Xiaomeng Li
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13705](https://arxiv.org/abs/2512.13705)
- **Reason:** 研究LLM训练中退火策略的可转移性，提出改进的预测框架优化学习率调度，为LLM训练的超参数选择提供实践指导。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] A Complete Guide to Spherical Equivariant Graph Transformers
- **Authors:** Sophia Tang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13927](https://arxiv.org/abs/2512.13927)
- **Reason:** 系统介绍球面等变图Transformer的理论和实现，包括群表示、球形谐波等，为几何图模型的研究提供基础。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits
- **Authors:** Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar (University of California, Berkeley)
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14338](https://arxiv.org/abs/2512.14338)
- **Reason:** 揭示Hopfield网络通过范数效率的隐式偏差实现图同构类的高效学习，连接隐式偏差与不变性泛化机制，属于深度学习理论中网络架构与泛化的基础研究。
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training
- **Authors:** John Graham Reynolds
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13706](https://arxiv.org/abs/2512.13706)
- **Reason:** 提出混合训练策略缓解LLM微调数学推理时的灾难性遗忘，在保持专业性能的同时保留通用能力，探索深度学习模型的泛化性优化。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Dropout Neural Network Training Viewed from a Percolation Perspective
- **Authors:** Finley Devlin, Jaron Sanders
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13853](https://arxiv.org/abs/2512.13853)
- **Reason:** 用渗流理论分析dropout训练中的路径问题，解释无偏置NN训练时的崩溃现象，为dropout的理论理解提供新视角。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Sliding Window Recurrences for Sequence Models
- **Authors:** Dragos Secrieru, Garyk Brixi, Yoshua Bengio, Taiji Suzuki, Michael Poli, Stefano Massaroli
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13921](https://arxiv.org/abs/2512.13921)
- **Reason:** 提出滑动窗口递归结构，优化序列模型的GPU内存利用，提升速度和性能，属于深度学习理论的network architecture方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix
- **Authors:** Wei Tao, Sheng Long, Xin Liu, Wei Li, Qing Tao
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14188](https://arxiv.org/abs/2512.14188)
- **Reason:** 分析PGD等梯度攻击的优化缺陷，提出动量自适应矩阵的AdaMI攻击方法，理论证明凸问题下的最优收敛性，实验验证提升对抗迁移性与稳定性，属于深度学习理论中优化器方向的针对性研究。
Score: 7
Field: 深度学习理论

## 多模态智能体

### [Score: 8.0/10] ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning
- **Authors:** Boran Wang, Xinming Wang, Yi Chen, Xiang Li, Jian Xu, Jing Yuan, Chenglin Liu
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14040](https://arxiv.org/abs/2512.14040)
- **Reason:** 提出工具集成推理的图表理解框架，解决MLLMs依赖文本注释的问题，通过模块化工具库和Evidence Package实现可解释推理，属于多模态智能体的实用应用。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] MobileWorldBench: Towards Semantic World Modeling For Mobile Agents
- **Authors:** Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Aditya Grover
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14014](https://arxiv.org/abs/2512.14014)
- **Reason:** 提出GUI移动智能体的语义世界建模基准和框架，整合VLM提升任务成功率，对多模态智能体研究有推动作用。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models
- **Authors:** Zechen Bai, Chen Gao, Mike Zheng Shou
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14666](https://arxiv.org/abs/2512.14666)
- **Reason:** 该工作针对视觉-语言-动作（VLA）模型提出测试时训练框架，通过环境反馈解决模型难以适应部署条件变化的问题，涉及多模态智能体的自适应与交互学习，实验证明能显著提升长 horizons任务、少样本学习及跨任务泛化性能，与多模态智能体方向高度相关。
Score: 8
Field: 多模态智能体

## 大模型新技术

### [Score: 7.0/10] Repurposing 2D Diffusion Models for 3D Shape Completion
- **Authors:** Yao He, Youngjoong Kwon, Tiange Xiang, Wenxiao Cai, Ehsan Adeli
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.13991](https://arxiv.org/abs/2512.13991)
- **Reason:** 提出Shape Atlas将3D几何转为2D表示，利用预训练2D扩散模型解决3D形状补全，属于大模型新技术中扩散模型的跨模态应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution
- **Authors:** Hao Chen, Junyang Chen, Jinshan Pan, Jiangxin Dong
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14061](https://arxiv.org/abs/2512.14061)
- **Reason:** 提出可控一步扩散超分辨率方法，通过LQ引导特征调制和文本匹配引导解决保真度与真实感的矛盾，属于大模型新技术中扩散模型的细分应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Massive Editing for Large Language Models Based on Dynamic Weight Generation
- **Authors:** Wentao Wan, Qiqing Lao, Zhiwei Xie, Hefeng Wu, Runnan Lin, Liang Lin, Keze Wang
- **Published:** 2025-12-17
- **Link:** [https://arxiv.org/abs/2512.14395](https://arxiv.org/abs/2512.14395)
- **Reason:** 提出基于动态权重生成的大规模LLM知识编辑方法，通过扩散模型实现高效编辑，属于大模型新技术的创新。
Score: 7
Field: 大模型新技术

