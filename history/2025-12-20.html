<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-20</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >大模型新技术</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-20</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-20 12:31:51</p>
<p>已自动阅读了 250 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：135776</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</h3>
<p><strong>Authors:</strong> Jintao Zhang (Tsinghua University), Kaiwen Zheng (Tsinghua University), Kai Jiang (Tsinghua University), Haoxu Wang (Tsinghua University), Ion Stoica (University of California, Berkeley), Joseph E. Gonzalez (University of California, Berkeley), Jianfei Chen (Tsinghua University), Jun Zhu (Tsinghua University)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出TurboDiffusion框架，通过注意力加速、步数蒸馏和8位量化等方法，将视频扩散模型生成速度提升100-200倍，同时保持视频质量，对视频生成的实际部署具有重大意义。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16093' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</h3>
<p><strong>Authors:</strong> Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Kascade稀疏注意力方法，提升长上下文LLM推理速度且保持精度，属于高效大模型训练与推理方向，对长上下文场景有重要价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16391' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</h3>
<p><strong>Authors:</strong> Yifan Zhou, Zeqi Xiao, Tianyi Wei, Shuai Yang, Xingang Pan</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出可训练的对数线性稀疏注意力LLSA，将扩散Transformer的注意力复杂度从二次降为对数线性，显著提升扩散模型的训练与推理效率，解决了长序列扩散的计算瓶颈。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16615' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</h3>
<p><strong>Authors:</strong> Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Alchemist框架，通过元梯度数据选择从大规模文本-图像对中筛选有效子集，提升文本到图像模型的训练效率，解决了大模型训练的数据冗余问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs</h3>
<p><strong>Authors:</strong> Anshul Kumar, Gagan Raj Gupta, Manisha Chawla</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出自适应梯度引导的层选择方法，通过Dirichlet采样和epsilon-greedy策略提升小语言模型的微调效率，属于高效大模型训练与推理中的高效微调方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.15764' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</h3>
<p><strong>Authors:</strong> Caner Erden</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出动态秩强化学习自适应调整MHSA的低秩分解，结合在线矩阵扰动理论提升LLM推理效率，属于高效大模型训练与推理中的高效推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.15973' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> INTELLECT-3: Technical Report</h3>
<p><strong>Authors:</strong> Prime Intellect Team, Mika Senghaas, Fares Obeid, Sami Jaghouar, William Brown, Jack Min Ong, Daniel Auras, Matej Sirovatka, Jannik Straube, Andrew Baker, Sebastian M\"uller, Justus Mattern, Manveer Basra, Aiman Ismail, Dominik Scherm, Cooper Miller, Ameen Patel, Simon Kirsten, Mario Sieg, Christian Reetz, Kemal Erdem, Vincent Weisser, Johannes Hagemann</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 介绍106B参数的MoE大模型INTELLECT-3及其训练推理基础设施，开源训练栈，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16144' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity</h3>
<p><strong>Authors:</strong> Jinhao Zhang, Yunquan Zhang, Daning Chen</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出基于CKA的模块化量化方法，优化各层量化策略，提升模型性能，属于高效大模型训练与推理方向（量化是高压缩关键技术）。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</h3>
<p><strong>Authors:</strong> Hao Liang, Xiaochen Ma, Zhou Liu, Zhen Hao Wong, Zhengyang Zhao, Zimo Meng, Runming He, Chengyu Shen, Qifeng Cai, Zhaoyang Han, Meiyi Qiang, Yalin Feng, Tianyi Bai, Zewei Pan, Ziyi Guo, Yizhen Jiang, Jingwen Deng, Qijie You, Peichao Lai, Tianyu Guo, Chi Hsu Tsai, Hengyi Feng, Rui Hu, Wenkai Yu, Junbo Niu, Bohan Zeng, Ruichuan An, Lu Ma, Jihao Huang, Yaowei Zheng, Conghui He, Linpeng Tang, Bin Cui, Weinan E, Wentao Zhang</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出DataFlow框架用于LLM数据准备自动化，提升数据准备效率和质量，属于高效大模型训练与推理方向（LLM infra）。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16676' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MEPIC: Memory Efficient Position Independent Caching for LLM Serving</h3>
<p><strong>Authors:</strong> Qian Wang, Zahra Yousefijamarani, Morgan Lindsay Heisler, Rongzhi Gu, Bai Xiaolong, Shan Yizhou, Wei Zhang, Wang Lan, Ying Xiong, Yong Zhang, Zhenan Fan</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出MEPIC内存高效缓存方法，提升长上下文LLM内存利用率，属于高效大模型训练与推理方向（LLM serving infra）。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16822' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision</h3>
<p><strong>Authors:</strong> Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, Igor Gitman</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出高效长上下文蒸馏方法，用于数学推理模型训练，属于高效大模型训练与推理研究，整合多模态监督数据，提升模型性能和效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.15489' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning</h3>
<p><strong>Authors:</strong> Pengfei Sun, Wenyu Jiang, Piew Yoong Chee, Paul Devos, Dick Botteldooren</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出无BN的全整数量化网络，实现端到端整数推理，适用于边缘设备，属于高效大模型训练与推理方向（量化压缩）。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16476' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Kling-Omni Technical Report</h3>
<p><strong>Authors:</strong> Kling Team, Jialu Chen, Yuanzheng Ci, Xiangyu Du, Zipeng Feng, Kun Gai, Sainan Guo, Feng Han, Jingbin He, Kang He, Xiao Hu, Xiaohua Hu, Boyuan Jiang, Fangyuan Kong, Hang Li, Jie Li, Qingyu Li, Shen Li, Xiaohan Li, Yan Li, Jiajun Liang, Borui Liao, Yiqiao Liao, Weihong Lin, Quande Liu, Xiaokun Liu, Yilun Liu, Yuliang Liu, Shun Lu, Hangyu Mao, Yunyao Mao, Haodong Ouyang, Wenyu Qin, Wanqi Shi, Xiaoyu Shi, Lianghao Su, Haozhi Sun, Peiqin Sun, Pengfei Wan, Chao Wang, Chenyu Wang, Meng Wang, Qiulin Wang, Runqi Wang, Xintao Wang, Xuebo Wang, Zekun Wang, Min Wei, Tiancheng Wen, Guohao Wu, Xiaoshi Wu, Zhenhua Wu, Da Xie, Yingtong Xiong, Yulong Xu, Sile Yang, Zikang Yang, Weicai Ye, Ziyang Yuan, Shenglong Zhang, Shuaiyu Zhang, Yuanxing Zhang, Yufan Zhang, Wenzheng Zhao, Ruiliang Zhou, Yan Zhou, Guosheng Zhu, Yongjie Zhu</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Kling-Omni端到端多模态生成框架，支持文本、图像、视频上下文等多输入的高保真视频生成，实现了多模态大模型的端到端整合，具有强工业应用价值。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16776' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Davide Caffagni (aimagelab), Sara Sarto (aimagelab), Marcella Cornia (aimagelab), Lorenzo Baraldi (aimagelab), Pier Luigi Dovesi (aimagelab), Shaghayegh Roohi (aimagelab), Mark Granroth-Wilding (aimagelab), Rita Cucchiara (aimagelab)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 针对多模态大语言模型（MLLMs）依赖文本监督导致视觉推理能力有限的问题，提出JARVIS框架整合I-JEPA自监督视觉学习，提升MLLMs的视觉理解能力，实验验证在视觉基准上的性能提升，方向匹配且方法创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.15885' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs</h3>
<p><strong>Authors:</strong> Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, Ruixuan Li</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Sketch-in-Latents范式，让多模态大模型在潜空间生成视觉草稿token，实现文本与视觉的统一推理，突破了传统MLLMs的模态分离局限。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16584' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</h3>
<p><strong>Authors:</strong> Tianyuan Qu, Lei Ke, Xiaohang Zhan, Longxiang Tang, Yuqi Liu, Bohao Peng, Bei Yu, Dong Yu, Jiaya Jia</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出RePlan框架，通过视觉语言 planner 分解复杂指令并grounding到目标区域，结合扩散编辑器实现精确多区域编辑，解决了多模态大模型在复杂指令理解与图像生成的协同问题。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16864' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SFTok: Bridging the Performance Gap in Discrete Tokenizers</h3>
<p><strong>Authors:</strong> Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 针对多模态模型中离散tokenizer性能落后于连续tokenizer的问题，提出SFTok通过多步迭代机制提升图像重建质量，是原生多模态大模型中tokenizer方向的重要改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16910' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs</h3>
<p><strong>Authors:</strong> Tejas Anvekar (Arizona State University), Fenil Bardoliya (Arizona State University), Pavan K. Turaga (Arizona State University), Chitta Baral (Arizona State University), Vivek Gupta (Arizona State University)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Perceptual Observatory框架，系统评估MLLMs的视觉鲁棒性、接地性和推理能力，揭示MLLMs依赖文本先验而非视觉细节的局限性，对理解MLLMs的感知能力有重要学术价值。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.15949' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EasyV2V: A High-quality Instruction-based Video Editing Framework</h3>
<p><strong>Authors:</strong> Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出EasyV2V框架解决视频编辑中的一致性、控制和泛化问题，通过数据构建、模型设计和控制机制实现高质量指令驱动视频编辑，属于原生多模态大模型中的图像生成研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16920' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</h3>
<p><strong>Authors:</strong> Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出WorldCanvas框架结合参考图像、轨迹和文本生成可控的世界事件，支持多模态输入的用户导向模拟，属于原生多模态大模型中的图像生成研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16924' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> LLaDA2.0: Scaling Up Diffusion Language Models to 100B</h3>
<p><strong>Authors:</strong> Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Ling Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, Yihong Zhuang</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 将扩散语言模型扩展到100B参数，通过知识继承、渐进适应和效率感知设计实现大规模部署，属于大模型新技术中的diffusion LLM研究。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.15745' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> NRGPT: An Energy-based Alternative for GPT</h3>
<p><strong>Authors:</strong> Nima Dehmamy, Benjamin Hoover, Bishwajit Saha, Leo Kozachkov, Jean-Jacques Slotine, Dmitry Krotov</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出能量基的GPT替代模型NRGPT，探索能量基模型在语言建模中的应用，属于大模型新技术方向，是大模型新范式。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.16762' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models</h3>
<p><strong>Authors:</strong> Mariam Hassan (EPFL), Bastien Van Delft (EPFL), Wuyang Li (EPFL), Alexandre Alahi (EPFL)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 分解文本到视频（T2V）扩散模型的生成过程为推理、构图和时间合成，解决复杂场景生成和时间逻辑不一致的问题，实验验证在CompBench和VBench2上的SOTA性能，同时加速采样。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.16371' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FlowDet: Unifying Object Detection and Generative Transport Flows</h3>
<p><strong>Authors:</strong> Enis Baty, C. P. Bridges, Simon Hadfield</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 将条件流匹配引入目标检测，提出FlowDet框架，统一生成式传输与检测任务，突破了传统检测模型的判别式局限，属于生成式大模型的新应用方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.16771' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Unification of Discrete, Gaussian, and Simplicial Diffusion</h3>
<p><strong>Authors:</strong> Nuria Alina Chandra, Yucen Lily Li, Alan N. Amin, Alex Ali, Joshua Rollins, Sebastian W. Ober, Aniruddh Raghu, Andrew Gordon Wilson</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 基于Wright-Fisher模型提出离散、高斯和单纯形扩散的统一理论，解决扩散模型的稳定性和多域适配问题，属于大模型新技术中的diffusion研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.15923' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</h3>
<p><strong>Authors:</strong> Shaohua Wu, Tong Yu, Shenling Wang, Xudong Zhao</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 将Swin-transformer引入文本条件扩散模型，增强模型对长程语义的捕捉能力，提升文本到图像生成的语义一致性，属于扩散大模型的结构优化方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.16586' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</h3>
<p><strong>Authors:</strong> Giorgos Petsangourakis, Christos Sgouropoulos, Bill Psomas, Theodoros Giannakopoulos, Giorgos Sfikas, Ioannis Kakogeorgiou</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出REGLUE框架，将VAE潜码、局部视觉基础模型语义与全局[CLS]token联合建模，强化扩散模型的语义监督，提升生成内容的语义准确性。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.16636' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</h3>
<p><strong>Authors:</strong> Lulu Xue, Shengshan Hu, Linqiang Qian, Peijin Guo, Yechao Zhang, Minghui Li, Yanjun Zhang, Dayong Ye, Leo Yu Zhang</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 揭示机器遗忘在双视图设置下的隐私风险，提出DVIA攻击验证隐私泄露问题，对大模型安全有重要警示作用，属于大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16126' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</h3>
<p><strong>Authors:</strong> Barna P\'asztor, Thomas Kleine Buening, Andreas Krause</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Stackelberg学习框架用于偏好优化，解决RLHF局限性，提升对齐效果，属于大模型安全与对齐方向（alignment）。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16626' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> A Decision-Theoretic Approach for Managing Misalignment</h3>
<p><strong>Authors:</strong> Daniel A. Herrmann, Abinav Chari, Isabelle Qian, Sree Sharvesh, B. A. Levinstein</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出决策理论框架管理AI系统的错位问题，属于大模型安全与对齐研究，量化分析对齐、准确性和可达性的权衡，为理性 delegation提供依据。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.15584' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</h3>
<p><strong>Authors:</strong> Amita Kamath, Kai-Wei Chang, Ranjay Krishna, Luke Zettlemoyer, Yushi Hu, Marjan Ghazvininejad</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 针对文本到图像模型评估的基准漂移问题，构建GenEval 2基准并提出Soft-TIFA评估方法，提升评估与人类判断的对齐性，解决了大模型评估的核心挑战。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16853' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Feature-Selective Representation Misdirection for Machine Unlearning</h3>
<p><strong>Authors:</strong> Taozhao Chen, Linghan Huang, Kim-Kwang Raymond Choo, Huaming Chen</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出特征选择的表示误导方法用于机器遗忘，解决纠缠数据下的遗忘问题，提升大模型安全合规性，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16297' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Impacts of Racial Bias in Historical Training Data for News AI</h3>
<p><strong>Authors:</strong> Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 分析新闻AI模型中历史训练数据的种族偏见问题，属于大模型安全与对齐研究，揭示模型偏见对新闻应用的影响，提出改进方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</h3>
<p><strong>Authors:</strong> Zhiwei Li, Yitian Pang, Weining Wang, Zhenan Sun, Qi Li</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出轻量级测试时防御框架TTP，通过填充扰动检测对抗样本并自适应修复，有效提升视觉语言模型的对抗鲁棒性，属于大模型安全研究的关键方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16523' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DeContext as Defense: Safe Image Editing in Diffusion Transformers</h3>
<p><strong>Authors:</strong> Linghui Shen, Mingyue Cui, Xingyi Yang</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 针对扩散模型的图像编辑安全问题，通过扰动跨注意力路径阻断输入与输出的关联，有效防止未经授权的图像篡改，填补了扩散模型安全防御的空白。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16625' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Pixel Seal: Adversarial-only training for invisible image and video watermarking</h3>
<p><strong>Authors:</strong> Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Pixel Seal方法，通过对抗训练实现不可见图像/视频水印，解决生成内容的溯源问题，为大模型生成内容的安全管理提供了关键技术。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.16874' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DSO: Direct Steering Optimization for Bias Mitigation</h3>
<p><strong>Authors:</strong> Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出直接转向优化方法通过强化学习纠正模型偏差，实现可控的偏差缓解，属于大模型安全与对齐中的bias mitigation方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.15926' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions</h3>
<p><strong>Authors:</strong> Haishan Ye</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 研究基于秩的零阶优化算法的非渐近查询复杂度，解决现有理论仅渐近分析的问题，属于深度学习理论中的optimizer方向，具有重要理论价值。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16200' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</h3>
<p><strong>Authors:</strong> Sahil Rajesh Dhayalkar</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 从向量符号架构视角分析Transformer的注意力机制，属于深度学习理论中的网络架构研究，解释Transformer的推理能力及失败模式，有理论价值。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.14709' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> In-Context Multi-Operator Learning with DeepOSets</h3>
<p><strong>Authors:</strong> Shao-Ting Chiu, Aditya Nambiar, Ali Syed, Jonathan W. Siegel, Ulisses Braga-Neto</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出基于DeepOSets的多算子上下文学习方法，DeepOSets是非注意力、非自回归的神经架构，证明了其通用近似性，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16074' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Geometric Laplace Neural Operator</h3>
<p><strong>Authors:</strong> Hao Tang, Jiongyu Zhu, Zimeng Feng, Hao Li, Chao Li</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出几何拉普拉斯神经算子，解决非欧几何和非周期信号问题，拓展神经算子应用范围，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16409' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling</h3>
<p><strong>Authors:</strong> Lei Wang, Xin Tan, Mingwei Wang, Ying Zhang</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出KOSS选择性状态空间模型，解决现有SSM的上下文选择问题，属于深度学习理论中的网络架构方向，理论与实践结合。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16723' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Surrogate Neural Architecture Codesign Package (SNAC-Pack)</h3>
<p><strong>Authors:</strong> Jason Weitz, Dmitri Demler, Benjamin Hawks, Nhan Tran, Javier Duarte</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 聚焦神经架构搜索与硬件感知优化，解决现有方法难以准确优化真实硬件性能的问题，属于深度学习理论中的网络架构方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.15998' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sharpness-aware Federated Graph Learning</h3>
<p><strong>Authors:</strong> Ruiyu Li, Peige Zhao, Guangxia Li, Pengcheng Wu, Xingyu Gao, Zhiqiang Xu</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 将Sharpness-aware优化应用于联邦图学习，解决联邦学习中的数据异质性问题，属于深度学习理论中的optimizer方向，对优化器设计有启发。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16247' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data</h3>
<p><strong>Authors:</strong> Jialiang Wang, Xueyan Bao, Hao Wu</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出Sharpness-aware的二阶潜在因子模型，解决高维不完整数据的优化问题，属于深度学习理论中的optimizer方向，理论与实践结合。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16277' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</h3>
<p><strong>Authors:</strong> Hesham G. Moussa, Aroosa Hameed, Arashmid Akhavain</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出基于任务排序的方法缓解持续学习中的灾难性遗忘，属于深度学习理论中的优化策略研究，通过零-shot评分算法优化任务顺序，提升持续学习性能。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16871' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</h3>
<p><strong>Authors:</strong> Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 训练可扩展的端到端可解释性助手，属于深度学习可解释性研究，通过编码器-解码器架构提取概念，用于检测jailbreak、秘密提示等，有实际应用价值。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.15712' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic</h3>
<p><strong>Authors:</strong> Mélissa Tamine, Otmane Sakhi, Benjamin Heymann</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 利用语言模型算术高效计算Shapley值，解决LLM微调中的数据估值问题，属于深度学习可解释性中的Shapley值应用方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.15765' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</h3>
<p><strong>Authors:</strong> Vegard Flovik</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出SALVE框架通过稀疏自编码器发现潜在特征并进行向量编辑，实现神经网络的机制控制，属于深度学习可解释性中的white-box explanation方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.15938' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explainable AI in Big Data Fraud Detection</h3>
<p><strong>Authors:</strong> Ayush Jain, Rahul Kulkarni, Siyi Lin</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 研究可解释AI在大数据欺诈检测中的应用，涉及SHAP等可解释性方法，分析了可解释性方法在大规模场景下的优缺点，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.16037' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explaining the Reasoning of Large Language Models Using Attribution Graphs</h3>
<p><strong>Authors:</strong> Chase Walker, Rickard Ewetz</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出归因图框架解释LLM的推理过程，属于深度学习可解释性研究，解决现有归因方法忽略代际影响的问题，提升解释的忠实性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.15663' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</h3>
<p><strong>Authors:</strong> Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出AuditDM框架通过审计模型分歧发现并纠正MLLM的失败模式，生成可解释的弱点示例用于模型改进，属于深度学习可解释性中的模型诊断与理解方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.16921' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I</h3>
<p><strong>Authors:</strong> Seok-Hyun Ga, Chun-Yen Chang</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 分析LLM在科学推理中的认知局限（感知-认知差距、计算-概念差异等），属于深度学习可解释性研究，揭示模型推理弱点，为设计AI-resistant问题提供依据。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.15298' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</h3>
<p><strong>Authors:</strong> Dwip Dalal (University of Illinois at Urbana-Champaign), Utkarsh Mishra (University of Illinois at Urbana-Champaign), Narendra Ahuja (University of Illinois at Urbana-Champaign), Nebojsa Jojic (University of Illinois at Urbana-Champaign)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 针对具身智能体在真实世界导航中的知识密集型推理挑战，提出CityNav基准和Verbalization of Path（VoP）方法，利用MLLMs的web-scale知识提升导航成功率，方向匹配且问题具有实际意义。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.15933' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</h3>
<p><strong>Authors:</strong> Tin Stribor Sohn (University of Michigan), Maximilian Dillitzer (University of Michigan), Jason J. Corso (University of Michigan), Eric Sax (University of Michigan)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出训练-free的4D时空检索增强框架，为视觉语言模型（VLMs）添加结构化记忆，解决VLMs缺乏时空接地的问题，提升具身推理中的时空信息处理能力，实验验证优于基线。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.15940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</h3>
<p><strong>Authors:</strong> Tin Stribor Sohn (University of Michigan), Maximilian Dillitzer (University of Michigan), Jason J. Corso (University of Michigan), Eric Sax (University of Michigan)</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 提出SNOW框架，整合VLM语义、点云几何和时间一致性，实现4D场景理解，解决VLMs缺乏时空接地的问题，提升具身系统的开放世界推理能力，训练-free且backbone-agnostic。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.16461' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</h3>
<p><strong>Authors:</strong> Beitong Zhou, Zhexiao Huang, Yuan Guo, Zhangxuan Gu, Tianyu Xia, Zichen Luo, Fei Tang, Dehan Kong, Yanyi Shang, Suling Ou, Zhenlin Guo, Changhua Meng, Shuheng Shen</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Reason:</strong> 针对多模态智能体的核心任务GUI grounding，构建了首个覆盖多平台、多任务的基准VenusBench-GD，提供了系统的评估体系，直接支撑GUI智能体的研究。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.16501' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>