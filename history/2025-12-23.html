<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-23</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>原生多模态大模型</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >多模态智能体</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习可解释性</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-23</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-23 12:42:18</p>
<p>已自动阅读了 222 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：121020</p>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression</h3>
<p><strong>Authors:</strong> Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 基于信息论提出自适应视频tokenizer InfoTok，优化token分配以提升压缩效率与准确性，属于原生多模态大模型中的tokenizer关键技术研究。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.16975' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Xiaomi MiMo-VL-Miloco Technical Report</h3>
<p><strong>Authors:</strong> Jiaze Li (Xiaomi), Jingyang Chen (Xiaomi), Yuxun Qu (Xiaomi), Jianzhong Ju (Xiaomi), Zhenbo Luo (Xiaomi), Jian Luan (Xiaomi), Shijie Xu (Xiaomi), Zhenru Lin (Xiaomi), Junyou Zhu (Xiaomi), Boshen Xu (Xiaomi), Wenhui Tan (Xiaomi), Pei Fu (Xiaomi)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出面向家庭场景的多模态大模型MiMo-VL-Miloco，采用两阶段训练（监督微调+强化学习）、链思维监督和token预算感知推理等策略，平衡场景专业化与通用推理能力，性能优于主流开源/闭源基线，且开源模型与工具。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17436' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis</h3>
<p><strong>Authors:</strong> Qilong Wang (Tsinghua University), Xiaofan Ming (Tsinghua University), Zhenyi Lin (Tsinghua University), Jinwen Li (Tsinghua University), Dongwei Ren (Tsinghua University), Wangmeng Zuo (Tsinghua University), Qinghua Hu (Tsinghua University)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 构建RoomBench++数据集，提出参数共享的扩散架构RoomEditor++，解决室内家具合成的几何一致性与背景完整性问题，实验显示在定量指标、定性评估和人类偏好上均优于SOTA，且开源数据集与代码。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17573' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</h3>
<p><strong>Authors:</strong> Rang Li (Chinese Academy of Sciences), Lei Li (Chinese Academy of Sciences), Shuhuai Ren (Chinese Academy of Sciences), Hao Tian (Chinese Academy of Sciences), Shuhao Gu (Chinese Academy of Sciences), Shicheng Li (Chinese Academy of Sciences), Zihao Yue (Chinese Academy of Sciences), Yudong Wang (Chinese Academy of Sciences), Wenhan Ma (Chinese Academy of Sciences), Zhe Yang (Chinese Academy of Sciences), Jingyuan Ma (Chinese Academy of Sciences), Zhifang Sui (Chinese Academy of Sciences), Fuli Luo (Chinese Academy of Sciences)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 构建多维度基准GroundingME，系统评估MLLMs的视觉接地能力，发现现有模型存在严重的接地 gap（如拒答任务准确率近0%），提出测试时缩放与数据混合训练策略，为多模态大模型的安全对齐提供关键诊断工具。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17495' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</h3>
<p><strong>Authors:</strong> Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出4D-RGPT，通过感知蒸馏增强多模态大语言模型的4D（3D+时间）理解能力，构建区域级提示基准R4D-Bench，属于原生多模态大模型的时空推理研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17012' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</h3>
<p><strong>Authors:</strong> Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出ABE-CLIP，训练-free的属性绑定增强方法，通过语义细化与局部token-patch对齐提升CLIP的属性-对象关联能力，属于原生多模态大模型的多模态对齐研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17178' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</h3>
<p><strong>Authors:</strong> Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, Lin Ma</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出课程学习框架，分离逻辑推理与视觉感知能力，解决多模态大语言模型的“视觉遗忘”问题，属于原生多模态大模型的推理 grounding 研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17227' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</h3>
<p><strong>Authors:</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 优化表示编码器兼顾语义与重建，提升文本到图像生成与编辑性能，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17909' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</h3>
<p><strong>Authors:</strong> Min-Jung Kim, Jeongho Kim, Hoiyeong Jin, Junha Hyung, Jaegul Choo</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出InfCam，深度无关的相机控制视频生成框架，通过无限单应性扭曲提升 pose 保真度与视觉质量，属于原生多模态大模型的图像生成研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17040' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos</h3>
<p><strong>Authors:</strong> Henghui Du, Chang Zhou, Chunjie Zhang, Xi Chen, Di Hu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出Video Detective，通过问题感知记忆机制循环提取长视频关键信息，提升长视频问答效率，属于原生多模态大模型的长视频理解研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17229' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Vision-Language Model Guided Image Restoration</h3>
<p><strong>Authors:</strong> Cuixin Yang, Rongkang Dong, Kin-Man Lam</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出VLMIR，结合视觉语言模型的视觉-语言知识引导图像恢复，通过跨注意力融合多模态特征提升语义一致性，属于原生多模态大模型的多模态融合研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17292' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model</h3>
<p><strong>Authors:</strong> SuBeen Lee, GilHan Park, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出ADK，视觉语言模型的少样本适应框架，通过大语言模型生成描述性提示增强文本表示，提升分布外任务性能，属于原生多模态大模型的适应研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.17313' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</h3>
<p><strong>Authors:</strong> Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出ProCache，动态特征缓存框架加速扩散Transformer，通过约束感知缓存模式与选择性计算实现训练-free加速，属于高效大模型训练与推理的加速研究。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17298' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs</h3>
<p><strong>Authors:</strong> Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出GPA优化器改进DiLoCo，提升LLM训练速度（如Llama-160M提速24.22%），并提供收敛保证，符合高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17131' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Learning What to Write: Write-Gated KV for Efficient Long-Context Inference</h3>
<p><strong>Authors:</strong> Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出Write-Gated KV优化KV缓存，减少内存占用（46-57%）并提升长上下文推理速度（预填充3.03-3.45×），符合高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17452' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</h3>
<p><strong>Authors:</strong> Yikang Yue, Yishu Yin, Xuehai Qian</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出垂直调度和优化步骤重叠的方法，解决SSD卸载LLM训练的I/O瓶颈，显著提升吞吐量（GPT-65B在1 GPU上比ZeRO-Infinity快1.96倍），代码开源，对高效大模型训练有重要实用价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17570' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</h3>
<p><strong>Authors:</strong> Yichen Jiang (University of Waterloo), Mohammed Talha Alam (University of Waterloo), Sohail Ahmed Khan (University of Waterloo), Duc-Tien Dang-Nguyen (University of Waterloo), Fakhri Karray (University of Waterloo)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出AdaptPrompt框架，通过学习任务特定文本提示与视觉适配器，在冻结VLMs backbone的情况下实现参数高效适应，解决Deepfake检测的泛化问题，实验显示在25个测试集上优于基线。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17730' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models</h3>
<p><strong>Authors:</strong> Zhongpan Tang</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出“压缩即路由”理念，用重构误差作为模块路由信号，解决LLM上下文、成本与遗忘问题，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.16963' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation</h3>
<p><strong>Authors:</strong> Zhenyu Liu, Yunzhen Liu, Zehao Fan, Garrett Gagnon, Yayue Hou, Nan Wu, Yangwook Kang, Liu Liu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出低秩补偿的MoE模型，提升带宽效率与吞吐量，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17073' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mitigating Forgetting in Low Rank Adaptation</h3>
<p><strong>Authors:</strong> Joanna Sliwa, Frank Schneider, Philipp Hennig, Jose Miguel Hernandez-Lobato</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 针对LoRA的灾难性遗忘问题，提出LaLoRA用拉普拉斯近似正则化LoRA权重，平衡新知识学习与旧知识保留，在Llama模型的数学推理任务上验证了学习-遗忘权衡的改进，对参数高效微调有重要意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.17720' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning</h3>
<p><strong>Authors:</strong> Qi Song, Honglin Li, Yingchen Yu, Haoyi Zhou, Lin Yang, Song Bai, Qi She, Zilong Huang, Yunqing Zhao</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出CodeDance，通过代码编排工具的多模态大语言模型，实现可执行视觉推理，性能优于GPT-4o等模型，契合多模态智能体的工具使用与推理研究。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.17312' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> V-Agent: An Interactive Video Search System Using Vision-Language Models</h3>
<p><strong>Authors:</strong> SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出多代理平台V-Agent，结合视觉语言模型与检索模型实现视频搜索及用户交互，涉及多模态智能体的协作与视频内容理解，契合多模态智能体研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.16925' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</h3>
<p><strong>Authors:</strong> Mohammed Irfan Kurpath, Jaseel Muhammad Kaithakkodan, Jinxing Zhou, Sahal Shaji Mullappilly, Mohammad Almansoori, Noor Ahsan, Beknur Kalmakhanbet, Sambal Shikhar, Rishabh Lalla, Jean Lahoud, Mariette Awad, Fahad Shahbaz Khan, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出长视频多模态推理基准LongShOTBench及代理框架LongShOTAgent，支持多模态工具使用与对话，针对长视频理解的代理能力提升，符合多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.16978' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</h3>
<p><strong>Authors:</strong> Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出DAVE，专为文档理解与Web Agents设计的视觉语言模型编码器，提升结构与空间信息提取，支持Web Agents的GUI grounding与导航，符合多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.17221' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents</h3>
<p><strong>Authors:</strong> Yun He (University of California San Diego), Francesco Pittaluga (University of California San Diego), Ziyu Jiang (University of California San Diego), Matthias Zwicker (University of California San Diego), Manmohan Chandraker (University of California San Diego), Zaid Tasneem (University of California San Diego)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出LangDriveCTRL，通过多模态智能体实现自然语言控制的驾驶场景编辑，结合3D场景分解（静态背景+动态物体）与扩散模型，解决场景编辑的几何一致性与视觉连贯性问题，实验优于现有研究与商用模型。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.17445' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PAACE: A Plan-Aware Automated Agent Context Engineering Framework</h3>
<p><strong>Authors:</strong> Kamer Ali Yuksel</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 针对LLM智能体的多步计划感知上下文优化，提出PAACE框架通过任务相关性建模、计划结构分析和压缩，提升智能体正确性并减少上下文负载，在AppWorld、OfficeBench等基准上验证有效，对多模态智能体的上下文管理有重要贡献。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.16970' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reinforcement Learning for Self-Improving Agent with Skill Library</h3>
<p><strong>Authors:</strong> Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出SAGE框架用强化学习整合技能库，提升LLM智能体的自我改进能力，通过Sequential Rollout和Skill-integrated Reward优化技能学习与应用，在AppWorld上实现更高任务完成率和效率，对多模态智能体的持续学习有重要意义。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.17102' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images</h3>
<p><strong>Authors:</strong> Wenhao Yang, Yu Xia, Jinlong Huang, Shiyin Lu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Yuanyu Wan, Lijun Zhang</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出DRIM，多轮自反思推理的多模态大语言模型，通过工具调用解决视觉任务，符合多模态智能体的推理研究方向。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.17306' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Task Schema and Binding: A Double Dissociation Study of In-Context Learning</h3>
<p><strong>Authors:</strong> Chaeha Kim</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 揭示ICL的双机制（Task Schema与Binding），通过激活修补实验证明神经可分离性，为ICL机制研究提供因果证据，符合深度学习理论方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.17325' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dion2: A Simple Method to Shrink Matrix in Muon</h3>
<p><strong>Authors:</strong> Kwangjun Ahn, Noah Amsel, John Langford</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出简化Muon优化器矩阵收缩的方法，减少正交化计算成本，提升 scalability，符合深度学习理论的优化器方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.16928' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks</h3>
<p><strong>Authors:</strong> Salar Beigzad</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 针对Forward-Forward算法的层间隔离问题，提出协作式Forward-Forward学习框架（F-CFF和A-CFF），通过层间协作提升深层模型的收敛效率，在MNIST和Fashion-MNIST上验证了性能提升，对深度学习理论中的正向传播优化有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.17531' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Reasoning Meets Its Laws</h3>
<p><strong>Authors:</strong> Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出LoRe框架形式化大模型的推理行为（计算定律和精度定律），开发LoRe-Bench基准测量单调性和组合性，通过finetuning提升模型对计算定律的合规性，显著改进推理性能，对深度学习理论中的推理模型研究有重要推动作用。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.17901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</h3>
<p><strong>Authors:</strong> Tung-Ling Li, Yuhao Wu, Hongliang Liu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 发现LLM裁判系统的对抗脆弱性，通过控制令牌翻转二元决策，揭示奖励hacking风险并提出缓解方法，符合大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.17375' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</h3>
<p><strong>Authors:</strong> Lu Wei, Yuta Nakashima, Noa Garcia</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出EMMA，概念擦除基准，评估隐私、偏见等问题，涵盖多维度指标与类别，属于大模型安全与对齐的基准研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.17320' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adversarial Robustness of Vision in Open Foundation Models</h3>
<p><strong>Authors:</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 研究LLaVA与Llama 3.2 Vision的视觉模态对抗鲁棒性，发现视觉是攻击向量，为大模型安全提供实证 insights，符合大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.17902' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance</h3>
<p><strong>Authors:</strong> Ankit Yadav, Ta Duc Huy, Lingqiao Liu</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出EMAG，指数移动平均引导的扩散采样方法，提升生成质量与人类偏好得分，属于大模型新技术的扩散大模型研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.17303' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</h3>
<p><strong>Authors:</strong> Sarah Rastegar (Amazon), Violeta Chatalbasheva (Amazon), Sieger Falkena (Amazon), Anuj Singh (Amazon), Yanbo Wang (Amazon), Tejas Gokhale (Amazon), Hamid Palangi (Amazon), Hadi Jamali-Rad (Amazon)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出InfSplign，在推理时通过复合损失调整扩散模型的噪声，增强text-to-image的空间关系对齐（如物体位置与比例），实验显示在VISOR和T2I-CompBench上优于现有推理时方法甚至微调方法。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.17851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> InSPECT: Invariant Spectral Features Preservation of Diffusion Models</h3>
<p><strong>Authors:</strong> Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出在扩散模型正向和反向过程中保持不变谱特征的方法，提升生成质量、多样性与收敛速度，是扩散模型领域的创新尝试，符合大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.17873' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</h3>
<p><strong>Authors:</strong> Rujiao Long, Yang Li, Xingyao Zhang, Weixun Wang, Tianqianjin Lin, Xi Zhao, Yuchi Xu, Wenbo Su, Junchi Yan, Bo Zheng</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出Reasoning Palette，通过潜在变量引导大模型的推理策略，实现可控的推理探索，属于大模型新技术的推理控制研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.17206' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection</h3>
<p><strong>Authors:</strong> Zhaolin Cai (Zhejiang University), Fan Li (Zhejiang University), Ziwei Zheng (Zhejiang University), Haixia Bi (Zhejiang University), Lijun He (Zhejiang University)</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出HeadHunt-VAD，通过多标准分析（显著性+稳定性）筛选MLLM中的异常敏感注意力头，绕过文本生成直接利用视觉特征，解决现有方法的信息损失与 prompt敏感性问题，实验优于调优-free基线。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.17601' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</h3>
<p><strong>Authors:</strong> Kristoffer Wickström, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出无需训练将ViT转化为自解释模型的方法，通过关键点计数实现可解释决策，提升模型透明度，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.17891' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability</h3>
<p><strong>Authors:</strong> Michael Merry, Pat Riddle, Jim Warren</p>
<p><strong>Published:</strong> 2025-12-22</p>
<p><strong>Reason:</strong> 提出可解释性的可测试标准，用图论与注释解决可解释性缺乏统一定义的问题，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.17316' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>