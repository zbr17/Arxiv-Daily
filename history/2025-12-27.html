<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-12-27</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>深度学习理论</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-12-27</h1>
<div class='meta-info'><p>更新于北京时间：2025-12-27 12:29:34</p>
<p>已自动阅读了 31 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：21679</p>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA</h3>
<p><strong>Authors:</strong> Esmail Gumaan</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出动态选择注意力机制（MHA/GQA/MQA）的混合架构，通过学习路由器优化建模质量与推理效率的权衡，实验验证动态路由优于静态混合，属于深度学习理论中的网络架构改进方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.20650' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers</h3>
<p><strong>Authors:</strong> Yawei Liu</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出对抗性注意力反馈机制，通过动态 masking 策略与判别器博弈，引导模型重新分配注意力权重至任务相关词，解决Transformer过度关注常见词的问题，属于深度学习理论中的注意力机制改进方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.20661' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Safety Alignment of LMs via Non-cooperative Games</h3>
<p><strong>Authors:</strong> Anselm Paulus, Ilia Kulikov, Brandon Amos, R\'emi Munos, Ivan Evtimov, Kamalika Chaudhuri, Arman Zharmagambetov</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 将语言模型的安全对齐建模为攻击者-防御者的非合作游戏，通过在线强化学习联合训练，提升防御模型的有用性与抗攻击能力，属于大模型安全与对齐中的对齐方法方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.20806' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data</h3>
<p><strong>Authors:</strong> Aayam Bansal, Ishaan Gangwani</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出仅用100个策略选择的探针样本实现基础模型可靠性评估的方法，结合提示多样性、不确定性量化和自适应加权，解决传统评估数据量大、成本高的问题，属于大模型安全与对齐中的可靠性评估方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.20630' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents</h3>
<p><strong>Authors:</strong> Miles Q. Li, Benjamin C. M. Fung, Martin Weiss, Pulei Xiong, Khalil Al-Hussaeni, Claude Fachkha</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出评估自主AI智能体结果驱动约束违反的基准，包含40个多步骤场景，分析12个SOTA LLM的违反率，揭示推理能力与安全性的非正相关关系，属于大模型安全与对齐中的对齐评估方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.20798' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Context: Large Language Models Failure to Grasp Users Intent</h3>
<p><strong>Authors:</strong> Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 实证分析ChatGPT、Claude等SOTA LLM无法理解用户意图的安全漏洞，揭示情感框架、渐进式披露等攻击方法的有效性，指出当前安全机制忽视意图理解的局限性，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.21110' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning</h3>
<p><strong>Authors:</strong> Leo Lu, Jonathan Zhang, Sean Chua, Spencer Kim, Kevin Zhu, Sean O'Brien, Vasu Sharma</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 研究不同LLM之间推理链的互换性，通过截断基线模型推理链并让其他模型继续，评估推理的稳定性与连贯性，涉及LLM推理的trustworthiness，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.20647' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AIAuditTrack: A Framework for AI Security system</h3>
<p><strong>Authors:</strong> Zixun Luo, Yuhang Fan, Yufei Li, Youzhi Zhang, Hengyu Lin, Ziqi Wang</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出基于区块链的AI安全框架，利用DID和VC建立可信AI实体，通过链上记录交互轨迹实现跨系统监督与审计，并设计风险扩散算法溯源危险行为，属于大模型安全与对齐中的安全治理方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.20649' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Bridging the AI Trustworthiness Gap between Functions and Norms</h3>
<p><strong>Authors:</strong> Daan Di Scala, Sophie Lathouwers, Michael van Bekkum</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 分析功能型AI（FTAI）与规范型AI（NTAI）之间的可信度差距，提出语义语言作为桥梁，帮助开发者将规范转化为具体实现，属于大模型安全与对齐中的可信度评估方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.20671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation</h3>
<p><strong>Authors:</strong> Tomoaki Yamaguchi, Yutong Zhou, Masahiro Ryo, Keisuke Katsura</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出结合SHAP可解释性与多模态LLM迭代优化的Agentic XAI框架，通过11轮细化提升解释的特异性、清晰性等指标，验证其在农业推荐系统中的有效性，属于深度学习可解释性方向。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.21066' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization</h3>
<p><strong>Authors:</strong> Ravi Gupta, Shabista Haider</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出1-bit量化大语言模型与深度Q网络结合的框架，用于边缘设备智能家庭照明控制，实现71.4倍能耗降低和32%节能，同时保持92%任务准确率，属于高效大模型训练与推理中的模型压缩方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.20623' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation</h3>
<p><strong>Authors:</strong> Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出结合多模态知识图谱的检索增强生成框架，将视觉线索整合到知识图谱构建、检索和生成过程，解决传统RAG仅处理文本的局限，提升多模态内容理解与推理能力，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.20626' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</h3>
<p><strong>Authors:</strong> Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Reason:</strong> 提出基于混合推理（反向反射推理+正向预测推理）的安全框架，保护视觉语言模型驱动的具身智能体，减少危险行为，实验验证其在物理机器人上的实用性，属于多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.21220' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>