<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-02</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型新技术</a>
<a href='#' >多模态智能体</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习可解释性</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-02</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-02 12:46:56</p>
<p>已自动阅读了 264 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：145578</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Pretraining Frame Preservation in Autoregressive Video Memory Compression</h3>
<p><strong>Authors:</strong> Lvmin Zhang, Shengqu Cai, Muyang Li, Chong Zeng, Beijia Lu, Anyi Rao, Song Han (MIT), Gordon Wetzstein (Stanford), Maneesh Agrawala (UC Berkeley)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出PFP结构压缩长视频为短上下文，降低长历史记忆成本，作者含高效推理专家Song Han，对长视频大模型高效推理有价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.23851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention</h3>
<p><strong>Authors:</strong> Aiyue Chen, Yaofu Liu, Junjian Huang, Guang Lian, Yiwu Yao, Wangli Lan, Jing Lin, Zhixin Ma, Tingting Zhou, Harry Yang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出硬件高效块级稀疏注意力，加速扩散Transformer生成（80% sparsity下1.5~1.8X speedup），对大模型高效推理有重要价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24086' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding</h3>
<p><strong>Authors:</strong> Yue Guan, Changming Yu, Shihan Fang, Weiming Hu, Zaifeng Pan, Zheng Wang, Zihan Liu, Yangjie Zhou, Yufei Ding, Minyi Guo, Jingwen Leng</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出上下文感知的树结构 drafting 与编译器友好执行框架，解决动态投机与静态运行时的不匹配问题，实现LLM延迟最优解码，显著提升推理速度。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.23858' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Recursive Language Models</h3>
<p><strong>Authors:</strong> Alex L. Zhang, Tim Kraska, Omar Khattab</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出递归语言模型RLMs，通过推理时递归处理长上下文，解决长文本输入问题，在多个长上下文任务中显著优于基线，且成本相当或更低，属于高效大模型训练与推理的重要方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24601' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice</h3>
<p><strong>Authors:</strong> Jiachen T. Wang, Tong Wu, Kaifeng Lyu, James Zou, Dawn Song, Ruoxi Jia, Prateek Mittal</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 研究小模型实验对大模型数据筛选的可靠性问题，发现固定配置实验的结论偏差，提出调整学习率的改进方案，显著提升小样本实验与大模型训练的相关性，是高效大模型训练中数据优化的重要工作。
Score: 8.5
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24503' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization</h3>
<p><strong>Authors:</strong> Yuma Ichikawa, Yoshihiko Fujisawa, Yudai Fujimoto, Akira Sakai, Katsuki Fujisawa</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 改进双二进制因子分解（DBF）为多包络DBF（MDBF），通过共享符号矩阵提升极端量化（如二进制）的LLM性能，同时保持部署友好的推理原语，是高效大模型压缩的关键进展。
Score: 8.5
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24545' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference</h3>
<p><strong>Authors:</strong> Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出FPGA硬件-软件协同设计方案，支持N:M稀疏与低比特量化的LLM推理，显著提升推理速度与吞吐量，是高效大模型推理硬件优化的重要成果。
Score: 8.5
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24713' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing</h3>
<p><strong>Authors:</strong> Mustafa Munir, Md Mostafijur Rahman, Kartikeya Bhardwaj, Paul Whatmough, Radu Marculescu (Carnegie Mellon University)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出流水线处理和运动感知帧选择，大幅提升长视频编辑速度（最高31.7X），对长视频大模型高效推理有价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24026' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers</h3>
<p><strong>Authors:</strong> Yonglak Son, Suhyeok Kim, Seungryong Kim, Young Geun Kim</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出贡献引导块级缓存，无训练加速扩散Transformer推理（最高2.0X speedup），对大模型高效推理有价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24195' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers</h3>
<p><strong>Authors:</strong> Zheng Liu, Jinchao Zhu, Gao Huang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 改进LoRA方法，通过共享基空间与多样性增强平衡参数效率与性能，属于高效大模型训练与推理中的低秩适应微调方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24603' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation</h3>
<p><strong>Authors:</strong> Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出阶段感知多模型采样策略，在保持视频质量的同时提升推理速度，属于高效大模型训练与推理中的推理加速方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24724' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Trellis: Learning to Compress Key-Value Memory in Attention Models</h3>
<p><strong>Authors:</strong> Mahdi Karami, Ali Behrouz, Praneeth Kacham, Vahab Mirrokni</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出Trellis架构，通过固定大小内存与递归压缩机制动态压缩注意力模型的KV缓存，提升长上下文序列推理效率，直接针对高效大模型推理的核心问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.23852' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficiently Estimating Data Efficiency for Language Model Fine-tuning</h3>
<p><strong>Authors:</strong> Gyung Hyun Je, Colin Raffel</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出基于低置信度样本梯度余弦相似性的方法，估计LLM微调的数据效率，减少不必要的标注成本，是高效大模型训练中数据优化的实用技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24991' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Structure-Guided Allocation of 2D Gaussians for Image Representation and Compression</h3>
<p><strong>Authors:</strong> Huanxiong Liang, Yunuo Chen, Yicheng Pan, Sixian Wang, Jincheng Dai, Guo Lu, Wenjun Zhang (Shanghai Jiao Tong University)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出结构引导2D高斯分配，提升图像压缩率失真性能并保持高效解码，对大模型图像高效表示有价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24018' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation</h3>
<p><strong>Authors:</strong> Siyang Wang, Hanting Li, Wei Li, Jie Hu, Xinghao Chen, Feng Zhao</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 重新排序自回归生成顺序，采用径向拓扑与嵌套注意力提升视觉生成推理效率，属于高效大模型训练与推理中的推理加速方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24639' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining</h3>
<p><strong>Authors:</strong> Ruizhe Huang, Kexuan Zhang, Yihao Fang, Baifeng Yu</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 研究Infini-attention在小模型预训练中的压缩内存效果，提升长上下文外推能力，为小模型高效训练提供实证依据，属于高效大模型训练范畴。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.23862' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning</h3>
<p><strong>Authors:</strong> Xusheng Zhang, Tuan Nguyen, Ting He</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出时变混合矩阵设计框架，通过多阶段拓扑激活平衡迭代能耗与收敛速度，优化去中心化联邦学习的能量效率，属于高效大模型训练范畴。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24069' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization</h3>
<p><strong>Authors:</strong> Advait Gadhikar, Riccardo Grazzi, James Hensman</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出数据无关的旋转方法（OptRot）缓解LLM权重异常值，提升后训练量化效果，属于高效大模型推理中的压缩优化。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2512.24124' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design</h3>
<p><strong>Authors:</strong> Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte (ETH Zurich)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出少样本提示和高效验证方法，用LLM生成神经网络架构，对深度学习理论中的自动架构设计有重要价值。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24120' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Generalising E-prop to Deep Networks</h3>
<p><strong>Authors:</strong> Beren Millidge</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 将E-prop（RTRL的近似方法）扩展到深度网络，实现在线前向梯度传播，避免BPTT的历史存储与反向回放，解决深度循环网络的优化效率问题，是深度学习理论中优化方法的重要扩展。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24506' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Nested Learning: The Illusion of Deep Learning Architectures</h3>
<p><strong>Authors:</strong> Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出嵌套学习范式，将模型表示为多层嵌套优化问题，解决持续学习、自改进与记忆能力的核心挑战，是深度学习理论中架构与学习机制的根本性创新。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24695' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Gradient Descent as Implicit EM in Distance-Based Neural Models</h3>
<p><strong>Authors:</strong> Alan Oursland</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 证明距离基神经模型的梯度下降等价于隐式EM算法，统一优化与推理过程，揭示了深度学习优化的概率本质，是深度学习理论中优化方法的重要理论进展。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24780' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback</h3>
<p><strong>Authors:</strong> Shulun Chen, Runlong Zhou, Zihan Zhang, Maryam Fazel, Simon S. Du</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 研究零和游戏中无正则化OMWU算法的线性收敛性，解决偏好反馈中的对齐问题，为大模型对齐的优化算法提供了理论保障，属于深度学习理论中的优化收敛分析。
Score: 8.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24818' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Generative Classifiers Avoid Shortcut Solutions</h3>
<p><strong>Authors:</strong> Alexander C. Li, Ananya Kumar, Deepak Pathak</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 证明生成式分类器通过建模完整数据分布避免shortcut依赖，提升分布外鲁棒性，揭示了生成式模型在泛化能力上的优势，属于深度学习理论中的分类器设计与泛化分析。
Score: 8.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.25034' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Mechanics of CNN Filtering with Rectification</h3>
<p><strong>Authors:</strong> Liam Frija-Altrac, Matthew Toews</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 基于正交分解与光谱分析研究CNN整流滤波的信息处理机制，属于深度学习理论中的网络结构分析方向，有助于理解CNN工作原理。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24338' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Renormalization Group Guided Tensor Network Structure Search</h3>
<p><strong>Authors:</strong> Maolin Wang, Bowen Yu, Sheng Zhang, Linjie Mi, Wanyu Wang, Yiqi Wang, Pengyue Jia, Xuetao Wei, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 用重整化群指导张量网络结构搜索，解决单尺度优化与离散搜索问题，属于深度学习理论中的网络架构搜索方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24663' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling</h3>
<p><strong>Authors:</strong> Mahdi Karami, Ali Behrouz, Peilin Zhong, Razvan Pascanu, Vahab Mirrokni</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出多尺度状态空间模型（MS-SSM），通过多分辨率处理序列动态并引入输入依赖的尺度融合器，改进传统SSM的长程依赖与内存效率问题，属于深度学习理论中的网络架构创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.23824' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks</h3>
<p><strong>Authors:</strong> Peter Farag</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出SPM结构替代稠密线性层，通过分阶段成对混合实现近线性训练复杂度，同时保留 compositional 归纳偏置，属于深度学习理论中的网络架构创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.23905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improved Balanced Classification with Theoretically Grounded Loss Functions</h3>
<p><strong>Authors:</strong> Corinna Cortes, Mehryar Mohri, Yutao Zhong</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出GLA与GCA损失函数，理论分析其贝叶斯一致性与H-一致性，解决不平衡分类中的损失函数设计问题，属于深度学习理论中的损失函数研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.23947' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</h3>
<p><strong>Authors:</strong> Akash Samanta, Sheldon Williamson</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出基于误差分解（偏差、噪声、对齐）的自适应学习框架，通过在线诊断误差结构动态调整学习策略，解决非平稳环境下优化与RL的稳定性问题，属于深度学习理论中自适应优化机制的关键创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24445' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the geometry and topology of representations: the manifolds of modular addition</h3>
<p><strong>Authors:</strong> Gabriela Moisescu-Pareja, Gavin McCracken, Harley Wiltzer, Vincent L\'etourneau, Colin Daniels, Doina Precup, Jonathan Love</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 研究模块化加法任务的表示流形，揭示不同架构（如Transformer、RNN）的表示等价性，通过拓扑工具分析表示的几何结构，是深度学习理论中表示学习的重要几何分析工作。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.25060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OCP-LS: An Efficient Algorithm for Visual Localization</h3>
<p><strong>Authors:</strong> Jindi Zhong, Hongxia Wang, Huanshui Zhang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出新的二阶优化算法，结合OCP方法与Hessian近似，提升深度学习大规模优化的收敛速度与稳定性，属于深度学习理论中的优化器方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24552' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A Comprehensive Study of Deep Learning Model Fixing Approaches</h3>
<p><strong>Authors:</strong> Hanmo You, Zan Wang, Zishuo Dong, Luanqi Mo, Jianjun Zhao, Junjie Chen</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 对16种先进深度学习模型修复方法（覆盖模型级、层级、神经元级）开展大规模实证研究，分析其修复效果及对鲁棒性、公平性等关键属性的影响，为模型修复的理论与应用提供重要参考，属于深度学习理论范畴。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.23745' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Flow Matching Neural Processes</h3>
<p><strong>Authors:</strong> Hussen Abu Hamad, Dan Rosenbaum</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 将流匹配（Flow Matching）引入神经过程，提出简单易实现的生成模型，支持条件采样与精度-速度权衡，属于深度学习理论中的生成模型方法创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.23853' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias</h3>
<p><strong>Authors:</strong> Xia Chen</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 通过相空间分析揭示时间动态作为归纳偏置对泛化的作用，指出 dissipative 动力学促进特征抽象，属于深度学习理论中的归纳偏置研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.23916' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study</h3>
<p><strong>Authors:</strong> Yves Ruffenach</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 实证分析GP-VAE语言模型中 latent 自回归的作用，揭示其对长程稳定性与 latent 结构的影响，属于深度学习理论中的语言模型结构研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2512.24102' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</h3>
<p><strong>Authors:</strong> Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng (Microsoft)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出DiffThinker将多模态推理转化为扩散生成，大幅提升视觉-centric推理性能（超GPT-5 314.2%），属于大模型新技术研究。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.24165' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> GARDO: Reinforcing Diffusion Models without Reward Hacking</h3>
<p><strong>Authors:</strong> Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 针对扩散模型强化学习中的奖励黑客与模式崩溃问题，提出GARDO框架，通过选择性正则化、自适应参考模型更新和多样性奖励放大，平衡样本效率与探索能力，有效提升生成多样性与对齐性，是扩散大模型优化的重要进展。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.24138' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</h3>
<p><strong>Authors:</strong> Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出动态大概念模型（DLCM），将计算从token级转移到自适应概念空间，通过分层压缩与参数化优化提升大模型的效率与零-shot性能，是大模型架构创新的重要方向。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.24617' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Diffusion Language Models are Provably Optimal Parallel Samplers</h3>
<p><strong>Authors:</strong> Haozhe Jiang, Nika Haghtalab, Lijie Chen</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 从理论上证明扩散语言模型是最优并行采样器，提出修订机制进一步优化空间复杂度，为扩散大模型的高效推理提供了理论基础，是大模型新技术的重要理论进展。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.25014' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Many Minds from One Model: Bayesian Transformers for Population Intelligence</h3>
<p><strong>Authors:</strong> Diji Yang, Yi Zhang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出贝叶斯Transformer（B-Trans），将LLM转化为贝叶斯模型，通过采样多样模型实例提升群体决策能力，是大模型新技术中贝叶斯方法的创新应用。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.25063' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On Exact Editing of Flow-Based Diffusion Models</h3>
<p><strong>Authors:</strong> Zixiang Li, Yue Song, Jianing Peng, Ting Liu, Jun Huang, Xiaochao Qu, Luoqi Liu, Wei Wang, Yao Zhao, Yunchao Wei (Beihang University)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出CVC框架解决流基扩散模型编辑的轨迹漂移问题，提升编辑 fidelity，属于扩散大模型新技术研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.24015' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Guiding a Diffusion Transformer with the Internal Dynamics of Itself</h3>
<p><strong>Authors:</strong> Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu (The Chinese University of Hong Kong)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 利用扩散Transformer内部动态引导生成，提升生成质量（SiT-XL/2+IG达FID=5.31），属于扩散Transformer优化的新技术研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.24176' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Iterative Deployment Improves Planning Skills in LLMs</h3>
<p><strong>Authors:</strong> Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出迭代部署提升LLM规划能力，理论连接RL，发现迭代部署可显著改善规划技能，具有AI安全和训练机制的启示，属于大模型新技术的重要探索。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2512.24940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ShowUI-$\pi$: Flow-based Generative Models as GUI Dexterous Hands</h3>
<p><strong>Authors:</strong> Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出Flow模型实现GUI的灵巧操作（离散点击+连续拖拽），构建ScreenDrag基准，直接对应多模态智能体中的GUI Agent方向。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.24965' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations</h3>
<p><strong>Authors:</strong> Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出多模态智能体RSAgent，通过多轮工具调用实现文本引导分割，融合推理与行动，对多模态智能体研究有价值。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.24023' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 构建多模态代理推理框架，通过强化学习增强VLMs的工具调用与视觉推理能力，设计HR-MMSearch基准，属于多模态智能体中的代理推理方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.24330' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</h3>
<p><strong>Authors:</strong> Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出Youtu-Agent框架，通过自动化生成和混合政策优化提升代理生产力，解决现有代理框架配置成本高和能力静态的问题，实验在WebWalkerQA和GAIA上表现优异，属于多模态智能体的关键改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.24615' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots</h3>
<p><strong>Authors:</strong> Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出统一框架实现人形机器人多模态指令下的实时运动生成与动作流，属于多模态智能体中的机器人代理方向，解决高延迟问题并提升响应能力。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.24321' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents</h3>
<p><strong>Authors:</strong> Xunyi Zhao, Gengze Zhou, Qi Wu</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 构建基准评估MLLMs作为视觉导航代理的能力，分析其空间推理与序列决策局限性，属于多模态智能体中的视觉导航方向。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2512.24851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</h3>
<p><strong>Authors:</strong> Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke H\"ullermeier</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出ResponseRank方法，利用偏好强度信号提升奖励模型的样本效率，通过局部分层比较解决强度信号的噪声问题，是大模型对齐中偏好学习的关键优化。
Score: 8.5
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.25023' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models</h3>
<p><strong>Authors:</strong> Rahul Baxi</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出DDFT协议，通过语义压缩与对抗伪造测量LLM的认知鲁棒性，发现模型大小与鲁棒性无关，为大模型安全与对齐中的鲁棒性评估提供了标准方法。
Score: 8.5
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.23850' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models</h3>
<p><strong>Authors:</strong> Changzhen Li, Yuecong Min, Jie Zhang, Zheng Yuan, Shiguang Shan (Institute of Automation, Chinese Academy of Sciences), Xilin Chen (Institute of Automation, Chinese Academy of Sciences)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 针对文本到视频扩散模型提出两种对抗攻击，揭示其语义和 temporal 漏洞，对大模型安全与对齐研究有参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.23953' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning</h3>
<p><strong>Authors:</strong> Chubin Chen, Sujie Hu, Jiashu Zhu, Meiqi Wu, Jintao Chen, Yanxun Li, Nisha Huang, Chengyu Fang, Jiahong Wu, Xiangxiang Chu (Zhejiang University), Xiu Li</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出D²-Align解决扩散强化学习的偏好模式崩溃，提升生成多样性和质量，对大模型安全与对齐（偏好对齐）有价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.24146' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</h3>
<p><strong>Authors:</strong> Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu (Zhejiang University), Sheng Lu, Haoqian Wang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出反事实视频生成框架DualityForge，减少MLLM视频理解幻觉（Qwen2.5-VL-7B提升24.0%），对大模型安全与对齐（幻觉抑制）有价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.24271' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improved Bounds for Private and Robust Alignment</h3>
<p><strong>Authors:</strong> Wenqian Weng, Yi He, Xingyu Zhou</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 从理论角度建立了私人与鲁棒对齐的次优性边界，覆盖离线与在线场景，直接关联大模型安全与对齐的核心问题，为对齐算法设计提供理论指导。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.23816' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</h3>
<p><strong>Authors:</strong> Honglin Gao, Lan Zhao, Junhao Ren, Xiang Li, Gaoxi Xiao</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 针对异构图神经网络的后门攻击问题，提出HeteroHBA框架，通过生成式触发节点与特征合成提升攻击成功率与隐蔽性，是大模型安全领域中异构图攻击的关键研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.24665' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment</h3>
<p><strong>Authors:</strong> Natchaya Temyingyong, Daman Jain, Neeraj Kumarsahu, Prabhat Kumar, Rachata Phondi, Wachiravit Modecrua, Krittanon Kaewtawee, Krittin Pachtrachai, Touchapon Kraisingkorn</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 针对零样本代理对齐问题，提出反射优化框架ROAD，通过多代理架构将故障日志转化为决策树协议，提升样本效率和代理性能，实验在学术基准和生产环境中均有显著提升，与大模型安全与对齐方向高度相关。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.24040' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment</h3>
<p><strong>Authors:</strong> Lijun Zhang, Lin Li, Wei Wei, Yajie Qi, Huizhong Song, Jun Wang, Yaodong Yang, Jiye Liang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出风险感知逐步对齐方法RSA，将安全对齐转化为token级风险感知约束优化，解决现有方法风险中性的局限，实验提升安全性和抑制高影响有害行为，属于大模型安全与对齐的关键问题。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2512.24263' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Factorized Learning for Temporally Grounded Video-Language Models</h3>
<p><strong>Authors:</strong> Wenzheng Zeng, Difei Gao, Mike Zheng Shou (National University of Singapore), Hwee Tou Ng (National University of Singapore)</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出分解学习框架解决视频语言模型时间接地问题，通过“grounding then answering”提升事件级感知，属于原生多模态大模型研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.24097' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation</h3>
<p><strong>Authors:</strong> Yijie Qian, Juncheng Wang, Yuxiang Feng, Chao Xu, Wang Lu, Yang Liu, Baigui Sun, Yiqiang Chen, Yong Liu, Shujun Wang</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出Latent Motion Reasoning框架解决文本到运动的语义-运动不匹配，通过双粒度tokenizer提升生成质量，属于原生多模态大模型研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.24100' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme</h3>
<p><strong>Authors:</strong> Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 针对VLM的多模态幽默生成任务，提出HUMOR框架，通过分层CoT推理与群体偏好对齐，提升模型对视觉内容与幽默逻辑的理解能力，是原生多模态大模型在复杂场景下的重要应用优化。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.24555' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks</h3>
<p><strong>Authors:</strong> Shota Suzuki, Satoshi Ono</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出自监督神经架构搜索方法，用于多模态深度网络的自动设计，无需标注数据即可优化多模态融合结构，是原生多模态大模型架构优化的关键技术。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2512.24793' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns</h3>
<p><strong>Authors:</strong> Haoyue Bai, Yiyou Sun, Wenjie Hu, Shi Qiu, Maggie Ziyu Huan, Peiyang Song, Robert Nowak, Dawn Song</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 将LLM推理分解为原子核心技能（计算、事实检索等），细粒度分析泛化从认知行为到低层次模式的机制，属于深度学习可解释性的深入研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.24063' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Attribution-Guided Distillation of Matryoshka Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Cristina P. Martin-Linares, Jonathan P. Ling</p>
<p><strong>Published:</strong> 2026-01-01</p>
<p><strong>Reason:</strong> 提出归因引导的蒸馏方法，改进Matryoshka稀疏自编码器的特征一致性与可解释性，通过归因分析优化特征蒸馏过程，是深度学习可解释性与模型压缩结合的重要工作。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2512.24975' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>