# ArXiv 每日推荐 - 2026-01-07

> 更新于北京时间：2026-01-07 12:49:46
> 已自动阅读了 399 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：203106

## 原生多模态大模型

### [Score: 9.0/10] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation
- **Authors:** Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02204](https://arxiv.org/abs/2601.02204)
- **Reason:** 提出统一自回归Transformer用于文本-图像多模态理解与生成，实现原生多模态能力及高效图像生成，相关工作具有较强创新性与应用价值。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] A unified multimodal understanding and generation model for cross-disciplinary scientific research
- **Authors:** Xiaomeng Yang (), Zhiyu Tan (), Xiaohui Zhong (), Mengping Yang (), Qiusheng Huang (), Lei Chen (), Libo Wu (), Hao Li ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01363](https://arxiv.org/abs/2601.01363)
- **Reason:** 提出原生多模态大模型FuXi-Uni，统一跨学科科学数据的理解与生成，支持天气预测、生物医学等任务，属于原生多模态大模型的应用与架构研究。
Score: 9
Field: 原生多模态大模型

### [Score: 8.0/10] NarrativeTrack: Evaluating Video Language Models Beyond the Frame
- **Authors:** Hyeonjeong Ha, Jinjin Ge, Bo Feng, Kaixin Ma, Gargi Chakraborty
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01095](https://arxiv.org/abs/2601.01095)
- **Reason:** 提出首个评估多模态大模型叙事理解的基准NarrativeTrack，通过实体中心推理框架分析模型在 temporal dynamics 和 entity continuity 上的表现，揭示了感知接地与时间推理的关键权衡，对原生多模态大模型的叙事能力研究有重要推动作用。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Causality-Aware Temporal Projection for Video Understanding in Video-LLMs
- **Authors:** Zhengjian Kang, Qi Chen, Rui Liu, Kangtong Mo, Xingyu Zhang, Xiaoyu Deng, Ye Zhang
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01804](https://arxiv.org/abs/2601.01804)
- **Reason:** 针对Video-LLM的时间顺序与因果一致性缺陷，提出V-CORE框架（含因果感知时间投影器），通过4位QLoRA实现高效训练，显著提升时间与因果推理能力，对原生多模态大模型的视频理解有重要贡献
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation
- **Authors:** Weijian Ma, Shizhao Sun, Tianyu Yu, Ruiyu Wang, Tat-Seng Chua, Jiang Bian
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01984](https://arxiv.org/abs/2601.01984)
- **Reason:** 针对视觉语言模型的空间推理缺陷，提出基于结构化蓝图表示的框架，结合蓝图嵌入、强化学习奖励与反捷径数据增强，显著提升空间语义理解能力，对原生多模态大模型的推理改进有价值
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] VINO: A Unified Visual Generator with Interleaved OmniModal Context
- **Authors:** Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02358](https://arxiv.org/abs/2601.02358)
- **Reason:** 提出统一视觉生成框架，通过 interleaved 多模态上下文实现图像/视频的生成与编辑，强化指令跟随与参考保留能力，属于原生多模态大模型的核心研究方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] HyperCLOVA X 8B Omni
- **Authors:** NAVER Cloud HyperCLOVA X Team
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01792](https://arxiv.org/abs/2601.01792)
- **Reason:** 提出8B规模的全模态模型，支持文本、音频、视觉的any-to-any输入输出，通过共享token预测接口统一模态，实验验证多模态任务竞争力，开源模型推动原生多模态大模型研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] DatBench: Discriminative, Faithful, and Efficient VLM Evaluations
- **Authors:** Siddharth Joshi (), Haoli Yin (), Rishabh Adiga (), Ricardo Monti (), Aldo Carranza (), Alex Fang (), Alvin Deng (), Amro Abbas (), Brett Larsen (), Cody Blakeney (), Darren Teh (), David Schwab (), Fan Pan (), Haakon Mongstad (), Jack Urbanek (), Jason Lee (), Jason Telanoff (), Josh Wills (), Kaleigh Mentzer (), Luke Merrick (), Parth Doshi (), Paul Burstein (), Pratyush Maini (), Scott Loftin (), Spandan Das (), Tony Jiang (), Vineeth Dorna (), Zhengping Wang (), Bogdan Gaza (), Ari Morcos (), Matthew Leavitt ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02316](https://arxiv.org/abs/2601.02316)
- **Reason:** 针对多模态大模型（VLM）评估的缺陷，提出DatBench基准 suite，通过转换与过滤提升评估的忠实性与区分性，属于原生多模态大模型的评估方法研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications
- **Authors:** YuanLab. ai, :, Shawn Wu (), Sean Wang (), Louie Li (), Darcy Chen (), Allen Wang (), Jiangang Luo (), Xudong Zhao (), Joseph Shen (), Gawain Ma (), Jasper Jia (), Marcus Mao (), Claire Wang (), Hunter He (), Carol Wang (), Zera Zhang (), Jason Wang (), Chonly Shen (), Leo Zhang (), Logan Chen (), Qasim Meng (), James Gong (), Danied Zhao (), Penn Zheng (), Owen Zhu (), Tong Yu ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01718](https://arxiv.org/abs/2601.01718)
- **Reason:** 发布企业级开源多模态大模型Yuan3.0 Flash，采用MoE架构提升性能与效率，支持复杂任务如RAG、表格理解，属于原生多模态大模型的工程化研究。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] Improving Flexible Image Tokenizers for Autoregressive Image Generation
- **Authors:** Zixuan Fu, Lanqing Guo, Chong Wang, Binbin Song, Ding Liu, Bihan Wen
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01535](https://arxiv.org/abs/2601.01535)
- **Reason:** 针对自回归图像生成中灵活tokenizer的信息集中问题，提出ReToK（冗余token填充+分层语义正则化），提升token利用效率和生成性能，属于原生多模态大模型中的tokenizer与图像生成方向。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation
- **Authors:** Haonan Cai, Yuxuan Luo, Zhouhui Lian
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01593](https://arxiv.org/abs/2601.01593)
- **Reason:** 提出GAR-Font框架，结合global-aware tokenizer与多模态风格编码器（视觉+语言指导），提升少样本字体生成的全局风格一致性，属于原生多模态大模型中的multimodal与图像生成方向。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing
- **Authors:** Xijie Huang, Chengming Xu, Donghao Luo, Xiaobin Hu, Peng Tang, Xu Peng, Jiangning Zhang, Chengjie Wang, Yanwei Fu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01720](https://arxiv.org/abs/2601.01720)
- **Reason:** 构建FFP-300K大规模视频编辑数据集，提出guidance-free FFP框架（AST-RoPE+自蒸馏），提升视频编辑的 temporal stability与语义一致性，属于原生多模态大模型中的图像/视频生成方向。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement
- **Authors:** Guangqian Guo, Aixi Ren, Yong Guo, Xuehui Yu, Jiacheng Tian, Wenli Li, Yaoxing Wang, Shan Gao
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02018](https://arxiv.org/abs/2601.02018)
- **Reason:** 针对Segment Anything Model在低质量图像上的性能下降问题，提出GleSAM++框架（含生成 latent 增强与退化感知自适应机制），显著提升分割鲁棒性，对多模态大模型的图像理解有实际应用价值
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] VIBE: Visual Instruction Based Editor
- **Authors:** Grigorii Alekseenko, Aleksandr Gordeev, Irina Tolstykh, Bulat Suleimanov, Vladimir Dokholyan, Georgii Fedorov, Sergey Yakubson, Aleksandra Tsybina, Mikhail Chernyshov, Maksim Kuprashevich
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02242](https://arxiv.org/abs/2601.02242)
- **Reason:** 基于小型多模态模型实现高效指令式图像编辑，在保持生成质量的同时降低推理成本，相关工作对原生多模态模型的轻量化应用具有参考价值。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes
- **Authors:** Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02356](https://arxiv.org/abs/2601.02356)
- **Reason:** 利用RL驱动的扩散模型实现文本指令下的物体几何变换，提升场景编辑的精确性与一致性，属于原生多模态大模型的扩展应用。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models
- **Authors:** Yunlin Zeng
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01062](https://arxiv.org/abs/2601.01062)
- **Reason:** 针对视觉语言模型的生成叙事任务提出基准，属于原生多模态大模型方向（视觉-语言模型）。
Score: 7
Field: 原生多模态大模型

## 高效大模型训练与推理

### [Score: 9.0/10] You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference
- **Authors:** Ryan Shamim
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00847](https://arxiv.org/abs/2601.00847)
- **Reason:** 提出MFEE控制平面架构，实现Transformer的选择性推理，在保持正确性的同时降低78.1%的执行成本，对大模型高效推理具有突破性贡献。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] ELLA: Efficient Lifelong Learning for Adapters in Large Language Models
- **Authors:** Shristi Das Biswas (), Yue Zhang (), Anwesan Pal (), Radhika Bhargava (), Kaushik Roy ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02232](https://arxiv.org/abs/2601.02232)
- **Reason:** 提出选择性子空间解相关的持续学习框架ELLA，解决大模型连续微调中的灾难性遗忘，无需数据回放或架构扩展，提升终身学习效率与零样本泛化能力。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling
- **Authors:** Falcon LLM Team (), Iheb Chaabane (), Puneesh Khanna (), Suhail Mohmad (), Slim Frikha (), Shi Hu (), Abdalgader Abubaker (), Reda Alami (), Mikhail Lubinets (), Mohamed El Amine Seddik (), Hakim Hacid ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02346](https://arxiv.org/abs/2601.02346)
- **Reason:** 提出7B参数的混合模型Falcon-H1R，通过数据筛选和目标训练策略，在推理基准上超过更大模型，同时提升推理速度和token效率，属于高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression
- **Authors:** Zunhai Su, Weihao Ye, Hansen Feng, Keyu Fan, Jing Zhang, Dahai Yu, Zhengwu Liu, Ngai Wong
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01204](https://arxiv.org/abs/2601.01204)
- **Reason:** 针对流式3D视觉几何Transformer的KV缓存内存膨胀问题，提出tuning-free的联合剪枝与量化压缩方法，实现4.42×内存 reduction和5.48×推理加速，且性能损失可忽略，对高效大模型推理有实际应用价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] LinMU: Multimodal Understanding Made Linear
- **Authors:** Hongjie Wang (Princeton University), Niraj K. Jha (Princeton University)
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01322](https://arxiv.org/abs/2601.01322)
- **Reason:** 针对Vision-Language Models的二次自注意力复杂度问题，提出线性复杂度的M-MATE块及三阶段蒸馏框架，显著降低推理延迟（TTFT减少2.7倍）并提升吞吐量（9倍），属于高效大模型训练与推理的核心方向，作者团队有影响力。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference
- **Authors:** Aayush Kumar
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00850](https://arxiv.org/abs/2601.00850)
- **Reason:** 利用小型模型集成实现边缘设备上的真实问答，在提升真实性的同时保持推理效率，属于高效大模型训练与推理的边缘应用场景。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs
- **Authors:** Xingsheng Chen, Regina Zhang, Bo Gao, Xingwei He, Xiaofeng Liu, Pietro Lio, Kwok-Yan Lam, Siu-Ming Yiu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00920](https://arxiv.org/abs/2601.00920)
- **Reason:** 融合低秩Neural ODE与Mamba实现高效时间序列预测，提升准确性与可扩展性，属于高效大模型训练与推理的架构优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures
- **Authors:** Kabir Grover
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00942](https://arxiv.org/abs/2601.00942)
- **Reason:** 分析稀疏MoE架构与密集LLM在解码随机性下的可靠性，稀疏架构是高效大模型训练的关键方向，结果对稀疏模型部署有指导意义。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions
- **Authors:** Abidemi Koledoye, Chinemerem Unachukwu, Gold Nwobu, Hasin Rana
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01237](https://arxiv.org/abs/2601.01237)
- **Reason:** 对比SSM与Transformer在长上下文任务中的效率，SSM是高效大模型训练的重要架构，结果对高效模型选择有指导意义。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting
- **Authors:** Muxi Diao (), Lele Yang (), Wuxuan Gong (), Yutong Zhang (), Zhonghao Yan (), Yufei Han (), Kongming Liang (), Weiran Xu (), Zhanyu Ma ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02151](https://arxiv.org/abs/2601.02151)
- **Reason:** 提出熵自适应微调框架EAFT，通过token级熵区分认知不确定性与知识冲突，解决监督微调中的灾难性遗忘问题，提升大模型微调效率与泛化能力。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Heterogeneous Low-Bandwidth Pre-Training of LLMs
- **Authors:** Yazan Obeidi (), Amir Sarfi (), Joel Lidin (), Paul Janson (), Eugene Belilovsky ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02360](https://arxiv.org/abs/2601.02360)
- **Reason:** 提出异质低带宽预训练框架，结合SparseLoCo与激活压缩，解决大模型预训练中的带宽限制问题，提升分布式训练效率，属于高效大模型训练与推理研究。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] Efficient Unrolled Networks for Large-Scale 3D Inverse Problems
- **Authors:** Romain Vo, Julián Tachella
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02141](https://arxiv.org/abs/2601.02141)
- **Reason:** 针对大规模3D逆问题的深度展开网络内存瓶颈，提出域划分与正态算子近似方法，支持单GPU高效训练推理，对高效大模型训练与推理研究有贡献
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training
- **Authors:** Gihyeon Sim
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00894](https://arxiv.org/abs/2601.00894)
- **Reason:** 提出训练-free的门控策略，实现代码生成的自适应计算分配，提升OOD场景性能，属于高效大模型推理的针对性优化。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Accelerated Full Waveform Inversion by Deep Compressed Learning
- **Authors:** Maayan Gelboim, Amir Adler, Mauricio Araya-Polo
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01268](https://arxiv.org/abs/2601.01268)
- **Reason:** 利用深度压缩学习加速全波形反演，属于高效大模型训练中的数据压缩方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware
- **Authors:** Jorge L. Ruiz Williams
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01298](https://arxiv.org/abs/2601.01298)
- **Reason:** 提出Warp Cortex架构实现多agent内存高效缩放，属于高效大模型训练中的内存优化方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Spectral-Window Hybrid (SWH)
- **Authors:** Vladimer Khasia
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01313](https://arxiv.org/abs/2601.01313)
- **Reason:** 提出SWH架构结合全局谱分支与局部窗口注意力，实现长上下文高效建模，属于高效大模型训练中的架构优化方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Accelerating Decentralized Optimization via Overlapping Local Steps
- **Authors:** Yijie Zhou, Shi Pu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01493](https://arxiv.org/abs/2601.01493)
- **Reason:** 提出OLDSGD方法，通过计算-通信重叠减少分布式训练通信瓶颈，理论分析非凸目标收敛率，实验提升wall-clock时间，对高效分布式大模型训练有帮助。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations
- **Authors:** Tao An ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00821](https://arxiv.org/abs/2601.00821)
- **Reason:** 提出训练无关的CogCanvas框架，通过提取对话中的认知 artifact 并构建时序图，解决长对话中的上下文压缩与检索问题，提升大模型推理效率。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Energy-Aware Routing to Large Reasoning Models
- **Authors:** Austin R. Ellis-Mohr (), Max Hartman (), Lav R. Varshney ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00823](https://arxiv.org/abs/2601.00823)
- **Reason:** 研究大模型推理的能量感知路由策略，分析能量供应与推理性能的平衡，优化推理过程的能量效率，属于高效大模型推理研究。
Score: 7
Field: 高效大模型训练与推理

## 深度学习理论

### [Score: 9.0/10] Attention Needs to Focus: A Unified Perspective on Attention Allocation
- **Authors:** Zichuan Fu, Wentao Song, Guojing Li, Yejing Wang, Xian Wu, Yimin Deng, Hanyu Yan, Yefeng Zheng, Xiangyu Zhao
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00919](https://arxiv.org/abs/2601.00919)
- **Reason:** 提出Lazy Attention解决Transformer的注意力过载/欠载问题，缓解注意力 sink 与坍塌，同时实现高稀疏性，属于深度学习理论中的注意力机制创新。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] Geometric and Dynamic Scaling in Deep Transformers
- **Authors:** Haoran Su, Chenyu You
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01014](https://arxiv.org/abs/2601.01014)
- **Reason:** 研究深度Transformer的几何漂移与特征积累问题，提出Manifold-Geometric Transformer，属于深度学习理论中的网络架构方向。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD
- **Authors:** Ze Peng, Jian Zhang, Yisen Wang, Lei Qi, Yinghuan Shi, Yang Gao
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01465](https://arxiv.org/abs/2601.01465)
- **Reason:** 利用平坦性改进SGD的信息论泛化界，属于深度学习理论中的泛化理论方向，结果更紧致且反映平坦性对泛化的影响。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] Learning with Monotone Adversarial Corruptions
- **Authors:** Kasper Green Larsen (), Chirag Pabbaraju (), Abhishek Shetty ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02193](https://arxiv.org/abs/2601.02193)
- **Reason:** 研究单调对抗性污染下的机器学习问题，揭示现有最优算法对数据交换性与独立性的过度依赖，属于深度学习理论中学习算法鲁棒性的基础研究。
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion
- **Authors:** Binglei Li, Mengping Yang, Zhiyu Tan, Junping Zhang, Hao Li
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02211](https://arxiv.org/abs/2601.02211)
- **Reason:** 系统分析MMDiT块在文本条件扩散模型中的功能，通过训练-free方法优化模型性能，为扩散Transformer架构设计提供理论支撑。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] BEDS: Bayesian Emergent Dissipative Structures
- **Authors:** Laurent Caraffa
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02329](https://arxiv.org/abs/2601.02329)
- **Reason:** 融合非平衡热力学、贝叶斯推理与机器学习，提出统一理论框架解释学习的耗散本质，对深度学习理论的基础研究具有重要意义。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Horizon Reduction as Information Loss in Offline Reinforcement Learning
- **Authors:** Uday Kumar Nidadala, Venkata Bhumika Guthi
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00831](https://arxiv.org/abs/2601.00831)
- **Reason:** 形式化分析离线RL中的 horizon reduction 问题，证明其会导致不可恢复的信息损失，为RL理论研究提供关键 insights。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Path Integral Solution for Dissipative Generative Dynamics
- **Authors:** Xidi Wang
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00860](https://arxiv.org/abs/2601.00860)
- **Reason:** 证明耗散量子动力学可生成连贯文本，将量子力学与生成模型关联，为深度学习理论提供新颖的物理视角。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] FANoS: Friction-Adaptive Nos\'e--Hoover Symplectic Momentum for Stiff Objectives
- **Authors:** Nalin Dhiman
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00889](https://arxiv.org/abs/2601.00889)
- **Reason:** 提出物理启发的优化器FANoS，结合辛动量与自适应摩擦，在 stiff 目标上超越AdamW，属于深度学习理论中的优化器创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification
- **Authors:** Harshita Narnoli, Mihai Surdeanu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01290](https://arxiv.org/abs/2601.01290)
- **Reason:** 研究ICL机制，对比LLM与监督分类器行为，属于深度学习理论中的ICL方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space
- **Authors:** Changhoon Song, Seungchan Ko, Youngjoon Hong
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01295](https://arxiv.org/abs/2601.01295)
- **Reason:** 研究深度ReLU网络在log-weighted Barron空间中的Sobolev逼近，属于深度学习理论中的网络表示方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Towards a Principled Muon under $\mu\mathsf{P}$: Ensuring Spectral Conditions throughout Training
- **Authors:** John Zhao
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01306](https://arxiv.org/abs/2601.01306)
- **Reason:** 研究μP下的Muon优化器，确保训练中的谱条件，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory
- **Authors:** Itay Safran
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01417](https://arxiv.org/abs/2601.01417)
- **Reason:** 证明ReLU网络计算最大值的深度层次，属于深度学习理论中的网络架构（ReLU网络）方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models
- **Authors:** Jian Feng, Zhihong Huang
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01452](https://arxiv.org/abs/2601.01452)
- **Reason:** 提出BSZO优化器改进LLM的零阶优化，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission
- **Authors:** Emrah Mete (), Emin Erkan Korkmaz ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02253](https://arxiv.org/abs/2601.02253)
- **Reason:** 受生物神经信号传输启发，提出无乘法运算的Neuro-Channel Networks架构，用通道宽度与神经递质参数替代权重，实现高效前向传播，属于深度学习理论中的新型网络结构研究。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Context Collapse: In-Context Learning and Model Collapse
- **Authors:** Josef Ott ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00923](https://arxiv.org/abs/2601.00923)
- **Reason:** 研究大模型的上下文学习（ICL）与模型崩溃现象，通过线性Transformer分析参数学习的相变，属于深度学习理论中的模型行为研究。
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] Value-guided action planning with JEPA world models
- **Authors:** Matthieu Destrade, Oumayma Bounou, Quentin Le Lidec, Jean Ponce, Yann LeCun
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00844](https://arxiv.org/abs/2601.00844)
- **Reason:** 增强JEPA世界模型的动作规划能力，通过塑造表示空间近似价值函数，提升规划性能，属于深度学习理论中的世界模型研究方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments
- **Authors:** Sameer Rahil, Zain Abdullah Ahmad, Talha Asif
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00853](https://arxiv.org/abs/2601.00853)
- **Reason:** 提出联邦学习中的SAM变体，通过聚类聚合与调制提升异质环境下的鲁棒性，属于深度学习理论中的优化器研究方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation
- **Authors:** Andrea Thomas Nava, Lijo Johny, Fabio Azzalini, Johannes Schneider, Arianna Casanova
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00932](https://arxiv.org/abs/2601.00932)
- **Reason:** 研究基于梯度的优化（Projected Gradient Descent）与不确定性估计结合的产品开发方法，涉及深度学习理论中的优化器方向，具有理论与实践价值。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Learning from Historical Activations in Graph Neural Networks
- **Authors:** Yaniv Galron, Hadar Sinai, Haggai Maron, Moshe Eliasof
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01123](https://arxiv.org/abs/2601.01123)
- **Reason:** 提出HISTOGRAPH架构利用GNN历史激活改进图池化，属于深度学习理论中的网络架构（GNN）方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Sparse Bayesian Message Passing under Structural Uncertainty
- **Authors:** Yoonhyuk Choi, Jiho Choi, Chanran Kim, Yumin Lee, Hawon Shin, Yeowon Jeon, Minjeong Kim, Jiwoo Kang
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01207](https://arxiv.org/abs/2601.01207)
- **Reason:** 提出稀疏符号消息传递网络处理图结构不确定性，属于深度学习理论中的网络架构（GNN）方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines
- **Authors:** Itai Morad, Nir Shlezinger, Yonina C. Eldar
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01484](https://arxiv.org/abs/2601.01484)
- **Reason:** 从贝叶斯视角分析知识蒸馏的SGD收敛行为，提出贝叶斯教师优于确定性教师的理论依据，实验验证准确性与收敛稳定性提升，对蒸馏的优化理论有贡献。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Output Embedding Centering for Stable LLM Pretraining
- **Authors:** Felix Stollenwerk, Anna Lokrantz, Niclas Hertzberg
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02031](https://arxiv.org/abs/2601.02031)
- **Reason:** 分析LLM预训练的输出logit发散问题，提出输出嵌入中心化策略，理论证明抑制发散，实验验证比z-loss更稳定，对LLM预训练稳定性有理论与实践贡献。
Score: 7
Field: 深度学习理论

## 大模型安全与对齐

### [Score: 9.0/10] Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance
- **Authors:** Jiawen Zhang, Lipeng He, Kejia Chen, Jian Lou, Jian Liu, Xiaohu Yang, Ruoxi Jia
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01887](https://arxiv.org/abs/2601.01887)
- **Reason:** 发现仅用单个安全实例即可恢复微调LLM的安全对齐，分析安全梯度的低秩结构，实验验证跨模型和数据集的有效性，对LLM安全对齐的高效修复有重大贡献。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] Admissibility Alignment
- **Authors:** Chris Duffey ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01816](https://arxiv.org/abs/2601.01816)
- **Reason:** 提出可容许性对齐框架，将大模型对齐视为概率决策属性，通过Monte Carlo方法评估结果分布的对齐度，属于大模型安全与对齐中的决策理论研究。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing
- **Authors:** Tianbo Wang, Yuqing Ma, Kewei Liao, Zhange Zhang, Simin Li, Jinyang Guo, Xianglong Liu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01957](https://arxiv.org/abs/2601.01957)
- **Reason:** 针对大视觉语言模型的对象幻觉问题，提出事实引导的自适应激活编辑框架（含FAS与QAO模块），有效减少语言偏差，提升模型输出可信度，对大模型安全与对齐研究有重要意义
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models
- **Authors:** Sunny Gupta, Shounak Das, Amit Sethi
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02147](https://arxiv.org/abs/2601.02147)
- **Reason:** 针对视觉语言模型的视觉-文本偏差问题，提出双模态提示优化框架（含结构注意力擦除与平衡提示归一化），减少非因果特征依赖，提升因果推理能力，对大模型安全与对齐有重要价值
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs
- **Authors:** Zibo Zhao (Arizona State University), Yuanting Zha (ShanghaiTech University), Haipeng Zhang (ShanghaiTech University), Xingcheng Xu (Shanghai Artificial Intelligence Laboratory)
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01580](https://arxiv.org/abs/2601.01580)
- **Reason:** 提出两阶段决策采样假设，通过梯度归因理论解释RL训练LLM自反思能力的涌现机制，实验验证自反思主要来自决策能力提升，对LLM的RL对齐有理论贡献。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs
- **Authors:** Dasol Choi (), DongGeon Lee (), Brigitta Jesica Kartono (), Helena Berndt (), Taeyoun Kwon (), Joonwon Jang (), Haon Park (), Hwanjo Yu (), Minsuk Kahng ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01836](https://arxiv.org/abs/2601.01836)
- **Reason:** 提出首个系统评估LLM组织特定政策对齐的框架COMPASS，解决现有安全评估仅关注通用危害的问题，覆盖8个行业场景，实验验证当前LLM在政策合规性上的不足，对大模型安全对齐有重要实用价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors
- **Authors:** Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02359](https://arxiv.org/abs/2601.02359)
- **Reason:** 基于个性化扩散模型实现零样本人脸伪造检测，在多个数据集上超越现有方法，对大模型的安全与对齐研究具有实践价值。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors
- **Authors:** Yash Thesia, Meera Suthar
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01688](https://arxiv.org/abs/2601.01688)
- **Reason:** 利用潜在扩散模型的语义先验解决数据-free模型提取的冷启动问题，提出DiMEx框架提升查询效率，同时提出HSE防御识别攻击轨迹，对大模型安全防护有实践价值。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck
- **Authors:** Dina El Zein (), James Henderson ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02307](https://arxiv.org/abs/2601.02307)
- **Reason:** 提出非参数变分差分隐私框架NVDP，通过信息瓶颈层向Transformer嵌入注入噪声，平衡文本数据共享的实用性与隐私保护，属于大模型安全与对齐中的隐私增强研究。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback
- **Authors:** Ismail Ahmad Abdullah ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00816](https://arxiv.org/abs/2601.00816)
- **Reason:** 提出可验证学习基板MathLedger，结合形式验证与密码学证明，实现学习过程的审计与反馈认证，属于大模型安全与对齐中的可验证性研究。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation
- **Authors:** Letian Kong (Jenny), Qianran (Jenny), Jin, Renyu Zhang ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01546](https://arxiv.org/abs/2601.01546)
- **Reason:** 提出上下文形成与导航框架，提升大模型社会模拟中的行为对齐度，解决复杂决策环境中的行为偏差问题，属于大模型安全与对齐中的行为一致性研究。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs
- **Authors:** Farzan Karimi-Malekabadi (), Suhaib Abdurahman (), Zhivar Sourati (), Jackson Trager (), Morteza Dehghani ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01878](https://arxiv.org/abs/2601.01878)
- **Reason:** 针对LLM社会认知评估的理论基础缺失问题，提出Theory Trace Card（TTC）明确评估的理论依据、目标能力组件和局限性，解决评估-部署差距，提升评估可解释性和复用性，对大模型安全对齐中的评估方法有重要贡献。
Score: 7
Field: 大模型安全与对齐

## 深度学习可解释性

### [Score: 9.0/10] Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning
- **Authors:** Deep Pankajbhai Mehta ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00830](https://arxiv.org/abs/2601.00830)
- **Reason:** 通过实验发现大模型Chain-of-Thought解释存在系统性漏报问题，质疑AI解释的可靠性，属于深度学习可解释性中的解释信任度研究。
Score: 9
Field: 深度学习可解释性

### [Score: 8.0/10] VerLM: Explaining Face Verification Using Natural Language
- **Authors:** Syed Abdul Hannan, Hazim Bukhari, Thomas Cantalapiedra, Eman Ansar, Massa Baali, Rita Singh, Bhiksha Raj
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01798](https://arxiv.org/abs/2601.01798)
- **Reason:** 提出结合两种解释风格（简洁总结与详细差异描述）的视觉语言模型，实现面部验证决策的自然语言解释，改进跨模态方法以提升准确性与可解释性，对深度学习可解释性研究有直接价值
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment
- **Authors:** Chorok Lee
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00908](https://arxiv.org/abs/2601.00908)
- **Reason:** 利用SHAP分析解释分布偏移下的 conformal prediction 失败原因，发现单特征依赖与灾难性失败的关联，属于深度学习可解释性研究。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks
- **Authors:** Longwei Wang, Mohammad Navid Nayyem, Abdullah Al Rakin, KC Santosh, Chaowei Zhang, Yang Zhou
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00968](https://arxiv.org/abs/2601.00968)
- **Reason:** 利用LIME解释方法识别并抑制虚假特征，将可解释性作为主动训练信号提升对抗鲁棒性，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train
- **Authors:** Ungsik Kim, Suwon Lee
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01903](https://arxiv.org/abs/2601.01903)
- **Reason:** 提出TT-FSI方法，利用张量训练加速忠实Shapley交互的计算，理论证明指数级时间和内存优化，实验验证多数据集上的速度与内存优势，对Shapley值的可扩展计算有重要意义。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis
- **Authors:** Yin Li ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00828](https://arxiv.org/abs/2601.00828)
- **Reason:** 将大模型自校正分解为错误检测、定位与纠正三个子能力，发现准确性-校正悖论并提出错误深度假设，揭示自校正的内在机制，属于深度学习可解释性研究。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration
- **Authors:** Albert Sadowski (), Jarosław A. Chudziak ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01609](https://arxiv.org/abs/2601.01609)
- **Reason:** 将大模型与语义网推理结合，通过结构化分解实现可审计的规则推理，提升推理的可解释性与确定性，属于深度学习可解释性中的符号-神经融合研究。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents
- **Authors:** Sourena Khanzadeh ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02314](https://arxiv.org/abs/2601.02314)
- **Reason:** 提出基于结构因果模型（SCM）和反事实逻辑的框架，审计LLM代理推理的因果忠实性，发现当前CoT推理存在“因果解耦”问题，为可解释AI提供因果视角的评估方法，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix
- **Authors:** Fanzhe Fu ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01532](https://arxiv.org/abs/2601.01532)
- **Reason:** 提出正则化逆混淆矩阵方法，量化推理模型的认知信念深度，解决现有评估仅关注知识广度的问题，属于深度学习可解释性中的信念评估研究。
Score: 7
Field: 深度学习可解释性

### [Score: 6.0/10] The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context
- **Authors:** Md Muhtasim Munif Fahim, Humyra Ankona, Md Monimul Huq, Md Rezaul Karim
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01231](https://arxiv.org/abs/2601.01231)
- **Reason:** 使用SHAP和ALE分析解释学生满意度驱动因素，属于深度学习可解释性方向。
Score: 6
Field: 深度学习可解释性

## 大模型新技术

### [Score: 8.0/10] Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment
- **Authors:** Bac Nguyen, Yuhta Takida, Naoki Murata, Chieh-Hsin Lai, Toshimitsu Uesaka, Stefano Ermon, Yuki Mitsufuji
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01224](https://arxiv.org/abs/2601.01224)
- **Reason:** 针对基于扩散模型的物体中心学习中的slot纠缠与对齐问题，提出CODA框架，通过register slots吸收残余注意力并引入对比对齐损失，显著提升物体发现（COCO上+6.1% FG-ARI）和生成性能，属于大模型新技术（扩散模型）的重要改进。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Guiding Token-Sparse Diffusion Models
- **Authors:** Felix Krause, Stefan Andreas Baumann, Johannes Schusterbauer, Olga Grebenkova, Ming Gui, Vincent Tao Hu, Björn Ommer
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01608](https://arxiv.org/abs/2601.01608)
- **Reason:** 解决token稀疏扩散模型的推理引导问题，提出Sparse Guidance提升生成质量与效率（如ImageNet-256上FID 1.58且FLOPs减少25%），属于大模型新技术中的diffusion方向，作者团队有影响力。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Dichotomous Diffusion Policy Optimization
- **Authors:** Ruiming Liang, Yinan Zheng, Kexin Zheng, Tianyi Tan, Jianxiong Li, Liyuan Mao, Zhihao Wang, Guang Chen, Hangjun Ye, Jingjing Liu, Jinqiao Wang, Xianyuan Zhan
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00898](https://arxiv.org/abs/2601.00898)
- **Reason:** 提出二分扩散策略优化方法DIPOLE，通过双策略设计提升RL训练稳定性，属于大模型新技术中的扩散RL方向。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI
- **Authors:** Tatsuaki Tsuruyama
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01045](https://arxiv.org/abs/2601.01045)
- **Reason:** 提出基于粗粒度KL控制的扩散模型反向过程，改进扩散模型生成控制，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts
- **Authors:** Ruofeng Yang, Yongcan Li, Bo Jiang, Cheng Chen, Shuai Li
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01475](https://arxiv.org/abs/2601.01475)
- **Reason:** 研究扩散模型的多子空间多模态潜变量建模，结合MoE结构，理论分析估计误差与收敛性，实验验证生成性能优于高斯得分模型，对扩散模型的理论深化与多模态应用有价值。
Score: 8
Field: 大模型新技术

### [Score: 7.0/10] Forget Less by Learning from Parents Through Hierarchical Relationships
- **Authors:** Arjun Ramesh Kaushik, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Nalini K. Ratha, Venu Govindaraju
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01892](https://arxiv.org/abs/2601.01892)
- **Reason:** 针对Custom Diffusion Models的灾难性遗忘问题，提出基于双曲空间层次父子关系的FLLP框架，实现持续概念学习与知识保留，对扩散大模型的持续学习研究有创新价值
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Learning Action Hierarchies via Hybrid Geometric Diffusion
- **Authors:** Arjun Ramesh Kaushik, Nalini K. Ratha, Venu Govindaraju
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01914](https://arxiv.org/abs/2601.01914)
- **Reason:** 提出HybridTAS框架，结合欧几里得与双曲几何的扩散模型处理动作层次结构，通过粗到细去噪提升时间动作分割性能，对扩散大模型在视频理解中的应用有推动作用
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation
- **Authors:** Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02256](https://arxiv.org/abs/2601.02256)
- **Reason:** 针对视觉自回归模型的异步策略冲突问题，提出基于RL的优化框架，改善样本质量与目标对齐，属于大模型新技术范畴的创新应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] LLMize: A Framework for Large Language Model-Based Numerical Optimization
- **Authors:** M. Rizki Oktavian
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.00874](https://arxiv.org/abs/2601.00874)
- **Reason:** 提出LLM-based数值优化框架，通过自然语言注入约束，实现 domain-specific 任务的高效求解，属于大模型新技术的扩展应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations
- **Authors:** Amin Abyaneh, Charlotte Morissette, Mohamad H. Danesh, Anas El Houssaini, David Meger, Gregory Dudek, Hsiu-Chin Lin
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01003](https://arxiv.org/abs/2601.01003)
- **Reason:** 提出收缩扩散策略改进扩散模型在连续控制中的鲁棒性，属于大模型新技术中的扩散模型方向。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] GDRO: Group-level Reward Post-training Suitable for Diffusion Models
- **Authors:** Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02036](https://arxiv.org/abs/2601.02036)
- **Reason:** 针对扩散模型提出群体奖励后训练框架，理论证明离线训练的效率与采样无关性，实验验证奖励提升与抗奖励黑客，对扩散模型的奖励对齐有重要价值。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Learning Diffusion Policy from Primitive Skills for Robot Manipulation
- **Authors:** Zhihao Gu (), Ming Yang (), Difan Zou (), Dong Xu ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01948](https://arxiv.org/abs/2601.01948)
- **Reason:** 提出技能条件的扩散政策SDP，将原始技能与扩散模型结合，实现可解释的机器人学习，属于大模型新技术中的diffusion LLM方向。
Score: 7
Field: 大模型新技术

## 多模态智能体

### [Score: 8.0/10] Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization
- **Authors:** Xinyu Qiu, Heng Jia, Zhengwen Zeng, Shuheng Shen, Changhua Meng, Yi Yang, Linchao Zhu
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01483](https://arxiv.org/abs/2601.01483)
- **Reason:** 提出ADPO框架统一多模态模型的生成与自验证，在GUI Odyssey等多模态智能体任务（如AndroidControl）上提升step success rate，解决了传统分离模型的高成本问题，相关度高。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents
- **Authors:** Keyu Wang (), Bingchen Miao (), Wendong Bu (), Yu Wu (), Juncheng Li (), Shengyu Zhang (), Wenqiao Zhang (), Siliang Tang (), Jun Xiao (), Yueting Zhuang ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02201](https://arxiv.org/abs/2601.02201)
- **Reason:** 提出代码驱动的逆自训练框架CORE，结合语义代码抽象与策略图扩展，解决虚拟代理模仿学习多样性不足与强化学习依赖人工奖励的问题，提升多模态智能体的行为泛化能力。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding
- **Authors:** Chenyang Ma (), Guangyu Yang (), Kai Lu (), Shitong Xu (), Bill Byrne (), Niki Trigoni (), Andrew Markham ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.02295](https://arxiv.org/abs/2601.02295)
- **Reason:** 提出CycleVLA框架，为Vision-Language-Action（VLA）模型赋予主动自纠正能力，结合进度感知VLA、VLM故障预测和MBR解码，提升VLA执行性能，属于多模态智能体方向。
Score: 8
Field: 多模态智能体

### [Score: 7.0/10] KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models
- **Authors:** Zixian Liu (), Sihao Liu (), Yuqi Zhao ()
- **Published:** 2026-01-06
- **Link:** [https://arxiv.org/abs/2601.01366](https://arxiv.org/abs/2601.01366)
- **Reason:** 提出知识增强的双图评估框架KGCE，针对跨平台教育智能体的任务执行与效率进行细粒度评估，属于多模态智能体的基准与评估研究。
Score: 7
Field: 多模态智能体

