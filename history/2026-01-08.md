# ArXiv 每日推荐 - 2026-01-08

> 更新于北京时间：2026-01-08 12:39:48
> 已自动阅读了 178 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：91273

## 高效大模型训练与推理

### [Score: 9.0/10] DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation
- **Authors:** Jiajun jiao, Haowei Zhu, Puyuan Yang, Jianghui Wang, Ji Liu, Ziqiong Liu, Dong Li, Yuejian Fang, Junhai Yong, Bin Wang, Emad Barsoum
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03178](https://arxiv.org/abs/2601.03178)
- **Reason:** 提出LLM驱动的扩散模型加速框架，包括DiffBench基准和DiffAgent代码生成，解决扩散模型推理效率问题，对高效大模型推理有实践价值
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth
- **Authors:** Arjun S. Nair
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02609](https://arxiv.org/abs/2601.02609)
- **Reason:** 提出Chronicals框架，通过多个优化实现LLM微调3.51倍加速，对高效大模型训练有显著实践价值
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models
- **Authors:** Ruiyang Zhang, Dongzhan Zhou, Zhedong Zheng
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02825](https://arxiv.org/abs/2601.02825)
- **Reason:** 针对大模型长推理的高token成本和慢响应问题，提出sketch-style推理方法，通过三阶段训练减少64%推理token成本且不影响答案准确性，有效提升大模型推理效率。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] GEM-Style Constraints for PEFT with Dual Gradient Projection in LoRA
- **Authors:** Brian Tekmen, Jason Yin, Qianqian Tong
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02500](https://arxiv.org/abs/2601.02500)
- **Reason:** 将GEM约束应用于LoRA subspace，提升持续学习的效率和性能，对高效大模型训练中的持续学习有贡献
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance
- **Authors:** Xuanyu Wang, Haisen Su, Jingtao Zhang, Xiangxiang Wang, Yongbin Yu, Manping Fan, Bo Gong, Siqi Chen, Mingsheng Cao, Liyong Ren
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02888](https://arxiv.org/abs/2601.02888)
- **Reason:** 提出RPIQ量化框架，将大模型压缩到4位并减少内存消耗，对高效大模型推理的部署有实践价值
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Joint Encoding of KV-Cache Blocks for Scalable LLM Serving
- **Authors:** Joseph Kampeas, Emir Haleva
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03067](https://arxiv.org/abs/2601.03067)
- **Reason:** 提出KV-Cache块联合编码方法，解决LLM服务中的内存瓶颈问题，实现4.38倍KV-Cache压缩且精度损失小，显著提升LLM serving的可扩展性
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Sparse Knowledge Distillation: A Mathematical Framework for Probability-Domain Temperature Scaling and Multi-Stage Compression
- **Authors:** Aaron R. Flouro, Shawn P. Chadwick
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03195](https://arxiv.org/abs/2601.03195)
- **Reason:** 提出稀疏知识蒸馏的概率域数学框架，包含偏差-方差分解、收敛性保证等理论支撑，支持黑盒蒸馏和隐私保护压缩
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] SOP: A Scalable Online Post-Training System for Vision-Language-Action Models
- **Authors:** Mingjie Pan, Siyuan Feng, Qinglin Zhang, Xinchen Li, Jianheng Song, Chendi Qu, Yi Wang, Chuankang Li, Ziyu Xiong, Zhi Chen, Yi Liu, Jianlan Luo
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03044](https://arxiv.org/abs/2601.03044)
- **Reason:** 提出SOP在线后训练系统，支持VLA模型的分布式、多任务在线适应，提升真实世界任务性能，对高效大模型训练与推理有借鉴意义
Score: 7
Field: 高效大模型训练与推理

## 多模态智能体

### [Score: 9.0/10] WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks
- **Authors:** Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02439](https://arxiv.org/abs/2601.02439)
- **Reason:** 提出WebGym大规模视觉web智能体训练环境，提升智能体的网页任务性能，对多模态智能体的训练与评估有重要价值
Score: 9
Field: 多模态智能体

### [Score: 8.0/10] SimpleMem: Efficient Lifelong Memory for LLM Agents
- **Authors:** Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02553](https://arxiv.org/abs/2601.02553)
- **Reason:** 提出SimpleMem框架，通过语义压缩、递归整合和自适应检索，提升LLM智能体的记忆效率，减少30倍推理token消耗
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents
- **Authors:** Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03204](https://arxiv.org/abs/2601.03204)
- **Reason:** 提出InfiAgent框架，通过外部化状态解决长horizon任务的上下文增长问题，实验显示其在长期任务上优于上下文中心基线
Score: 8
Field: 多模态智能体

### [Score: 7.0/10] A Versatile Multimodal Agent for Multimedia Content Generation
- **Authors:** Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03250](https://arxiv.org/abs/2601.03250)
- **Reason:** 提出MultiMedia-Agent，用agent系统自动化复杂多媒体内容生成，结合工具库和偏好对齐，对多模态智能体的内容生成应用有价值
Score: 7
Field: 多模态智能体

### [Score: 7.0/10] MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents
- **Authors:** Dongming Jiang, Yi Li, Guanpeng Li, Bingzhe Li
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03236](https://arxiv.org/abs/2601.03236)
- **Reason:** 提出MAGMA多图记忆架构，分离记忆表示与检索逻辑，提升长horizon推理的准确性，优于现有智能体记忆系统
Score: 7
Field: 多模态智能体

## 原生多模态大模型

### [Score: 8.0/10] Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning
- **Authors:** Wenting Lu, Didi Zhu, Tao Shen, Donglin Zhu, Ayong Ye, Chao Wu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02422](https://arxiv.org/abs/2601.02422)
- **Reason:** 针对多模态复杂视觉推理中CoT方法依赖单区域、推理碎片化的问题，提出CoCoT框架及70K数据集，提升LLaVA-1.5和Qwen2-VL的推理准确性，对原生多模态大模型的跨模态推理能力有重要改进。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs
- **Authors:** Chenchen Lin, Sanbao Su, Rachel Luo, Yuxiao Chen, Yan Wang, Marco Pavone, Fei Miao
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03100](https://arxiv.org/abs/2601.03100)
- **Reason:** 提出文本引导的层融合模块TGIF，增强多模态LLM的视觉grounding并减少幻觉，对原生多模态大模型的可靠性提升有价值
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Unified Thinker: A General Reasoning Modular Core for Image Generation
- **Authors:** Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03127](https://arxiv.org/abs/2601.03127)
- **Reason:** 提出Unified Thinker模块化推理核心，解耦推理与生成，提升图像生成的逻辑指令遵循能力，对原生多模态大模型的生成推理有创新
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision
- **Authors:** Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03193](https://arxiv.org/abs/2601.03193)
- **Reason:** 提出UniCorn自改进框架，解决统一多模态模型的生成与理解不一致问题，提升图像生成性能，对原生多模态大模型的自监督优化有贡献
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] LTX-2: Efficient Joint Audio-Visual Foundation Model
- **Authors:** Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03233](https://arxiv.org/abs/2601.03233)
- **Reason:** 提出高效的音视频联合基础模型LTX-2，实现高质量音视频生成且计算成本低，对原生多模态大模型的高效设计有参考
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation
- **Authors:** Junhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He, Lei Jiang, Hang Li, Hengjie Li, Yang Li, Yufei Liu, Yanan Lu, Qi Lv, Haoxiang Ma, Jiangmiao Pang, Yu Qiao, Zherui Qiu, Yanqing Shen, Xu Shi, Yang Tian, Bolun Wang, Hanqing Wang, Jiaheng Wang, Tai Wang, Xueyuan Wei, Chao Wu, Yiman Xie, Boyang Xing, Yuqiang Yang, Yuyin Yang, Qiaojun Yu, Feng Yuan, Jia Zeng, Jingjing Zhang, Shenghan Zhang, Shi Zhang, Zhuoma Zhaxi, Bowen Zhou, Yuanzhen Zhou, Yunsong Zhou, Hongrui Zhu, Yangkun Zhu, Yuchen Zhu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02456](https://arxiv.org/abs/2601.02456)
- **Reason:** 提出InternVLA-A1，采用混合Transformer架构统一视觉-语言-动作的机器人操纵，在真实世界任务中显著优于基线，对原生多模态大模型的研究有价值
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors
- **Authors:** Wei-Yuan Cheng, Kai-Po Chang, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02908](https://arxiv.org/abs/2601.02908)
- **Reason:** 针对VideoLLMs时间定位不准确、事件 grounding差的问题，提出TA-Prompting方法增强时间感知，提升dense video captioning及时间理解任务性能，改进原生多模态大模型的视频处理能力。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] DreamStyle: A Unified Framework for Video Stylization
- **Authors:** Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu, Qian He
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02785](https://arxiv.org/abs/2601.02785)
- **Reason:** 提出统一视频风格化框架，支持text、style image、first frame多种条件，用LoRA训练减少条件混淆，提升风格一致性和视频质量，属于原生多模态大模型的创新应用。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Empowering Reliable Visual-Centric Instruction Following in MLLMs
- **Authors:** Weilei He, Feng Ju, Zhiyuan Fan, Rui Min, Minhao Cheng, Yi R. Fung
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03198](https://arxiv.org/abs/2601.03198)
- **Reason:** 提出VC-IFEval基准，系统评估MLLMs的视觉指令跟随能力，通过微调提升模型对视觉-文本联合指令的对齐准确性
Score: 7
Field: 原生多模态大模型

## 大模型安全与对齐

### [Score: 8.0/10] Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench
- **Authors:** Zanting Ye, Xiaolong Niu, Xuanbin Wu, Xu Han, Shengyuan Liu, Jing Hao, Zhihao Peng, Hao Sun, Jieqin Lv, Fanghu Wang, Yanchao Huang, Hubing Wu, Yixuan Yuan, Habib Zaidi, Arman Rahmim, Yefeng Zheng, Lijun Lu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02737](https://arxiv.org/abs/2601.02737)
- **Reason:** 发现MLLMs在功能成像（PET）中存在“CoT幻觉陷阱”的安全隐患，提出AVA方法桥接功能感知 gap，将CoT转化为 robust推理工具，提升诊断准确性14.83%，属于大模型安全与对齐的关键研究。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control
- **Authors:** Harshvardhan Saini, Yiming Tang, Dianbo Liu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02896](https://arxiv.org/abs/2601.02896)
- **Reason:** 提出梯度上升框架，结合机制可解释性和提示工程，实现LLM的可解释persona控制，对大模型安全与对齐有贡献
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Trust in LLM-controlled Robotics: a Survey of Security Threats, Defenses and Challenges
- **Authors:** Xinyu Huang, Shyam Karthick V B, Taozhao Chen, Mitch Bryson, Thomas Chaffey, Huaming Chen, Kim-Kwang Raymond Choo, Ian R. Manchester
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02377](https://arxiv.org/abs/2601.02377)
- **Reason:** 系统综述LLM控制机器人的安全威胁（越狱、后门等）与防御策略，为embodied LLM系统的安全对齐提供 foundational roadmap
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs
- **Authors:** David Hartmann, Lena Pohlmann, Lelia Hanslik, Noah Gieβing, Bettina Berendt, Pieter Delobelle
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03087](https://arxiv.org/abs/2601.03087)
- **Reason:** 提出BAFA框架，通过主动采样减少黑盒LLM公平性审计的查询次数（最多40倍），为LLM公平性对齐提供高效审计工具
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] HAL: Inducing Human-likeness in LLMs with Alignment
- **Authors:** Masum Hasan, Junjie Zhao, Ehsan Hoque
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02813](https://arxiv.org/abs/2601.02813)
- **Reason:** 提出HAL框架，用可解释的人类偏好奖励对齐LLM，人类评估显示模型对话更接近人类，提升LLM的人类相似性
Score: 7
Field: 大模型安全与对齐

## 深度学习理论

### [Score: 8.0/10] On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning
- **Authors:** Siyi Lyu, Quan Liu, Feng Yan
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03048](https://arxiv.org/abs/2601.03048)
- **Reason:** 从群同态和复杂度理论角度分析ViT在非可解空间推理的内在限制，为Transformer架构的空间推理能力提供理论洞察
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Physical Transformer
- **Authors:** Tao Xu, Zhixin Hu, Li Luo, Momiao Xiong
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02433](https://arxiv.org/abs/2601.02433)
- **Reason:** 提出Physical Transformer，将Transformer与几何表示和物理动力学结合，为深度学习模型的物理接地提供理论框架，属于深度学习理论创新
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] mHC-GNN: Manifold-Constrained Hyper-Connections for Graph Neural Networks
- **Authors:** Subhankar Mishra
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02451](https://arxiv.org/abs/2601.02451)
- **Reason:** 提出mHC-GNN，解决GNN的过平滑问题，理论证明其表达能力超越1-WL，属于深度学习理论中的GNN架构理论
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] CRoPE: Efficient Parametrization of Rotary Positional Embedding
- **Authors:** Beicheng Lou, Zifei Xu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02728](https://arxiv.org/abs/2601.02728)
- **Reason:** 提出CRoPE，优化旋转位置嵌入的参数化，减少参数并保持性能，属于深度学习理论中的架构优化
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures
- **Authors:** Waleed Khalid, Dmitry Ignatov, Radu Timofte
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02997](https://arxiv.org/abs/2601.02997)
- **Reason:** 用LLM通过闭环合成框架设计新颖神经网络架构，提升生成性能，属于深度学习理论中的架构自动化设计
Score: 8
Field: 深度学习理论

## 深度学习可解释性

### [Score: 8.0/10] When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability
- **Authors:** Raphael Ronge, Markus Maier, Frederick Eberhardt
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03047](https://arxiv.org/abs/2601.03047)
- **Reason:** 针对LLM机械可解释性的SAE方法进行压力测试，揭示其在特征引导鲁棒性、泛化性上的局限性，对AI安全相关的可解释性研究有重要参考价值
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] HEXAR: a Hierarchical Explainability Architecture for Robots
- **Authors:** Tamlin Love, Ferran Gebellí, Pradip Pramanick, Antonio Andriella, Guillem Alenyà, Anais Garrell, Raquel Ros, Silvia Rossi
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03070](https://arxiv.org/abs/2601.03070)
- **Reason:** 提出分层可解释性框架HEXAR，结合LLM推理、因果模型等技术针对机器人模块生成解释，评估优于基线，对深度学习可解释性的应用有价值
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] Towards Faithful Reasoning in Comics for Small MLLMs
- **Authors:** Chengcheng Feng, Haojie Yin, Yucheng Jin, Kaizhu Huang
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02991](https://arxiv.org/abs/2601.02991)
- **Reason:** 针对小MLLMs在漫画推理中存在的状态纠缠、虚假过渡等问题，提出框架提升推理链的忠实性和可迁移性，通过modular CoT和GRPO强化学习实现，属于深度学习可解释性的重要研究。
Score: 7
Field: 深度学习可解释性

## 大模型新技术

### [Score: 7.0/10] DreamLoop: Controllable Cinemagraph Generation from a Single Photograph
- **Authors:** Aniruddha Mahapatra, Long Mai, Cusuh Ham, Feng Liu
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02646](https://arxiv.org/abs/2601.02646)
- **Reason:** 基于视频扩散模型提出可控cinemagraph生成框架，无需cinemagraph训练数据，通过时间桥接和运动条件化实现单图生成动态cinemagraph，属于大模型新技术（diffusion LLM）的创新应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Polynomial Convergence of Riemannian Diffusion Models
- **Authors:** Xingyu Xu, Ziyi Zhang, Yorie Nakahira, Guannan Qu, Yuejie Chi
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.02499](https://arxiv.org/abs/2601.02499)
- **Reason:** 分析黎曼扩散模型的多项式收敛性，为非欧几里得空间的扩散模型提供理论保证，属于大模型新技术中的扩散模型理论
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion
- **Authors:** Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv
- **Published:** 2026-01-07
- **Link:** [https://arxiv.org/abs/2601.03213](https://arxiv.org/abs/2601.03213)
- **Reason:** 用强化学习实现文本到图像扩散模型的unlearning，引入时间步感知critic提升遗忘效果和图像质量，属于扩散大模型新技术研究
Score: 7
Field: 大模型新技术

