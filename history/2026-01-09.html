<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-09</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>大模型新技术</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习理论</a>
<a href='#' >多模态智能体</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >高效大模型训练与推理</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-09</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-09 12:43:22</p>
<p>已自动阅读了 198 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：107593</p>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mass Concept Erasure in Diffusion Models with Concept Hierarchy</h3>
<p><strong>Authors:</strong> Jiahang Tu, Ye Li, Yiming Wu, Hanbin Zhao, Chao Zhang, Hui Qian</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文针对扩散模型多概念擦除的效率问题，提出概念分层结构和SuPLoRA方法，解决了多概念擦除的性能下降问题，属于大模型新技术的重要进展。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.03305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance</h3>
<p><strong>Authors:</strong> Siddarth Nilol Kundur Satish, Devesh Jaiswal, Hongyu Chen, Abhishek Bakshi</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出PhysVideoGenerator框架，将可学习的物理先验嵌入视频生成过程，解决生成中的物理不一致问题，属于大模型新技术的重要研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.03665' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SIGMA: Scalable Spectral Insights for LLM Collapse</h3>
<p><strong>Authors:</strong> Yi Gu, Lingyou Pang, Xiangkun Ye, Tianyu Wang, Jianyu Lin, Carey E. Priebe, Alexander Aue</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出SIGMA框架，通过嵌入Gram矩阵谱分析量化LLM递归训练的模型collapse问题，提供可扩展监测工具，解决高维空间量化挑战。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.03385' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</h3>
<p><strong>Authors:</strong> Akarsh Kumar, Ryan Bahlous-Boldi, Prafull Sharma, Phillip Isola, Sebastian Risi, Yujin Tang, David Ha</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 利用LLM进行Core War程序的对抗进化，研究Red Queen动态（持续适应），发现warriors变得更通用且行为收敛，属于大模型新技术中的对抗适应研究，作者团队有强影响力。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.03335' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Logic Tensor Network-Enhanced Generative Adversarial Network</h3>
<p><strong>Authors:</strong> Nijesh Upreti (The University of Edinburgh), Vaishak Belle (The University of Edinburgh)</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 结合Logic Tensor Networks（LTN）与GAN，利用逻辑约束增强生成样本的逻辑一致性，实验在合成和MNIST数据集上优于传统GAN，属于大模型新技术中的神经符号生成方法。
Score: 6
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.03839' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Xiangdong Hu, Yangyang Jiang, Qin Hu, Xiaojun Jia</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出GAMBIT框架，通过游戏化场景诱导多模态大模型越狱，针对推理型模型的安全漏洞，实验显示对主流模型有高攻击成功率，属于大模型安全与对齐的重要研究。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03416' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Understanding Reward Hacking in Text-to-Image Reinforcement Learning</h3>
<p><strong>Authors:</strong> Yunqi Hong, Kuei-Chun Kao, Hengguang Zhou, Cho-Jui Hsieh</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文系统分析了文本到图像强化学习中的奖励hacking问题，提出artifact reward作为正则化减少生成 artifacts，属于大模型安全与对齐的重要研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03468' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks</h3>
<p><strong>Authors:</strong> Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出RAILS框架，无需梯度或先验实现LLM越狱攻击，通过自回归损失与历史选择策略提升有效性与迁移性，在开源与闭源模型上取得高成功率。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03420' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification</h3>
<p><strong>Authors:</strong> Xiao Lin, Philip Li, Zhichen Zeng, Tingwei Li, Tianxin Wei, Xuying Ning, Gaotang Li, Yuzhong Chen, Hanghang Tong</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对LLM零样本越狱检测的挑战，提出ALERT框架分层放大良性与越狱prompt的内部特征差异，解决传统方法依赖训练模板的问题。实验在三个安全基准上显著优于基线，如平均Accuracy和F1-score比次优基线高至少10%，作者团队有影响力。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03600' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules</h3>
<p><strong>Authors:</strong> Di Wu, Yanyan Zhao, Xin Lu, Mingzhe Li, Bing Qin</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出STAR-S框架，通过自监督推理安全规则提升大模型对 jailbreak 攻击的防御能力，直接针对大模型安全对齐的核心问题，实验验证其有效性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03537' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Inference Attacks Against Graph Generative Diffusion Models</h3>
<p><strong>Authors:</strong> Xiuling Wang, Xin Huang, Guibo Luo, Jianliang Xu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对图生成扩散模型的隐私风险，提出三种黑盒推理攻击（重构、属性推理、成员推理），实验在六个真实图上验证有效性，并提出防御方法，属于大模型安全与对齐中的隐私保护研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03701' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Detecting Semantic Backdoors in a Mystery Shopping Scenario</h3>
<p><strong>Authors:</strong> Arpad Berta, Gabor Danner, Istvan Hegedus, Mark Jelasity</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对语义后门检测的挑战（无明确触发模式），提出基于清洁数据集和训练流程的检测方法，通过生成参考模型池和模型距离阈值识别后门，实验优于SOTA。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03805' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs</h3>
<p><strong>Authors:</strong> Su-Hyeon Kim, Hyundong Jin, Yejin Lee, Yo-Sub Han</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 研究思维步骤对大模型安全的影响，提出熵基安全提醒方法，在不更新模型参数的情况下有效提升安全性，为大模型安全研究提供了新视角。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.03662' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Variational Inference, Entropy, and Orthogonality: A Unified Theory of Mixture-of-Experts</h3>
<p><strong>Authors:</strong> Ye Su, Yong Liu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 建立MoE统一理论框架，从贝叶斯与信息论角度推导Top-k路由与负载均衡策略，证明正交正则化对提升贪心路由性能的作用，为MoE设计提供理论支持。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03577' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Illusion of Specialization: Unveiling the Domain-Invariant "Standing Committee" in Mixture-of-Experts Models</h3>
<p><strong>Authors:</strong> Yan Wang, Yitao Xu, Nanhan Shen, Jinyan Su, Jimin Huang, Zining Zhu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 揭示MoE模型中存在领域不变的“Standing Committee”专家 coalition，挑战传统专家specialization假设，为MoE结构设计与训练提供指导。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spectral Manifold Regularization for Stable and Modular Routing in Deep MoE Architectures</h3>
<p><strong>Authors:</strong> Ibrahim Delibasoglu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对MoE架构的专家崩溃问题，提出SR-MoE，通过谱范数约束和稳定秩惩罚保持路由流形的结构模块化，实验显示改进深度MoE的稳定性和终身学习能力，属于深度学习理论中的MoE架构研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03889' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Gap Between Decision Trees and Neural Networks</h3>
<p><strong>Authors:</strong> Akash Kumar</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 分析决策树（分段线性）与神经网络的差异，指出决策树指标的Radon总变分无限，提出光滑 barrier score 平衡准确性与复杂度，实验验证于合成数据集，属于深度学习理论中的piece-wise linear研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03919' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Controllable LLM Reasoning via Sparse Autoencoder-Based Steering</h3>
<p><strong>Authors:</strong> Yi Fang, Wenjie Wang, Mingfeng Xue, Boyi Deng, Fengli Xu, Dayiheng Liu, Fuli Feng</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 利用稀疏自编码器分解LLM隐藏状态，实现细粒度推理策略控制，解决了推理策略自主选择的低效问题，属于深度学习理论中模型结构与控制的创新研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03595' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias</h3>
<p><strong>Authors:</strong> Joonwon Seo</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对多声部音乐生成的“Missing Middle”问题，通过结构归纳偏置提出Smart Embedding架构，利用信息论、Rademacher复杂度等理论证明稳定性和泛化性，减少48.3%参数，属于深度学习理论中的network architecture研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03612' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning Shrinks the Hard Tail: Training-Dependent Inference Scaling in a Solvable Linear Model</h3>
<p><strong>Authors:</strong> Noam Levi</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对LLM推理缩放的训练依赖性问题，提出Latent Instance Difficulty（LID）模型，分析学习对误差分布“硬尾”的影响，给出闭形式预测和计算分配规则，实验验证于CIFAR-10H和数学蒸馏任务。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03764' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Group and Exclusive Sparse Regularization-based Continual Learning of CNNs</h3>
<p><strong>Authors:</strong> Basile Tousside, Janis Mohr, Jörg Frochte</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对CNN持续学习中的灾难性遗忘问题，提出GESCL方法，通过分组和排他稀疏正则化平衡模型稳定性与可塑性，减少参数和计算量。实验在多个视觉基准上优于SOTA，属于深度学习理论中的network architecture正则化研究。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.03658' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MobileDreamer: Generative Sketch World Model for GUI Agent</h3>
<p><strong>Authors:</strong> Yilin Cao, Yufeng Zhong, Zhixiong Zeng, Liming Zheng, Jing Huang, Haibo Qiu, Peng Shi, Wenji Mao, Wan Guanglu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对移动GUI Agent提出生成式草图世界模型，解决了长 horizon 任务中的空间感知和预测问题，实验验证其任务成功率提升5.25%，是多模态智能体中GUI相关的重要成果。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.04035' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models</h3>
<p><strong>Authors:</strong> Jianke Zhang, Xiaoyu Chen, Qiuyue Wang, Mingsheng Li, Yanjiang Guo, Yucheng Hu, Jiajun Zhang, Shuai Bai, Junyang Lin, Jianyu Chen</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文研究视觉语言模型在视觉语言动作模型中的应用，分析了VLM在具身控制中的性能瓶颈，提出VLM4VLA框架，属于多模态智能体的重要研究。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.03309' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</h3>
<p><strong>Authors:</strong> Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shou, Hwee Tou Ng</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对UI grounding任务提出高效框架，解决视觉token冗余和位置连续性问题，通过patch-level监督与PosPad策略提升性能，实验在多基准上优于GUI-specific基线且降低计算开销。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.03928' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows</h3>
<p><strong>Authors:</strong> Jinwei Su, Qizhen Lan, Zeyu Wang, Yinghui Xia, Hairu Wen, Yiqun Duan, Xi Xiao, Tianyu Shi, Yang Jingsong, Lewei He</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对ComfyUI工作流提出自主探索推理框架，提升复杂创意管道的可执行性和质量，直接解决多模态智能体中GUI工作流的核心问题，实验验证其性能优于现有方法。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.04060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HyperCLOVA X 32B Think</h3>
<p><strong>Authors:</strong> NAVER Cloud HyperCLOVA X Team</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出针对韩国语境的视觉语言模型，强调多模态理解、推理和agent能力，开源有助于推动多模态大模型的研究与应用，作者团队具有工业界影响力。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.03286' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VideoMemory: Toward Consistent Video Generation via Memory Integration</h3>
<p><strong>Authors:</strong> Jinsong Zhou, Yihua Du, Xinli Xu, Luozhou Wang, Zijie Zhuang, Yehang Zhang, Shuaibo Li, Xiaojun Hu, Bolan Su, Ying-cong Chen</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出VideoMemory框架，通过动态记忆库保持视频生成中的实体一致性，解决长视频生成的一致性问题，属于原生多模态大模型的重要进展。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.03655' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing</h3>
<p><strong>Authors:</strong> Jinghan Yu, Junhao Xiao, Chenyu Zhu, Jiaming Li, Jia Li, HanMing Deng, Xirui Wang, Guoli Jia, Jianjun Li, Zhiyuan Ma, Xiang Bai, Bowen Zhou</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出I2E框架，通过分解-动作范式解决复杂文本引导图像编辑问题，提高多目标空间推理和精确编辑能力，属于原生多模态大模型的重要进展。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.03741' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation</h3>
<p><strong>Authors:</strong> Xu Zhang, Cheng Da, Huan Yang, Kun Gai, Ming Lu, Zhan Ma</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出ResTok 1D视觉tokenizer，通过分层残差学习改善自回归图像生成的表征能力与采样效率，在ImageNet-256上取得gFID 2.34的优异性。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.03955' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval</h3>
<p><strong>Authors:</strong> Zhipeng Qian, Zihan Liang, Yufei Ma, Ben Chen, Huangyu Dai, Yiwei Ma, Jiayi Ji, Chenyi Lei, Han Li, Xiaoshuai Sun</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出CSMCIR框架，通过CoT增强的对称对齐和记忆库解决组合图像检索的模态错位问题，属于原生多模态大模型的重要进展。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.03728' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models</h3>
<p><strong>Authors:</strong> Yang Shi, Yifeng Xie, Minzhe Guo, Liangsi Lu, Mingxuan Huang, Jingchao Wang, Zhihong Zhu, Boyan Xu, Zhiqi Huang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出MMErroR基准，针对视觉语言模型的推理错误进行过程级评估，揭示了模型在推理错误检测中的不足，属于深度学习可解释性的重要基准。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03331' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Yuxuan Xia, Siheng Wang, Peng Li</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文识别出视觉语言模型中的视觉统计偏差导致幻觉，提出SDCD方法抑制偏差减少幻觉，属于深度学习可解释性的重要进展。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03500' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> xDNN(ASP): Explanation Generation System for Deep Neural Networks powered by Answer Set Programming</h3>
<p><strong>Authors:</strong> Ly Ly Trieu (New Mexico State University), Tran Cao Son (New Mexico State University)</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出xDNN(ASP)系统，用Answer Set Programming生成DNN的全局解释，揭示模型结构、特征重要性和隐藏节点影响，为深度学习可解释性提供了全面的解决方案。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03847' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> In Search of Grandmother Cells: Tracing Interpretable Neurons in Tabular Representations</h3>
<p><strong>Authors:</strong> Ricardo Knauer, Erik Rodner</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对表格基础模型（TabPFN）的可解释性问题，提出信息论度量量化神经元的显著性和选择性，发现模型中存在对高层概念有统计显著性的可解释神经元，无需复杂技术即可识别。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03657' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Formally Explaining Decision Tree Models with Answer Set Programming</h3>
<p><strong>Authors:</strong> Akihiro Takemura (National Institute of Informatics, Tokyo, Japan), Masayuki Otani (Tokyo Institute of Technology, Tokyo, Japan), Katsumi Inoue (National Institute of Informatics, Tokyo, Japan)</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 用Answer Set Programming生成决策树的形式化解释（充分、对比、多数等类型），解决了复杂决策树的可解释性问题，属于深度学习可解释性的白盒方法创新。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03845' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Improving Compactness and Reducing Ambiguity of CFIRE Rule-Based Explanations</h3>
<p><strong>Authors:</strong> Sebastian Müller, Tobias Schneider, Ruben Kemna, Vanessa Toborek</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对CFIRE规则解释的冗余和歧义问题，提出后剪枝策略，在保持 fidelity的同时减少规则数量和歧义，实验验证于多个表格数据集。
Score: 6
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03776' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions</h3>
<p><strong>Authors:</strong> Agostino Dovier (DMIF - University of Udine), Talissa Dreossi (DMIF - University of Udine), Andrea Formisano (DMIF - University of Udine), Benedetta Strizzolo (DMIF - University of Udine)</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 利用逻辑编程工具生成法律决策的可解释性，支持规则学习和矛盾处理，属于深度学习可解释性中白盒方法的跨领域应用，方法具有通用性。
Score: 6
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.03844' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning</h3>
<p><strong>Authors:</strong> Yu Luo, Shuo Han, Yihan Hu, Dong Li, Jianye Hao</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出R²VPO框架，通过约束政策比率方差解决硬剪辑的梯度丢失问题，提升LLM微调的样本效率与性能，在数学推理基准上取得显著增益。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03320' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs</h3>
<p><strong>Authors:</strong> Kaiyuan Deng, Hangyu Zheng, Minghai Qing, Kunxiong Zhu, Gen Li, Yang Xiao, Lan Emily Zhang, Linke Guo, Bo Hui, Yanzhi Wang, Geng Yuan, Gagan Agrawal, Wei Niu, Xiaolong Ma</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出HAQA自动化量化部署框架，利用LLM优化硬件感知的量化策略，提升LLM推理速度与准确性，适应不同硬件平台并减少手动 effort。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03484' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> R$^3$L: Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification</h3>
<p><strong>Authors:</strong> Weijie Shi, Yanxi Chen, Zexi Li, Xuchen Pan, Yuchang Sun, Jiajie Xu, Xiaofang Zhou, Yaliang Li</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对LLM强化学习中的探索低效和训练不稳定问题，提出R³L框架，通过“反思-重试”生成高质量轨迹，结合关键信用分配和正向放大优化，实验在代理和推理任务上优于基线5%-52%。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03715' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition</h3>
<p><strong>Authors:</strong> Muyang Zhao, Qi Qi, Hao Sun</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出ROI-Reasoning框架，通过元认知预计算优化推理资源分配，在严格token约束下提升多任务推理性能，为高效大模型推理提供了新的优化思路。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03822' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models</h3>
<p><strong>Authors:</strong> Wei Wu, Liyi Chen, Congxi Xiao, Tianfu Wang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Hui Xiong</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出动态异常截断方法解决“长度偏移”问题，在不损失复杂推理能力的情况下减少冗余token，显著提升推理模型效率，属于高效大模型训练的关键优化技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03969' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction</h3>
<p><strong>Authors:</strong> Xiaokun Sun, Zezhong Wu, Zewen Ding, Linli Xu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 该论文提出MVP方法，通过自监督掩码视频预测增强视频大模型的时间一致性和因果理解，属于高效大模型训练与推理的重要研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03781' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Lightweight Transformer Architectures for Edge Devices in Real-Time Applications</h3>
<p><strong>Authors:</strong> Hema Hariharan Samson</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 系统综述轻量级Transformer架构，分析压缩、量化等技术在边缘设备的部署，实验表明现代轻量级模型在保持精度同时降低大小与延迟，支撑实时应用。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03290' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries</h3>
<p><strong>Authors:</strong> Sumedh Pendurkar, Guni Sharon</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对Tree-of-Thoughts（ToT）框架下LLM推理的高计算成本问题，提出Levin Tree Search（LTS）算法，利用LLM的概率作为启发式引导搜索，减少思想评估次数。理论证明搜索步数的 bounds，实验在三个任务上验证效率和准确性的提升。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03606' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AMIR-GRPO: Inducing Implicit Preference Signals into GRPO</h3>
<p><strong>Authors:</strong> Amir Hossein Yari, Fajri Koto</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对GRPO在LLM推理任务中的长度偏差和样本效率问题，提出AMIR-GRPO，通过隐式对比正则化放大偏好信号，改进数学推理性能，实验显示优于基线。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03661' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL</h3>
<p><strong>Authors:</strong> Lang Cao, Hui Ruan, Yongqian Li, Peng Chao, Wu Ning, Haonan Song, Renhong Chen, Yitong Li</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对Group-based RL（如GRPO）中轨迹级优势分配的样本效率问题，提出TreeAdv，通过树结构显式建模rollout轨迹，重新分配优势，减少token使用并提高数学推理性能。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03703' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization</h3>
<p><strong>Authors:</strong> Shijie Zhang, Kevin Zhang, Zheyuan Gu, Xiang Guo, Rujun Guo, Shaoyu Liu, Guanjun Jiang, Xiaozhao Wang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对GRPO的静态信任区域限制，提出ETR动态调整信任区域，适应结果驱动学习的异质信号，改进数学推理性能并缓解熵 collapse。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03723' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning</h3>
<p><strong>Authors:</strong> Jing-Cheng Pang, Liu Sun, Chang Zhou, Xian Tang, Haichuan Ma, Kun Jiang, Jianlong Wang, Kai Zhang, Sijie Wu, Haoran Cai, Chenwei Wu, Xubin Li, Xin Chen</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对领域特定LLM微调中的静态课程问题，提出EDCO动态课程框架，基于推理熵自适应选择样本，改进通信、医疗、法律领域的微调性能，减少83.5%计算时间。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03725' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training</h3>
<p><strong>Authors:</strong> Chi Liu, Xin Chen</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对GRPO的裁剪机制缺陷，提出ABC-GRPO，通过不对称自适应裁剪改进数学推理性能，保持更高熵，实验优于标准GRPO。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03895' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning</h3>
<p><strong>Authors:</strong> Yujie Feng, Hao Wang, Jian Li, Xu Chu, Zhaolu Kang, Yiran Liu, Yasha Wang, Philip S. Yu, Xiao-Ming Wu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对LLM持续学习的灾难性遗忘问题，提出FOREVER，基于Ebbinghaus遗忘曲线设计记忆回放策略，改进三个CL基准的性能。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03938' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models</h3>
<p><strong>Authors:</strong> Magnus Bühler, Lennart Purucker, Frank Hutter</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对表格基础模型微调的数据稀缺问题，提出CausalMixFT，利用因果结构生成合成样本，改进低数据 regime的微调稳定性和性能，实验优于统计生成器。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.04110' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization</h3>
<p><strong>Authors:</strong> Alberto Purpura, Li Wang, Sahil Badyal, Eugenio Beaufrand, Adam Faulkner</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 针对LLM指令遵循的形式约束问题，提出多代理工作流，分离任务描述与约束优化，改进Llama 3.1和Mixtral的合规性。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ReEfBench: Quantifying the Reasoning Efficiency of LLMs</h3>
<p><strong>Authors:</strong> Zhizhang Fu, Yuancheng Gu, Chenkai Hu, Hanmeng Liu, Yue Zhang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出神经符号框架量化LLM推理效率，分析推理模式、训练策略和模型规模对效率的影响，为高效大模型推理提供了基准和关键 insights。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03550' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction</h3>
<p><strong>Authors:</strong> Chen Zhang, Kepu Zhang, Jiatong Zhang, Xiao Zhang, Jun Xu</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 提出Sandwich Reasoning范式，在保持推理准确性的同时将查询修正延迟降低40%-70%，解决了实时场景下的 latency-accuracy 权衡问题，属于高效大模型推理的实用研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03672' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation</h3>
<p><strong>Authors:</strong> Zihang Li, Yuhang Wang, Yikun Zong, Wenhan Yu, Xiaokun Yuan, Runhan Jiang, Zirui Liu, Tong Yang, Arthur Jiang</p>
<p><strong>Published:</strong> 2026-01-08</p>
<p><strong>Reason:</strong> 通过熵引导的自适应分割优化CoT数据，过滤“答案正确但推理错误”的低质量样本，显著提升LLM数学推理性能，属于高效大模型训练中数据优化的关键工作。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.03769' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>