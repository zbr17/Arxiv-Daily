<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-14</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>大模型新技术</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-14</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-14 12:57:22</p>
<p>已自动阅读了 343 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：179978</p>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur</h3>
<p><strong>Authors:</strong> Yani Meziani</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出融合哈密顿状态空间对偶性与视觉-语言联合嵌入的新架构，引入物理启发的归纳偏置（如辛积分、哈密顿流匹配），实现超低成本的视觉合成和时空一致性，为大模型架构设计提供新范式。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.06212' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mosaic: Unlocking Long-Context Inference for Diffusion LLMs via Global Memory Planning and Dynamic Peak Taming</h3>
<p><strong>Authors:</strong> Liang Zheng, Bowen Shi, Yitao Hu, Jiawei Zhang, Ruofan Li, Sheng Chen, Wenxin Li, Keqiu Li</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散LLM的内存瓶颈，提出全局内存规划框架，通过掩码仅日志内核、惰性分块优化等技术显著降低内存峰值并提升长上下文推理能力，是diffusion LLM实用化的关键突破。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.06562' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation</h3>
<p><strong>Authors:</strong> Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao, Hao Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散LLM（dLLM）的准确性-并行性权衡问题，提出d3LLM，通过伪轨迹蒸馏和熵基多块解码，实现10倍于vanilla dLLM的速度提升，同时保持精度，引入AUP metric评估准确性与并行性。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.07568' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</h3>
<p><strong>Authors:</strong> Yuanyang Yin, Yufan Deng, Shenghai Yuan, Kaipeng Zhang, Xiao Yang, Feng Zhao</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对视频扩散模型的文本可控性问题，提出Focal Guidance机制增强语义弱层的文本引导，属于大模型新技术（diffusion LLM）的可控性优化。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.07287' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control</h3>
<p><strong>Authors:</strong> Wonhyeok Choi, Minwoo Choi, Jungwan Woo, Kyumin Hwang, Jaeyeul Kim, Sunghoon Im</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 综述在线扩散政策强化学习（Online DPRL）算法，提出四类分类（Action-Gradient、Q-Weighting等），在NVIDIA Isaac Lab上评估12个机器人任务，分析算法权衡（样本效率与扩展性），属于大模型新技术中扩散政策的重要综述与分析。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.06133' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Forget Many, Forget Right: Scalable and Precise Concept Unlearning in Diffusion Models</h3>
<p><strong>Authors:</strong> Kaiyuan Deng, Gen Li, Yang Xiao, Bo Hui, Xiaolong Ma</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散模型多概念遗忘的三大挑战，提出ScaPre框架，实现高效精确的大规模概念移除，同时保持生成质量，属于diffusion LLM方向的关键改进，实验验证优于现有基线。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.06162' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Teach Diffusion Language Models to Learn from Their Own Mistakes</h3>
<p><strong>Authors:</strong> Liming Liu, Binxuan Huang, Xin Liu, Bing Yin, Tuo Zhao</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散语言模型（DLM）并行生成的错误问题，提出解耦自校正框架，通过双阶段训练和未来上下文增强提升生成质量，在数学推理和代码生成上显著缓解质量退化，属于diffusion LLM的关键优化。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.06428' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> BabyVision: Visual Reasoning Beyond Language</h3>
<p><strong>Authors:</strong> Liang Chen, Weichu Xie, Yiyan Liang, Hongfeng He, Hans Zhao, Zhibo Yang, Zhiqi Huang, Haoning Wu, Haoyu Lu, Y. charles, Yiping Bao, Yuantao Fan, Guopeng Li, Haiyang Shen, Xuanzhong Chen, Wendong Xu, Shuzheng Si, Zefan Cai, Wenhao Chai, Ziqi Huang, Fangfu Liu, Tianyu Liu, Baobao Chang, Xiaobo Hu, Kaiyuan Chen, Yixin Ren, Yang Liu, Yuan Gong, Kuan Li</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出BabyVision基准评估多模态大模型的基础视觉推理能力，发现现有模型远低于人类水平，为多模态模型的视觉理解研究提供关键 benchmarks，推动模型向人类级视觉能力发展。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06521' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> CLIMP: Contrastive Language-Image Mamba Pretraining</h3>
<p><strong>Authors:</strong> Nimrod Shabtay, Itamar Zimerman, Eli Schwartz, Raja Giryes</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 首次用Mamba替代CLIP的视觉与文本编码器，构建全Mamba的对比视觉语言模型，解决Transformer的 quadratic复杂度问题，提升OOD鲁棒性与分辨率适应性，属于原生多模态大模型的架构创新。
Score: 8.5
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06891' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding</h3>
<p><strong>Authors:</strong> Zhiyong Ma, Zhenpeng Li, Yuanjie Shi, Zhengping Li, Jiahao Chen, Qingyuan Chuai</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出训练-free框架TBDN，通过Hint Instruction注入归纳偏置和Query Contrastive Decoding抑制幻觉，解决文本到图像上下文学习中的两个核心瓶颈，实验在多个基准上取得SOTA性能，提升多模态生成的可靠性。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06169' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</h3>
<p><strong>Authors:</strong> Hao Tang, Ting Huang, Zeyu Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出结合对比学习与测试时搜索的3D字幕框架，解决3D场景理解的泛化问题，在多个基准上提升性能，为3D多模态大模型的场景理解提供新方法。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06496' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models</h3>
<p><strong>Authors:</strong> Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Yuhua Zhu, Wenhui Zhao</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出结合Grounding DINO和LLaVA-OneVision的语义多目标跟踪框架，将几何定位与认知推理结合，提升跟踪的语义理解能力，在基准上取得SOTA性能，拓展多模态模型的应用场景。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06550' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ArrowGEV: Grounding Events in Video via Learning the Arrow of Time</h3>
<p><strong>Authors:</strong> Fangxu Yu, Ziyao Lu, Liqiang Niu, Fandong Meng, Jie Zhou</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出强化学习框架ArrowGEV，通过学习时间箭头（temporal directionality）提升视频事件接地的准确性和时间理解能力，实验验证其在事件接地和推理中的有效性，为视频多模态理解提供新方法。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06559' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Junyan Lin, Junlong Tong, Hao Wu, Jialiang Zhang, Jinming Liu, Xin Jin, Xiaoyu Shen</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对多模态大模型的实时视频理解瓶颈，提出并行流框架放松位置连续性约束，实现感知与生成并行，提升实时交互能力，属于原生多模态大模型的关键性能优化。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06843' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unified Personalized Understanding, Generating and Editing</h3>
<p><strong>Authors:</strong> Yu Zhong, Tianwei Lin, Ruike Zhu, Yuqian Yuan, Haoyu Zheng, Liang Liang, Wenqiao Zhang, Feifei Shao, Haoyuan Li, Wanggui He, Hao Jiang, Yueting Zhuang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出OmniPersona框架，首次在统一多模态大模型中整合个性化理解、生成与编辑，解决现有方法的任务干扰问题，属于原生多模态大模型的个性化能力拓展。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06965' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding</h3>
<p><strong>Authors:</strong> Jiapeng Shi, Junke Wang, Zuyao You, Bo He, Zuxuan Wu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出VideoLoom视频大模型，整合时空定位能力，构建LoomData与LoomBench，属于原生多模态大模型的视频理解方向，提升时空联合性能。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07290' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training</h3>
<p><strong>Authors:</strong> Shezheng Song, Shasha Li, Jie Yu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对多模态大语言模型（MLLMs）内部推理中“视觉关注正确但输出错误”的矛盾，提出无需训练的双视角解码优化策略DualPD，通过层间注意力引导对比对数模块和层内低贡献头过滤模块，有效提升模型视觉理解与输出一致性，在LLaVA、Qwen-VL等模型上验证了普适性，与原生多模态大模型研究高度相关。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation</h3>
<p><strong>Authors:</strong> Yuze He, Yanning Zhou, Wang Zhao, Jingwen Ye, Zhongkai Wu, Ran Yi, Yong-Jin Liu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出语义分解的3D角色生成系统，通过双分支语义感知模型联合重建几何、颜色与组件语义，支持非破坏性编辑和物理兼容动画，解决传统方法生成结果缺乏结构灵活性的问题，属于原生多模态大模型中3D内容生成的关键成果。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07660' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding</h3>
<p><strong>Authors:</strong> Yanxiang Huang, Guohua Gao, Zhaoyang Wei, Jianyuan Ni</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对大视觉语言模型（LVLMs）视频推理中计算成本高与幻觉问题，提出证据链（CoE）框架，通过轻量级证据接地模块和强化学习优化的证据锚定协议，构建CoE-Instruct数据集，在Video-MME、MVBench等基准上达到SOTA，属于原生多模态大模型中视频推理的核心突破。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07761' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> More Images, More Problems? A Controlled Analysis of VLM Failure Modes</h3>
<p><strong>Authors:</strong> Anurag Das, Adrian Bulat, Alberto Baldrati, Ioannis Maniadis Metaxas, Bernt Schiele, Georgios Tzimiropoulos, Brais Martinez</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 构建MIMIC基准，分析LVLMs的多图像理解缺陷（如跨图像信息聚合失败），提出数据生成策略和注意力掩码方案，显著提升跨图像推理性能，属于原生多模态大模型中多图像理解的关键研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07812' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sissi: Zero-shot Style-guided Image Synthesis via Semantic-style Integration</h3>
<p><strong>Authors:</strong> Yingying Deng, Xiangyu He, Fan Tang, Weiming Dong, Xucheng Yin</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出训练-free的风格引导图像合成框架，通过语义-风格融合解决现有方法的语义偏离和风格不忠问题，实验验证其高保真和一致性，提升多模态生成的灵活性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06605' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Improving Video Question Answering through query-based frame selection</h3>
<p><strong>Authors:</strong> Himanshu Patil, Geo Jolly, Ramana Raja Buddala, Ganesh Ramakrishnan, Rohit Saluja</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对视频问答中均匀帧采样无法捕捉问题相关上下文的缺陷，提出基于子模互信息（SMI）的查询式帧选择方法，替换传统均匀采样，使所选帧更贴合问题需求，在Video-LLaVA、LLaVA-NeXT上实现4%准确率提升，属于原生多模态大模型中视频理解的重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07459' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> UIKA: Fast Universal Head Avatar from Pose-Free Images</h3>
<p><strong>Authors:</strong> Zijian Wu, Boyao Zhou, Liangxiao Hu, Hongyu Liu, Yuan Sun, Xuan Wang, Xun Cao, Yujun Shen, Hao Zhu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出UV引导的头像建模策略，从无姿态图像生成通用可动画3D头部模型，通过UV空间映射整合多视图信息，解决传统方法依赖多相机或长优化的问题，显著优于现有单目/多视图方法，属于原生多模态大模型中3D生成的重要进展。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07603' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</h3>
<p><strong>Authors:</strong> Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLMs）3D空间推理中数值预测不准确的问题，提出平滑数值奖励激活（SNRA）和绝对保留GRPO（AP-GRPO）框架，构建Numerical3D-50k数据集，激活VLM的潜在3D推理能力，在不修改架构的情况下达到与监督方法相当的性能，属于原生多模态大模型中空间推理的重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07695' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Evaluating the encoding competence of visual language models using uncommon actions</h3>
<p><strong>Authors:</strong> Chen Ling, Nai Ding</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 构建UAIT数据集（不常见动作的图像-文本对），评估VLMs对语义合理性的理解能力，发现模型在区分语法正确与语义合理上远逊于人类，为VLMs的语义推理改进提供诊断工具，属于原生多模态大模型的评估与优化。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.07737' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Hellinger Multimodal Variational Autoencoders</h3>
<p><strong>Authors:</strong> Huyen Khanh Vo, Isabel Valera</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出基于Hellinger pooling的多模态变分自编码器，改进联合 posterior近似，在多模态生成任务上实现更优的生成一致性和质量，属于原生多模态大模型的基础研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06572' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DaQ-MSA: Denoising and Qualifying Diffusion Augmentations for Multimodal Sentiment Analysis</h3>
<p><strong>Authors:</strong> Jiazhang Liang, Jianheng Dai, Miaosen Luo, Menghua Jiang, Sijie Mai</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 用扩散模型增强多模态情感分析数据，提出质量评分模块过滤噪声样本，提升多模态大模型性能，属于原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.06870' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers</h3>
<p><strong>Authors:</strong> Guantao Chen, Shikang Zheng, Yuqi Lin, Linfeng Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散Transformer（DiT）迭代采样的高计算成本问题，提出子空间感知特征缓存框架SVD-Cache，通过SVD分解特征为可预测的主 subspace 和稳定的残差 subspace，结合EMA预测与直接复用策略，实现FLUX、HunyuanVideo等模型的5.55×推理加速，属于高效大模型推理的关键优化。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07396' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> From Sketch to Fresco: Efficient Diffusion Transformer with Progressive Resolution</h3>
<p><strong>Authors:</strong> Shikang Zheng, Guantao Chen, Lixuan He, Jiacheng Liu, Yuqi Lin, Chang Zou, Linfeng Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散Transformer生成效率低的问题，提出渐进分辨率框架Fresco，通过统一跨阶段重噪声与全局结构、逐步上采样收敛区域，实现FLUX的10×推理加速和HunyuanVideo的5×加速，同时保持生成质量，属于高效大模型推理的核心创新。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07462' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training</h3>
<p><strong>Authors:</strong> Lingchen Sun, Rongyuan Wu, Zhengqiang Zhang, Ruibin Li, Yujing Sun, Shuaizheng Liu, Lei Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对扩散Transformer训练依赖外部语义特征（如DINO）的问题，提出Self-Transcendence方法，利用模型内部特征引导训练，无需外部网络，在生成质量和收敛速度上超过REPA，属于高效大模型训练的重要优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07773' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Enabling Long FFT Convolutions on Memory-Constrained FPGAs via Chunking</h3>
<p><strong>Authors:</strong> Peter Wang, Neelesh Gupta, Viktor Prasanna</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对FPGAs上长FFT卷积的内存限制问题，提出分块FFT卷积方法，通过分块和重叠-相加重建，实现450K长度序列的卷积，在Alveo U200上保持高吞吐量（仅7%性能下降），属于高效大模型推理中硬件加速的重要进展。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06065' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Latent Space Communication via K-V Cache Alignment</h3>
<p><strong>Authors:</strong> Lucio M. Dery, Zohar Yahav, Henry Prior, Qixuan Feng, Jiajun Shen, Arthur Szlam</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对多模型协作中文字通信效率低的问题，提出通过K-V缓存对齐构建共享 latent 空间，无需修改预训练参数，实现多模型的高效通信和技能转移（如软prompt迁移），属于高效大模型推理中多模型协作的重要创新。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06123' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AIConfigurator: Lightning-Fast Configuration Optimization for Multi-Framework LLM Serving</h3>
<p><strong>Authors:</strong> Tianhao Xu, Yiming Liu, Xianglong Lu, Yijia Zhao, Xuting Zhou, Aichen Feng, Yiyi Chen, Yi Shen, Qin Zhou, Xumeng Chen, Ilya Sherstyuk, Haorui Li, Rishi Thakkar, Ben Hamm, Yuanzhe Li, Xue Huang, Wenpeng Wu, Anish Shanbhag, Harry Kim, Chuan Chen, Junjie Lai</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出多框架LLM服务配置优化框架，通过解析推理原语和校准内核数据库实现快速配置搜索，显著提升dense和MoE模型的服务性能，属于高效LLM推理的关键工具。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06288' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Monkey Jump : MoE-Style PEFT for Efficient Multi-Task Learning</h3>
<p><strong>Authors:</strong> Nusrat Jahan Prottasha, Md Kowsher, Chun-Nam Yu, Chen Chen, Ozlem Garibay</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出MoE风格的参数高效微调方法，无需额外专家或路由器参数，通过k-means路由实现token级自适应，在多任务基准上用更少参数和内存取得 competitive性能，属于高效LLM训练的重要改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06356' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MoE-DisCo:Low Economy Cost Training Mixture-of-Experts Models</h3>
<p><strong>Authors:</strong> Xin Ye, Daning Cheng, Boyang Zhang, Yunquan Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出分阶段MoE训练框架，通过数据聚类与独立训练降低成本，性能匹配全参数训练，成本降低47.6%-69.5%，解决MoE高成本问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06857' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HAS-VQ: Hessian-Adaptive Sparse Vector Quantization for High-Fidelity LLM Compression</h3>
<p><strong>Authors:</strong> Vladimer Khasia</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出Hessian自适应稀疏向量量化方法，分离敏感参数与主体权重，保持LLM压缩后的高保真度，优于标准整数量化，属于高效大模型推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06959' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics</h3>
<p><strong>Authors:</strong> Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出Fisher对齐子空间压缩方法，基于二阶敏感性保留知识，比方差基方法多保留6-8%的知识密集型任务 accuracy，属于高效大模型推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07197' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training</h3>
<p><strong>Authors:</strong> Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, Bo Zhou</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对RLVR中PPO的优势估计不可靠问题，提出SAE框架，通过分割序列为信息丰富的子段并选择性计算优势估计，提升LLM训练的稳定性、样本效率和最终性能，实验验证跨模型大小的一致改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07320' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs</h3>
<p><strong>Authors:</strong> Haoqian Meng, Yilun Luo, Yafei Zhao, Wenyuan Liu, Peng Zhang, Xindian Ma</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对NVFP4量化的挑战，提出ARCQuant框架，通过增强残差通道将误差补偿整合到矩阵维度，保持统一NVFP4格式，实验验证在LLaMA和Qwen模型上实现接近全精度的性能，同时提升推理速度。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07475' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding</h3>
<p><strong>Authors:</strong> Yuxuan Zhou, Fei Huang, Heng Li, Fengyi Wu, Tianyu Wang, Jianwei Zhang, Junyang Lin, Zhi-Qi Cheng</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对推测解码中联合不可处理的验证瓶颈，提出分层推测解码方法，通过平衡分支概率质量提升接受率，集成到EAGLE-3后性能提升超12%，为高效大模型推理提供了新方案。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.05724' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Revisiting Training Scale: An Empirical Study of Token Count, Power Consumption, and Parameter Efficiency</h3>
<p><strong>Authors:</strong> Joe Dwyer</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 实证分析训练token数对参数效率和能耗的影响，填补训练规模与计算成本研究 gap，为高效大模型训练提供实践指导。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06649' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget</h3>
<p><strong>Authors:</strong> Zohaib Khan, Omer Tafveez, Zoha Hayat Bhatti</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 研究微预算下低秩适配器对小模型推理的影响，发现适配器容量与模型初始化的关键作用，解决小模型推理可塑性问题。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.06677' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Stable On-Policy Distillation through Adaptive Target Reformulation</h3>
<p><strong>Authors:</strong> Ijun Jang, Jewon Yeom, Juan Yeo, Hyunggu Lim, Taesup Kim</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出自适应目标重构的在线知识蒸馏方法，解决分布 mismatch 与训练不稳定问题，提升小模型推理性能，属于高效大模型训练方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.07155' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</h3>
<p><strong>Authors:</strong> Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对线性注意力中全局上下文崩溃的问题，提出令牌级多头线性注意力（MHLA），通过令牌维度的多头计算恢复表示能力，在ImageNet分类、NLP、图像/视频生成上实现3.6%~41%的性能提升，属于深度学习理论中网络架构的重要创新。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07832' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Hessian of tall-skinny networks is easy to invert</h3>
<p><strong>Authors:</strong> Ali Rahimi</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出瘦高网络（tall-skinny networks）Hessian逆的精确算法，无需存储Hessian或其逆，时间和空间复杂度与层数线性相关，解决了传统方法 quadratic 存储和 cubic 计算的问题，属于深度学习理论中优化算法的核心突破。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06096' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Filtering Beats Fine Tuning: A Bayesian Kalman View of In Context Learning in LLMs</h3>
<p><strong>Authors:</strong> Andrew Kiruluta</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 将LLMs的上下文学习（ICL）解释为在线贝叶斯状态估计，提出Kalman递归框架，分析不确定性动态和prompt信息积累，证明优化基适应是贝叶斯推理的退化近似，属于深度学习理论中上下文学习的重要理论贡献。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06100' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CliffordNet: All You Need is Geometric Algebra</h3>
<p><strong>Authors:</strong> Zhongping Ji</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出基于几何代数的CliffordNet，以统一的几何乘积交互机制替代传统CNN/Transformer的启发式模块，属于深度学习理论中的网络架构创新，实验显示小参数模型性能优于ResNet-18，有较强理论与应用价值。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06793' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Tree-Preconditioned Differentiable Optimization and Axioms as Layers</h3>
<p><strong>Authors:</strong> Yuexin Liao</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出将随机效用模型（RUM）的公理结构嵌入深度网络的可微框架，利用布尔格上的流守恒推导树预处理共轭梯度求解器，解决RUM投影的NP难问题，属于深度学习理论中优化与模型结构的核心创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06036' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths</h3>
<p><strong>Authors:</strong> Xuezhe Ma, Shicheng Wen, Linghao Jin, Bilge Acun, Ruihang Lai, Bohan Hou, Will Lin, Hao Zhang, Songlin Yang, Ryan Lee, Mengxi Wu, Jonathan May, Luke Zettlemoyer, Carole-Jean Wu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出高效处理任意长度序列的神经架构，解决Transformer的二次复杂度和长度外推问题，通过时间步衰减归一化等设计实现长上下文处理，属于深度学习理论中网络架构方向的重要突破。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06463' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Implicit bias as a Gauge correction: Theory and Inverse Design</h3>
<p><strong>Authors:</strong> Nicola Aladrah, Emanuele Ballarin, Matteo Biagetti, Alessio Ansuini, Alberto d'Onofrio, Fabio Anselmi</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 研究深度学习理论中的隐式偏差，提出gauge校正机制，统一现有隐式偏差结果并支持逆设计，具有理论创新性。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06597' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Why are there many equally good models? An Anatomy of the Rashomon Effect</h3>
<p><strong>Authors:</strong> Harsh Parikh</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 系统分析Rashomon效应的三类原因（统计、结构、过程），为理解模型多样性与泛化提供统一框架，属于深度学习理论核心问题。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06730' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Artificial Entanglement in the Fine-Tuning of Large Language Models</h3>
<p><strong>Authors:</strong> Min Chen, Zihan Wang, Canyu Chen, Zeguan Wu, Manling Li, Junyu Liu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 从量子信息视角分析LLM微调中的人工纠缠，发现LoRA与全微调的纠缠差异及"无毛发"性质，深化对LLM微调机制的理论理解。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06788' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Tight Analysis of Decentralized SGD: A Markov Chain Perspective</h3>
<p><strong>Authors:</strong> Lucas Versini, Paul Mangold, Aymeric Dieuleveut</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 用马尔可夫链视角分析去中心化SGD，给出收敛界与线性加速结论，深化对优化器动态的理论理解，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07021' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Innovation Capacity of Dynamical Learning Systems</h3>
<p><strong>Authors:</strong> Anthony M. Polloreno</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出创新能力概念，统一可预测与创新容量的分解，证明守恒定律，深化对动态学习系统信息处理的理论理解，属于深度学习理论方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07257' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Free-RBF-KAN: Kolmogorov-Arnold Networks with Adaptive Radial Basis Functions for Efficient Function Learning</h3>
<p><strong>Authors:</strong> Shao-Ting Chiu, Siu Wun Cheung, Ulisses Braga-Neto, Chak Shing Lee, Rui Peng Li</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出Free-RBF-KAN，用自适应径向基函数（RBF）改进Kolmogorov-Arnold Networks（KAN），提升计算效率和函数逼近性能，提供RBF-KAN的通用性证明，实验验证在多任务上优于B-spline KAN。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07760' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Revisiting the Ordering of Channel and Spatial Attention: A Comprehensive Study on Sequential and Parallel Designs</h3>
<p><strong>Authors:</strong> Zhongming Liu, Bingbing Jiang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 系统分析通道与空间注意力的顺序与并行设计，提出数据规模与方法选择的规律，属于深度学习理论中的注意力机制研究，有指导意义。
Score: 7.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07310' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Parent-Guided Adaptive Reliability (PGAR): A Behavioural Meta-Learning Framework for Stable and Trustworthy AI</h3>
<p><strong>Authors:</strong> Anshum Rankawat</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出行为元学习框架，通过父层调节学习率提升模型稳定性与校准性，属于深度学习理论中的优化器方向，理论上有Lyapunov稳定性证明，实验验证优于标准优化器。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06167' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FlexAct: Why Learn when you can Pick?</h3>
<p><strong>Authors:</strong> Ramnath Kumar, Kyle Ritscher, Junmin Judy, Lawrence Liu, Cho-Jui Hsieh</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出动态激活函数选择框架，利用Gumbel-Softmax实现离散可微分的激活函数挑选，改进网络架构的适应性，属于深度学习理论中网络架构方向的创新尝试。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06441' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Deriving Decoder-Free Sparse Autoencoders from First Principles</h3>
<p><strong>Authors:</strong> Alan Oursland</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 从第一性原理推导无解码器的稀疏自编码器，解决传统自编码器的 collapse问题，通过InfoMax正则化保持表示质量，属于深度学习理论中网络架构方向的理论创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06478' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Softly Induced Functional Simplicity Implications for Neural Network Generalisation, Robustness, and Distillation</h3>
<p><strong>Authors:</strong> Maciej Glowacki</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 分析软对称归纳偏置对神经网络功能复杂度的影响，揭示低复杂度模型在泛化、鲁棒性和可蒸馏性上的优势，属于深度学习理论中归纳偏置与模型性能关系的重要研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06584' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Variational decomposition autoencoding improves disentanglement of latent representations</h3>
<p><strong>Authors:</strong> Ioannis Ziogas, Aamna Al Shehhi, Ahsan H. Khandoker, Leontios J. Hadjileontiadis</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出变分分解自编码器改进隐表示解纠缠，结合信号分解与对比学习，在多领域数据上优于现有VAE方法，属于深度学习理论中的表示学习方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.06844' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training</h3>
<p><strong>Authors:</strong> Xueyan Niu, Bo Bai, Wei Han, Weixi Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 证明了监督微调（SFT）与强化学习（RL）在LLM后训练中无法解耦，理论分析并实验验证两者顺序执行会导致性能下降，为LLM训练流程设计提供重要理论依据。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07389' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Sharpness: A Flatness Decomposition Framework for Efficient Continual Learning</h3>
<p><strong>Authors:</strong> Yanan Chen, Tieliang Gong, Yunjiao Zhang, Wen Wen</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出FLAD框架，分解尖锐度扰动为梯度对齐和随机噪声分量，仅保留噪声分量提升持续学习的泛化能力，结合轻量调度方案，实验验证在多种CL范式上优于标准和尖锐度感知优化器。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.07636' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis</h3>
<p><strong>Authors:</strong> Zihao Fu, Xufeng Duan, Zhenguang G. Cai</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出SCALPEL框架，将LLM能力表示为低秩参数子空间，通过LoRA训练实现目标能力的精准消融而不影响其他能力，解决传统可解释性方法的粗粒度问题，提供细粒度的能力分布 insights。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.07411' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition</h3>
<p><strong>Authors:</strong> Jakob Paul Zimmermann, Georg Loho</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出通过DC分解（将ReLU网络分解为两个单调凸函数之和）解释深度模型，开发SplitCAM和SplitLRP显著性方法，在VGG16、ResNet18上优于现有可解释性方法，属于深度学习可解释性中基于单调性的白盒解释创新。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.07700' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GroupSegment-SHAP: Shapley Value Explanations with Group-Segment Players for Multivariate Time Series</h3>
<p><strong>Authors:</strong> Jinwoong Kim, Sangjin Park</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对时间序列SHAP方法中特征与时间轴独立的问题，提出GroupSegment-SHAP，通过组-段玩家（cross-variable依赖和时间分布转移）量化贡献，在人类活动识别等四个领域提升faithfulness（1.7×）并降低 runtime（40%），属于深度学习可解释性中Shapley值的重要扩展。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.06114' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explainability of Complex AI Models with Correlation Impact Ratio</h3>
<p><strong>Authors:</strong> Poushali Sengupta, Rabindra Khadka, Sabita Maharjan, Frank Eliassen, Yan Zhang, Shashi Raj Pandey, Pedro G. Lind, Anis Yazidi</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出ExCIR方法解决复杂模型可解释性问题，改进现有方法对相关特征的误排序和高维数据 scalability 问题，理论与实证支持充分。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.06701' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explaining Machine Learning Predictive Models through Conditional Expectation Methods</h3>
<p><strong>Authors:</strong> Silvia Ruiz-Espa~na (ITI, Universitat Polit\`ecnica de Val\`encia, Val\`encia, Spain), Laura Arnal (ITI, Universitat Polit\`ecnica de Val\`encia, Val\`encia, Spain), Fran\c{c}ois Signol (ITI, Universitat Polit\`ecnica de Val\`encia, Val\`encia, Spain), Juan-Carlos Perez-Cortes (ITI, Universitat Polit\`ecnica de Val\`encia, Val\`encia, Spain), Joaquim Arlandis (ITI, Universitat Polit\`ecnica de Val\`encia, Val\`encia, Spain)</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出MUCE方法通过条件期望解释模型预测，提供图形与定量分析，解决复杂模型的局部可解释性问题，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.07313' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Triadic Concept Analysis for Logic Interpretation of Simple Artificial Networks</h3>
<p><strong>Authors:</strong> Ingo Schmitt</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 利用三元概念分析将简单人工神经网络转换为可解释的逻辑树，解决了ANN的白盒解释问题，为深度学习可解释性提供了基于逻辑的新方法。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.06229' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Forget-It-All: Multi-Concept Machine Unlearning via Concept-Aware Neuron Masking</h3>
<p><strong>Authors:</strong> Kaiyuan Deng, Bo Hui, Gen Li, Jie Ji, Minghai Qin, Geng Yuan, Xiaolong Ma</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对多概念机器遗忘问题提出训练-free的FIA框架，通过概念感知神经元masking解决现有方法在多概念遗忘中的有效性、生成质量和超参数敏感性问题，实验验证其在多任务中的可靠性，对大模型安全中的敏感概念删除有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06163' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization</h3>
<p><strong>Authors:</strong> Miao Pan, Wangjie Gan, Jintao Chen, Wenqi Zhang, Bing Sun, Jianwei Yin, Xuhong Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 系统分析多模态大模型幻觉的三大成因，提出包含字幕反馈、多样性采样和冲突正则的框架，有效降低幻觉率并提升推理准确性，对大模型安全对齐中的幻觉问题有针对性解决。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06224' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Adversarial Robustness of 3D Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Chao Liu, Ngai-Man Cheung</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 首次系统研究3D多模态大模型的对抗鲁棒性，提出视觉攻击和字幕攻击两种策略，发现模型在无目标攻击下的脆弱性和目标攻击下的 resilience，为3D大模型的安全部署提供指导。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06464' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> APEX: Learning Adaptive Priorities for Multi-Objective Alignment in Vision-Language Generation</h3>
<p><strong>Authors:</strong> Dongliang Chen, Xinlin Zhuang, Junjie Xu, Luojian Xie, Zehui Wang, Jiaxi Zhuang, Haolin Yang, Liang Dou, Xiao He, Xingjiao Wu, Ying Qian</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出自适应优先级的多目标对齐框架APEX，解决静态权重导致的优化不平衡问题，提升文本到图像生成的多目标平衡性能，对大模型的多目标对齐有重要贡献。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Impact of Post-training on Data Contamination</h3>
<p><strong>Authors:</strong> Muhammed Yusuf Kocyigit, Caglar Yildirim</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 研究后训练（SFT、GRPO）对数据污染的影响，发现SFT仅 inflated 污染任务得分，GRPO会扩散到未污染任务，模型规模放大这些趋势，为大模型训练中数据污染的审计和缓解提供关键 insights，属于大模型安全与对齐的重要研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06103' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stress Testing Machine Learning at $10^{10}$ Scale: A Comprehensive Study of Adversarial Robustness on Algebraically Structured Integer Streams</h3>
<p><strong>Authors:</strong> HyunJun Jeon</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 利用10^10规模的结构化数学数据（毕达哥拉斯三元组）测试机器学习的对抗鲁棒性，提出Hypothesis-driven Negative Dataset（HND），发现LightGBM依赖二次模式而非代数验证，为对抗鲁棒性研究提供大规模基准，属于大模型安全与对齐的重要成果。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06117' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ECLIPTICA - A Framework for Switchable LLM Alignment via CITA - Contrastive Instruction-Tuned Alignment</h3>
<p><strong>Authors:</strong> Kapil Wanaskar, Gaytri Jena, Vinija Jain, Aman Chadha, Amitava Das</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出可切换的LLM对齐框架，解决传统静态对齐局限，实现基于自然语言指令的 runtime行为调控，在多基准上显著优于DPO等方法，对大模型对齐的动态性研究有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06157' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MixDPO: Modeling Preference Strength for Pluralistic Alignment</h3>
<p><strong>Authors:</strong> Saki Imai, Pedram Heydari, Anthony Sicilia, Asteria Kaeberlein, Katherine Atwell, Malihe Alikhani</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 扩展DPO模型以捕捉偏好强度异质性，提出MixDPO，在保持子群体偏好的同时提升整体对齐性能，解决了传统对齐方法忽略偏好强度的问题，对大模型偏好建模有重要贡献。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06180' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Projecting Out the Malice: A Global Subspace Approach to LLM Detoxification</h3>
<p><strong>Authors:</strong> Zenghao Duan, Zhiyi Yin, Zhichao Shi, Liang Pang, Shaoling Jing, Zihe Huang, Jiayi Wu, Yu Yan, Jingcheng Deng, Huawei Shen, Xueqi Cheng</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出GLOSS框架，通过识别并消除FFN参数中的全局有毒子空间实现LLM去毒，在Qwen3等模型上实现SOTA去毒效果且保留通用能力，是大模型安全的重要实践。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06226' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Should We Introduce Safety Interventions During Pretraining?</h3>
<p><strong>Authors:</strong> Dylan Sam, Sachin Goyal, Pratyush Maini, Alexander Robey, J. Zico Kolter</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 研究预训练中安全干预的时机，发现早期干预提升模型鲁棒性与可引导性，不增加过拒绝率，为大模型安全对齐提供实践指导。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07087' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment</h3>
<p><strong>Authors:</strong> Haozhong Wang, Zhuo Li, Yibo Yang, He Zhao, Hongyuan Zha, Dandan Guo</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出SOT框架通过推拉分布对齐保护LLM微调安全，提升安全-效用 trade-off，优于现有数据过滤方法，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07200' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Universal Adversarial Purification with DDIM Metric Loss for Stable Diffusion</h3>
<p><strong>Authors:</strong> Li Zheng, Liangbin Xie, Jiantao Zhou, He YiMin</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出UDAP框架，针对Stable Diffusion的 adversarial攻击（如VAE、UNet目标攻击），用DDIM metric loss净化数据，属于大模型安全与对齐方向的防御机制创新。
Score: 7.5
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07253' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs</h3>
<p><strong>Authors:</strong> Weihao Hong, Zhiyuan Jiang, Bingyu Shen, Xinlei Guan, Yangyi Feng, Meng Xu, Boyang Li</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出Ghost-100数据集研究 prompt 语言 tone 对多模态模型幻觉的影响，发现模型对语义敌意和结构强制的不同响应，揭示当前安全对齐的局限性，为优化prompt设计提供依据。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06460' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> L2CU: Learning to Complement Unseen Users</h3>
<p><strong>Authors:</strong> Dileepa Pitawela, Gustavo Carneiro, Hsiang-Ting Chen</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对现有L2C方法忽略用户变异性的问题，提出L2CU框架，通过代表性标注者轮廓匹配 unseen 用户，提升人机协作分类的泛化性，在CIFAR-10N等数据集上验证有效性，属于大模型安全与对齐中人际对齐的重要进展。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06119' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers</h3>
<p><strong>Authors:</strong> Arion Das, Partha Pratim Saha, Amit Dhanda, Vinija Jain, Aman Chadha, Amitava Das</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出SPINAL诊断工具，层间追踪DPO对齐对LLM表示的几何影响，揭示对齐集中在最后几层的规律，对理解对齐的内部机制和故障预测有重要意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06238' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Leveraging Soft Prompts for Privacy Attacks in Federated Prompt Tuning</h3>
<p><strong>Authors:</strong> Quan Minh Nguyen, Min-Seon Kim, Hoang M. Ngo, Trong Nghia Hoang, Hyuk-Yoon Kwon, My T. Thai</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对联邦prompt调优提出PromptMIA隐私攻击，揭示大模型安全新漏洞，分析现有防御局限性，对大模型安全有重要意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06641' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Forgetting Similar Samples: Can Machine Unlearning Do it Better?</h3>
<p><strong>Authors:</strong> Heng Xu, Tianqing Zhu, Dayong Ye, Lefeng Zhang, Le Wang, Wanlei Zhou</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 研究机器遗忘相似样本的有效性，发现现有方法未完全消除目标样本影响，为改进机器遗忘算法提供实证基础，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06938' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A Robust Certified Machine Unlearning Method Under Distribution Shift</h3>
<p><strong>Authors:</strong> Jinduo Guo, Yinzhi Cao</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出分布感知的认证机器遗忘方法，解决非独立同分布删除下的不稳定性，提供更紧的梯度残差界，提升大模型安全鲁棒性。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.06967' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Hallucinations Live in Variance</h3>
<p><strong>Authors:</strong> Aaron R. Flouro, Shawn P. Chadwick</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 发现LLM幻觉源于语义等价提示的输出方差，提出语义稳定性指标，通过稀疏化降低幻觉，对大模型安全与可靠性有重要意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07058' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PRPO: Aligning Process Reward with Outcome Reward in Policy Optimization</h3>
<p><strong>Authors:</strong> Ruiyi Ding, Yongxuan Lv, Xianhui Meng, Jiahe Song, Chao Wang, Chen Jiang, Yuan Cheng</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出PRPO方法对齐过程奖励与结果奖励，解决多步推理任务的稀疏奖励问题，提升Qwen2.5-Math-1.5B的数学推理 accuracy，属于大模型安全与对齐中的奖励对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07182' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization</h3>
<p><strong>Authors:</strong> Murtaza Nikzad, Raghuram Ramanujan</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 比较直接偏好优化中的前向推理与后向验证目标，发现前向训练提升准确性、后向训练降低假阳性，为大模型对齐策略提供新 insights。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07199' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MAESTRO: Meta-learning Adaptive Estimation of Scalarization Trade-offs for Reward Optimization</h3>
<p><strong>Authors:</strong> Yang Zhao, Hepeng Wang, Xiao Ding, Yangou Ouyang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出元学习框架优化奖励标量化权衡，解决多目标冲突问题，提升GRPO在开放域任务的性能，属于大模型安全与对齐中的奖励对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07208' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Are LLM Decisions Faithful to Verbal Confidence?</h3>
<p><strong>Authors:</strong> Jiawei Wang, Yanfei Zhou, Siddartha Devic, Deqing Fu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 提出RiskEval框架，研究LLM口头信心与决策的一致性，发现模型在高penalty条件下不调整弃权策略，揭示校准的口头信心不足以保证可信决策，对大模型安全对齐有重要启示。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07767' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility</h3>
<p><strong>Authors:</strong> G M Shahariar, Zabir Al Nazi, Md Olid Hasan Bhuiyan, Zhouxing Shi</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 构建PII-VisBench基准，评估VLMs在不同主体在线可见性下的PII泄露问题，揭示模型隐私安全的异质性，为大模型隐私对齐提供了关键评估工具。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.05739' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PROTEA: Securing Robot Task Planning and Execution</h3>
<p><strong>Authors:</strong> Zainab Altaweel, Mohaiminul Al Nahian, Jake Juettner, Adnan Siraj Rakin, Shiqi Zhang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对机器人任务规划的安全漏洞，提出LLM-as-a-Judge的防御机制PROTEA，通过评估任务计划的安全性解决 adversarial attacks问题，为大模型驱动的机器人任务规划安全提供了新方法。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.07186' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation</h3>
<p><strong>Authors:</strong> Zezhou Wang, Ziyun Zhang, Xiaoyi Zhang, Zhuzhong Qian, Yan Lu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对GUI Agent小样本专家轨迹的分布偏移问题，提出双级专家同化方法BEPA，提升OSWorld-Verified等多个GUI基准的任务成功率，为GUI智能体的强化学习优化提供了有效途径。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.05787' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> ShowUI-Aloha: Human-Taught GUI Agent</h3>
<p><strong>Authors:</strong> Yichun Zhang, Xiangwu Guo, Yauhong Goh, Jessica Hu, Zhiheng Chen, Xin Wang, Difei Gao, Mike Zheng Shou</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Reason:</strong> 针对GUI智能体的训练数据问题，提出从人类屏幕录制中提取结构化任务的pipeline，属于多模态智能体中的GUI Agent方向，解决实际部署中的数据瓶颈。
Score: 7.5
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.07181' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>