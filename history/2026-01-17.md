# ArXiv 每日推荐 - 2026-01-17

> 更新于北京时间：2026-01-17 12:30:58
> 已自动阅读了 203 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：95873

## 大模型安全与对齐

### [Score: 9.0/10] Understanding and Preserving Safety in Fine-Tuned LLMs
- **Authors:** Jiawen Zhang, Yangfan Hu, Kejia Chen, Lipeng He, Jiachen Ma, Jian Lou, Dan Li, Jian Liu, Xiaohu Yang, Ruoxi Jia
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10141](https://arxiv.org/abs/2601.10141)
- **Reason:** 通过分析安全与效用梯度的几何交互，提出SPF方法在微调中保持LLM安全对齐，解决安全-效用困境，对大模型安全与对齐有重要理论和实践价值
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5
- **Authors:** Xingjun Ma, Yixu Wang, Hengyuan Xu, Yutao Wu, Yifan Ding, Yunhan Zhao, Zilong Wang, Jiabin Hua, Ming Wen, Jianan Liu, Ranjie Duan, Yifeng Gao, Yingshui Tan, Yunhao Chen, Hui Xue, Xin Wang, Wei Cheng, Jingjing Chen, Zuxuan Wu, Bo Li, Yu-Gang Jiang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10527](https://arxiv.org/abs/2601.10527)
- **Reason:** 对7个前沿LLM与多模态模型进行多模态安全评估，揭示安全性能的多维性（模态、语言、评估方案），为大模型安全开发提供关键参考。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models
- **Authors:** Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10416](https://arxiv.org/abs/2601.10416)
- **Reason:** 提出token级流引导偏好优化，实现高效测试时对齐，在保留生成多样性的同时提升对齐性能，解决了传统对齐方法的计算成本与灵活性问题。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing
- **Authors:** Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10543](https://arxiv.org/abs/2601.10543)
- **Reason:** 通过解码过程中的潜在安全信号探测早期识别不安全内容，有效防御越狱攻击，同时保持低误拒率与响应质量，提升了LLM的安全性。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models
- **Authors:** Zefan Zhang, Kehua Zhu, Shijie Jiang, Hongyuan Lu, Shengkai Sun, Tian Bai
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10010](https://arxiv.org/abs/2601.10010)
- **Reason:** 构建VERHallu基准评估VideoLLMs事件关系幻觉问题，提出KFP策略缓解该问题，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models
- **Authors:** Peng-Fei Zhang, Zi Huang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10313](https://arxiv.org/abs/2601.10313)
- **Reason:** 提出HRA框架生成多模态通用对抗攻击，评估VLMs脆弱性，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents
- **Authors:** Hanna Foerster, Robert Mullins, Tom Blanchard, Nicolas Papernot, Kristina Nikolić, Florian Tramèr, Ilia Shumailov, Cheng Zhang, Yiren Zhao
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09923](https://arxiv.org/abs/2601.09923)
- **Reason:** 针对计算机使用代理（CUA）的prompt injection攻击问题，提出Single-Shot Planning架构，实现系统级安全防御，在OSWorld基准上保持性能同时提升安全性，属于大模型安全与对齐的重要研究。
Score: 7
Field: 大模型安全与对齐

## 高效大模型训练与推理

### [Score: 9.0/10] TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks
- **Authors:** Vansh Kapoor, Aman Gupta, Hao Chen, Anurag Beniwal, Jing Huang, Aviral Kumar
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10245](https://arxiv.org/abs/2601.10245)
- **Reason:** 通过目标分步路由将关键推理步骤分配给大模型、非关键步骤用小模型，显著提升多步推理的效率与成本效益，是高效大模型推理的突破性方法。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration
- **Authors:** Subhajit Sanyal, Srinivas Soumitri Miriyala, Akshay Janardan Bankar, Sravanth Kodavanti, Harshit, Abhishek Ameta, Shreyas Pandith, Amit Satish Unde
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09823](https://arxiv.org/abs/2601.09823)
- **Reason:** 提出边缘高效的扩散基础模型NanoSD，通过全管道协同设计优化模型大小与推理速度，适配边缘设备实时图像恢复，贴合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Transition Matching Distillation for Fast Video Generation
- **Authors:** Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie, Arash Vahdat
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09881](https://arxiv.org/abs/2601.09881)
- **Reason:** 提出TMD框架将视频扩散模型蒸馏为高效少步生成器，提升视频生成推理速度同时保持视觉质量，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Global Context Compression with Interleaved Vision-Text Transformation
- **Authors:** Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10378](https://arxiv.org/abs/2601.10378)
- **Reason:** 提出VIST2框架压缩全局上下文，减少tokens与计算成本，提升生成速度，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment
- **Authors:** Jacob Sander, Brian Jalaian, Venkat R. Dasari
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09865](https://arxiv.org/abs/2601.09865)
- **Reason:** 提出整合GPTQ量化、LoRA和数据蒸馏的框架，用Muon优化器提升LLM部署效率，减少模型大小同时保持性能，针对高效大模型训练与推理有实际应用价值
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers
- **Authors:** Aryan Karmore
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10155](https://arxiv.org/abs/2601.10155)
- **Reason:** 针对Transformer的KV缓存压缩问题，提出LOOKAT框架，通过乘积量化和不对称距离计算将注意力从内存绑定转为计算绑定，实现高压缩率且保持输出保真度，对高效大模型推理有重要价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs
- **Authors:** Cheng Feng, Chaoliang Zhong, Jun Sun, Yusuke Oishi
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10114](https://arxiv.org/abs/2601.10114)
- **Reason:** 提出调度 checkpoint蒸馏方法，模拟教师模型收敛过程以减少师生能力差距，有效提升领域特定LLM的蒸馏效率，是高效大模型训练的重要创新。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] FlowAct-R1: Towards Interactive Humanoid Video Generation
- **Authors:** Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10103](https://arxiv.org/abs/2601.10103)
- **Reason:** 提出FlowAct-R1框架实现实时交互式人形视频生成，优化推理延迟与 temporal consistency，符合高效大模型训练与推理方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement
- **Authors:** Yichong Xia, Yimin Zhou, Jinpeng Wang, Bin Chen
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10373](https://arxiv.org/abs/2601.10373)
- **Reason:** 提出DiffCR框架优化低码率图像压缩的推理速度与质量，符合高效大模型训练与推理方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts
- **Authors:** Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10079](https://arxiv.org/abs/2601.10079)
- **Reason:** 提出Sparse-RL解决LLM强化学习中的KV缓存内存瓶颈，通过稀疏rollout和偏差校正保持训练稳定性和性能，针对高效大模型训练与推理中的RL场景有研究价值
Score: 7
Field: 高效大模型训练与推理

## 原生多模态大模型

### [Score: 8.0/10] Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP
- **Authors:** Anant Mehta, Xiyuan Wei, Xingyu Chen, Tianbao Yang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09859](https://arxiv.org/abs/2601.09859)
- **Reason:** 针对开放权重CLIP提出自监督微调框架TuneCLIP，解决微调性能退化问题，提升多模态模型泛化能力，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning
- **Authors:** Linquan Wu, Tianxiang Jiang, Yifei Dong, Haoyu Yang, Fengji Zhang, Shichaang Meng, Ai Xuan, Linqi Song, Jacky Keung
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10129](https://arxiv.org/abs/2601.10129)
- **Reason:** 提出LaViT框架对齐多模态推理中的潜在视觉思想，提升视觉 grounding与推理性能，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset
- **Authors:** Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10305](https://arxiv.org/abs/2601.10305)
- **Reason:** 构建大规模中文视觉语言预训练数据集DanQing，提升中文多模态模型性能，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders
- **Authors:** Siqi Kou, Jiachun Jin, Zetong Zhou, Ye Ma, Yugang Wang, Quan Chen, Peng Jiang, Xiao Yang, Jun Zhu, Kai Yu, Zhijie Deng
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10332](https://arxiv.org/abs/2601.10332)
- **Reason:** 提出T2G范式结合LLM推理能力与扩散模型生成能力，提升文本到图像生成的语义一致性，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] mergetune: Continued fine-tuning of vision-language models
- **Authors:** Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10497](https://arxiv.org/abs/2601.10497)
- **Reason:** 提出mergetune框架实现VLMs持续微调，缓解灾难性遗忘，提升泛化性能，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models
- **Authors:** Yulin He, Wei Chen, Zhikang Jian, Tianhang Guo, Wenjuan Zhou, Minglong Li
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09981](https://arxiv.org/abs/2601.09981)
- **Reason:** 提出DR²Seg框架优化多模态大语言模型推理分割，减少冗余推理并提升准确性，属于原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation
- **Authors:** Han Wang, Yi Yang, Jingyuan Hu, Minfeng Zhu, Wei Chen
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10094](https://arxiv.org/abs/2601.10094)
- **Reason:** 提出V-Zero框架通过无标注图像实现多模态模型自改进，提升视觉推理性能，属于原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text
- **Authors:** Piyush Singh Pasi
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10096](https://arxiv.org/abs/2601.10096)
- **Reason:** 提出METAL方法，用单语语文本将多语言嵌入对齐到多模态空间，解决多语言多模态资源不足问题，属于原生多模态大模型中的跨语言对齐研究
Score: 7
Field: 原生多模态大模型

## 深度学习理论

### [Score: 8.0/10] An analytic theory of convolutional neural network inverse problems solvers
- **Authors:** Minh Hai Nguyen, Quoc Bao Do, Edouard Pauwels, Pierre Weiss
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10334](https://arxiv.org/abs/2601.10334)
- **Reason:** 提出CNN逆问题求解器的解析理论，属于深度学习理论方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit
- **Authors:** Faruk Alpay, Bilge Senturk
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09775](https://arxiv.org/abs/2601.09775)
- **Reason:** 从热带半环和动态规划视角分析Transformer自注意力机制的理论本质，为Transformer的几何解释和链式思维推理提供新的理论视角，属于深度学习理论中的网络架构理论研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Unlabeled Data Can Provably Enhance In-Context Learning of Transformers
- **Authors:** Renpu Liu, Jing Yang
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10058](https://arxiv.org/abs/2601.10058)
- **Reason:** 理论研究无标签数据对Transformer上下文学习的增强作用，证明其能通过链式思维提示模拟期望最大化算法提升性能，属于深度学习理论中的上下文学习理论研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction
- **Authors:** Hongru Duan, Yongle Chen, Lei Guan
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10251](https://arxiv.org/abs/2601.10251)
- **Reason:** 从谱和几何角度分析SAM的优化缺陷，提出X-SAM通过主导特征向量梯度校正改进Sharpness-Aware Minimization，有理论收敛性证明和实验验证，属于深度学习优化器方向的重要改进。
Score: 8
Field: 深度学习理论

## 多模态智能体

### [Score: 8.0/10] GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents
- **Authors:** Chen Chen, Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao, Wu Liu
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09770](https://arxiv.org/abs/2601.09770)
- **Reason:** 针对GUI代理的视觉接地问题，提出主动视觉感知框架GUI-Eyes，通过两阶段策略决策视觉工具使用，设计空间连续奖励函数，在ScreenSpot-Pro基准上显著提升接地 accuracy，属于多模态智能体的关键技术改进。
Score: 8
Field: 多模态智能体

## 深度学习可解释性

### [Score: 8.0/10] Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection
- **Authors:** Frank Bobe III, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, Jose Salas-Vernis
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10524](https://arxiv.org/abs/2601.10524)
- **Reason:** 用SHAP分析与机械可解释性方法诊断微调LLM的泛化失败原因，揭示架构、数据多样性与训练策略对泛化的影响，提升了LLM的可解释性。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] Adversarial Evasion Attacks on Computer Vision using SHAP Values
- **Authors:** Frank Mollard, Marcus Becker, Florian Roehrbein
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.10587](https://arxiv.org/abs/2601.10587)
- **Reason:** 结合SHAP值（深度学习可解释性工具）研究计算机视觉模型的对抗逃避攻击，探讨可解释性技术在攻击场景中的应用，对理解可解释性技术的实际影响有研究价值
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models
- **Authors:** Khalid Oublal, Quentin Bouniot, Qi Gan, Stephan Clémencçon, Zeynep Akata
- **Published:** 2026-01-16
- **Link:** [https://arxiv.org/abs/2601.09776](https://arxiv.org/abs/2601.09776)
- **Reason:** 提出TimeSAE框架，结合稀疏自编码器和因果性解决时间序列黑盒模型的可解释性问题，针对分布外场景的鲁棒解释有研究价值
Score: 7
Field: 深度学习可解释性

