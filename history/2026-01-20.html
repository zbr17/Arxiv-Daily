<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-20</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >多模态智能体</a>
<a href='#' >原生多模态大模型</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-20</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-20 12:43:00</p>
<p>已自动阅读了 159 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：77558</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mugi: Value Level Parallelism For Efficient LLMs</h3>
<p><strong>Authors:</strong> Daniel Price, Prabhu Vellaisamy, John Shen, Di Wu</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出Mugi架构，通过值级并行（VLP）优化LLM的非线性近似、小批量GEMM及全 workload 支持，实验显示softmax吞吐量提升45×、能效提升668×，属于高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.10823' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization</h3>
<p><strong>Authors:</strong> Haiyang Xiao, Weiqing Li, Jinyue Guo, Guochao Jiang, Guohua Liu, Yuewei Zhang</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出FAQ框架，利用同家族大模型生成携带CoT推理的高保真校准数据，优化后训练量化（PTQ）的误差，实验显示Qwen3-8B精度损失减少28.5%，属于高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11200' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</h3>
<p><strong>Authors:</strong> Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 针对视觉语言模型推理时KV缓存的内存与计算瓶颈，提出参数高效的MHA到MLA转换框架，通过模态自适应策略与低秩近似实现高效推理，实验验证可恢复模型性能并减少缓存占用，对高效大模型推理有重要实用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11464' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training</h3>
<p><strong>Authors:</strong> Aakriti, Zhe Li, Dandan Liang, Chao Huang, Rui Li, Haibo Yang</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出HOSL框架，结合零阶（ZO）与一阶（FO）优化解决边缘设备split learning的内存问题，理论证明收敛率依赖客户端模型维度，实验显示OPT模型内存减少3.7×，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.10940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Differentially Private Subspace Fine-Tuning for Large Language Models</h3>
<p><strong>Authors:</strong> Lele Zheng, Xiang Wang, Tao Zhang, Yang Cao, Ke Cheng, Yulong Shen</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出DP-SFT框架，通过主梯度方向识别任务子空间，仅在子空间注入DP噪声，减少噪声对无关参数的影响，实验显示多数据集上准确性与稳定性优于DP微调基线，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11113' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients</h3>
<p><strong>Authors:</strong> Zhikang Shen, Jianrong Lu, Haiyuan Wan, Jianhai Chen</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出SDFLoRA，将LoRA分解为捕捉通用知识的全局模块与保留客户特异性的局部模块，解决联邦学习中的秩异质性问题，实验显示GLUE基准上优于联邦LoRA基线，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11219' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Low-Rank Key Value Attention</h3>
<p><strong>Authors:</strong> James O'Neill, Robert Clancy, Mariia Matskevichus, Fergal Reid</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出低秩键值注意力机制LRKV，通过共享全秩KV投影加低秩残差的方式减少Transformer的KV缓存内存占用，同时保留头多样性，实验验证在2.5B规模模型上用约一半KV缓存实现更优性能，属于高效大模型训练与推理的核心优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11471' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning</h3>
<p><strong>Authors:</strong> Qianyue Wang, Jinwu Hu, Yufeng Wang, Huanxiang Lin, Bolin Chen, Zhiquan Wen, Yaofo Chen, Mingkui Tan</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出Test-Time交互推理范式Think-with-Me，通过过渡连词处的外部反馈干预减少LLM推理中的过度思考与冗余，在保持准确性的同时缩短推理长度，属于高效大模型训练与推理中的推理效率优化方向，实验验证在AIME24上有显著提升。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11252' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VLAgents: A Policy Server for Efficient VLA Inference</h3>
<p><strong>Authors:</strong> Tobias J\"ulg, Khaled Gamal, Nisarga Nilavadi, Pierre Krack, Seongjin Bien, Michael Krawez, Florian Walter, Wolfram Burgard (RobotControlStack)</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出VLAgents模块化策略服务器，针对视觉-语言-动作（VLA）模型推理部署的碎片化接口与通信延迟问题，通过统一协议及自适应通信层（零拷贝共享内存+压缩流）优化效率，比OpenVLA等默认服务器性能更优，属于高效大模型推理的重要实践，项目开源且作者团队有影响力。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11250' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent</h3>
<p><strong>Authors:</strong> Ning Yang, Yikuan Zhang, Qi Ouyang, Chao Tang, Yuhai Tu</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 分析SGD学习动力学，揭示其逃离sharp valleys的非平衡机制（ transient exploratory phase + 冻结效应），建立损失 landscape 几何与泛化的统一物理框架，属于深度学习理论中的优化与损失 landscape 方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.10962' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Analytic Bijections for Smooth and Interpretable Normalizing Flows</h3>
<p><strong>Authors:</strong> Mathis Gerdes, Miranda C. N. Cheng</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出三类分析双射函数，解决归一化流中双射函数的expressivity、smoothness与invertibility trade-off，实现全局光滑、闭形式可逆且高表达力，对深度学习理论中的归一化流架构设计有突破性贡献。
Score: 8.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.10774' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework</h3>
<p><strong>Authors:</strong> Qingyue Zhang, Chang Chu, Haohao Fu, Tianren Peng, Yanru Wu, Guanbo Huang, Yang Li, Shao-Lun Huang</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出UOWQ框架，基于KL散度泛化误差的渐近分析联合优化多源迁移学习的源权重与迁移量，理论证明全源样本最优性并给出闭解，实验验证优于DomainNet等基准的强基线，属于深度学习理论中的优化方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.10779' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention</h3>
<p><strong>Authors:</strong> Ruibang Li, Guan Luo, Yiwei Zhang, Jin Gao, Bing Li, Weiming Hu</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 针对softmax注意力二次复杂度与线性注意力容量缺陷的问题，提出层-wise线性-softmax混合注意力架构，通过细粒度混合策略平衡计算效率与模型性能，对深度学习理论中的注意力机制设计有创新性贡献。
Score: 7.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.11164' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications</h3>
<p><strong>Authors:</strong> Jeffrey Uhlmann</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 针对正齐次网络的 gauge symmetry 问题，提出单位一致（UC）伴随方法，推导UC gauge-consistent梯度下降与反向传播，解决标准梯度下降的参数化依赖问题，属于深度学习理论中的优化方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.10873' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models</h3>
<p><strong>Authors:</strong> Chuanyue Yu, Jiahui Wang, Yuhan Li, Heng Chang, Ge Lan, Qingyun Sun, Jia Li, Jianxin Li, Ziwei Zhang</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 系统研究RAG在扩散语言模型（DLM）中的应用，提出SPREAD框架通过查询相关性引导去噪解决语义漂移问题，实验验证生成精度提升，属于大模型新技术中的diffusion LLM方向。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.11342' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models</h3>
<p><strong>Authors:</strong> Rapha\"el Razafindralambo, R\'emy Sun, Fr\'ed\'eric Precioso, Damien Garreau, Pierre-Alexandre Mattei</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 系统研究扩散模型的集成策略（如Deep Ensembles、Monte Carlo Dropout）对生成质量的影响，分析分数集成与图像质量的关联，属于大模型新技术中的diffusion模型优化方向，为扩散模型的性能提升提供理论与实验 insights。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.11444' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning</h3>
<p><strong>Authors:</strong> Rajat Ghosh, Debojyoti Dutta</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 提出Action Shapley指标，基于Shapley值解决强化学习世界模型的训练数据选择问题，设计随机动态算法将计算复杂度从指数级降低80%，实验验证优于随机选择，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.10905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</h3>
<p><strong>Authors:</strong> Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, Wenxi Li, Vincent Wang, Chris Lee</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 研究RLVR中虚假奖励导致LLM记忆捷径的机制，揭示Anchor-Adapter电路（中间层Functional Anchor触发记忆检索，后续层Structural Adapters适配信号），提供因果调控方法，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.11061' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Building Production-Ready Probes For Gemini</h3>
<p><strong>Authors:</strong> J\'anos Kram\'ar, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 针对LLM滥用检测问题，设计能处理长上下文等生产环境分布偏移的探针架构，已部署到Gemini的用户端实例，解决大模型安全与对齐中的滥用 mitigation 问题，具有实际应用价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.11516' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</h3>
<p><strong>Authors:</strong> Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, Guanghui Ren</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 针对视觉-语言-动作（VLA）模型直接生成动作的局限性，提出Action Chain-of-Thought（ACoT）范式，通过粗动作意图序列推理指导精确动作执行，融合显式/隐式动作推理组件，在LIBERO等数据集上取得98.5%的顶尖成绩，显著提升多模态智能体的操纵任务性能。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.11404' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</h3>
<p><strong>Authors:</strong> Wenhui Tan, Ruihua Song, Jiaze Li, Jianzhong Ju, Zhenbo Luo</p>
<p><strong>Published:</strong> 2026-01-19</p>
<p><strong>Reason:</strong> 针对多模态大模型长视频理解的计算约束与帧选择问题，提出训练-free框架，通过多查询推理与clip-level慢快采样平衡局部细节与全局上下文，提升长视频理解性能与效率，属于原生多模态大模型的关键优化。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.11359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>