<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-22</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>多模态智能体</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习理论</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >大模型新技术</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-22</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-22 13:06:04</p>
<p>已自动阅读了 614 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：317482</p>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents</h3>
<p><strong>Authors:</strong> Yurun Song, Jiong Yin, Rongjunchen Zhang, Ian G. Harris</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对多轮GUI代理的上下文膨胀问题，提出坐标压缩策略优化框架，提升GUI导航效率与 grounding精度，直接对应GUI Agent研究方向
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.11631' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction</h3>
<p><strong>Authors:</strong> Wenqi Zhang, Yulin Shen, Changyue Jiang, Jiarun Dai, Geng Hong, Xudong Pan</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对计算机使用代理（CUAs）的安全风险，提出基于文本模拟环境的推理修正框架，通过模拟生成高风险轨迹并训练修正模型，有效降低不安全行为率，属于多模态智能体中的GUI Agent安全增强方向。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.12822' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Agentic Reasoning for Large Language Models</h3>
<p><strong>Authors:</strong> Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang, Jiaru Zou, Zhichen Zeng, Ruizhong Qiu, Xiao Lin, Dongqi Fu, Zihao Li, Mengting Ai, Duo Zhou, Wenxuan Bao, Yunzhe Li, Gaotang Li, Cheng Qian, Yu Wang, Xiangru Tang, Yin Xiao, Liri Fang, Hui Liu, Xianfeng Tang, Yuji Zhang, Chi Wang, Jiaxuan You, Heng Ji, Hanghang Tong, Jingrui He</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 全面综述大语言模型的Agentic推理框架，从单Agent基础能力、自进化Agent到多Agent协作，覆盖规划、工具使用、记忆等核心模块，为Agentic AI研究提供系统roadmap。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.12538' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents</h3>
<p><strong>Authors:</strong> Arunkumar V, Gangadharan G. R., Rajkumar Buyya</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 系统综述Agentic AI的架构、分类和评估，涵盖从单Agent到多Agent的设计，讨论工具使用、记忆、反馈等关键组件，为Agentic AI的研究和部署提供全面参考。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.12560' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux</h3>
<p><strong>Authors:</strong> Zecheng Li, Zhihui Cao, Wenke Huang, Yudong Zhang, Keying Qi, Rui Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan, Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, He Yang, Minqi Xiang, Xingyu Liu, Zuojian Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出多代理奖励模型系统MagicGUI-RMS，通过自动化反馈回流机制生成高质量奖励数据，实现GUI Agents的自进化，解决GUI代理的奖励评估和数据生成问题，属于多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.13060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search</h3>
<p><strong>Authors:</strong> Shahrzad Esmat, Mahdi Banisharif, Ali Jannesari</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 利用LLM驱动的策略搜索实现MAC约束的模型压缩，解决现有剪枝方法无法直接控制计算成本的问题，通过Profiling、Master、Analysis三个Agent协同优化，实验验证在ResNet、ConvNeXt等模型上保持精度的同时降低MAC，适合资源受限设备部署。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.12272' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Scaling Test-time Inference for Visual Grounding</h3>
<p><strong>Authors:</strong> Guanqi Zhan, Changye Li, Zhijian Liu, Yao Lu, Yi Wu, Song Han, Ligeng Zhu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出EGM方法，通过扩展小模型测试时计算量提升视觉grounding性能，显著优于大模型且推理更快，属于高效大模型推理的关键技术。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13633' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference</h3>
<p><strong>Authors:</strong> Chaeyoung Jung, Youngjoon Jang, Seungwoo Lee, Joon Son Chung</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出音频-视觉大语言模型的高效 token 剪枝策略，显著降低推理 FLOPs，属于高效大模型推理研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13143' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow</h3>
<p><strong>Authors:</strong> Haonan Wei, Linyuan Wang, Nuolin Sun, Zhizhong Zheng, Lei Li, Bin Yan</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 将28层Transformer蒸馏为单一层，显著减少模型参数（从675M到4.3M）并提升图像生成稳定性，属于大模型蒸馏优化的高效训练技术
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11630' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers</h3>
<p><strong>Authors:</strong> Yuxi Liu, Yipeng Hu, Zekun Zhang, Kunze Jiang, Kun Yuan</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出动态稀疏注意框架MOD-DiT，解决视频扩散Transformer的二次复杂度问题，提升生成效率，属于注意力优化的高效模型技术
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11641' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control</h3>
<p><strong>Authors:</strong> Quang-Hung Bui, Anh Son Ta</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 改进FRUGAL框架，通过动态调整subspace ratio和update frequency解决静态超参数问题，减少LLM训练的GPU内存和时间，同时保持性能，对高效大模型训练有实用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11568' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Activation Sensitivity as a Unifying Principle for Post-Training Quantization</h3>
<p><strong>Authors:</strong> Bruce Changlong Xu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出激活灵敏度作为后训练量化（PTQ）的统一框架，连接AWQ（激活感知）和GPTQ（二阶方法），为PTQ方法提供理论基础，提升量化的有效性。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11663' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Speculative Sampling with Reinforcement Learning</h3>
<p><strong>Authors:</strong> Chenan Wang, Daniel H. Shi, Haipeng Chen</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Introduces Re-SpS, an RL-based framework to dynamically optimize draft tree hyperparameters for speculative sampling, achieving up to 5.45x speedup over backbone LLMs and 1.12x over EAGLE-3 with no fidelity loss. Aligns with efficient LLM inference.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.12212' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement</h3>
<p><strong>Authors:</strong> Aaron R. Flouro, Shawn P. Chadwick</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提供递归知识蒸馏的公理框架，属于高效大模型训练中的知识压缩研究
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13100' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits</h3>
<p><strong>Authors:</strong> Aryan Karmore</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出ButterflyMoE架构，通过结构化蝴蝶轨道实现子线性三元专家，显著降低MoE模型内存消耗，属于高效大模型训练与推理中的高压缩方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13563' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models</h3>
<p><strong>Authors:</strong> Ahmad Al-Zuraiqi</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出NOT框架，通过模块化checkpoint移植实现Transformer领域适应，显著提升微调效率，属于高效大模型训练与推理中的模块化适应方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13580' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks</h3>
<p><strong>Authors:</strong> Xiaohong Yang, Tong Xie, Minghui Liwang, Chikai Shang, Yang Lu, Zhenzhen Jiao, Liqun Fu, Seyyedali Hosseinalipour</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出ELSA框架，解决资源受限边缘网络的LLM联邦学习效率问题，属于高效大模型训练与推理中的edge LLM训练方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13824' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow</h3>
<p><strong>Authors:</strong> Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai, Yilong Zhao, Shuo Yang, Kurt Keutzer, Song Han (MIT, NVIDIA, UC Berkeley), Ligeng Zhu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对强化学习训练中rollout阶段的效率瓶颈，提出统一FP8精度流的Jet-RL框架，解决了BF16训练+FP8 rollout的不稳定问题，实现33%rollout加速、41%训练加速且精度损失可忽略。作者团队包含高效训练领域权威，结果可靠。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14243' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning</h3>
<p><strong>Authors:</strong> Kangye Ji, Yuan Meng, Zhou Jianbo, Ye Li, Hanyun Cui, Zhi Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Sparse ActionGen框架，通过实时剪枝、缓存复用和环境感知自适应策略，解决diffusion policy多步去噪的实时性问题，实现4倍生成速度提升且不牺牲性能，属于diffusion模型的高效推理方法。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.12894' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Federated Balanced Learning</h3>
<p><strong>Authors:</strong> Jiaze Li, Haoran Xu, Wanyi Wu, Changwei Wang, Shuaiguang Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Youyang Qu, Longxiang Gao, Xudong Yang, Lumin Xing</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出FBL框架，通过样本平衡解决联邦学习的非iid问题，提升模型性能，属于高效大模型训练的联邦学习优化研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14042' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition</h3>
<p><strong>Authors:</strong> Emily Kim, Allen Wu, Jessica Hodgins</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出curriculum-based训练策略，提升跨域动作识别的训练效率，属于高效大模型训练的课程学习优化研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14101' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction</h3>
<p><strong>Authors:</strong> Xiaojie Xia, Huigang Zhang, Chaoliang Zhong, Jun Sun, Yusuke Oishi</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 通过块级局部蒸馏和贪心层替换构建混合注意力模型，平衡全注意力的性能和线性注意力的效率，无需重新训练或架构搜索，对高效Transformer设计有实用价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.11667' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data</h3>
<p><strong>Authors:</strong> Zhipeng Chang, Ting He, Wenrui Hao</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出基于Fisher信息的参数级聚合方法，提升异构数据下联邦学习效率，属于高效大模型训练与推理中的联邦学习方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.13608' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents</h3>
<p><strong>Authors:</strong> Yilun Yao, Shan Huang, Elsie Dai, Zhewen Tan, Zhenyu Duan, Shousheng Jia, Yanbing Jiang, Tong Yang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对长文本推理中的“上下文衰退”问题，提出ARC框架通过主动反射驱动的上下文管理动态调整工作上下文，在BrowseComp-ZH基准上用Qwen2.5-32B-Instruct实现11% accuracy提升，显著提升长文本推理效率。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.12030' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models</h3>
<p><strong>Authors:</strong> Raphi Kang, Hongqiao Chen, Georgia Gkioxari, Pietro Perona</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出VLMs通过线性绑定空间ID和时间ID到文本激活来编码时空信息的机制，通过因果干预验证了这些ID对模型中间层信念的介导作用，揭示了VLMs时空推理的内部机制，属于深度学习可解释性的重要研究。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12626' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders</h3>
<p><strong>Authors:</strong> Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Insight框架，提取视觉语言编码器中的可解释语义层次，提供细粒度的模型决策解释，属于深度学习可解释性的关键研究。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.13798' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition</h3>
<p><strong>Authors:</strong> Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出 HAGD 框架从百亿参数语言模型中提取可解释的稀疏电路，属于深度学习可解释性中的白盒解释研究
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12879' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Actionable Interpretability Must Be Defined in Terms of Symmetries</h3>
<p><strong>Authors:</strong> Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Filippo Bonchi, Mateja Jamnik, Giuseppe Marra</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 指出当前可解释性定义缺乏可操作性，提出基于对称性的可解释性框架，认为对称性可推导解释性属性、刻画可解释模型类并统一解释性推理，为深度学习可解释性提供理论基础。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12913' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> \textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions</h3>
<p><strong>Authors:</strong> Chenchen Zhao, Muxi Chen, Qiang Xu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出逻辑驱动的视觉模型可解释性框架，解决现有方法依赖白盒或定量性不足的问题，通过提取视觉焦点并转化为逻辑表达式，实现透明、结构化的模型决策解释，且提出量化指标评估解释效果。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12049' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations</h3>
<p><strong>Authors:</strong> Shizhan Gong, Xiaofan Zhang, Qi Dou</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出PCBM-ReD pipeline，自动从预训练模型提取视觉概念，结合MLLM标注过滤任务相关概念，通过表示分解适配概念瓶颈模型，在提升可解释性的同时缩小与端到端模型的性能差距，实验验证跨11个分类任务的有效性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12303' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data</h3>
<p><strong>Authors:</strong> Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 通过可控1D图像-文本测试床研究CLIP风格模型的左右空间关系理解机制，发现标签多样性是泛化的关键，且注意力梯度打破了左右对称性，属于深度学习可解释性中模型空间推理机制的分析。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12809' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Local-to-Global Logical Explanations for Deep Vision Models</h3>
<p><strong>Authors:</strong> Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对深度视觉模型的可解释性问题，提出局部与全局逻辑解释方法，生成单调析取范式（MDNF）的逻辑公式，保持高保真度与覆盖度，实验验证在视觉数据集上的有效性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.13404' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explanation Multiplicity in SHAP: Characterization and Assessment</h3>
<p><strong>Authors:</strong> Hyunseung Hwang, Seungeun Lee, Lucas Rosenblatt, Julia Stoyanovich, Steven Euijong Whang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Studies the phenomenon of explanation multiplicity in SHAP, characterizing how stochasticity in the explanation pipeline leads to conflicting results for the same prediction. Directly relevant to deep learning explainability (Shapley value-based methods).
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12654' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Spectroscopy: Susceptibility Clusters in Language Models</h3>
<p><strong>Authors:</strong> Andrew Gordon, Garrett Baker, George Wang, William Snell, Stan van Wingerden, Daniel Murfet</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 利用随机梯度朗之万动力学测量语言模型的敏感性并聚类 token，揭示模型内部可解释结构，符合深度学习可解释性方向
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12703' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Patterning: The Dual of Interpretability</h3>
<p><strong>Authors:</strong> George Wang, Daniel Murfet</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出patterning作为机械可解释性的对偶问题，通过敏感性分析反向工程模型内部结构，直接关联深度学习可解释性核心方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.13548' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability</h3>
<p><strong>Authors:</strong> Hanwei Zhang, Luo Cheng, Rui Wen, Yang Zhang, Lijun Zhang, Holger Hermanns</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对概念瓶颈模型（CBM）空间局部性不足的问题，提出SL-CBM通过1×1卷积层和交叉注意力机制增强概念与图像区域的对齐，提升可解释性和干预效果，直接关联深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.12804' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Inverting Self-Organizing Maps: A Unified Activation-Based Framework</h3>
<p><strong>Authors:</strong> Alessandro Londei, Matteo Benati, Denise Lanzieri, Vittorio Loreto</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出自组织映射的激活基逆框架，通过激活模式恢复输入，属于深度学习可解释性中的white-box explanation方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.13851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance</h3>
<p><strong>Authors:</strong> Sahil Rajesh Dhayalkar</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出训练时可解释性指标“推理稳定点（RSP）”，通过跟踪微调过程的token-level归因漂移，识别模型依赖稳定证据的阶段，帮助选择更可靠的checkpoint，解决了微调中解释漂移的问题。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.11625' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF</h3>
<p><strong>Authors:</strong> Wang Zixian</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Proposes OPO framework to decouple sampling geometry from optimization geometry in RLHF, addressing instability in existing alignment methods (PPO/DPO/IPO) and providing a unifying perspective for robust reasoning-oriented training. Aligns with LLM safety and alignment.
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.12415' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability</h3>
<p><strong>Authors:</strong> Diego Gosmar, Deborah A. Dahl</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出基于代理AI、嵌套学习和语义缓存的prompt injection缓解方法，有效降低注入风险（零高风险违规）并提升计算效率（减少41.6% LLM调用），属于大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13186' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> AgenticRed: Optimizing Agentic Systems for Automated Red-teaming</h3>
<p><strong>Authors:</strong> Jiayi Yuan, Jonathan Nöther, Natasha Jaques, Goran Radanović</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出自动化红队系统AgenticRed，通过进化选择优化红队系统设计，显著提升对Llama、GPT等模型的攻击成功率（如Llama-2-7B攻击成功率从60%提升至96%），属于大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13518' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal</h3>
<p><strong>Authors:</strong> Haonan An, Guang Hua, Wei Du, Hangcheng Cao, Yihang Tao, Guowen Xu, Susanto Rahardja, Yuguang Fang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出解码器梯度shield方法，防御基于梯度的水印移除攻击，保护模型知识产权，属于大模型安全的模型保护技术
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.11952' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Proxy Robustness in Vision Language Models is Effortlessly Transferable</h3>
<p><strong>Authors:</strong> Xiaowei Fu, Fuxiang Huang, Lei Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出HPT-GPD框架，利用CLIP变体间的跨架构鲁棒性蒸馏实现VLM的对抗鲁棒性迁移，解决了对抗训练的高计算成本问题，同时通过GPD平衡了自然泛化与对抗鲁棒性，对大模型安全与对齐有实践价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.12865' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain</h3>
<p><strong>Authors:</strong> Sung Ju Lee, Nam Ik Cho</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对扩散模型生成图像的水印需求，提出PhaseMark框架，在VAE潜在频率域调制相位，实现单步、无优化的水印嵌入，速度快且抗攻击能力强，优于现有方法。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13128' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs</h3>
<p><strong>Authors:</strong> Yujin Jo, Sangyoon Bae, Taesup Kim</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出注意力空间对比引导机制，高效缓解大视觉语言模型（LVLMs）的幻觉问题，提升输出的视觉一致性和可靠性，属于大模型安全与对齐的关键技术。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning</h3>
<p><strong>Authors:</strong> Renmiao Chen, Yida Lu, Shiyao Cui, Xuan Ouyang, Victor Shea-Jay Huang, Shumin Zhang, Chengwei Pan, Han Qiu, Minlie Huang (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对多模态大模型（MLLMs）的多图像推理安全风险构建首個基准MIR-SafetyBench，评估19个模型发现更先进的推理模型可能更脆弱，分析不安全生成的注意力熵特征，对大模型安全研究有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.14127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment</h3>
<p><strong>Authors:</strong> Lukas Abrie Nel</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出GRADE方法，用Gumbel-softmax替代RLHF中的政策梯度，解决高方差问题，在IMDB数据集上比PPO提升50%的测试奖励，提升LLM对齐的稳定性和效果。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.11574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning</h3>
<p><strong>Authors:</strong> Duygu Nur Yaldiz, Evangelia Spiliopoulou, Zheng Qi, Siddharth Varia, Srikanth Doss, Nikolaos Pappas</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出校准感知强化学习方法平衡决策大模型的准确性与校准性，属于大模型安全与对齐研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13284' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PAC-Private Responses with Adversarial Composition</h3>
<p><strong>Authors:</strong> Xiaochen Zhu, Mayuri Sridhar, Srinivas Devadas</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出PAC隐私的对抗性组合方法，保护LLM响应隐私，属于大模型安全与对齐中的隐私保护方向，实现高 utility下的强隐私保证。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.14033' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</h3>
<p><strong>Authors:</strong> Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出InT干预训练，解决LLM推理的信用分配问题，通过定位错误步骤优化推理过程，属于大模型安全与对齐中的推理对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.14209' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward</h3>
<p><strong>Authors:</strong> Gourab K Patro, Himanshi Agrawal, Himanshu Gharat, Supriya Panigrahi, Nim Sherpa, Vishal Vaddina, Dagnachew Birru</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 综述通用大模型的负责任AI（RAI）原则，分析其与传统任务特定AI的差异，提出C2V2（Control、Consistency、Value、Veracity） desiderata，为大模型的安全和对齐提供框架，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13122' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games</h3>
<p><strong>Authors:</strong> Christopher Kao, Vanshika Vats, James Davis</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 通过社交推理游戏（Mafia）测量LLM的欺骗能力，发现LLM比人类更擅长隐藏身份（检测器对LLM的预测准确率低于人类），揭示LLM欺骗的风险，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13709' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adversarial Defense in Vision-Language Models: An Overview</h3>
<p><strong>Authors:</strong> Xiaowei Fu, Lei Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 综述了视觉语言模型（VLMs）对抗防御的三大范式（训练时防御、测试时适应防御、无训练防御）及其最新进展，分析了各方法的优缺点与挑战，对大模型安全与对齐中的对抗鲁棒性研究具有重要参考价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.12443' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content</h3>
<p><strong>Authors:</strong> Shengyi Wu, Yan Hong, Shengyao Chen, Zheng Wang, Xianbing Sun, Jiahui Zhan, Jun Lan, Jianfu Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出VTONGuard基准和多任务检测框架，自动检测AI生成的虚拟试穿内容，解决生成内容的真实性问题，属于大模型安全与对齐的关键应用。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13951' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning</h3>
<p><strong>Authors:</strong> Qian Feng, JiaHang Tu, Mintong Kang, Hanbin Zhao, Chao Zhang, Hui Qian</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出特征-梯度正交的增量遗忘框架，解决预训练模型不完全遗忘问题，属于大模型安全与对齐中的数据删除和隐私保护方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.13578' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Preconditioning Benefits of Spectral Orthogonalization in Muon</h3>
<p><strong>Authors:</strong> Jianhao Ma, Yu Huang, Yuejie Chi, Yuxin Chen</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 研究Muon优化器的谱正交化预处理机制，通过矩阵分解和线性收敛分析揭示优化器优势，属于深度学习理论中的optimizer方向，具有重要理论价值。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13474' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models</h3>
<p><strong>Authors:</strong> Samuel Cyrenius Anderson</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 分析25000+推理轨迹，发现大模型规模会引发推理结构的领域特定相变（如法律推理的“结晶化”、代码推理的“晶格化”），揭示推理的几何机制，属于深度学习理论方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13358' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems</h3>
<p><strong>Authors:</strong> Benedikt Hartl, Léo Pio-Lopez, Chris Fields, Michael Levin</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出嵌入空间的重映射（将输入转化为latent空间）和导航（迭代误差最小化）是自然与人工系统认知的基本原理，统一深度学习与生物系统的认知机制，属于深度学习理论方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14096' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Revisiting Multi-Task Visual Representation Learning</h3>
<p><strong>Authors:</strong> Shangzhe Di, Zhonghua Zhai, Weidi Xie</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出MTV多任务视觉预训练框架，整合视觉-语言对比、自监督和密集空间目标，系统研究多任务学习的机制，属于深度学习理论的多任务学习研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13886' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis</h3>
<p><strong>Authors:</strong> Shenyang Deng, Boyao Liao, Zhuoli Ouyang, Tianyu Pang, Minhak Song, Yaoqing Yang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 在高维二次设置中分析SGD的step size对alignment的影响，提出自适应临界step size条件，解释两阶段行为，对优化器（SGD）的理论理解有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.11789' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays</h3>
<p><strong>Authors:</strong> Chang-Wei Shi, Shi-Shang Wang, Wu-Jun Li</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Proposes OrLoMo, an asynchronous distributed momentum SGD with local updates, and proves its convergence for non-convex problems under arbitrary delays. Directly contributes to optimizer research in deep learning theory.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.12322' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization</h3>
<p><strong>Authors:</strong> Safwan Labbi, Daniil Tiapkin, Paul Mangold, Eric Moulines</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Proposes f-softargmax parameterization with coupled regularization to address softmax's ill-conditioning in policy gradients, providing non-asymptotic convergence guarantees for stochastic policy gradient methods. Directly contributes to optimizer research in deep learning theory.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.12604' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off</h3>
<p><strong>Authors:</strong> Zhaochun Li, Chen Wang, Jionghao Bai, Shisheng Cui, Ge Lan, Zhou Zhao, Yue Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出分布中心的优化器 DCPO 解决大语言模型强化学习中的探索-利用权衡问题，属于深度学习理论中的优化器研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.12730' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks</h3>
<p><strong>Authors:</strong> Thibaut Boissin (IRIT-MISFIT), Franck Mamalet (ANITI, IMT), Valentin Lafargue (ANITI, IMT), Mathieu Serrurier (IRIT-MISFIT)</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出正交和1-Lipschitz神经网络模块库，支持鲁棒和稳定模型设计，属于深度学习理论中的network architecture方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13776' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Universal Approximation Theorem for Input-Connected Multilayer Perceptrons</h3>
<p><strong>Authors:</strong> Vugar Ismailov</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 证明输入连接MLP的通用逼近定理，属于深度学习理论中的network architecture和泛化理论方向，具有基础理论价值。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14026' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A model of errors in transformers</h3>
<p><strong>Authors:</strong> Suvrat Raju, Praneeth Netrapalli</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Transformer的误差模型，分析LLM在重复任务中的错误来源，属于深度学习理论中的Transformer架构和误差分析方向，具有实践指导意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14175' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Graph Neural Networks are Heuristics</h3>
<p><strong>Authors:</strong> Yimeng Min, Carla P. Gomes</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 证明单训练轨迹可将GNN转化为组合优化的无监督启发式，无需监督或搜索，重构GNN在组合优化中的角色，属于深度学习理论方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13465' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning is a Modality</h3>
<p><strong>Authors:</strong> Zhiguang Liu, Yi Shang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出推理是一种独立模态，设计角色分离的Transformer块分离控制器与工作区，提升ARC任务性能（62.6% accuracy，超过人类平均60.2%），属于深度学习理论方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13562' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection</h3>
<p><strong>Authors:</strong> Hojin Kim, Jaehyung Kim</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 分析大模型Best-of-N选择中的置信度，发现其依赖表面流畅度而非推理结构，提出对比因果metric提升推理忠实性，属于深度学习理论方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising</h3>
<p><strong>Authors:</strong> Vikram R Lakkavalli</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Additive U-Net，将传统U-Net的拼接跳跃连接替换为门控加法连接，解决通道膨胀和信息流动不透明问题，提升去噪模型的鲁棒性与可解释性，在Kodak-17基准上取得 competitive 性能。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13208' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation</h3>
<p><strong>Authors:</strong> Pavlo Melnyk, Cuong Le, Urs Waldmann, Per-Erik Forssén, Bastian Wandt</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 研究旋转等变性在单目3D人体姿态估计中的作用，发现数据增强学习等变性优于设计性等变模型，属于深度学习理论的等变性研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13913' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Equivariant Learning for Unsupervised Image Dehazing</h3>
<p><strong>Authors:</strong> Zhang Wen, Jiangwei Xie, Dongdong Chen</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出等变学习框架EID，利用图像信号的对称性解决无监督图像去雾，属于深度学习理论的等变性应用研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13986' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management</h3>
<p><strong>Authors:</strong> Nattapong Kurpukdee, Adrian G. Bors</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出无监督视频增量学习方法，通过深度嵌入聚类管理提升增量学习性能，属于深度学习理论的增量学习研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14069' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VENI: Variational Encoder for Natural Illumination</h3>
<p><strong>Authors:</strong> Paul Walker, James A. D. Gardner, Andreea Ardelean, William A. P. Smith, Bernhard Egger</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出VENI等变变分编码器，建模自然光照的球形对称性，属于深度学习理论的变分自编码器（VAE）改进研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14079' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Two-Stream temporal transformer for video action classification</h3>
<p><strong>Authors:</strong> Nattapong Kurpukdee, Adrian G. Bors</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出双流temporal transformer框架，整合空间和运动信息提升视频动作分类性能，属于深度学习理论的Transformer结构改进研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14086' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces</h3>
<p><strong>Authors:</strong> Timo Aukusti Laine</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 用线性代数和哈密顿形式主义分析LLM嵌入空间的离散语义状态，探索语义关系和嵌入空间动力学，对理解LLM内部表示的几何结构有理论贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.11572' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective</h3>
<p><strong>Authors:</strong> Feilong Liu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 从几何视角分析Mixture-of-Experts（MoE），将路由解释为软聚类，通过Jacobian-PCA分析局部函数几何和表示几何，对MoE的工作机制提供新的理论理解。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.11616' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention</h3>
<p><strong>Authors:</strong> Luis Rosario Freytes</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 定义注意力层的几何语义，分解不变结构与建模选择，统一Gibbs权重、softmax等注意力机制，对Transformer注意力的理论框架有贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.11618' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization</h3>
<p><strong>Authors:</strong> Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Emre Neftci</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Proposes a taxonomy of linear recurrent models and SelectivBench (a lightweight synthetic benchmark) to systematically evaluate their selectivity and generalization. Contributes to network architecture research in deep learning theory.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.12598' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning</h3>
<p><strong>Authors:</strong> Ishir Garg, Neel Kolhe, Andy Peng, Rohan Gopalam</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出 Fisher 正交投影自然梯度下降优化器，解决持续学习中的灾难性遗忘，属于深度学习理论中的优化器方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.12816' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients</h3>
<p><strong>Authors:</strong> Pancheng Niu, Jun Guo, Qiaolin He, Yongming Chen, Yanchao Shi</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 结合注意力架构与冲突解决优化策略改进物理信息神经网络，属于深度学习理论中的架构与优化协同设计
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.12971' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Analysis of Long Range Dependency Understanding in State Space Models</h3>
<p><strong>Authors:</strong> Srividya Ravikumar, Abhinav Anand, Shweta Verma, Mira Mezini</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 分析状态空间模型（如 S4D）的长程依赖能力，属于深度学习理论中的网络架构研究
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13048' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Training instability in deep learning follows low-dimensional dynamical principles</h3>
<p><strong>Authors:</strong> Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 从动力系统视角研究深度学习训练不稳定性，属于深度学习理论中的训练动力学研究
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13160' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Relation of State Space Models and Hidden Markov Models</h3>
<p><strong>Authors:</strong> Aydin Ghojogh, M. Hadi Sepanj, Benyamin Ghojogh</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 对比状态空间模型（如 S4、Mamba）与隐马尔可夫模型的理论关系，属于深度学习理论中的网络架构研究
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13357' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Quadratic Upper Bound for Boosting Robustness</h3>
<p><strong>Authors:</strong> Euijin You, Hyang-Won Lee</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出对抗训练损失的二次上界，提升模型鲁棒性，属于深度学习理论中的鲁棒优化方向，具有理论指导意义。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13645' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Optimal L2 Regularization in High-dimensional Continual Linear Regression</h3>
<p><strong>Authors:</strong> Gilad Karpel, Edward Moroshko, Ran Levinstein, Ron Meir, Daniel Soudry, Itay Evron</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 研究高维持续线性回归的最优L2正则化，属于深度学习理论中的正则化和持续学习方向，提出T scaling law的重要结论。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13844' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition</h3>
<p><strong>Authors:</strong> Gorgi Pavlov</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出可微分逻辑合成框架，结合谱系数选择和Sinkhorn约束，属于深度学习理论中的神经符号和网络架构方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13953' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A universal linearized subspace refinement framework for neural networks</h3>
<p><strong>Authors:</strong> Wenbo Cao, Weiwei Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出线性化子空间细化框架，提升神经网络预测精度，属于深度学习理论中的网络优化和refinement方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.13989' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Riemannian Liquid Spatio-Temporal Graph Network</h3>
<p><strong>Authors:</strong> Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出黎曼流形上的液体时空图网络，属于深度学习理论中的几何深度学习和网络架构方向，提升非欧数据建模能力。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14115' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Penalizing Localized Dirichlet Energies in Low Rank Tensor Products</h3>
<p><strong>Authors:</strong> Paris A. Karakasis, Nicholas D. Sidiropoulos</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 研究低秩张量乘积的局部Dirichlet能量惩罚，属于深度学习理论中的张量模型和正则化方向，提升模型泛化能力。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14173' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior</h3>
<p><strong>Authors:</strong> Junwoo Chang, Joseph Park, Roberto Horowitz, Jongmin Lee, Jongeun Choi</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出组不变无监督技能发现框架GISD，将群结构嵌入技能发现目标，理论证明等变策略与群不变评分函数的最优性，属于深度学习理论中的表示学习与网络架构设计，与研究方向高度相关。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14000' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering</h3>
<p><strong>Authors:</strong> Zhiyang Li, Ao Ke, Yukun Cao, Xike Xie</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 融合常识图与场景图到多模态LLM，解决知识幻觉与视觉感知不足问题，提升VQA性能，属于多模态大模型的知识与视觉融合技术
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.11632' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation</h3>
<p><strong>Authors:</strong> Jing Zhang, Bingjie Fan</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 构建多模态情感关联知识图（MSA-KG），结合MLLM推理情感视觉线索，设计解耦的结构-情感编辑模块，实现训练-free的图像情感编辑，解决情感与内容解耦问题，属于原生多模态大模型的应用。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.12326' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch</h3>
<p><strong>Authors:</strong> Zheng Liu, Honglin Lin, Chonghan Qin, Xiaoyang Wang, Xin Gao, Yu Li, Mengzhang Cai, Yun Zhu, Zhanping Zhong, Qizhi Pei, Zhuoshi Pan, Xiaoran Shang, Bin Cui, Conghui He, Wentao Zhang, Lijun Wu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLMs）图表推理任务，提出可靠的程序化数据合成框架，解决现有数据集简单性与幻觉问题，提升图表推理性能，属于原生多模态大模型的关键数据支撑研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13606' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Donghee Lee, Rui Cai, Zhe Zhao</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出上下文感知的视觉表示优先框架，优化大视觉语言模型（LVLMs）的视觉-文本模态整合，提升视觉任务性能，属于原生多模态大模型的视觉能力增强研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13622' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Human detectors are surprisingly powerful reward models</h3>
<p><strong>Authors:</strong> Kumar Ashutosh, XuDong Wang, Xi Yin, Kristen Grauman, Adam Polyak, Ishan Misra, Rohit Girdhar</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出HuDA奖励模型，利用人类检测器提升视频生成的运动真实性，属于原生多模态大模型的视频生成质量优化研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.14037' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration</h3>
<p><strong>Authors:</strong> Yongcong Ye, Kai Zhang, Yanghai Zhang, Enhong Chen, Longfei Li, Jun Zhou</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出CVSI框架，整合视觉-语义互补信息提升零样本组合图像检索性能，属于原生多模态大模型的检索能力增强研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.14060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning</h3>
<p><strong>Authors:</strong> Zhiyan Hou, Haiyun Guo, Haokai Ma, Yandu Sun, Yonghui Yang, Jinqiao Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出基于路径激活子空间的 MoE-LoRA 方法，解决多模态大模型持续指令调优中的路由器-专家协同漂移问题，属于原生多模态大模型研究
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration</h3>
<p><strong>Authors:</strong> Jinyoung Park, Minseong Bae, Jeehye Na, Hyunwoo J. Kim</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出CoLLaMo框架，通过关系感知的多模态协作投影器整合分子1D序列、2D图和3D构象信息，提升分子语言模型的泛化能力，在分子captioning等多任务上取得最优性能，是原生多模态大模型的重要改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.12256' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation</h3>
<p><strong>Authors:</strong> Amine Rostane</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出用于评估text-to-image模型空间prompt跟随的基准SpatialBench-UC，考虑不确定性并支持可重复比较，涉及image generation，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13462' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</h3>
<p><strong>Authors:</strong> Hao Luo, Ye Wang, Wanpeng Zhang, Sipeng Zheng, Ziheng Xi, Chaoyi Xu, Haiweng Xu, Haoqi Yuan, Chi Zhang, Yiqing Wang, Yicheng Feng, Zongqing Lu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Being-H0.5，一个基于人类交互轨迹的Vision-Language-Action (VLA)多模态大模型，通过35000小时多模态数据预训练和跨embodiment统一动作空间设计，实现机器人跨平台泛化，属于多模态大模型的基础研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.12993' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</h3>
<p><strong>Authors:</strong> Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Xinming Wang, Bailing Wang, Cong Huang, Kai Chen</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出TwinBrainVLA架构，通过不对称混合Transformer协调通用VLM与专业VLM，解决多模态大模型在具身任务中的灾难性遗忘，属于原生多模态大模型的架构创新，与研究方向高度相关。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.14133' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles</h3>
<p><strong>Authors:</strong> Maria Lymperaiou, Vasileios Karampinis, Giorgos Filandrianos, Angelos Vlachos, Chrysoula Zerva, Athanasios Voulodimos</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 用视觉谜题探针分析大视觉语言模型（LVLMs）的推理能力，揭示其模式匹配与真正推理的差异，属于原生多模态大模型的推理机制研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13705' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search</h3>
<p><strong>Authors:</strong> Xinlei Yin, Xiulian Peng, Xiao Li, Zhiwei Xiong, Yan Lu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出HAVEN框架，通过层次结构和多模态实体整合提升长视频理解，解决信息碎片化问题，属于原生多模态大模型的长视频理解研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13719' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval</h3>
<p><strong>Authors:</strong> Gabriele Serussi, David Vainshtein, Jonathan Kouchly, Dotan Di Castro, Chaim Baskin</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出PREGEN框架，利用冻结大视觉语言模型（VLMs）提升组合视频检索性能，属于原生多模态大模型的检索应用研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13797' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology</h3>
<p><strong>Authors:</strong> Kaiyu Wu, Pucheng Han, Hualong Zhang, Naigeng Wu, Keze Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Weather-R1模型和LoCo-RFT框架，优化气象领域的多模态推理一致性，属于原生多模态大模型的领域化应用研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.14044' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model</h3>
<p><strong>Authors:</strong> Haoran Xu, Yanlin Liu, Zizhao Tong, Jiaze Li, Kexue Fu, Yuyang Zhang, Longxiang Gao, Shuaiguang Li, Xingyu Li, Yanran Xu, Changwei Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出MM-OOD框架，利用多模态大语言模型（MLLMs）提升OOD检测性能，属于原生多模态大模型的安全相关应用研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.14052' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System</h3>
<p><strong>Authors:</strong> Muhayy Ud Din, Waseem Akram, Ahsan B. Bakht, Irfan Hussain</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出LLM与VLM融合的框架，将自然语言任务指令转化为机器人可执行的符号计划，并通过VLM进行实时语义检查，实现港口检查的多模态感知与规划，属于LLM与VLM的融合应用。
Score: 6
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.13096' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SDiT: Semantic Region-Adaptive for Diffusion Transformers</h3>
<p><strong>Authors:</strong> Bowen Lin, Fanjiang Ye, Yihua Liu, Zhenghui Guo, Boyuan Zhang, Weijian Zheng, Yufan Xu, Tiancheng Xing, Yuke Wang, Chengming Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对Diffusion Transformers的计算瓶颈，提出语义区域自适应框架，通过快速分割、复杂度驱动调度和边界细化，根据区域复杂度分配计算，实现3倍推理加速且保持生成质量，属于diffusion LLM的效率优化。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.12283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Moaw: Unleashing Motion Awareness for Video Diffusion Models</h3>
<p><strong>Authors:</strong> Tianqi Zhang, Ziyi Wang, Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Zhengyang Huang, Jie Zhou, Jiwen Lu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Moaw框架将视频扩散模型从图像到视频生成转向视频到密集跟踪，通过训练扩散模型感知运动并注入运动特征到生成模型，实现零样本运动转移，属于大模型新技术（扩散模型扩展）的研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.12761' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations</h3>
<p><strong>Authors:</strong> Junyi Zhang, Yiming Wang, Yunhong Lu, Qichao Wang, Wenzhe Qian, Xiaoyin Xu, David Gu, Min Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出球面几何表示与Spherical Geometry Diffusion条件扩散框架，解决3D人脸生成的几何质量问题，实现可控的3D人脸生成、重建与编辑，实验优于现有方法。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13371' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation</h3>
<p><strong>Authors:</strong> Boyuan Cao, Xingbo Yao, Chenhui Wang, Jiaxin Ye, Yujie Wei, Hongming Shan</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出动态差分线性注意力机制，改进扩散Transformer的注意力计算，提升图像生成质量，属于扩散模型（大模型新技术）的关键优化。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13683' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion</h3>
<p><strong>Authors:</strong> Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno Lepri, Nicu Sebe</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出POCI-Diff框架，通过3D布局引导扩散模型生成一致的物体位置，属于大模型新技术的3D扩散生成研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.14056' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models</h3>
<p><strong>Authors:</strong> Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出Diffusion in Diffusion框架，解决块扩散模型的自回归瓶颈，属于大模型新技术中的diffusion LLM方向，提升扩散模型效率。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13599' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Generating metamers of human scene understanding</h3>
<p><strong>Authors:</strong> Ritik Raina, Abe Leite, Alexandros Graikos, Seoyoung Ahn, Dimitris Samaras, Gregory J. Zelinsky</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 用latent diffusion模型生成符合人类场景理解的metamers，结合视觉注意与场景上下文，属于diffusion相关的大模型新技术
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.11675' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study</h3>
<p><strong>Authors:</strong> A. Nieto Juscafresa, Á. Mazcuñán Herreros, J. Sullivan</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 探索冻结扩散模型作为自监督特征编码器的潜力，在浮游生物细粒度分类任务中，冻结扩散特征优于其他自监督方法，且在分布移位下保持鲁棒性。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13416' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models</h3>
<p><strong>Authors:</strong> Yang Yu, Yunze Deng, Yige Zhang, Yanjie Xiao, Youkun Ou, Wenhao Hu, Mingchao Li, Bin Feng, Wenyu Liu, Dandan Zheng, Jingdong Chen</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出首个多层虚拟试穿方法GO-MLVTON，用扩散模型处理服装遮挡与变形，生成高质量多层试穿结果，构建MLG数据集与LACD metric，实验验证SOTA性能。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13524' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation</h3>
<p><strong>Authors:</strong> Jinmei Liu, Haoru Li, Zhenhong Sun, Chaofeng Chen, Yatao Bian, Bo Wang, Daoyi Dong, Chunlin Chen, Zhi Wang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> Proposes DRIFT to mitigate diversity collapse in reinforcement fine-tuning of generative models (diffusion/flow), balancing task alignment with generation diversity. Aligns with large model new technologies (diffusion LLM subarea).
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.12401' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning</h3>
<p><strong>Authors:</strong> Doheon Kim</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 研究分数扩散模型的确定性动力学，属于大模型新技术中的扩散模型方向
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.12965' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Principled Latent Diffusion for Graphs via Laplacian Autoencoders</h3>
<p><strong>Authors:</strong> Antoine Siraudin, Christopher Morris</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出基于拉普拉斯自编码器的图潜在扩散模型，属于大模型新技术中的diffusion方向，解决图生成的二次复杂度问题。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13780' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart</h3>
<p><strong>Authors:</strong> Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zijun Yao, Heng Wang, Minshen Yu, Yixin Cao</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 识别长CoT中的“思维陷阱”（早期错误导致后续自我一致但错误的推理），提出TAAR框架通过诊断政策预测陷阱位置并自适应重启解码，显著提升数学和科学推理性能，是大模型推理技术的重要改进。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.11940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion</h3>
<p><strong>Authors:</strong> Ang Gao, Changshuo Zhang, Xiao Zhang, Deyang Li, Minjun Zhao, Fangchao Liu, Xinyu Zhang</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 针对静态ICL无法适应推理过程中动态困惑点的问题，提出PICL框架动态插入相关演示，解决数学推理中的中途困惑，提升复杂推理准确性，是ICL在推理任务中的重要扩展。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.11979' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PlannerRFT: Reinforcing Diffusion Planners through Closed-Loop and Sample-Efficient Fine-Tuning</h3>
<p><strong>Authors:</strong> Hongchen Li, Tianyu Li, Jiazhi Yang, Haochen Tian, Caojun Wang, Lei Shi, Mingyang Shang, Zengrong Lin, Gaoqiang Wu, Zhihui Hao, Xianpeng Lang, Jia Hu, Hongyang Li</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出PlannerRFT框架，通过强化学习闭式循环微调diffusion规划器，解决现有方法多模态轨迹生成能力不足的问题，提升自动驾驶场景下轨迹生成的鲁棒性，属于diffusion模型的强化学习优化新技术。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.12901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation</h3>
<p><strong>Authors:</strong> Weize Xie, Yi Ding, Ying He, Leilei Wang, Binwen Bai, Zheyi Zhao, Chenyang Wang, F. Richard Yu</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 提出ForeDiffusion，通过注入未来视图表示引导diffusion policy的去噪过程，解决现有方法依赖短期观测导致的轨迹偏差问题，提升机器人操纵任务的成功率，属于diffusion策略的前瞻性改进技术。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.12925' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Diffusion-based Inverse Model of a Distributed Tactile Sensor for Object Pose Estimation</h3>
<p><strong>Authors:</strong> Ante Marić, Giammarco Caroleo, Alessandro Albini, Julius Jankowski, Perla Maiolino, Sylvain Calinon</p>
<p><strong>Published:</strong> 2026-01-21</p>
<p><strong>Reason:</strong> 用denoising diffusion学习分布式触觉传感器的逆模型，通过触觉观测生成物体位姿假设，结合粒子滤波实现位姿估计，解决触觉数据部分可观测的问题，属于diffusion模型在触觉模态的创新应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.13250' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>