<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-23</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >大模型新技术</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习理论</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-23</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-23 12:40:37</p>
<p>已自动阅读了 216 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：98727</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models</h3>
<p><strong>Authors:</strong> YuanLab. ai, Shawn Wu, Jiangang Luo, Tong Yu, Darcy Chen, Sean Wang, Xudong Zhao, Louie Li, Claire Wang, Hunter He, Carol Wang, Allen Wang</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出层自适应专家剪枝算法LAEP，针对MoE LLM预训练阶段剪枝未充分利用的专家，显著提升训练效率并减少参数规模
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14327' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</h3>
<p><strong>Authors:</strong> Haowei Zhang, Shudong Yang, Jinlan Fu, See-Kiong Ng, Xipeng Qiu</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 将KV缓存设计为分层内存框架，实现流式视频理解的高效推理（减少70%视频token仍保持精度），属于高效大模型训练与推理研究
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14724' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Understanding Best Practices for Quantization of Vision-Language Models</h3>
<p><strong>Authors:</strong> Gautom Das, Vincent La, Ethan Lau, Abhinav Shrivastava, Matthew Gwilliam</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 研究多模态模型量化的最佳实践，分析不同量化方法、位宽及量化对象对性能的影响，为MLLM高效部署提供实用 insights
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.15287' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct</h3>
<p><strong>Authors:</strong> Uygar Kurt</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 对llama.cpp量化格式进行统一实证研究，覆盖多任务性能、困惑度、吞吐量等指标，为本地部署选择量化方案提供实用指南
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14277' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents</h3>
<p><strong>Authors:</strong> Xiucheng Xu, Bingbing Xu, Xueyun Tian, Zihe Huang, Rongxin Chen, Yunfan Li, Huawei Shen</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出轻量级记忆构造框架CoM，通过动态进化组织记忆片段并自适应截断无关噪声，提升LLM Agents长文本推理性能同时降低计算开销
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14287' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design</h3>
<p><strong>Authors:</strong> Nilesh Prasad Pandey, Jangseon Park, Onat Gungor, Flavio Ponzina, Tajana Rosing</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 针对小语言模型边缘推理的量化与内存协同设计，通过离群值感知量化和异质内存架构优化，显著降低内存占用与延迟，提升部署效率
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14549' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study</h3>
<p><strong>Authors:</strong> Keyu Lv, Manyi Zhang, Xiaobo Xia, Jingchen Ni, Shannan Yan, Xianzhi Yu, Lu Hou, Chun Yuan, Haoli Bai</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 系统研究推理LLM的低比特量化感知训练，发现知识蒸馏、PTQ初始化等关键因素，提出优化工作流Reasoning-QAT，显著提升低比特场景下的推理性能
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14888' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RadixMLP -- Intra-batch Deduplication for Causal Transformers</h3>
<p><strong>Authors:</strong> Michael Feil, Julius Lipp</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 利用因果Transformer的位置无关计算特性，通过前缀树压缩共享前缀的重复计算，提升推理速度，在真实 reranking 任务中实现1.44-1.59倍加速
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.15013' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> POTR: Post-Training 3DGS Compression</h3>
<p><strong>Authors:</strong> Bert Ramlot, Martijn Courteaux, Peter Lambert, Glenn Van Wallendael</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出3D高斯Splatting（3DGS）的后训练压缩方法，通过剪枝和重新计算光照系数提升存储效率（减少2-4倍splats），属于高效大模型训练与推理中的压缩研究
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14821' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Limits of Learned Importance Scoring for KV Cache Compression</h3>
<p><strong>Authors:</strong> Brady Steele</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 研究KV缓存压缩中学习重要性评分的局限性，发现位置启发式等简单方法优于复杂学习方法，为缓存优化提供新见解
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.14279' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FSX: Message Flow Sensitivity Enhanced Structural Explainer for Graph Neural Networks</h3>
<p><strong>Authors:</strong> Bizu Feng, Zhimu Yang, Shaode Yu, Zixin Hu</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 结合GNN内部消息流分析与基于Shapley-like值的协作博弈，提升可解释性的 fidelity与效率，揭示模型通过关键计算路径实现结构推理的逻辑，直接涉及可解释性核心方法
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.14730' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Auditing Language Model Unlearning via Information Decomposition</h3>
<p><strong>Authors:</strong> Anmol Goel, Alan Ritter, Iryna Gurevych</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 用部分信息分解审计LLM遗忘效果，揭示残留知识与 adversarial 重建攻击的关联，提出表示级风险分数指导敏感输入 abstention，属于大模型安全与对齐中的隐私保护方向
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.15111' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity</h3>
<p><strong>Authors:</strong> Jun Liu, Leo Yu Zhang, Fengpeng Li, Isao Echizen, Jiantao Zhou</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 研究硬标签黑盒设置下的梯度信息恢复问题，提出结合频域初始化和模式驱动优化的攻击框架，超越现有硬标签攻击的成功率和查询效率
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.14300' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution</h3>
<p><strong>Authors:</strong> Chen Qian, Peng Wang, Dongrui Liu, Junyao Yang, Dadi Guo, Ling Tang, Jilin Mei, Qihan Ren, Shuai Shao, Yong Liu, Jie Fu, Jing Shao, Xia Hu</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出分层Agent行为归因框架，识别Agent行为的内部驱动因素，解决大模型Agent的责任可追溯性问题，实验覆盖工具使用、记忆偏差等场景，对大模型安全与对齐中的 accountability 有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.15075' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption</h3>
<p><strong>Authors:</strong> Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 针对扩散模型换脸的身份安全问题，提出多阶段扰动注入的防御方法，破坏身份传递路径，属于大模型安全与对齐研究
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.14738' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> GCG Attack On A Diffusion LLM</h3>
<p><strong>Authors:</strong> Ruben Neyroud, Sam Corley</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 探索GCG风格对抗攻击对扩散LLM的适用性，评估不同攻击变体在有害提示上的效果，为扩散语言模型鲁棒性分析提供初始见解
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.14266' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories</h3>
<p><strong>Authors:</strong> Qian Xiong, Yuekai Huang, Yujia Zheng, Tianhao Li, Ziyou Jiang, Zhiyuan Chang, Zhaoyang Li, Huanxiang Feng, Mingyang Li</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出“真实到虚拟”的数据合成方法RISE，解决工具使用Agent的意图偏差问题，通过生成负样本微调模型，实现任务完成率（+35.28%）与意图对齐度（+23.27%）的显著提升，属于大模型安全与对齐中的意图对齐关键进展。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.15120' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models</h3>
<p><strong>Authors:</strong> Mengyu Sun, Ziyuan Yang, Andrew Beng Jin Teoh, Junxu Liu, Haibo Hu, Yi Zhang</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 针对扩散模型中被擦除概念的重唤醒问题，提出 latent space 重建与梯度正交化等方法，深入分析扩散生成的多因素影响，属于扩散模型相关的大模型新技术研究
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.14330' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models</h3>
<p><strong>Authors:</strong> Injin Kong, Hyoungjoon Lee, Yohan Jo</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 分析自回归模型转掩码扩散模型的内部机制迁移，揭示扩散后训练对模型计算路径的重构，属于大模型新技术中Diffusion LLM方向的理论探索
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.14758' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> 3D Space as a Scratchpad for Editable Text-to-Image Generation</h3>
<p><strong>Authors:</strong> Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 引入3D空间作为视觉语言模型的空间推理 substrates，将文本意图与3D场景规划结合，提升文本到图像生成的空间一致性和可编辑性，属于原生多模态大模型研究
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.14602' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Runway Cascade of Transformers for Language Modeling</h3>
<p><strong>Authors:</strong> Hunjae Lee, Corey Clark</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 研究因果Transformer中因信息传播模式错位导致的Runway Cascade现象，提出跑道感知重连机制改进信息传播平衡，提升语言模型性能及外推能力，属于Transformer结构核心问题的探索
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14522' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum</h3>
<p><strong>Authors:</strong> Jingru Li, Yibo Fan, Huan Li</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 改进Muon优化器，引入方差自适应的NSR调制与方差缩放动量机制，加速LLM预训练收敛，在GPT-2和LLaMA上验证了收敛速度提升
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14603' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search</h3>
<p><strong>Authors:</strong> Bostan Khan, Masoud Daneshtalab</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 针对联邦神经架构搜索的核心瓶颈（无指导超网训练、耗时子网发现）提出统一框架，结合多目标适应度函数与无预测器搜索，实现精度提升（CIFAR-100上1.21%）和61倍速度加快，对深度学习理论中的联邦架构搜索有重要推动作用。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.15127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mirai: Autoregressive Visual Generation Needs Foresight</h3>
<p><strong>Authors:</strong> Yonghao Yu, Lang Huang, Zerun Wang, Runyi Li, Toshihiko Yamasaki</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 针对自回归视觉生成的因果性监督局限，提出引入未来信息（foresight）的训练框架，提升收敛速度和生成质量，属于深度学习理论中的训练策略与网络架构研究
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective</h3>
<p><strong>Authors:</strong> Xiao Hu, Hong Xie, Tao Tan, Defu Lian, Jianyu Han</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 从多臂老虎机视角重新审视LLM的强化微调，通过极简配置与逐层扩展实验，揭示微调设计选择的作用与瓶颈，为RL微调提供理论新视角
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.14599' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ZENITH: Automated Gradient Norm Informed Stochastic Optimization</h3>
<p><strong>Authors:</strong> Dhrubo Saha</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出基于梯度范数时间演化的自适应学习率优化器，解决现有自适应优化器的计算开销与泛化问题，实验覆盖CNN、目标检测等多任务，结果优于基线，属于深度学习理论中的优化器方向关键进展。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.15212' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CI4A: Semantic Component Interfaces for Agents Empowering Web Automation</h3>
<p><strong>Authors:</strong> Zhi Qiu, Jiazheng Sun, Chenxiao Xia, Jun Zheng, Xin Peng</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 为Agent设计UI组件语义封装机制，将复杂交互抽象为工具原语，解决Web自动化中的Agent-UI对齐问题，在WebArena基准上实现86.3%的任务成功率，显著提升多模态智能体的Web交互能力。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.14790' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</h3>
<p><strong>Authors:</strong> Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</p>
<p><strong>Published:</strong> 2026-01-22</p>
<p><strong>Reason:</strong> 提出贝叶斯分解框架解决Vision-Language-Action模型的“信息坍塌”问题，强制语言指令与动作的关联，提升多模态智能体的语言接地能力，在SimplerEnv基准上实现11.3%的泛化性能提升。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.15197' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>