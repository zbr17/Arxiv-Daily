# ArXiv 每日推荐 - 2026-01-23

> 更新于北京时间：2026-01-23 12:40:37
> 已自动阅读了 216 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：98727

## 高效大模型训练与推理

### [Score: 9.0/10] Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models
- **Authors:** YuanLab. ai, Shawn Wu, Jiangang Luo, Tong Yu, Darcy Chen, Sean Wang, Xudong Zhao, Louie Li, Claire Wang, Hunter He, Carol Wang, Allen Wang
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14327](https://arxiv.org/abs/2601.14327)
- **Reason:** 提出层自适应专家剪枝算法LAEP，针对MoE LLM预训练阶段剪枝未充分利用的专家，显著提升训练效率并减少参数规模
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding
- **Authors:** Haowei Zhang, Shudong Yang, Jinlan Fu, See-Kiong Ng, Xipeng Qiu
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14724](https://arxiv.org/abs/2601.14724)
- **Reason:** 将KV缓存设计为分层内存框架，实现流式视频理解的高效推理（减少70%视频token仍保持精度），属于高效大模型训练与推理研究
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Towards Understanding Best Practices for Quantization of Vision-Language Models
- **Authors:** Gautom Das, Vincent La, Ethan Lau, Abhinav Shrivastava, Matthew Gwilliam
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15287](https://arxiv.org/abs/2601.15287)
- **Reason:** 研究多模态模型量化的最佳实践，分析不同量化方法、位宽及量化对象对性能的影响，为MLLM高效部署提供实用 insights
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct
- **Authors:** Uygar Kurt
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14277](https://arxiv.org/abs/2601.14277)
- **Reason:** 对llama.cpp量化格式进行统一实证研究，覆盖多任务性能、困惑度、吞吐量等指标，为本地部署选择量化方案提供实用指南
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents
- **Authors:** Xiucheng Xu, Bingbing Xu, Xueyun Tian, Zihe Huang, Rongxin Chen, Yunfan Li, Huawei Shen
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14287](https://arxiv.org/abs/2601.14287)
- **Reason:** 提出轻量级记忆构造框架CoM，通过动态进化组织记忆片段并自适应截断无关噪声，提升LLM Agents长文本推理性能同时降低计算开销
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design
- **Authors:** Nilesh Prasad Pandey, Jangseon Park, Onat Gungor, Flavio Ponzina, Tajana Rosing
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14549](https://arxiv.org/abs/2601.14549)
- **Reason:** 针对小语言模型边缘推理的量化与内存协同设计，通过离群值感知量化和异质内存架构优化，显著降低内存占用与延迟，提升部署效率
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study
- **Authors:** Keyu Lv, Manyi Zhang, Xiaobo Xia, Jingchen Ni, Shannan Yan, Xianzhi Yu, Lu Hou, Chun Yuan, Haoli Bai
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14888](https://arxiv.org/abs/2601.14888)
- **Reason:** 系统研究推理LLM的低比特量化感知训练，发现知识蒸馏、PTQ初始化等关键因素，提出优化工作流Reasoning-QAT，显著提升低比特场景下的推理性能
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] RadixMLP -- Intra-batch Deduplication for Causal Transformers
- **Authors:** Michael Feil, Julius Lipp
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15013](https://arxiv.org/abs/2601.15013)
- **Reason:** 利用因果Transformer的位置无关计算特性，通过前缀树压缩共享前缀的重复计算，提升推理速度，在真实 reranking 任务中实现1.44-1.59倍加速
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] POTR: Post-Training 3DGS Compression
- **Authors:** Bert Ramlot, Martijn Courteaux, Peter Lambert, Glenn Van Wallendael
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14821](https://arxiv.org/abs/2601.14821)
- **Reason:** 提出3D高斯Splatting（3DGS）的后训练压缩方法，通过剪枝和重新计算光照系数提升存储效率（减少2-4倍splats），属于高效大模型训练与推理中的压缩研究
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] On the Limits of Learned Importance Scoring for KV Cache Compression
- **Authors:** Brady Steele
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14279](https://arxiv.org/abs/2601.14279)
- **Reason:** 研究KV缓存压缩中学习重要性评分的局限性，发现位置启发式等简单方法优于复杂学习方法，为缓存优化提供新见解
Score: 7
Field: 高效大模型训练与推理

## 深度学习可解释性

### [Score: 9.0/10] FSX: Message Flow Sensitivity Enhanced Structural Explainer for Graph Neural Networks
- **Authors:** Bizu Feng, Zhimu Yang, Shaode Yu, Zixin Hu
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14730](https://arxiv.org/abs/2601.14730)
- **Reason:** 结合GNN内部消息流分析与基于Shapley-like值的协作博弈，提升可解释性的 fidelity与效率，揭示模型通过关键计算路径实现结构推理的逻辑，直接涉及可解释性核心方法
Score: 9
Field: 深度学习可解释性

## 大模型安全与对齐

### [Score: 9.0/10] Auditing Language Model Unlearning via Information Decomposition
- **Authors:** Anmol Goel, Alan Ritter, Iryna Gurevych
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15111](https://arxiv.org/abs/2601.15111)
- **Reason:** 用部分信息分解审计LLM遗忘效果，揭示残留知识与 adversarial 重建攻击的关联，提出表示级风险分数指导敏感输入 abstention，属于大模型安全与对齐中的隐私保护方向
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity
- **Authors:** Jun Liu, Leo Yu Zhang, Fengpeng Li, Isao Echizen, Jiantao Zhou
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14300](https://arxiv.org/abs/2601.14300)
- **Reason:** 研究硬标签黑盒设置下的梯度信息恢复问题，提出结合频域初始化和模式驱动优化的攻击框架，超越现有硬标签攻击的成功率和查询效率
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution
- **Authors:** Chen Qian, Peng Wang, Dongrui Liu, Junyao Yang, Dadi Guo, Ling Tang, Jilin Mei, Qihan Ren, Shuai Shao, Yong Liu, Jie Fu, Jing Shao, Xia Hu
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15075](https://arxiv.org/abs/2601.15075)
- **Reason:** 提出分层Agent行为归因框架，识别Agent行为的内部驱动因素，解决大模型Agent的责任可追溯性问题，实验覆盖工具使用、记忆偏差等场景，对大模型安全与对齐中的 accountability 有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption
- **Authors:** Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14738](https://arxiv.org/abs/2601.14738)
- **Reason:** 针对扩散模型换脸的身份安全问题，提出多阶段扰动注入的防御方法，破坏身份传递路径，属于大模型安全与对齐研究
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] GCG Attack On A Diffusion LLM
- **Authors:** Ruben Neyroud, Sam Corley
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14266](https://arxiv.org/abs/2601.14266)
- **Reason:** 探索GCG风格对抗攻击对扩散LLM的适用性，评估不同攻击变体在有害提示上的效果，为扩散语言模型鲁棒性分析提供初始见解
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories
- **Authors:** Qian Xiong, Yuekai Huang, Yujia Zheng, Tianhao Li, Ziyou Jiang, Zhiyuan Chang, Zhaoyang Li, Huanxiang Feng, Mingyang Li
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15120](https://arxiv.org/abs/2601.15120)
- **Reason:** 提出“真实到虚拟”的数据合成方法RISE，解决工具使用Agent的意图偏差问题，通过生成负样本微调模型，实现任务完成率（+35.28%）与意图对齐度（+23.27%）的显著提升，属于大模型安全与对齐中的意图对齐关键进展。
Score: 7
Field: 大模型安全与对齐

## 大模型新技术

### [Score: 8.0/10] LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models
- **Authors:** Mengyu Sun, Ziyuan Yang, Andrew Beng Jin Teoh, Junxu Liu, Haibo Hu, Yi Zhang
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14330](https://arxiv.org/abs/2601.14330)
- **Reason:** 针对扩散模型中被擦除概念的重唤醒问题，提出 latent space 重建与梯度正交化等方法，深入分析扩散生成的多因素影响，属于扩散模型相关的大模型新技术研究
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models
- **Authors:** Injin Kong, Hyoungjoon Lee, Yohan Jo
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14758](https://arxiv.org/abs/2601.14758)
- **Reason:** 分析自回归模型转掩码扩散模型的内部机制迁移，揭示扩散后训练对模型计算路径的重构，属于大模型新技术中Diffusion LLM方向的理论探索
Score: 8
Field: 大模型新技术

## 原生多模态大模型

### [Score: 8.0/10] 3D Space as a Scratchpad for Editable Text-to-Image Generation
- **Authors:** Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14602](https://arxiv.org/abs/2601.14602)
- **Reason:** 引入3D空间作为视觉语言模型的空间推理 substrates，将文本意图与3D场景规划结合，提升文本到图像生成的空间一致性和可编辑性，属于原生多模态大模型研究
Score: 8
Field: 原生多模态大模型

## 深度学习理论

### [Score: 8.0/10] On the Runway Cascade of Transformers for Language Modeling
- **Authors:** Hunjae Lee, Corey Clark
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14522](https://arxiv.org/abs/2601.14522)
- **Reason:** 研究因果Transformer中因信息传播模式错位导致的Runway Cascade现象，提出跑道感知重连机制改进信息传播平衡，提升语言模型性能及外推能力，属于Transformer结构核心问题的探索
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum
- **Authors:** Jingru Li, Yibo Fan, Huan Li
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14603](https://arxiv.org/abs/2601.14603)
- **Reason:** 改进Muon优化器，引入方差自适应的NSR调制与方差缩放动量机制，加速LLM预训练收敛，在GPT-2和LLaMA上验证了收敛速度提升
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search
- **Authors:** Bostan Khan, Masoud Daneshtalab
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15127](https://arxiv.org/abs/2601.15127)
- **Reason:** 针对联邦神经架构搜索的核心瓶颈（无指导超网训练、耗时子网发现）提出统一框架，结合多目标适应度函数与无预测器搜索，实现精度提升（CIFAR-100上1.21%）和61倍速度加快，对深度学习理论中的联邦架构搜索有重要推动作用。
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] Mirai: Autoregressive Visual Generation Needs Foresight
- **Authors:** Yonghao Yu, Lang Huang, Zerun Wang, Runyi Li, Toshihiko Yamasaki
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14671](https://arxiv.org/abs/2601.14671)
- **Reason:** 针对自回归视觉生成的因果性监督局限，提出引入未来信息（foresight）的训练框架，提升收敛速度和生成质量，属于深度学习理论中的训练策略与网络架构研究
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective
- **Authors:** Xiao Hu, Hong Xie, Tao Tan, Defu Lian, Jianyu Han
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14599](https://arxiv.org/abs/2601.14599)
- **Reason:** 从多臂老虎机视角重新审视LLM的强化微调，通过极简配置与逐层扩展实验，揭示微调设计选择的作用与瓶颈，为RL微调提供理论新视角
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] ZENITH: Automated Gradient Norm Informed Stochastic Optimization
- **Authors:** Dhrubo Saha
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15212](https://arxiv.org/abs/2601.15212)
- **Reason:** 提出基于梯度范数时间演化的自适应学习率优化器，解决现有自适应优化器的计算开销与泛化问题，实验覆盖CNN、目标检测等多任务，结果优于基线，属于深度学习理论中的优化器方向关键进展。
Score: 7
Field: 深度学习理论

## 多模态智能体

### [Score: 8.0/10] CI4A: Semantic Component Interfaces for Agents Empowering Web Automation
- **Authors:** Zhi Qiu, Jiazheng Sun, Chenxiao Xia, Jun Zheng, Xin Peng
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.14790](https://arxiv.org/abs/2601.14790)
- **Reason:** 为Agent设计UI组件语义封装机制，将复杂交互抽象为工具原语，解决Web自动化中的Agent-UI对齐问题，在WebArena基准上实现86.3%的任务成功率，显著提升多模态智能体的Web交互能力。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries
- **Authors:** Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen
- **Published:** 2026-01-22
- **Link:** [https://arxiv.org/abs/2601.15197](https://arxiv.org/abs/2601.15197)
- **Reason:** 提出贝叶斯分解框架解决Vision-Language-Action模型的“信息坍塌”问题，强制语言指令与动作的关联，提升多模态智能体的语言接地能力，在SimplerEnv基准上实现11.3%的泛化性能提升。
Score: 8
Field: 多模态智能体

