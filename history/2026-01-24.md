# ArXiv 每日推荐 - 2026-01-24

> 更新于北京时间：2026-01-24 12:35:45
> 已自动阅读了 199 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：99344

## 原生多模态大模型

### [Score: 9.0/10] Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling
- **Authors:** Hongyang Wei, Hongbo Liu, Zidong Wang, Yi Peng, Baixin Xu, Size Wu, Xuying Zhang, Xianglong He, Zexiang Liu, Peiyu Wang, Xuchen Song, Yangguang Li, Yang Liu, Yahui Zhou
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15664](https://arxiv.org/abs/2601.15664)
- **Reason:** 提出统一多图合成与单图编辑的原生多模态框架，通过序列建模和高效采样提升性能，在多图合成基准上超越现有模型，对原生多模态大模型的图像合成任务有重要推进
Score: 9
Field: 原生多模态大模型

### [Score: 8.0/10] Controllable Layered Image Generation for Real-World Editing
- **Authors:** Jinrui Yang, Qing Liu, Yijun Li, Mengwei Ren, Letian Zhang, Zhe Lin, Cihang Xie, Yuyin Zhou
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15507](https://arxiv.org/abs/2601.15507)
- **Reason:** 提出可控分层图像生成框架，解决现有方法层间一致性和视觉效果问题，对原生多模态大模型的图像生成与编辑任务有重要贡献
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] SAMTok: Representing Any Mask with Two Words
- **Authors:** Yikang Zhou, Tao Zhang, Dengxian Gong, Yuanzheng Wu, Ye Tian, Haochen Wang, Haobo Yuan, Jiacong Wang, Lu Qi, Hao Fei, Anran Wang, Zhuochen Wang, Yujing Wang, Cheng Chen, Shunping Ji, Xiangtai Li
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.16093](https://arxiv.org/abs/2601.16093)
- **Reason:** 提出离散掩码tokenizer SAMTok，将任意掩码转换为两个离散token，使多模态大模型（如QwenVL）通过标准训练获得强像素级能力，在区域captioning、VQA、分割等多任务上取得SOTA，直接关联原生多模态大模型的tokenizer与像素级能力提升方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing
- **Authors:** Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, Xudong Jiang
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.16200](https://arxiv.org/abs/2601.16200)
- **Reason:** 提出Feature-space Smoothing (FS)框架，为MLLMs提供特征级鲁棒性证明。结合PSM模块提升高斯鲁棒性分数，将白盒攻击成功率从近90%降至约1%，同时保持任务性能，对MLLM安全至关重要。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition
- **Authors:** Hatef Otroshi Shahreza, Anjith George, S\'ebastien Marcel
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15406](https://arxiv.org/abs/2601.15406)
- **Reason:** 系统评估了多模态大语言模型在异质人脸识别中的性能，揭示了其在跨光谱场景下的局限性，对原生多模态大模型的应用与改进有参考价值
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] VIOLA: Towards Video In-Context Learning with Minimal Annotations
- **Authors:** Ryo Fujii, Hideo Saito, Ryo Hachiuma
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15549](https://arxiv.org/abs/2601.15549)
- **Reason:** 针对多模态大语言模型在视频领域的少标注问题，提出基于最小注释的in-context learning框架，提升低资源场景下的适应能力，对原生多模态大模型的泛化有帮助
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation
- **Authors:** Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen, Dong-Hwan Jang, Inderjit S Dhillon, Ismini Lourentzou
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.16210](https://arxiv.org/abs/2601.16210)
- **Reason:** 提出语言对齐的金字塔视频tokenizer，学习多尺度离散latents以提升跨模态对齐与零样本迁移，针对视频理解与生成任务优化，属于原生多模态大模型的tokenizer与视频方向核心研究。
Score: 7
Field: 原生多模态大模型

## 深度学习理论

### [Score: 9.0/10] PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction
- **Authors:** Dongchen Huang
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15540](https://arxiv.org/abs/2601.15540)
- **Reason:** 从Maximum Coding Rate Reduction（MCR²）原则出发推导Transformer，将其视为信号降噪算子，引入过完备字典与π-RoPE等几何归纳偏置，实现无监督功能解耦，属于深度学习理论的Transformer架构理论核心研究。
Score: 9
Field: 深度学习理论

### [Score: 7.0/10] Partially Lazy Gradient Descent for Smoothed Online Learning
- **Authors:** Naram Mhaisen, George Iosifidis
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15984](https://arxiv.org/abs/2601.15984)
- **Reason:** 提出k-lazyGD在线学习算法，桥接OGD与lazy GD，分析Smoothed Online Convex Optimization的动态regret，证明可达最优界O(√(P_T T))，实验验证反应性与稳定性平衡，为优化器设计提供新思路。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Decoupling Return-to-Go for Efficient Decision Transformer
- **Authors:** Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li, Qirui Zheng, Xionghui Yang, Wenxin Li
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15953](https://arxiv.org/abs/2601.15953)
- **Reason:** 识别Decision Transformer（DT）中Return-to-Go（RTG）的冗余，提出Decoupled DT（DDT）简化架构，在多个离线RL任务上显著优于DT及变种，属于深度学习理论中网络架构改进的重要工作，提升DT效率和性能。
Score: 7
Field: 深度学习理论

## 高效大模型训练与推理

### [Score: 9.0/10] Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models
- **Authors:** Alfred Shen, Aaron Shen
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15305](https://arxiv.org/abs/2601.15305)
- **Reason:** 提出Gated Sparse Attention (GSA)，结合稀疏注意力与门控机制解决长上下文LLM的效率与稳定性矛盾。理论分析复杂度、表达性与收敛性，实验用1.7B模型验证，实现12-16倍速度提升，困惑度从6.03降至5.70，注意力sink现象显著缓解，训练稳定性大幅提高。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] Improving MoE Compute Efficiency by Composing Weight and Data Sparsity
- **Authors:** Maciej Kilian, Oleg Mkrtchyan, Luke Zettlemoyer, Akshat Shrivastava, Armen Aghajanyan
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15370](https://arxiv.org/abs/2601.15370)
- **Reason:** 提出组合权重稀疏与数据稀疏的MoE框架，通过引入null专家实现数据稀疏，解决视觉-语言模型训练中的计算冗余问题，属于高效大模型训练与推理的MoE效率优化核心方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification
- **Authors:** Jingwei Song, Xinyu Wang, Hanbin Wang, Xiaoxuan Lei, Bill Shi, Shixin Han, Eric Yang, Xiao-Wen Chang, Lynn Ai
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15498](https://arxiv.org/abs/2601.15498)
- **Reason:** 提出Margin-Aware验证策略，改进推测解码的验证机制，在不损失生成质量的情况下显著提升大语言模型推理速度，属于高效大模型训练与推理的推理加速核心方向。
Score: 8
Field: 高效大模型训练与推理

## 多模态智能体

### [Score: 9.0/10] EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience
- **Authors:** Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han, Haozhe Wang, Jianing Wang, Xiaocheng Zhang, Xin Yang, Dengchang Zhao, Jinrui Ding, Xiandi Ma, Yuchen Xie, Peng Pei, Xunliang Cai, Xipeng Qiu (Fudan University, etc.)
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15876](https://arxiv.org/abs/2601.15876)
- **Reason:** 针对原生计算机使用agent（CUA）的静态数据瓶颈，提出EvoCUA框架通过可验证合成引擎和迭代进化学习，在OSWorld基准上取得56.7%成功率（开源SOTA），为多模态智能体能力提升提供scalable路径。
Score: 9
Field: 多模态智能体

### [Score: 8.0/10] VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning
- **Authors:** Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin, Yan Gong, Ruilin Li, Yin Zhang, Jiaqi Wang
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15724](https://arxiv.org/abs/2601.15724)
- **Reason:** 提出用LLM引导工具推理构建多模态视频智能体，解决长视频理解的信息丢失问题，对多模态智能体的工具使用与长视频处理有重要贡献
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Agentic Uncertainty Quantification
- **Authors:** Jiaxin Zhang, Prafulla Kumar Choubey, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15703](https://arxiv.org/abs/2601.15703)
- **Reason:** 提出Dual-Process Agentic UQ (AUQ)框架，将不确定性转化为主动控制信号，通过UAM与UAR机制平衡代理的高效执行与深度deliberation，实验提升闭环基准与开放任务的性能与校准度，推动可靠多模态智能体发展。
Score: 8
Field: 多模态智能体

### [Score: 7.0/10] Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification
- **Authors:** Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu (The Chinese University of Hong Kong, etc.)
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15808](https://arxiv.org/abs/2601.15808)
- **Reason:** 构建DRA Failure Taxonomy分类agent失败模式，提出DeepVerifier验证器，通过测试时rubric-guided反馈实现agent自进化，在GAIA等基准上提升性能，为多模态智能体自我改进提供有效方法。
Score: 7
Field: 多模态智能体

## 大模型安全与对齐

### [Score: 8.0/10] Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs
- **Authors:** Mingyu Yu, Lana Liu, Zhehao Zhao, Wei Wang, Sujuan Qin
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15698](https://arxiv.org/abs/2601.15698)
- **Reason:** 揭示了多模态大模型在视觉安全对齐中的漏洞，提出语义无关输入的越狱方法，对大模型安全与对齐的研究有警示和参考价值
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models
- **Authors:** Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, Shouling Ji, Songze Li
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15801](https://arxiv.org/abs/2601.15801)
- **Reason:** 提出GOSV框架，通过全局优化识别LLM中安全关键注意力头，区分出Malicious Injection Vectors与Safety Suppression Vectors。基于此开发的白盒越狱攻击显著优于现有方法，为LLM安全可解释性提供新视角。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing
- **Authors:** Xinyu Wang, Sicheng Lyu, Yu Gu, Jerry Huang, Peng Lu, Yufei Cui, Xiao-Wen Chang
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15686](https://arxiv.org/abs/2601.15686)
- **Reason:** 提出RLSEdit，通过软约束在线二次优化解决LLM终身编辑的plasticity-stability dilemma。支持10K次编辑，保持早期编辑效果与通用能力，理论推导偏差边界与多编辑权衡，实验优于强基线。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models
- **Authors:** Manish Bhatt
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15652](https://arxiv.org/abs/2601.15652)
- **Reason:** 结合预测编码与信息瓶颈提出幻觉检测框架，用75倍更少数据实现0.8669 AUROC，推理速度快1000倍且可解释，解决现有方法依赖大模型法官或数据量大的问题，对LLM安全部署意义重大。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Agentic Confidence Calibration
- **Authors:** Jiaxin Zhang, Caiming Xiong, Chien-Sheng Wu (Salesforce Research, etc.)
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15778](https://arxiv.org/abs/2601.15778)
- **Reason:** 首次系统提出agentic置信度校准问题，针对agent过度自信缺陷提出HTC框架，通过过程级特征提升校准效果，在多个基准上优于基线，对提升agent可靠性和安全性意义重大。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] Improving Methodologies for LLM Evaluations Across Global Languages
- **Authors:** Akriti Vij, Benjamin Chua, Darshini Ramiah, En Qi Ng, Mahran Morsidi, Naga Nikshith Gangarapu, Sharmini Johnson, Vanessa Wilfred, Vikneswaran Kumaran, Wan Sie Lee, Wenzhuo Yang, Yongsen Zheng, Bill Black, Boming Xia, Frank Sun, Hao Zhang, Qinghua Lu, Suyu Ma, Yue Liu, Chi-kiu Lo, Fatemeh Azadi, Isar Nejadgholi, Sowmya Vajjala, Agnes Delaborde, Nicolas Rolin, Tom Seimandi, Akiko Murakami, Haruto Ishi, Satoshi Sekine, Takayuki Semitsu, Tasuku Sasaki, Angela Kinuthia, Jean Wangari, Michael Michie, Stephanie Kasaon, Hankyul Baek, Jaewon Noh, Kihyuk Nam, Sang Seo, Sungpil Shin, Taewhi Lee, Yongsu Kim, Daisy Newbold-Harrop, Jessica Wang, Mahmoud Ghanem, Vy Hong (Singapore AISI, etc.)
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15706](https://arxiv.org/abs/2601.15706)
- **Reason:** 针对LLM全球部署中的多语言安全评估问题，通过10种语言的6000+提示测试揭示安全行为跨语言差异，提出改进多语言安全评估的方法论，对大模型安全对齐的实际落地具有重要价值。
Score: 7
Field: 大模型安全与对齐

## 深度学习可解释性

### [Score: 8.0/10] Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models
- **Authors:** Zhen Zhang, Runhao Zeng, Sicheng Zhao, Xiping Hu
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15906](https://arxiv.org/abs/2601.15906)
- **Reason:** 揭示了多模态基础模型中情感建模的关键模块（gate_proj），对深度学习可解释性和多模态模型的情感理解机制有重要 insights
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models
- **Authors:** Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15441](https://arxiv.org/abs/2601.15441)
- **Reason:** 提出CASL框架，通过稀疏自编码器（SAE）与线性映射将扩散模型的潜在空间与语义概念对齐，并用CASL-Steer因果探测验证概念对生成内容的影响，解决扩散模型可解释性难题，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models
- **Authors:** Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen (Ben-Gurion University, etc.)
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15812](https://arxiv.org/abs/2601.15812)
- **Reason:** 提出ErrorMap方法系统绘制LLM失败图谱，构建ErrorAtlas错误分类体系，揭示LLM隐藏失败模式（如输出遗漏、问题误解），为深度学习可解释性提供深入故障分析工具，助力理解模型局限性。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification
- **Authors:** Yimin Zhu, Lincoln Linlin Xu, Zhengsen Xu, Zack Dewis, Mabel Heffring, Saeid Taleghanidoozdoozan, Motasem Alkayid, Quinn Ledingham, Megan Greenwood
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15757](https://arxiv.org/abs/2601.15757)
- **Reason:** 提出频谱感知的白盒框架，显式建模特征交互，提升 hyperspectral图像分类的可解释性，对深度学习可解释性的研究有推进
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems
- **Authors:** Annemarie Jutte, Uraz Odyurt
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.16074](https://arxiv.org/abs/2601.16074)
- **Reason:** 应用SHAP值分析时间序列分解组件对工业CPS模型预测的影响，发现上下文信息不足问题，通过扩大数据窗口提升性能，展示可解释AI在工业场景中改进ML可靠性的实际价值。
Score: 7
Field: 深度学习可解释性

## 大模型新技术

### [Score: 8.0/10] HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models
- **Authors:** Xin Xie, Jiaxian Guo, Dong Gong
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.15968](https://arxiv.org/abs/2601.15968)
- **Reason:** 提出超网络框架高效实现扩散模型的测试时对齐，平衡了多样性与对齐效果，对大模型新技术中的扩散模型优化有重要贡献
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders
- **Authors:** Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.16208](https://arxiv.org/abs/2601.16208)
- **Reason:** 提出用Representation Autoencoders（RAE）替代VAE用于文本到图像扩散Transformer的scaling，解决VAE过拟合问题，实现更快收敛与更好生成质量，属于大模型新技术中的diffusion LLM方向，同时推动原生多模态大模型的图像生成能力提升。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Learning to Discover at Test Time
- **Authors:** Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun
- **Published:** 2026-01-23
- **Link:** [https://arxiv.org/abs/2601.16175](https://arxiv.org/abs/2601.16175)
- **Reason:** 提出TTT-Discover，通过测试时强化学习让LLM针对具体问题持续学习。在数学、GPU内核、算法设计等领域取得SOTA，用开源模型实现且代码可复现，推动大模型新技术在科学发现中的应用。
Score: 8
Field: 大模型新技术

