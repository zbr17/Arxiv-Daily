# ArXiv 每日推荐 - 2026-01-28

> 更新于北京时间：2026-01-28 12:51:30
> 已自动阅读了 377 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：190336

## 高效大模型训练与推理

### [Score: 9.0/10] VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding
- **Authors:** Zhihao He, Tieyuan Chen, Kangyu Wang, Ziran Qin, Yang Shao, Chaofan Gan, Shijie Li, Zuxuan Wu, Weiyao Lin
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17868](https://arxiv.org/abs/2601.17868)
- **Reason:** 提出基于双向扩散的视频LLM VidLaDA，解决自回归视频LLM的因果掩码偏见问题，并引入MARS-Cache实现12倍推理加速，在视频理解任务上超越现有基线，属于高效大模型训练与推理和原生多模态大模型的重要成果。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts
- **Authors:** Venmugil Elango, Nidhi Bhatia, Roger Waleffe, Rasoul Shafipour, Tomer Asida, Abhinav Khattar, Nave Assaf, Maximilian Golub, Joey Guman, Tiyasa Mitra, Ritchie Zhao, Ritika Borkar, Ran Zilberstein, Mostofa Patwary, Mohammad Shoeybi, Bita Rouhani
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18089](https://arxiv.org/abs/2601.18089)
- **Reason:** 优化MoE架构以提升每FLOP和参数的准确性，属于高效大模型训练与推理中的架构优化。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning
- **Authors:** Wen Luo, Peng Chen, Xiaotao Huang, LiQun Huang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17818](https://arxiv.org/abs/2601.17818)
- **Reason:** 提出视觉和文本语义协同剪枝框架ViTCoP，结合视觉编码器的冗余过滤和LLM的分层协同剪枝，有效保留关键视觉token，显著降低LVLM的推理延迟和内存消耗，属于高效大模型训练与推理方向的重要创新。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models
- **Authors:** Longteng Zhang, Sen Wu, Shuai Hou, Zhengyu Qing, Zhuo Zheng, Danning Ke, Qihong Lin, Qiang Wang, Shaohuai Shi, Xiaowen Chu
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16991](https://arxiv.org/abs/2601.16991)
- **Reason:** 针对大模型微调的高存储与计算成本问题，提出SALR框架，结合低秩适应与稀疏剪枝，通过理论证明静态剪枝冻结基权重可最小化误差，并通过截断SVD低秩适配器恢复残差信息，实验验证在保持性能的同时模型大小减半、推理速度提升1.7倍，对高效大模型训练有重要价值。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts
- **Authors:** Xuan-Phi Nguyen, Shrey Pandit, Austin Xu, Caiming Xiong, Shafiq Joty
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17111](https://arxiv.org/abs/2601.17111)
- **Reason:** 解决了MoE模型专家并行中的负载不平衡问题，提出LLEP动态重路由超额token与专家参数至低负载设备，理论分析与实验验证其在不同模型规模上提升速度5倍、减少峰值内存4倍，支持大模型（如gpt-oss-120b）的高效推理。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory
- **Authors:** Davide Ettori
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17357](https://arxiv.org/abs/2601.17357)
- **Reason:** 基于谱几何与随机矩阵理论，提出EigenTrack幻觉检测与RMT-KD压缩方法，解决大模型的可靠性与效率问题，其中RMT-KD通过谱分析指导知识蒸馏，有效提升压缩效率，属于高效大模型训练与推理的关键方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training
- **Authors:** Ruofan Wu, Jae-Won Chung, Mosharaf Chowdhury
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17654](https://arxiv.org/abs/2601.17654)
- **Reason:** 针对大模型训练的能量消耗问题，提出Kareus系统联合优化动态与静态能量，实验减少28.3%能量或27.5%时间，为高效训练提供系统级解决方案，属于高效大模型训练与推理的核心问题。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction
- **Authors:** Jang-Hyun Kim, Dongyoon Han, Sangdoo Yun
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17668](https://arxiv.org/abs/2601.17668)
- **Reason:** 针对LLM推理的KV缓存问题，提出轻量级门控eviction方法，保持近无损性能的同时压缩70%缓存，显著提升推理效率，属于高效大模型推理的关键优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models
- **Authors:** Longwei Ding, Anhao Zhao, Fanghua Ye, Ziyang Chen, Xiaoyu Shen
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18091](https://arxiv.org/abs/2601.18091)
- **Reason:** 研究推理增强LLM的剪枝策略，提升模型压缩效率，属于高效大模型训练与推理中的模型压缩。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning
- **Authors:** Zhaopeng Qiu, Shuang Yu, Jingqi Zhang, Shuai Zhang, Xue Huang, Jingyi Yang, Junjie Lai
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18150](https://arxiv.org/abs/2601.18150)
- **Reason:** 提出FP8低精度栈用于LLM强化学习，提升训练效率，属于高效大模型训练与推理中的高效训练。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs
- **Authors:** Fei Meng
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18255](https://arxiv.org/abs/2601.18255)
- **Reason:** 研究LLM持续学习中的结构安全与可塑性，属于高效大模型训练与推理中的持续学习。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] FGGM: Fisher-Guided Gradient Masking for Continual Learning
- **Authors:** Chao-Hong Tan, Qian Chen, Wen Wang, Yukun Ma, Chong Zhang, Chong Deng, Qinglin Zhang, Xiangang Li, Jieping Ye
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18261](https://arxiv.org/abs/2601.18261)
- **Reason:** 提出Fisher引导的梯度 masking用于持续学习，属于高效大模型训练与推理中的持续学习优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models
- **Authors:** Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, Aditya Grover
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18734](https://arxiv.org/abs/2601.18734)
- **Reason:** 提出自蒸馏框架OPSD，通过单模型同时作为教师与学生，提升LLM推理性能与token效率，属于高效大模型训练与推理中的蒸馏方法研究，推动LLM高效训练。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems
- **Authors:** Varun Chillara, Dylan Kline, Christopher Alvares, Evan Wooten, Huan Yang, Shlok Khetan, Cade Bauer, Tré Guillory, Tanishka Shah, Yashodhara Dhariwal, Volodymyr Pavlov, George Popstefanov
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16286](https://arxiv.org/abs/2601.16286)
- **Reason:** 提出SemanticALLI框架，通过缓存推理中间表示提升agentic系统效率，减少LLM调用与token消耗，属于高效大模型训练与推理中的推理缓存研究，优化agentic系统运行效率。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training
- **Authors:** Mengmeng Wang, Dengyang Jiang, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen, Xiangjie Kong, Yong Liu, Guang Dai, Jingdong Wang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17830](https://arxiv.org/abs/2601.17830)
- **Reason:** 提出轻量级内在引导框架VAE-REPA，利用预训练VAE的特征对齐扩散Transformer的中间潜在特征，加速扩散模型训练且仅增加4% GFLOPs，属于高效大模型训练与推理方向的有效方法。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices
- **Authors:** Byeongju Kim, Jungwan Lee, Donghyeon Han, Hoi-Jun Yoo, Sangyeob Kim
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17063](https://arxiv.org/abs/2601.17063)
- **Reason:** 针对MoE模型边缘推理的SSD I/O瓶颈问题，提出FlashMoE系统，通过轻量级ML缓存策略结合最近与频率信号最大化专家复用，在真实硬件上提升缓存命中率51%、推理速度2.6倍，为MoE模型的边缘高效部署提供了解决方案。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization
- **Authors:** A. El Ichi, K. Jbilou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17112](https://arxiv.org/abs/2601.17112)
- **Reason:** 提出基于余弦Lanczos双对角化的大模型权重低秩张量近似框架，利用cproduct代数结构捕捉多维相关性，比传统SVD更高效地压缩权重，降低大模型内存占用与计算成本，助力高效训练与推理。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning
- **Authors:** Wei Lin, Yining Jiang, Qingyu Song, Qiao Xiang, Hong Xu
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17261](https://arxiv.org/abs/2601.17261)
- **Reason:** 针对ZO优化的各向同性扰动缺陷，提出AGZO利用激活结构引导低秩子空间扰动，理论证明其梯度相似性更高，实验验证在Qwen3、Pangu模型上优于现有ZO方法，缩小与一阶微调的性能差距，提升大模型微调效率。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] LLM-42: Enabling Determinism in LLM Inference with Verified Speculation
- **Authors:** Raja Gond, Aditya K Kamath, Arkaprava Basu, Ramachandran Ramjee, Ashish Panwar
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17768](https://arxiv.org/abs/2601.17768)
- **Reason:** 针对LLM推理的非确定性问题，提出LLM-42通过verify-rollback loop保证确定性，不修改内核且保持吞吐量，属于高效大模型推理的系统优化。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging
- **Authors:** Jiapeng Wang, Changxin Tian, Kunlong Chen, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, Jun Zhou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17858](https://arxiv.org/abs/2601.17858)
- **Reason:** 针对数据混合比优化问题，提出MergeMix通过模型合并权重代替手动调参，减少搜索成本且性能接近手动调参，属于高效大模型训练的数据优化。
Score: 7
Field: 高效大模型训练与推理

## 深度学习理论

### [Score: 9.0/10] Dissipative Learning: A Framework for Viable Adaptive Systems
- **Authors:** Laurent Caraffa
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17933](https://arxiv.org/abs/2601.17933)
- **Reason:** 提出BEDS框架将学习视为耗散过程，证明Fisher-Rao正则化是热力学最优，统一现有正则化方法，解释过拟合与遗忘，为深度学习提供统一理论视角，理论贡献显著。
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs
- **Authors:** Daisuke Okanohara (Preferred Networks, Inc)
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17607](https://arxiv.org/abs/2601.17607)
- **Reason:** 从热力学视角建模学习过程，提出epistemic free-energy框架与ESL界限，解释学习的不可逆性与熵产生，为深度学习提供跨学科理论基础，具有严格数学推导与理论价值。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms
- **Authors:** Nikos Georgoudios, Konstantinos Spiliopoulos, Justin Sirignano
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17954](https://arxiv.org/abs/2601.17954)
- **Reason:** 研究Neural Actor Critic算法的缩放效应与不确定性量化，涉及网络宽度、训练步骤收敛性及超参数选择，属于深度学习理论中的优化算法与网络架构分析。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization
- **Authors:** Ziwei Zheng, Huizhi Liang, Vaclav Snasel, Vito Latora, Panos Pardalos, Giuseppe Nicosia, Varun Ojha
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17987](https://arxiv.org/abs/2601.17987)
- **Reason:** 系统分析最小深度学习架构的收敛性、剪枝与量化，属于深度学习理论中的网络架构研究。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Resonant Sparse Geometry Networks
- **Authors:** Hasi Hays
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18064](https://arxiv.org/abs/2601.18064)
- **Reason:** 提出脑启发的稀疏几何网络架构Resonant Sparse Geometry Networks，属于深度学习理论中的网络架构创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Neural Network Approximation: A View from Polytope Decomposition
- **Authors:** ZeYu Li, ShiJun Zhang, TieYong Zeng, FengLei Fan
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18264](https://arxiv.org/abs/2601.18264)
- **Reason:** 从多面体分解视角研究神经网络逼近，涉及分段线性模型（ReLU），属于深度学习理论中的网络表示能力分析。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Superlinear Multi-Step Attention
- **Authors:** Yufeng Huang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18401](https://arxiv.org/abs/2601.18401)
- **Reason:** 提出超线性多步注意力架构，属于深度学习理论中的网络架构（注意力机制）创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Gradient Regularized Natural Gradients
- **Authors:** Satya Prakash Dash, Hossein Abdi, Wei Pan, Samuel Kaski, Mingfei Sun
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18420](https://arxiv.org/abs/2601.18420)
- **Reason:** 提出梯度正则化的自然梯度优化器，属于深度学习理论中的优化算法创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning
- **Authors:** Yingxiao Huo, Satya Prakash Dash, Radu Stoican, Samuel Kaski, Mingfei Sun
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18626](https://arxiv.org/abs/2601.18626)
- **Reason:** 针对深度强化学习中自然梯度计算的Fisher矩阵逆问题，提出秩1近似方法，理论证明收敛性并在多环境验证优于基线，属于深度学习理论中的优化器研究，对自然策略梯度高效计算有重要价值。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks
- **Authors:** Spyros Rigas, Thanasis Papaioannou, Panagiotis Trakadas, Georgios Alexandridis
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18672](https://arxiv.org/abs/2601.18672)
- **Reason:** 针对Kolmogorov-Arnold Networks（KAN）网格自适应问题，提出基于曲率的动态框架，提升KAN在函数拟合、回归和PDE任务的性能，属于深度学习理论中的网络架构研究，推动KAN实用化。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning
- **Authors:** Olaf Yunus Laitinen Imanov
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18699](https://arxiv.org/abs/2601.18699)
- **Reason:** 系统分析LLM持续微调中的灾难性遗忘机制，识别注意力权重干扰、表示漂移和损失景观平坦化三个驱动因素，属于深度学习理论中的持续学习研究，助力理解LLM遗忘问题。
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] Power-based Partial Attention: Bridging Linear-Complexity and Full Attention
- **Authors:** Yufeng Huang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17334](https://arxiv.org/abs/2601.17334)
- **Reason:** 提出PPA注意力机制，将transformer注意力复杂度从线性到二次连续化（O(L^(1+p))），探索性能随p的变化规律，发现存在p<1时性能接近full attention，为注意力机制的复杂度-性能权衡提供新理论视角，属于深度学习理论中的架构研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Automatic Stability and Recovery for Neural Network Training
- **Authors:** Barak Or
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17483](https://arxiv.org/abs/2601.17483)
- **Reason:** 针对神经网络训练中不稳定更新问题，提出监督式运行时框架，通过创新信号检测并自动恢复，无需修改优化器且提供理论安全保证，属于深度学习理论中的训练稳定性研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning
- **Authors:** Fatema Siddika, Md Anwar Hossen, Tanwi Mallick, Ali Jannesari
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17616](https://arxiv.org/abs/2601.17616)
- **Reason:** 针对持续学习的plasticity-stability困境，提出SETA框架分解模型为任务专家与共享专家，通过弹性权重锚定保护共享知识，实验验证优于现有方法，属于深度学习理论中的持续学习与架构研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] $\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts
- **Authors:** Shota Takashiro, Takeshi Kojima, Shohei Taniguchi, Yusuke Iwasawa, Yutaka Matsuo
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17680](https://arxiv.org/abs/2601.17680)
- **Reason:** 将离散MoE扩展为连续参数采样的∞-MoE，解决专家增多的训练困难，实验显示小模型性能接近大模型，属于深度学习理论中的MoE架构泛化研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Spelling Bee Embeddings for Language Modeling
- **Authors:** Markus N. Rabe, Judith Clymo, Zheren Dong
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18030](https://arxiv.org/abs/2601.18030)
- **Reason:** 改进语言模型的嵌入层，通过拼写信息增强词嵌入，属于深度学习理论中的网络架构（嵌入层）设计。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Robust Learning of a Group DRO Neuron
- **Authors:** Guyang Cao, Shuyao Li, Sushrut Karmalkar, Jelena Diakonikolas
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18115](https://arxiv.org/abs/2601.18115)
- **Reason:** 研究Group DRO神经元的鲁棒学习，属于深度学习理论中的鲁棒优化算法。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients
- **Authors:** Rui Wu, Yongjun Li
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18189](https://arxiv.org/abs/2601.18189)
- **Reason:** 提出Smoothed Proximal Gradients实现因果发现的有限时间精确骨架恢复，属于深度学习理论中的优化算法。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Frequency-Based Hyperparameter Selection in Games
- **Authors:** Aniket Sanyal, Baraah A. M. Sidahmed, Rebekka Burkholz, Tatjana Chavdarova
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18409](https://arxiv.org/abs/2601.18409)
- **Reason:** 提出基于频率的游戏超参数选择方法，属于深度学习理论中的优化算法（超参数优化）。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Information Hidden in Gradients of Regression with Target Noise
- **Authors:** Arash Jamshidi, Katsiaryna Haitsiukevich, Kai Puolam\"aki
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18546](https://arxiv.org/abs/2601.18546)
- **Reason:** 分析回归梯度中的信息，涉及优化算法中的梯度利用，属于深度学习理论中的优化算法分析。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning
- **Authors:** Liheng Yu, Zhe Zhao, Yuxuan Wang, Pengkun Wang, Binwu Wang, Yang Wang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18650](https://arxiv.org/abs/2601.18650)
- **Reason:** 针对长尾数据遗忘问题，提出遗忘感知损失重加权方法，解决异质和偏态遗忘偏差，属于深度学习理论中的模型更新与遗忘研究，提升长尾遗忘性能。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data
- **Authors:** Willem Diepeveen, Oscar Leong
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18728](https://arxiv.org/abs/2601.18728)
- **Reason:** 提出Riemannian AmbientFlow框架，结合流形学习与生成模型处理corrupted数据，理论证明恢复数据分布能力，属于深度学习理论中的流形学习与生成模型研究，提升corrupted数据处理性能。
Score: 7
Field: 深度学习理论

## 深度学习可解释性

### [Score: 9.0/10] TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors
- **Authors:** Ido Andrew Atad, Itamar Zimerman, Shahar Katz, Lior Wolf
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17958](https://arxiv.org/abs/2601.17958)
- **Reason:** 提出TensorLens通过高阶注意力张量实现Transformer端到端分析，用于模型可解释性，符合深度学习可解释性方向。
Score: 9
Field: 深度学习可解释性

### [Score: 8.0/10] Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective
- **Authors:** Tianyuan Liu, Libin Hou, Linyuan Wang, Bin Yan
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17042](https://arxiv.org/abs/2601.17042)
- **Reason:** 提出了解耦成员-子空间建模的可解释稀疏线性注意力机制，基于MCR2目标实现白盒Transformer，结合可解释性与效率，对深度学习可解释性研究有理论贡献。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level
- **Authors:** Jinlong Hu, Jiacheng Liu
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18447](https://arxiv.org/abs/2601.18447)
- **Reason:** 提出GCFX生成图模型的模型级反事实解释，属于深度学习可解释性中的反事实解释。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Counterfactual Explanations on Robust Perceptual Geodesics
- **Authors:** Eslam Zaher, Maciej Trzaskowski, Quan Nguyen, Fred Roosta
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18678](https://arxiv.org/abs/2601.18678)
- **Reason:** 提出基于感知测地线的反事实解释方法，解决现有方法的off-manifold与语义漂移问题，在视觉数据集验证有效性，属于深度学习可解释性中的反事实解释研究，提升解释鲁棒性与语义有效性。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] What Do Learned Models Measure?
- **Authors:** Indr\.e \v{Z}liobait\.e
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18278](https://arxiv.org/abs/2601.18278)
- **Reason:** 研究学习模型的测量函数稳定性，涉及模型决策的可解释性，属于深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] Structural Gender Bias in Credit Scoring: Proxy Leakage
- **Authors:** Navya SD, Sreekanth D, SS Uma Sankari
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18342](https://arxiv.org/abs/2601.18342)
- **Reason:** 用SHAP分析信用评分中的性别偏见代理变量，属于深度学习可解释性中的Shapley值应用。
Score: 7
Field: 深度学习可解释性

## 大模型安全与对齐

### [Score: 9.0/10] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment
- **Authors:** Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18292](https://arxiv.org/abs/2601.18292)
- **Reason:** 提出三角色自玩强化学习框架用于LLM安全对齐，属于大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs
- **Authors:** Xinyue Zeng, Junhong Lin, Yujun Yan, Feng Guo, Liang Shi, Jun Wu, Dawei Zhou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18753](https://arxiv.org/abs/2601.18753)
- **Reason:** 提出HalluGuard框架，通过NTK得分联合检测数据驱动与推理驱动幻觉，在多基准与模型验证SOTA性能，属于大模型安全与对齐中的幻觉检测研究，提升LLM可靠性。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] Physical Prompt Injection Attacks on Large Vision-Language Models
- **Authors:** Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17383](https://arxiv.org/abs/2601.17383)
- **Reason:** 针对大视觉语言模型（LVLM）在真实物理环境中的prompt注入攻击问题，提出首个黑盒、查询无关的物理prompt注入攻击方法PPIA，通过视觉prompt设计和环境感知放置实现高攻击成功率（最高98%），对大模型安全与对齐中的物理场景攻击研究具有重要价值。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning
- **Authors:** Qi Li, Xinchao Wang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17566](https://arxiv.org/abs/2601.17566)
- **Reason:** 针对工具增强智能体的推理过程，提出Sponge Tool Attack（STA），通过重写prompt将高效推理轨迹转为冗长路径，实现 stealthy 的效率攻击，揭示了工具调用的新攻击面，对大模型安全与对齐研究有重要意义。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment
- **Authors:** Marco Pollanen
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17260](https://arxiv.org/abs/2601.17260)
- **Reason:** 研究DPO对齐中β参数对模型能力的影响，发现能力非单调性、架构特异性响应模式及滞后现象，指出对齐压力与推理能力可能反相关，打破“更高对齐压力更优”的认知，为大模型安全对齐的评估与优化提供关键启发。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment
- **Authors:** Tiejin Chen, Xiaoou Liu, Vishnu Nandam, Kuan-Ru Liou, Hua Wei
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17329](https://arxiv.org/abs/2601.17329)
- **Reason:** 针对RLHF中偏好标签噪声及答案可靠性未被充分建模的问题，提出CFA框架结合 conformal prediction量化答案级可靠性，为DPO/PPO训练提供 principled权重，实验验证提升了对齐鲁棒性与数据效率，是大模型对齐领域的重要改进。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming
- **Authors:** Alexandra Chouldechova, A. Feder Cooper, Solon Barocas, Abhinav Palia, Dan Vann, Hanna Wallach
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18076](https://arxiv.org/abs/2601.18076)
- **Reason:** 反思AI红队中攻击成功率比较的有效性，涉及大模型安全评估，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] AttenMIA: LLM Membership Inference Attack through Attention Signals
- **Authors:** Pedram Zaree, Md Abdullah Al Mamun, Yue Dong, Ihsen Alouani, Nael Abu-Ghazaleh
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18110](https://arxiv.org/abs/2601.18110)
- **Reason:** 利用注意力信号进行LLM成员推理攻击，涉及大模型隐私安全，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback
- **Authors:** Seyed Amir Hosseini, Maryam Abdolali, Amirhosein Tavakkoli, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18751](https://arxiv.org/abs/2601.18751)
- **Reason:** 针对多专家偏好反馈鲁棒性问题，提出TriTrust-PBRL框架，自动处理可靠、噪声与对抗性反馈，在多领域验证优于基线，属于大模型安全与对齐中的偏好强化学习研究，提升对齐鲁棒性。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values
- **Authors:** Henry Bell, Lara Neubauer da Costa Schertel, Bochu Ding, Brandon Fain
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18760](https://arxiv.org/abs/2601.18760)
- **Reason:** 提出GCAI框架，结合人类理由与价值生成对齐原则，提升对齐的代表性与道德grounded，属于大模型安全与对齐中的对齐原则研究，构建更符合人类价值的LLM。
Score: 8
Field: 大模型安全与对齐

## 大模型新技术

### [Score: 9.0/10] ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule
- **Authors:** Yilie Huang, Wenpin Tang, Xunyu Zhou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18681](https://arxiv.org/abs/2601.18681)
- **Reason:** 针对扩散模型时间步调度问题，提出基于强化学习的ART方法，提升CIFAR-10等数据集生成质量，属于大模型新技术中的扩散模型研究，优化扩散采样效率与质量。
Score: 9
Field: 大模型新技术

### [Score: 9.0/10] LongCat-Flash-Thinking-2601 Technical Report
- **Authors:** Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Pengcheng Jia, Pengtao Zhang, Peng Zhao, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiangcheng Liu, Xiandi Ma, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yaoming Wang, Yaoming Zhu, Yaqi Huo, Yanyu Chen, Yaorui Shi, Yerui Sun, Yi Zhang, Yihao Chen, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yuchuan Dai, Yuchen Xie, Yuchen Yu, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Yue Xu, Yueqing Sun, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16725](https://arxiv.org/abs/2601.16725)
- **Reason:** 介绍560B参数MoE推理模型LongCat-Flash-Thinking-2601，通过统一训练框架与RL优化提升agentic推理性能，属于大模型新技术中的MoE与RL训练研究，推动大模型agentic能力提升。
Score: 9
Field: 大模型新技术

### [Score: 8.0/10] Streaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding
- **Authors:** Zhongyu Xiao, Zhiwei Hao, Jianyuan Guo, Yong Luo, Jia Liu, Jie Xu, Han Hu
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17917](https://arxiv.org/abs/2601.17917)
- **Reason:** 针对Diffusion LLM推理的冗余问题，提出Streaming-dLLM通过suffix pruning与动态解码加速，训练-free且速度提升68.2X，为Diffusion LLM高效推理提供创新方案，属于大模型新技术的应用优化。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models
- **Authors:** Kai Hu, Haoqi Hu, Matt Fredrikson
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18513](https://arxiv.org/abs/2601.18513)
- **Reason:** 提出LipNeXt将Lipschitz鲁棒性扩展到十亿参数模型，属于大模型新技术中的鲁棒性架构创新。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation
- **Authors:** Tims Pecerskis, Aivars Smirnovs
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16863](https://arxiv.org/abs/2601.16863)
- **Reason:** 提出NSED协议构建Mixture-of-Models架构，通过动态专家经纪人和审议机制统一异质agent，在AIME 2025等基准验证优于大模型，属于大模型新技术中的混合模型研究，实现异质agent高效融合。
Score: 8
Field: 大模型新技术

### [Score: 7.0/10] ONRW: Optimizing inversion noise for high-quality and robust watermark
- **Authors:** Xuan Ding, Xiu Yan, Chuanlong Xie, Yao Zhu
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17388](https://arxiv.org/abs/2601.17388)
- **Reason:** 基于扩散模型提出高-quality鲁棒水印框架，通过优化反转噪声并结合自注意力约束和伪掩码策略，提升水印对图像损坏的鲁棒性，属于大模型新技术（diffusion相关）在水印领域的创新应用。
Score: 7
Field: 大模型新技术

## 原生多模态大模型

### [Score: 8.0/10] iFSQ: Improving FSQ for Image Generation with 1 Line of Code
- **Authors:** Bin Lin, Zongjian Li, Yuwei Niu, Kaixiong Gong, Yunyang Ge, Yunlong Lin, Mingzhe Zheng, JianWei Zhang, Miles Yang, Zhao Zhong, Liefeng Bo, Li Yuan
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17124](https://arxiv.org/abs/2601.17124)
- **Reason:** 通过替换激活函数改进FSQ的量化方法，解决图像生成中的激活坍塌问题，实现离散与连续表示的平衡，对原生多模态大模型的图像生成量化有实际贡献。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning
- **Authors:** Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu, Chen-Wei Xie, Zhaoyu Chen, Yun Zheng, Wenqiang Zhang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18543](https://arxiv.org/abs/2601.18543)
- **Reason:** 提出GenAgent框架通过agentic多模态推理解耦视觉理解与生成，利用工具调用提升文本到图像生成性能，对应原生多模态大模型中image generation方向，实验显示较FLUX.1-dev在GenEval++上提升23.6%，具备跨工具泛化和任务自适应推理能力。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting
- **Authors:** Trong Khiem Tran, Manh Cuong Dao, Phi Le Nguyen, Thao Nguyen Truong, Trong Nghia Hoang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18231](https://arxiv.org/abs/2601.18231)
- **Reason:** 重新思考跨模态微调中的特征对齐与目标拟合，属于原生多模态大模型中的跨模态学习。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] A Mechanistic View on Video Generation as World Models: State and Dynamics
- **Authors:** Luozhou Wang, Zhifei Chen, Yihua Du, Dongyu Yan, Wenhang Ge, Guibao Shen, Xinli Xu, Leyi Wu, Man Chen, Tianshuo Xu, Peiran Ren, Xin Tao, Pengfei Wan, Ying-Cong Chen
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17067](https://arxiv.org/abs/2601.17067)
- **Reason:** 从状态构建与动态建模视角分析视频生成模型作为世界模型的机制，涉及多模态视频生成与理解，对原生多模态大模型的世界模型研究有启发。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] SkyReels-V3 Technique Report
- **Authors:** Debang Li, Zhengcong Fei, Tuanhui Li, Yikun Dou, Zheng Chen, Jiangping Yang, Mingyuan Fan, Jingtao Xu, Jiahua Wang, Baoxuan Gu, Mingshan Chang, Yuqiang Xie, Binjie Mao, Youqiang Zhang, Nuo Pang, Hao Zhang, Yuzhe Jin, Zhiheng Xu, Dixuan Lin, Guibin Chen, Yahui Zhou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17323](https://arxiv.org/abs/2601.17323)
- **Reason:** 提出支持多模态输入的条件视频生成模型（参考图转视频、视频扩展、音频引导生成），属于原生多模态大模型的实践系统，覆盖image generation与multi-modal large model。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Federated learning for unpaired multimodal data through a homogeneous transformer model
- **Authors:** Anders Eklund
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17986](https://arxiv.org/abs/2601.17986)
- **Reason:** 提出联邦学习框架处理非配对多模态数据，训练同构多模态Transformer，属于原生多模态大模型中的联邦多模态学习。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Closing the Modality Gap Aligns Group-Wise Semantics
- **Authors:** Eleonora Grassucci, Giordano Cicchetti, Emanuele Frasca, Aurelio Uncini, Danilo Comminiello
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18525](https://arxiv.org/abs/2601.18525)
- **Reason:** 研究多模态中的模态差距与组级语义对齐，属于原生多模态大模型中的跨模态语义对齐。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] Federated learning for unpaired multimodal data through a homogeneous transformer model
- **Authors:** Anders Eklund
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17986](https://arxiv.org/abs/2601.17986)
- **Reason:** 提出联邦学习框架处理非配对多模态数据，训练同构多模态Transformer，属于原生多模态大模型中的联邦多模态学习。
Score: 7
Field: 原生多模态大模型

## 多模态智能体

### [Score: 8.0/10] PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation
- **Authors:** Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.17885](https://arxiv.org/abs/2601.17885)
- **Reason:** 提出多视图视觉-语言-动作（VLA）框架PEAfowl，通过深度分布预测、可微分3D提升和文本感知readout解决多视图特征融合和指令接地问题，显著提升双足操纵的成功率，属于多模态智能体方向的关键改进。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis
- **Authors:** Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.18305](https://arxiv.org/abs/2601.18305)
- **Reason:** 针对GUI代理滑动交互执行瓶颈，提出SwipeGen pipeline构建人类似滑动合成基准，设计GUISwiper提升GUI代理滑动执行能力，直接对应多模态智能体中GUI Agent研究方向，实验显示较VLM基线提升214%滑动执行准确率。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems
- **Authors:** Donghao Huang, Gauri Malwe, Zhaoxia Wang
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16280](https://arxiv.org/abs/2601.16280)
- **Reason:** 提出多agent系统工具调用可靠性诊断框架，构建12类错误 taxonomy并验证多模型性能，属于多模态智能体中的工具使用研究，提升agentic系统工具调用可靠性。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents
- **Authors:** Fan Nie, Junlin Wang, Harper Hua, Federico Bianchi, Yongchan Kwon, Zhenting Qi, Owen Queen, Shang Zhu, James Zou
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16344](https://arxiv.org/abs/2601.16344)
- **Reason:** 提出DSGym框架，标准化数据科学agent评估与训练，解决现有基准碎片化问题，属于多模态智能体中的数据科学agent研究，推动agent评估体系完善。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents
- **Authors:** Amin Rakhsha, Thomas Hehn, Pietro Mazzaglia, Fabio Valerio Massoli, Arash Behboodi, Tribhuvanesh Orekondy
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16649](https://arxiv.org/abs/2601.16649)
- **Reason:** 提出LUMINA框架，提升LLM在多回合长horizon agentic问题的性能，通过 oracle 干预分析能力贡献，属于多模态智能体中的长horizon交互研究，增强agent长程推理能力。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems
- **Authors:** Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16964](https://arxiv.org/abs/2601.16964)
- **Reason:** 构建AgentDrive数据集，包含30万LLM生成的自动驾驶场景与10万MCQ问题，评估多LLM推理性能，属于多模态智能体中的自动驾驶agent研究，推动agentic AI在 autonomous 系统的应用。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts
- **Authors:** Riyang Bao, Cheng Yang, Dazhou Yu, Zhexiang Tang, Gengchen Mai, Liang Zhao
- **Published:** 2026-01-27
- **Link:** [https://arxiv.org/abs/2601.16965](https://arxiv.org/abs/2601.16965)
- **Reason:** 提出Spatial-Agent，基于空间信息科学理论实现地理空间推理，将问题转化为GeoFlow Graphs，属于多模态智能体中的地理空间推理研究，提升agent地理空间推理准确性。
Score: 8
Field: 多模态智能体

