<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-29</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>大模型新技术</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >多模态智能体</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习理论</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-29</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-29 13:14:10</p>
<p>已自动阅读了 223 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：121311</p>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DeRaDiff: Denoising Time Realignment of Diffusion Models</h3>
<p><strong>Authors:</strong> Ratnavibusena Don Shahain Manujith, Yang Zhang, Teoh Tze Tzun, Kenji Kawaguchi</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对扩散模型对齐的正则化强度问题，提出采样时动态调整的方法，无需额外训练，属于大模型新技术中的扩散模型优化
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20198' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective</h3>
<p><strong>Authors:</strong> Qiyan Zhao, Xiaofeng Zhang, Shuochen Chang, Qianyu Chen, Xiaosong Yuan, Xuhang Chen, Luoqi Liu, Jiajun Zhang, Xu-Yao Zhang, Da-Han Wang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 从信息流角度分析diffusion-based MLLMs的重复问题（上下文token熵未收敛），提出CoTA方法增强上下文注意力并惩罚不确定token，有效缓解重复生成，属于大模型新技术中dMLLMs的关键改进。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20520' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Advancing Open-source World Models</h3>
<p><strong>Authors:</strong> Robbyant Team, Zelin Gao, Qiuyu Wang, Yanhong Zeng, Jiapeng Zhu, Ka Leong Cheng, Yixuan Li, Hanlin Wang, Yinghao Xu, Shuailei Ma, Yihang Chen, Jie Liu, Yansong Cheng, Yao Yao, Jiayi Zhu, Yihao Meng, Kecheng Zheng, Qingyan Bai, Jingye Chen, Zehong Shen, Yue Yu, Xing Zhu, Yujun Shen, Hao Ouyang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 开源world model LingBot-World，支持高保真视频生成、分钟级时间 horizon 和实时交互（<1秒延迟），缩小了开源与闭源world model的差距，属于大模型新技术的重要成果。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20540' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Order-Optimal Sample Complexity of Rectified Flows</h3>
<p><strong>Authors:</strong> Hari Krishna Sahoo, Mudit Gaur, Vaneet Aggarwal</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 证明整流流样本复杂度为O(ε^-2)，改进现有结果，属于大模型新技术中的生成模型理论分析
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20250' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku</h3>
<p><strong>Authors:</strong> Mariia Drozdova</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 研究连续时间扩散模型解决数独等离散约束问题，扩展扩散模型应用，属于大模型新技术
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20363' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reversible Efficient Diffusion for Image Fusion</h3>
<p><strong>Authors:</strong> Xingxin Xu, Bing Cao, DongDong Li, Qinghua Hu, Pengfei Zhu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出可逆高效diffusion模型用于图像融合，解决传统diffusion模型的细节丢失问题，通过多源结构先验和蒸馏驱动语义提取提升重建效果，属于大模型新技术中diffusion模型的创新应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20260' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching</h3>
<p><strong>Authors:</strong> Zhen Liu, Diedong Feng, Hai Jiang, Liaoyuan Zeng, Hao Wang, Chaoyu Feng, Lei Lei, Bing Zeng, Shuaicheng Liu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 用确定性 latent flow matching 解决RGB-to-RAW重建的细节丢失与颜色偏差问题，通过跨尺度上下文引导和双域 latent 自动编码器提升重建 fidelity，属于大模型新技术中flow matching的创新应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20364' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models</h3>
<p><strong>Authors:</strong> Hongyu Zhou, Zisen Shao, Sheng Miao, Pan Wang, Dongfeng Bai, Bingbing Liu, Yiyi Liao</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Fine-Tuning-Free的FreeFix框架，利用预训练图像扩散模型增强3D高斯溅射的外推视图渲染质量，结合了扩散模型与3D生成技术，符合大模型新技术方向中关于diffusion LLM的研究需求。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20857' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Hyperparameter Transfer with Mixture-of-Expert Layers</h3>
<p><strong>Authors:</strong> Tianze Jiang, Blake Bordelon, Cengiz Pehlevan, Boris Hanin</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出基于DMFT的MoE层超参数迁移方法，实现51M到2B参数模型的超参数复用，提升高效大模型训练效率
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20205' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching</h3>
<p><strong>Authors:</strong> Fengrui Zuo, Zhiwei Ke, Yiming Liu, Wenqi Lou, Chao Wang, Xvehai Zhou</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出窗口化token剪枝和缓存，加速扩散语言模型推理，高达99x加速，属于高效大模型训练与推理
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20332' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs</h3>
<p><strong>Authors:</strong> Guoan Wang (Unknown), Feiyu Wang (Unknown), Zongwei Lv (Unknown), Yikun Zong (Unknown), Tong Yang (Unknown)</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对极低比特LLM量化的梯度失配问题，提出Hessian引导的可微分量化感知训练框架，通过温度控制软松弛和Hessian trace驱动的温度退火，显著提升量化模型性能，在Llama-3.2上取得优于基线的结果，对高效大模型训练有重要价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20745' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient Token Pruning for LLaDA-V</h3>
<p><strong>Authors:</strong> Zhewen Wan, Tianchen Song, Chen Lin, Zhiyong Zhao, Xianpeng Lang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对diffusion-based多模态大模型LLaDA-V，提出中间层token剪枝策略，在减少65%计算量的同时保持95%的任务性能，为diffusion类多模态模型的高效推理提供了有效方案。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20168' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient Autoregressive Video Diffusion with Dummy Head</h3>
<p><strong>Authors:</strong> Hang Guo, Zhaoyang Jia, Jiahao Li, Bin Li, Yuanhao Cai, Jiangshan Wang, Yawei Li, Yan Lu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Dummy Head方法识别并过滤冗余上下文token，实现视频diffusion的2倍推理加速，同时保持生成质量，为 autoregressive 视频diffusion的高效部署提供了解决方案。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20499' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression</h3>
<p><strong>Authors:</strong> Wenzhuo Ma, Zhenzhong Chen</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出实时diffusion-based视频压缩框架，通过高效架构设计（模块替换+剪枝）、显式/隐式一致性建模和异步并行解码，实现80.1%的码率节省和实时编解码（720p@30fps），属于高效大模型训练与推理的重要突破。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20564' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference</h3>
<p><strong>Authors:</strong> Huanlin Gao, Ping Chen, Fuyuan Shi, Ruijia Wu, Li YanTao, Qiang Hui, Yuren You, Ting Lu, Chao Tan, Shaoan Zhao, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出训练无关的MeanCache框架，利用平均速度缓解Flow Matching推理的误差积累，在FLUX.1、Qwen-Image等模型上实现4倍以上加速，是高效大模型训练与推理方向的重要推理加速技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.19961' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery</h3>
<p><strong>Authors:</strong> Meng Xin, Sweta Priyadarshi, Jingyu Xin, Bilal Kartal, Aditya Vavre, Asma Kuriparambil Thekkumpate, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Ido Shahaf, Akhiad Bercovich, Kinjal Patel, Suguna Varshini Velury, Chenjie Luo, Zhiyu Cheng, Jenny Chen, Chen-Han Yu, Wei Ping, Oleg Rybakov, Nima Tajbakhsh, Oluwatobi Olabiyi, Dusan Stosic, Di Wu, Song Han, Eric Chung, Sharath Turuvekere Sreenivas, Bryan Catanzaro, Yoshi Suhara, Tijmen Blankevoort, Huizi Mao</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出量化感知蒸馏（QAD）以恢复NVFP4量化LLM的推理精度，适用于多阶段训练后的模型，是高效大模型训练与推理方向中关于量化与蒸馏的关键技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20088' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs</h3>
<p><strong>Authors:</strong> Minjae Lee, Wonjun Kang, Byeongkeun Ahn, Christian Classen, Kevin Galim, Seunghyuk Oh, Minghao Yan, Hyung Il Koo, Kangwook Lee</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出动态集成draft的投机解码，加速LVLM推理，1.74x加速，属于高效大模型训练与推理
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20357' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shallow-{\pi}: Knowledge Distillation for Flow-based VLAs</h3>
<p><strong>Authors:</strong> Boseong Jeon, Yunho Choi, Taehan Kim</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Shallow-π知识蒸馏框架，针对流基视觉-语言-动作（VLA）模型，将Transformer层数从18层压缩至6层，实现2倍以上推理加速且性能下降小于1%，并在工业级设备上验证了实时部署能力，属于高效大模型训练与推理方向的模型压缩研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20262' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> One Step Is Enough: Dispersive MeanFlow Policy Optimization</h3>
<p><strong>Authors:</strong> Guowei Zou, Haitao Wang, Hejun Wu, Yukun Qian, Yuhang Wang, Weibing Li</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出DMPO框架，通过MeanFlow实现单步生成式政策推理，结合分散正则化和RL微调，解决了多步采样的实时性问题，推理速度提升5-20倍（>120Hz），并在真实机器人上验证了适用性，属于高效大模型训练与推理方向的实时生成研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20701' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing</h3>
<p><strong>Authors:</strong> Zhuchenyang Liu, Ziyu Hu, Yao Zhang, Yu Xiao</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出训练无关的结构锚点剪枝方法，针对Visual RAG索引向量实现超90%压缩率，同时保持检索保真度，有效提升Visual RAG的 scalability，属于高效大模型推理的关键改进。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20107' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization</h3>
<p><strong>Authors:</strong> Jialuo He, Huangxun Chen</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出C-SAM框架，将剪枝与Sharpness-Aware Minimization结合，通过 mask 扰动优化模型结构的平坦性，在保持任务精度的同时提升模型紧凑性与鲁棒性，属于高效大模型训练的重要方法。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20301' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Continuous-Flow Data-Rate-Aware CNN Inference on FPGA</h3>
<p><strong>Authors:</strong> Tobias Habermann, Michael Mecik, Zhenyu Wang, César David Vera, Martin Kumm, Mario Garrido</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出数据率感知的连续流CNN架构，优化FPGA上的CNN推理效率，提高硬件利用率，符合高效大模型训练与推理方向中关于推理基础设施优化的需求。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.19940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs</h3>
<p><strong>Authors:</strong> Shih-Hsuan Chiu, Ming-Syan Chen</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对边缘设备的RAG系统，提出CiM-aware的噪声鲁棒与领域自适应框架TONEL，优化检索精度与推理效率，符合高效大模型训练与推理方向中关于边缘LLM优化的需求。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20041' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle</h3>
<p><strong>Authors:</strong> Viet Hung Dinh, Ming Ding, Youyang Qu, Kanchana Thilakarathna</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 基于编码特异性原则研究Transformer记忆机制，提取context-defining keywords，有助于理解LLM记忆检索，属于深度学习可解释性
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs</h3>
<p><strong>Authors:</strong> Yuhang Liu, Erdun Gao, Dong Gong, Anton van den Hengel, Javen Qinfeng Shi</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出ConCA框架，从LLM表示中恢复概念log-posterior，比SAE更具理论优势，属于深度学习可解释性
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20420' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning</h3>
<p><strong>Authors:</strong> Chuan Qin, Constantin Venhoff, Sonia Joseph, Fanyi Xiao, Stefan Scherer</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 将稀疏性集成到CLIP训练中，生成兼具可解释性与性能的表示，挑战了可解释性与准确性对立的传统认知，对深度学习可解释性研究有重要推动作用。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20075' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Decomposing multimodal embedding spaces with group-sparse autoencoders</h3>
<p><strong>Authors:</strong> Chiraag Kaushik, Davis Barch, Andrea Fanelli</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出分组稀疏自编码器分解多模态嵌入空间，改进模态对齐与特征语义性，提升多模态模型的可解释性，属于深度学习可解释性方向中关于嵌入分解与稀疏模型的研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20028' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations</h3>
<p><strong>Authors:</strong> Fatima Ezzeddine, Obaida Ammar, Silvia Giordano, Omran Ayoub</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 研究反事实解释的个体和群体公平性，提出强化学习方法生成公平解释，属于深度学习可解释性
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</h3>
<p><strong>Authors:</strong> Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 用多模态对齐（视觉-语言特征匹配）和强化学习（RLHF）实现可解释的deepfake检测，既提升检测准确性又提供推理依据，属于深度学习可解释性的重要进展。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20433' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Optimal Transport Group Counterfactual Explanations</h3>
<p><strong>Authors:</strong> Enrique Valero-Leal (Unknown), Bernd Bischl (Unknown), Pedro Larra~naga (Unknown), Concha Bielza (Unknown), Giuseppe Casalicchio (Unknown)</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对群体反事实解释的泛化性、模型假设依赖和几何扭曲问题，提出基于最优运输映射的方法，解决了现有方法的核心局限，理论分析和实验验证了其有效性，对深度学习可解释性研究有重要贡献。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.20692' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction</h3>
<p><strong>Authors:</strong> Tianyi Alex Qiu, Micah Carroll, Cameron Allen</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出peer prediction方法，弱监督下训练LLM真实性，对抗欺骗，属于大模型安全与对齐
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20299' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Reinforcement Unlearning via Group Relative Policy Optimization</h3>
<p><strong>Authors:</strong> Efstratios Zaradoukas, Bardh Prenkaj, Gjergji Kasneci</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出PURGE方法，解决LLM记忆敏感数据问题，提高遗忘效率和鲁棒性，属于大模型安全与对齐
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20568' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment</h3>
<p><strong>Authors:</strong> Haoyou Deng, Keyu Yan, Chaojie Mao, Xiang Wang, Yu Liu, Changxin Gao, Nong Sang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出密集奖励框架解决flow matching模型的稀疏奖励问题，通过步骤级奖励估计和奖励感知探索空间校准，显著提升人类偏好对齐效果，是大模型安全对齐的重要进展。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20218' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hallucination Begins Where Saliency Drops</h3>
<p><strong>Authors:</strong> Xiaofeng Zhang, Yuanchao Zhu, Chaochen Gu, Xiaosong Yuan, Qiyan Zhao, Jiawei Cao, Feilong Tang, Sinan Fan, Yaomin Shen, Chen Shen, Hao Tang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 通过显著性分析揭示LVLMs幻觉的根源（上下文记忆衰退），提出SGRS和LocoRE方法动态过滤低显著性token并强化上下文连贯性，有效降低幻觉率，属于大模型安全对齐的关键改进。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20279' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data</h3>
<p><strong>Authors:</strong> Minseo Kwak, Jaehyung Kim</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Gap-K%方法，基于top-1预测与目标token的对数概率差检测LLM预训练数据，解决了预训练数据的隐私与版权问题，是大模型安全与对齐方向的重要隐私保护技术。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.19936' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Membership Inference Attacks Against Fine-tuned Diffusion Language Models</h3>
<p><strong>Authors:</strong> Yuetian Chen, Kaiyuan Zhang, Yuntao Du, Edoardo Stoppa, Charles Fleming, Ashish Kundu, Bruno Ribeiro, Ninghui Li</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 首次系统研究扩散语言模型（DLMs）的成员推理攻击，提出SAMA方法实现高效隐私攻击，属于大模型安全与对齐方向中关于隐私攻击与防御的研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20125' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</h3>
<p><strong>Authors:</strong> Rohan Asthana, Vasileios Belagiannis</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出扩散模型记忆检测指标，结合各向同性和各向异性，比现有方法快5x，属于大模型安全与对齐
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20642' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reward Models Inherit Value Biases from Pretraining</h3>
<p><strong>Authors:</strong> Brian Christian (Unknown), Jessica A. F. Thompson (Unknown), Elle Michelle Yang (Unknown), Vincent Adam (Unknown), Hannah Rose Kirk (Unknown), Christopher Summerfield (Unknown), Tsvetomira Dumbalska (Unknown)</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 揭示了奖励模型从预训练模型继承价值偏差的现象（如Llama RMs偏好agency、Gemma RMs偏好communion），通过多个开源RM和心理语言学语料验证了结论，对大模型安全与对齐中的奖励模型设计有关键指导意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20838' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection</h3>
<p><strong>Authors:</strong> Yanzhu Liu, Xiao Liu, Yuexuan Wang, Mondal Soumik</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 利用生成器的最终组件（如归一化层、激活函数）“污染”真实图像，训练 detector 区分真实与污染图像，对 unseen 生成器的泛化性显著提升，属于大模型安全对齐的重要方法。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20461' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models</h3>
<p><strong>Authors:</strong> Haonan Zhong, Wei Song, Tingxu Han, Maurice Pagnucco, Jingling Xue, Yang Song</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对文本到视频扩散模型的性别偏见问题，提出训练无关的去偏框架FAIRT2V，通过中性化提示嵌入与动态去噪调度缓解模型偏差，直接对应大模型安全与对齐方向的偏见缓解研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20791' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers</h3>
<p><strong>Authors:</strong> Jinlin Liu, Wei Chen, Xiaojin Zhang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Perturbation-Induced Linearization（PIL）方法，仅用线性模型生成不可学习样本，显著降低计算成本，属于大模型安全与对齐方向中关于数据保护的研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.19967' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution</h3>
<p><strong>Authors:</strong> Le Zhang (Unknown), Yixiong Xiao (Unknown), Xinjiang Lu (Unknown), Jingjia Cao (Unknown), Yusai Zhao (Unknown), Jingbo Zhou (Unknown), Lang An (Unknown), Zikan Feng (Unknown), Wanxiang Sha (Unknown), Yu Shi (Unknown), Congxi Xiao (Unknown), Jian Xiong (Unknown), Yankai Zhang (Unknown), Hua Wu (Unknown), Haifeng Wang (Unknown)</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 构建了支持移动和桌面平台的通用GUI智能体，通过高质量数据构建管道和 decoupled 训练范式（SFT+GRPO）提升性能，在ScreenSpot-V2（96.3%）、AndroidControl（79.1%）等基准上取得SOTA，是GUI智能体的重要实践成果。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20380' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Continual GUI Agents</h3>
<p><strong>Authors:</strong> Ziwei Liu (Unknown), Borui Kang (Unknown), Hangjie Yuan (Unknown), Zixiang Zhao (Unknown), Wei Li (Unknown), Yifan Zhu (Unknown), Tao Feng (Unknown)</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对GUI智能体在动态环境下的持续学习问题，提出GUI-AiF强化微调框架，通过两个新颖奖励解决分布转移下的grounding稳定性问题，建立了首个GUI智能体持续学习框架，实验效果优于现有基线。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20732' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks</h3>
<p><strong>Authors:</strong> Jing Wu, Daphne Barretto, Yiye Chen, Nicholas Gydé, Yanan Jian, Yuhang He, Vibhav Vineet</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 构建了针对计算机使用智能体（CUAs）长时重复任务的OS-Marathon基准，提出基于少量示例的浓缩演示方法以提升任务执行效率，直接对应多模态智能体方向中关于GUI智能体长任务推理的研究需求。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20650' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents</h3>
<p><strong>Authors:</strong> Vishnu Sashank Dorbala, Dinesh Manocha</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出MemCtrl框架，将多模态大语言模型（MLLMs）用于具身代理的主动记忆控制，通过训练记忆头μ在线修剪记忆，解决了具身代理的内存与计算约束问题，显著提升了任务完成能力，属于多模态智能体方向的具身代理记忆管理研究。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20831' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Demonstration-Free Robotic Control via LLM Agents</h3>
<p><strong>Authors:</strong> Brian Y. Tsui, Alan Y. Fang, Tiffany J. Hwu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出FAEA框架，将通用LLM代理直接应用于具身操纵任务，无需任务特定演示或微调，在多个基准测试中取得接近有演示模型的性能，为多模态智能体的无数据依赖控制提供了新路径。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20334' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization</h3>
<p><strong>Authors:</strong> Baiqing Wang, Helei Cui, Bo Zhang, Xiaolong Zheng, Bin Guo, Zhiwen Yu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出MeCo框架，通过相似任务记忆化（memoization）优化LLM驱动的多机器人协作，减少冗余计算并提升成功率，构建了首个相似任务协作基准MeCoBench，属于多模态智能体方向的协作效率研究。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20577' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</h3>
<p><strong>Authors:</strong> Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出SpatialGenEval基准系统评估T2I模型的空间智能（位置、布局、遮挡等），并构建SpatialT2I数据集改进模型，为T2I模型的空间推理能力优化提供了重要支撑，属于原生多模态大模型的关键进展。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20354' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding</h3>
<p><strong>Authors:</strong> Kun Yin, Yunfei Wu, Bing Liu, Zhongpeng Cai, Xiaotian Li, Huang Chen, Xin Li, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun, Yunsheng Wu, Qianyu Li, Antai Guo, Yanzhen Liao, Yanqiu Qu, Haodong Lin, Chengxu He, Shuangyin Liu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出基于ViT和LLM的文档解析模型，通过高并行解码（token并行+query并行）实现5-11倍加速，支持文本、表格、公式等多元素提取，属于原生多模态大模型的高效应用。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20430' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits</h3>
<p><strong>Authors:</strong> Zelong Sun, Jiahui Wu, Ying Ba, Dong Jing, Zhiwu Lu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 构建CHEESE数据集（24K肖像集合+573K文本注释），提出SCheese框架通过自适应特征融合和ConsistencyNet保持身份与细节一致性，实现高质量肖像生成，属于原生多模态大模型中image generation的重要进展。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20511' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DeepSeek-OCR 2: Visual Causal Flow</h3>
<p><strong>Authors:</strong> Haoran Wei, Yaofeng Sun, Yukun Li</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Visual Causal Flow框架，让编码器根据图像语义动态重排视觉token（模拟人类因果感知），实现更有效的2D图像理解，显著提升OCR性能，属于原生多模态大模型的创新应用。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20552' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?</h3>
<p><strong>Authors:</strong> Zhuang Yu, Lei Shen, Jing Zhao, Shiliang Sun</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 构建了聚焦STEM讲座视频的多模态理解基准LEMON，包含2277个视频片段与4181个QA对，用于评估MLLMs的时序推理与跨模态整合能力，是原生多模态大模型方向的重要基准工具。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20705' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</h3>
<p><strong>Authors:</strong> Xin Jin, Jinming Liu, Yuntao Wei, Junyan Lin, Zhicheng Wang, Jianguo Huang, Xudong Yang, Yanxiao Liu, Wenjun Zeng</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 统一了经典视觉编码与生成式多模态大模型的视觉token技术，分析两者在语义信息保真与计算成本优化上的共性，涉及原生多模态大模型的tokenizer设计与高效表示学习，为多模态模型的 token 机制提供理论支撑。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20742' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models</h3>
<p><strong>Authors:</strong> Zhenchen Tang (Unknown), Songlin Yang (Unknown), Zichuan Wang (Unknown), Bo Peng (Unknown), Yang Li (Unknown), Beibei Dong (Unknown), Jing Dong (Unknown)</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对统一多模态模型（UMMs）的“认知 gap”问题，提出内生重提示机制，通过SEER框架将模型理解转化为生成推理步骤，提升生成质量和多模态能力，实验验证了其优于现有基线的效果。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models</h3>
<p><strong>Authors:</strong> Yuhao Sun, Chengyi Cai, Jiacheng Zhang, Zesheng Ye, Xingliang Yuan, Feng Liu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 通过视图（去除高IoU冗余 patches）和描述（去除高相似度冗余文本）的双向refinement，减少CLIP的冗余信息，显著提升文本-视觉对齐效果，属于原生多模态大模型的重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20419' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V</h3>
<p><strong>Authors:</strong> Meiqi Wu, Bingze Song, Ruimin Lin, Chen Zhu, Xiaokun Feng, Jiahong Wu, Xiangxiang Chu, Kaiqi Huang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 用潜在时间差异（帧间 latent 变化）作为运动先验，加权损失函数提升T2V模型的动态保真度，显著改善剧烈运动场景的生成效果，属于原生多模态大模型中T2V的重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20504' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval</h3>
<p><strong>Authors:</strong> Shaokun Wang, Weili Guan, Jizhou Han, Jianlong Wu, Yupeng Hu, Liqiang Nie</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出结构化跨模态对齐方法，通过simplex ETF几何先验对齐文本-视频特征，并用跨模态关系保持损失抑制特征漂移，有效缓解持续文本-视频检索的灾难性遗忘，属于原生多模态大模型的重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20597' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds</h3>
<p><strong>Authors:</strong> Faruk Alpay, Bugra Kilictas</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 从几何与统计物理角度分析Transformer的隐藏状态轨迹流形性质，研究多步推理的涌现机制，属于深度学习理论方向中关于网络架构与表示学习的基础研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.19942' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning</h3>
<p><strong>Authors:</strong> Bo Dai, Na Li, Dale Schuurmans</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 从谱表示角度统一分析自监督学习的表示充分性，揭示SSL算法的谱本质，为自监督学习提供理论支撑，属于深度学习理论方向中关于自监督学习与表示学习的基础研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20154' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Computational Complexity of Performative Prediction</h3>
<p><strong>Authors:</strong> Ioannis Anagnostides, Rohan Chauhan, Ioannis Panageas, Tuomas Sandholm, Jingming Yan</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 研究performative prediction的计算复杂度，证明ρ>1时为PPAD-complete，属于深度学习理论中的计算复杂度分析，对理解模型部署动态有重要价值
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20180' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations</h3>
<p><strong>Authors:</strong> Kadircan Aksoy, Peter Jung, Protim Bhattacharjee</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 通过二元假设检验研究神经网络训练动态，发现泛化好的模型符合Neyman-Pearson规则，属于深度学习理论
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20477' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Visual Prompt-Agnostic Evolution</h3>
<p><strong>Authors:</strong> Junze Wang, Lei Fan, Dezheng Zhang, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 分析Visual Prompt Tuning的训练动态问题（梯度振荡、跨层不匹配），提出PAE方法改进prompt演化，从频率域和Lyapunov稳定性理论角度优化训练，属于深度学习理论中网络架构的重要改进。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20232' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework</h3>
<p><strong>Authors:</strong> Shaokun Wang, Yifan Yu, Yuhang He, Weili Guan, Yihong Gong</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出黑白盒结合的prompt learning框架，通过白盒模块提取纠正知识（错误预测与正确认知的对比），指导黑盒模块优化prompt，显著提升下游任务适应能力，属于深度学习理论中prompt learning的重要进展。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20526' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information</h3>
<p><strong>Authors:</strong> Adnan Ahmad, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 针对去中心化联邦学习的异质性问题，提出利用二阶信息的聚合方法以提升本地模型泛化能力，属于深度学习理论方向中关于优化器与训练策略的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.19938' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning</h3>
<p><strong>Authors:</strong> Jiayu Xu, Junbiao Pang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出噪声补偿的Sharpness-Aware Minimization（NCSAM），通过模拟噪声增强模型对标签噪声的鲁棒性，属于深度学习理论方向中关于优化器与损失 landscape 分析的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.19947' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning</h3>
<p><strong>Authors:</strong> Chi-Yao Huang, Khoa Vo, Aayush Atul Verma, Duo Lu, Yezhou Yang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出Domain Expansion框架，通过正交池化构建多任务 latent 空间以防止表示崩溃，属于深度学习理论方向中关于多任务学习与表示学习的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20069' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering</h3>
<p><strong>Authors:</strong> Jim Maar, Denis Paperno, Callum Stuart McDougall, Neel Nanda</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 提出评估LLM隐式规划能力的指标，分析押韵生成与问答任务中的规划行为，属于深度学习理论方向中关于LLM行为机制的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20164' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Local Duality for Sparse Support Vector Machines</h3>
<p><strong>Authors:</strong> Penghe Zhang, Naihua Xiu, Houduo Qi</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 发展稀疏支持向量机（SSVM）的局部对偶理论，分析其与hinge-loss、ramp-loss SVM的关系，属于深度学习理论方向中关于模型优化与理论分析的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20170' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization</h3>
<p><strong>Authors:</strong> James Amarel, Robyn Miller, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Alexei Skurikhin, Earl Lawrence, Gerd J. Kunde</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Reason:</strong> 利用影响函数研究损失 landscape 几何与对称性学习的关系，分析模型泛化能力的来源，属于深度学习理论方向中关于损失 landscape 与泛化机制的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20172' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>