<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-30</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>深度学习理论</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型新技术</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-30</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-30 13:22:32</p>
<p>已自动阅读了 360 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：189833</p>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> When Gradient Optimization Is Not Enough: $\dagger$ Dispersive and Anchoring Geometric Regularizer for Multimodal Learning</h3>
<p><strong>Authors:</strong> Zixuan Xia, Hao Wang, Pengcheng Weng, Yanyu Qian, Yangxin Xu, William Dan, Fei Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出几何正则化框架解决多模态学习的表示坍缩与跨模态不一致，属于深度学习理论中的表示学习与正则化研究。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21670' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</h3>
<p><strong>Authors:</strong> Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出scalable的metric深度预训练框架，首次展示深度估计缩放规律，属于深度学习理论中的表示学习与缩放研究，应用广泛。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22054' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Why Adam Works Better with β₁ = β₂: The Missing Gradient Scale Invariance Principle</h3>
<p><strong>Authors:</strong> Alberto Fernández-Hernández, Cristian Pérez-Corral, Jose I. Mestre, Manuel F. Dolz, Enrique S. Quintana-Ortí</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 证明Adam的β₁=β₂时具有梯度尺度不变性，解释其性能优势，属于深度学习理论中的优化器研究
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21739' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer</h3>
<p><strong>Authors:</strong> Chenrui Xu, Wenjing Yan, Ying-Jun Angela Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Fisher结构的动量正交优化器，结合几何预条件与方差 reduction，属于深度学习理论中的优化器研究
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21750' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Just Noticeable Difference Modeling for Deep Visual Features</h3>
<p><strong>Authors:</strong> Rui Zhao, Wenrui Li, Lin Zhu, Yajing Zheng, Weisi Lin</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出FeatJND预测深度特征的可察觉扰动，属于深度学习理论中的特征质量控制研究，对特征压缩与量化有应用价值。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21933' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Deep Models, Shallow Alignment: Uncovering the Granularity Mismatch in Neural Decoding</h3>
<p><strong>Authors:</strong> Yang Du, Siyuan Dai, Yonghao Song, Paul M. Thompson, Haoteng Tang, Liang Zhan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Shallow Alignment用中间层表示对齐神经信号，解决粒度不匹配问题，属于深度学习理论中的表示对齐研究，提升神经解码性能。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21948' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation</h3>
<p><strong>Authors:</strong> Li Zhang, Pengtao Xie</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 用双级优化对齐YOLO与SAM解决目标mismatch问题，属于深度学习理论中的优化与对齐研究，提升实例分割鲁棒性。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22061' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Theory of Universal Agnostic Learning</h3>
<p><strong>Authors:</strong> Steve Hanneke (Carnegie Mellon University), Shay Moran (Hebrew University of Jerusalem)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 该论文提出了agnostic setting下二元分类的最优通用速率理论，扩展了可实现情况的学习理论，识别了决定概念类最优速率的组合结构，对深度学习理论中的学习理论分支有重要基础贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20961' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Depth Delusion: Why Transformers Should Be Wider, Not Deeper</h3>
<p><strong>Authors:</strong> Md Muhtasim Munif Fahim (University of Chicago), Md Rezaul Karim (University of Chicago)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出架构条件缩放定律，发现Transformer的最优宽度增长速度远快于深度，验证了“宽胜于深”的结论，对Transformer架构设计和缩放策略有重要指导意义，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20994' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity</h3>
<p><strong>Authors:</strong> Mutian Yang, Zisen Zhan, Yutong Chen, Haolin Li, Kaiwen Wang, Kaili Zheng, Yuguang Wang, Qi Wang, Jiandong Gao, Ji Wu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出梯度相似性框架解释灾难性遗忘，分析冲突与协作神经元，属于深度学习理论中的优化与遗忘机制研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21577' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamics Reveals Structure: Challenging the Linear Propagation Assumption</h3>
<p><strong>Authors:</strong> Hoyeon Chang, Bálint Mucsányi, Seong Joon Oh</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 挑战神经网络线性传播假设，分析关系代数操作的传播限制，属于深度学习理论中的网络机制研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21601' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Identifiable Equivariant Networks are Layerwise Equivariant</h3>
<p><strong>Authors:</strong> Vahid Shahverdi, Giovanni Luca Marchetti, Georg Bökman, Kathlén Kohn</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 证明可识别等变网络的层间等变性，属于深度学习理论中的网络结构研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21645' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Can Local Learning Match Self-Supervised Backpropagation?</h3>
<p><strong>Authors:</strong> Wu S. Zihan, Ariane Delrocq, Wulfram Gerstner, Guillaume Bellec</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 比较局部学习与自监督反向传播性能，提出局部学习改进变体，属于深度学习理论中的学习机制研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21683' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts</h3>
<p><strong>Authors:</strong> Qinglun Li, Anke Tang, Miao Zhang, Mengzhu Wang, Quanjun Yin, Li Shen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出模型合并的泛化理论框架，分析异质专家合并性能，属于深度学习理论中的模型合并研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21690' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics</h3>
<p><strong>Authors:</strong> Mohamed Elgaar, Hadi Amiri</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 分析LLM预训练的课程学习动态，比较不同课程轨迹，属于深度学习理论中的预训练机制研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21698' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions</h3>
<p><strong>Authors:</strong> Jinhang Chai, Xuyuan Liu, Elynn Chen, Yujun Yan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究迁移学习中结构化矩阵估计的理论框架，提出锚定交替投影估计器并建立确定性误差界，对深度学习迁移学习的理论发展有价值
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21873' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimistic Transfer under Task Shift via Bellman Alignment</h3>
<p><strong>Authors:</strong> Jinhang Chai, Enpei Zhang, Elynn Chen, Yujun Yan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于Bellman对齐的强化学习迁移框架，解决任务偏移下的迁移问题，建立 regret 界，属于深度学习理论中的迁移学习理论进展
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21924' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization</h3>
<p><strong>Authors:</strong> Chuanyang Zheng, Jiankai Sun, Yihang Gao, Chi Wang, Yuehao Wang, Jing Xiong, Liliang Ren, Bo Peng, Qingmei Wang, Xiaoran Shang, Mac Schwager, Anderson Schneider, Yuriy Nevmyvaka, Xiaodong Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 用测地线优化统一Pre-Norm和Post-Norm，提升Transformer性能，属于深度学习理论中归一化层与网络架构的创新
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22095' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</h3>
<p><strong>Authors:</strong> Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出矩阵函数的自适应计算框架加速神经网络训练，属于深度学习理论中的训练优化进展
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22137' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models</h3>
<p><strong>Authors:</strong> Rishi Upadhyay, Howard Zhang, Jim Solomon, Ayush Agrawal, Pranay Boreddy, Shruti Satya Narayana, Yunhao Ba, Alex Wong, Celso M de Melo, Achuta Kadambi</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对世界模型的物理一致性评估问题，提出解纠缠的WorldBench基准，分离评估单个物理概念，揭示模型的物理推理缺陷，属于深度学习理论的模型评估研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Variance & Greediness: A comparative study of metric-learning losses</h3>
<p><strong>Authors:</strong> Donghuo Zeng, Hao Niu, Zhi Li, Masato Taya</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对度量学习损失的效果差异问题，提出方差（intra-/inter-class）与贪婪性（active ratio等）的诊断框架，比较7种损失的性能与适用场景，属于深度学习理论的损失函数研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21450' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Monotone Optimisation with Learned Projections</h3>
<p><strong>Authors:</strong> Ahmed Rashwan (University of Oxford), Keith Briggs (University of Oxford), Chris Budd (University of Oxford), Lisa Kreusser (University of Oxford)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出了结构化神经架构HM-RI网络，将学习模型整合到单调优化算法中，解决了传统方法需显式函数的问题，通过单调和齐次性约束提升投影估计效率，对深度学习理论中的网络架构设计有启发。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20983' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization</h3>
<p><strong>Authors:</strong> Ruicheng Ao, Hongyu Chen, Haoyang Liu, David Simchi-Levi, Will Wei Sun</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 结合PPI与SVRG改进半监督优化的收敛性，推导了收敛界并验证了在 mean estimation 与 MNIST 上的性能提升，为半监督学习提供了理论与算法支持
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21470' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fast and Geometrically Grounded Lorentz Neural Networks</h3>
<p><strong>Authors:</strong> Robert van der Klis, Ricardo Chávez Torres, Max van Spengler, Yuhui Ding, Thomas Hofmann, Pascal Mettes</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 改进Lorentz线性层解决输出范数缩放问题，提出高效的双曲神经网络架构，为几何深度学习提供了更符合理论的模型
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21529' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dependence of Equilibrium Propagation Training Success on Network Architecture</h3>
<p><strong>Authors:</strong> Qingshan Wang, Clara C. Wanjura, Florian Marquardt</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究平衡传播训练成功与网络架构的关系，发现局部连接网络可达到 dense 网络性能，为网络架构设计提供理论依据
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21945' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models</h3>
<p><strong>Authors:</strong> Junming Huang, Weiwei Xu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出首个用多模态大模型实现3D内容描述与生成的框架，融合Mixture-of-Transformer与3D VAE，突破原生多模态大模型的3D处理能力。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21798' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</h3>
<p><strong>Authors:</strong> Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Closing the Multimodal Reasoning Gap via Open Data-Centric Methods
Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu
Published: 2026-01-30
Link: https://arxiv.org/abs/2601.21821
Reason: 构建1.8M样本的多模态推理数据集，微调模型达到SOTA，属于原生多模态大模型的推理能力研究，提升参数效率。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21821' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出多模态深度研究的MLLM，实现多轮多尺度视觉文本搜索，性能超过现有模型与闭源框架，属于原生多模态大模型的深度推理研究。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs</h3>
<p><strong>Authors:</strong> Haochen Zhang, Animesh Sinha, Felix Juefei-Xu, Haoyu Ma, Kunpeng Li, Zhipeng Fan, Meng Dong, Xiaoliang Dai, Tingbo Hou, Peizhao Zhang, Zecheng He</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对多轮对话式图像生成的非马尔可夫问题，提出历史条件的多模态大语言模型（MLLMs）框架，通过数据构建、token级缓存等策略提升多轮一致性与指令遵循性，属于原生多模态大模型的核心研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20911' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought</h3>
<p><strong>Authors:</strong> Yu Huo, Siyu Zhang, Kun Zeng, Haoyue Liu, Owen Lee, Junlin Chen, Yuquan Lu, Yifu Guo, Yaodong Liang, Xiaoying Tang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对多模态生成的结构约束问题，提出视觉Chain-of-Thought框架SoT，通过交错生成文本计划与中间状态提升组件计数、属性绑定等能力，属于原生多模态大模型的 compositional generation研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21081' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation</h3>
<p><strong>Authors:</strong> Zihan Su, Hongyang Wei, Kangrui Cen, Yong Wang, Guanhua Chen, Chun Yuan, Xiangxiang Chu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对统一多模态模型（UMMs）的理解-生成协同问题，提出多表示生成框架UniMRG，通过生成像素、深度等表示提升视觉理解与生成能力，属于原生多模态大模型的协同优化研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21406' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</h3>
<p><strong>Authors:</strong> Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出首个端到端的holistic OCR方法，统一文本中心与视觉中心OCR，涉及多模态大模型跨域训练与融合，对原生多模态大模型的多模态数据处理有价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21639' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ChartE$^{3}$: A Comprehensive Benchmark for End-to-End Chart Editing</h3>
<p><strong>Authors:</strong> Shuo Li, Jiajun Sun, Zhekai Wang, Xiaoran Fan, Hui Li, Dingwen Yang, Zhiheng Xi, Yijun Wang, Zifei Shan, Tao Gui, Qi Zhang, Xuanjing Huang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出首个端到端图表编辑基准，评估多模态大模型的细粒度控制与全局一致性，支撑原生多模态大模型的端到端编辑能力研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21694' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing</h3>
<p><strong>Authors:</strong> Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出0.9B多任务VLM提升野外文档解析鲁棒性，属于原生多模态大模型的文档理解研究，性能SOTA且开源。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21957' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</h3>
<p><strong>Authors:</strong> John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出音频驱动的说话头视频编辑框架，实现transcript-based修改，属于原生多模态大模型的视频编辑研究，解决现有编辑gap。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching</h3>
<p><strong>Authors:</strong> Li Ju, Mayank Nautiyal, Andreas Hellander, Ekta Vats, Prashant Singh</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Riemannian流匹配方法量化预训练VLMs的认知不确定性，属于原生多模态大模型研究
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21662' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models</h3>
<p><strong>Authors:</strong> Chenyu Huang, Peng Ye, Xudong Tan, Jinhan Mu, Shenghe Zheng, Li Shen, Tao Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLMs）推理能力增强的粗粒度合并问题，提出子空间级模型合并框架FRISM，通过SVD分解与自蒸馏提升推理能力同时保留视觉性能，属于原生多模态大模型的推理增强研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21187' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HiFi-Mesh: High-Fidelity Efficient 3D Mesh Generation via Compact Autoregressive Dependence</h3>
<p><strong>Authors:</strong> Yanfeng Li, Tao Tan, Qingquan Gao, Zhiwen Cao, Xiaohong liu, Yue Sun</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对高保真3D网格生成的效率瓶颈，提出Latent Autoregressive Network（LANE）与AdaGraph策略，提升生成序列长度与推理速度，属于原生多模态大模型的3D生成研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21314' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</h3>
<p><strong>Authors:</strong> Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出通用字符图像动画框架，将运动条件视为上下文学习问题，融合外观与运动模态，属于原生多模态大模型的多模态生成研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21716' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</h3>
<p><strong>Authors:</strong> Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin, Yuan Liu, Sibei Yang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出3D资产参考的扩散模型，融合多视图RGB与点云，提升生成一致性，属于原生多模态大模型的3D-2D生成研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22094' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention</h3>
<p><strong>Authors:</strong> Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang, Zifeng Cheng, Fei Shen, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出路径级干预框架断开非法语义传播，提升大模型对抗鲁棒性，属于大模型安全与对齐的防御研究，性能优于SOTA。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21900' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher</h3>
<p><strong>Authors:</strong> Yisheng Zhong (University of California, Berkeley), Zhengbang Yang (University of California, Berkeley), Zhuangdi Zhu (University of California, Berkeley)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出蒸馏式LLM遗忘方法，结合上下文教师模型和学生模型蒸馏，解决现有方法计算重或易受攻击的问题，提升遗忘效果和数据效率，属于大模型安全与对齐中的关键技术。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shaping capabilities with token-level data filtering</h3>
<p><strong>Authors:</strong> Neil Rathi, Alec Radford</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 通过token级数据过滤在预训练时移除不需要的能力（如医疗），提升模型安全性，在大模型上验证了有效性与鲁棒性，为预训练阶段的能力塑造提供了实用方法
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21571' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities</h3>
<p><strong>Authors:</strong> Tien Dang, The-Hai Nguyen, Dinh Mai Phuong, Nguyen Minh Phuong, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究机器遗忘的可控行为，发现遗忘可增强模型能力，属于大模型安全与对齐中的unlearning研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21702' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LoRA and Privacy: When Random Projections Help (and When They Don't)</h3>
<p><strong>Authors:</strong> Yaxi Hu, Johanna Düngler, Bernhard Schölkopf, Amartya Sanyal</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 分析LoRA的隐私特性，提出噪声变体增强隐私，属于大模型安全与对齐中的隐私研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21719' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Yejin Kim, Dongjun Hwang, Sungmin Cha, Junsuk Choe</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出无训练的知识向量削弱方法，实现LVLM的高效unlearning，属于大模型安全与对齐中的unlearning研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21794' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning</h3>
<p><strong>Authors:</strong> Chengyi Cai, Zesheng Ye, Peike Li, Bo Han, Jianzhong Qi, Feng Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出视觉引导的关键token正则化用于多模态大模型遗忘，提升遗忘效果并保持响应一致性，属于大模型安全与对齐的模型遗忘研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning</h3>
<p><strong>Authors:</strong> Haoran Tang, Rajiv Khanna</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出对比表征塑造用于LLM遗忘，减少遗忘-保留纠缠，提升遗忘的有效性和安全性，属于大模型安全与对齐的表征层面研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22028' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> BadDet+: Robust Backdoor Attacks for Object Detection</h3>
<p><strong>Authors:</strong> Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对目标检测模型的后门攻击弱点，提出BadDet+框架，通过对数屏障惩罚实现位置/尺度不变性与物理鲁棒性，揭露目标检测模型的安全漏洞，属于大模型安全与对齐的研究范畴。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21066' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models</h3>
<p><strong>Authors:</strong> Alvi Md Ishmam, Najibul Haque Sarker, Zaber Ibn Abdul Hakim, Chris Thomas</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对多图像多模态大模型（MLLMs）的黑盒攻击问题，提出LAMP方法，通过注意力约束与跨图像传染约束生成通用对抗扰动，提升攻击成功率，属于大模型安全与对齐的对抗攻击研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21220' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Factored Causal Representation Learning for Robust Reward Modeling in RLHF</h3>
<p><strong>Authors:</strong> Yupei Yang, Lin Yang, Wanxi Deng, Lin Qu, Fan Feng, Biwei Huang, Shikui Tu, Lei Xu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 通过因果分解将上下文嵌入分为因果与非因果因子，提升RLHF奖励模型的鲁棒性，缓解长度偏差与奉承偏差等奖励黑客问题，改善对齐效果
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21350' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Representation Unlearning: Forgetting through Information Compression</h3>
<p><strong>Authors:</strong> Antonio Almudévar, Alfonso Ortega</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出表示遗忘框架，通过信息压缩去除特定数据的影响，提升隐私与鲁棒性，比参数级方法更高效可靠，为模型遗忘提供了新范式
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21564' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation</h3>
<p><strong>Authors:</strong> Zihao Huang, Jundong Zhou, Xingwei Qu, Qiyang Min, Ge Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 动态合并语义相似token为概念表示，实现token级计算分配，在语言、长上下文与多模态任务上优于标准MoE，同时减少注意力计算与KV缓存，是高效大模型的关键创新
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21420' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</h3>
<p><strong>Authors:</strong> Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出无需训练的幂采样方法，减少LLM推理延迟10倍，属于高效大模型推理研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21590' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> HeRo-Q: A General Framework for Stable Low Bit Quantization via Hessian Conditioning</h3>
<p><strong>Authors:</strong> Jinhao Zhang, Yunquan Zhang, Zicheng yan, Boyang Zhang, Jun Sun, Daning Cheng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Hessian条件的低比特量化框架，解决“低误差高损失”问题，属于高效大模型压缩研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21626' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold</h3>
<p><strong>Authors:</strong> Luca Benfenati, Matteo Risso, Andrea Vannozzi, Ahmet Caner Yüzüğüler, Lukas Cavigelli, Enrico Macii, Daniele Jahier Pagliari, Alessio Burrello</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Stiefel流形上的KV缓存低秩近似，优化LLM推理内存瓶颈，属于高效大模型推理研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21686' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training</h3>
<p><strong>Authors:</strong> Xinwei Qiang, Hongmin Chen, Shixuan Sun, Jingwen Leng, Xin Liu, Minyi Guo</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出确定性注意力调度策略，提高LLM训练吞吐量与可重复性，属于高效大模型训练研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21824' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Where Do the Joules Go? Diagnosing Inference Energy Consumption</h3>
<p><strong>Authors:</strong> Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 大规模测量生成式AI推理的时间与能耗，提出诊断框架，为推理能耗优化提供关键 insights，属于高效大模型推理的重要研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22076' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</h3>
<p><strong>Authors:</strong> Ziming Dong, Hardik Sharma, Evan O'Toole, Jaya Prakash Champati, Kui Wu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出LLM提示引导SLM推理，大幅降低推理成本同时保持精度，属于高效大模型推理的成本优化关键技术
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22132' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models</h3>
<p><strong>Authors:</strong> Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对自回归视觉生成模型的量化挑战（通道异常值、token动态激活等），提出后训练量化框架PTQ4ARVG，通过Gain-Projected Scaling等策略提升量化效率与性能，属于高效大模型训练与推理的量化技术研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21238' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention</h3>
<p><strong>Authors:</strong> Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Ao Sun, Ziqi Yuan, Hao Zhou, Fandong Meng, Zhiyuan Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对长视频理解的预填充阶段效率瓶颈，提出序列并行近似注意力框架Spava，通过分布式计算与负载均衡提升推理速度，属于高效大模型训练与推理的长视频优化研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21444' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Bi-Anchor Interpolation Solver for Accelerating Generative Modeling</h3>
<p><strong>Authors:</strong> Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对生成模型的ODE求解延迟问题，提出双锚插值求解器BA-solver，通过SideNet近似双向速度与锚点集成提升生成效率（10 NFEs达到100+ NFEs性能），属于高效大模型训练与推理的生成加速研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21542' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion</h3>
<p><strong>Authors:</strong> Hanmo Chen, Chenghao Xu, Xu Yang, Xuan Chen, Cheng Deng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出PaFu-KV缓存策略，通过显著性估计保留重要token，提升自回归视频扩散的推理效率与质量，属于高效大模型训练与推理的缓存优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21896' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts</h3>
<p><strong>Authors:</strong> Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出低秩空间与Lipschitz控制的MoE路由框架，提升路由稳定性与专家专业化，在语言和视觉MoE任务上持续改进性能，属于高效大模型训练的关键优化
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21349' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving</h3>
<p><strong>Authors:</strong> Chendong Song, Meixuan Wang, Hang Zhou, Hong Liang, Yuan Lyu, Zixi Chen, Yuwei Fan, Zijie Zhou</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 推导了分拆LLM服务中Attention/FFN的最优比例，通过概率 workload 模型与仿真验证，提升吞吐量并减少空闲时间，为LLM推理优化提供了理论指导
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21351' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> L³: Large Lookup Layers</h3>
<p><strong>Authors:</strong> Albert Tseng, Christopher De Sa</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 将嵌入表扩展到解码器层，通过静态token路由与信息论分配优化，比dense和MoE模型在语言建模与下游任务上更优，为稀疏大模型提供了高效架构
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21461' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)</h3>
<p><strong>Authors:</strong> Sagi Meir, Tommer D. Keidar, Noam Levi, Shlomi Reuveni, Barak Hirshberg</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出ReD方法优化固定预算下LLM的推理覆盖，减少计算与成本，在HumanEval上验证了有效性，为LLM推理效率提升提供了实用方法
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21522' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking</h3>
<p><strong>Authors:</strong> Yiming Wang, Zhuosheng Zhang, Rui Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出T2方法优化并行思维的样本级并行度，减少计算成本，属于高效大模型推理研究
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21619' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models</h3>
<p><strong>Authors:</strong> Stanislav Budzinskiy, Marian Gloser, Tolunay Yilmaz, Ying Hong Tham, Yuanyi Lin, Wenyi Fang, Fan Wu, Philipp Petersen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出混合精度推理策略，提高Transformer推理准确性与效率，属于高效大模型推理研究
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21623' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Effective LoRA Adapter Routing using Task Representations</h3>
<p><strong>Authors:</strong> Akash Dhasade, Anne-Marie Kermarrec, Igor Pavlovic, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出任务表示的LoRA适配器路由，优化参数高效微调，属于高效大模型训练研究
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21795' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rate-Distortion Optimization for Transformer Inference</h3>
<p><strong>Authors:</strong> Anderson de Andrade, Alon Harell, Ivan V. Bajić</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Transformer推理的率失真优化框架，平衡比特率与精度，提升推理效率，属于高效大模型推理的重要进展
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22002' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Value-Based Pre-Training with Downstream Feedback</h3>
<p><strong>Authors:</strong> Shuqi Ke, Giulia Fanti</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出价值-based预训练框架，用下游任务反馈引导预训练，提升模型推理性能和效率，属于高效大模型训练的预训练优化
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22108' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</h3>
<p><strong>Authors:</strong> Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对文本到视频检索的效率瓶颈，提出生成式召回+密集重排框架GRDR，通过多视图语义ID与共享码本降低存储并加速推理，属于高效大模型训练与推理的检索效率优化研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21193' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Soft Quantization: Model Compression Via Weight Coupling</h3>
<p><strong>Authors:</strong> Daniel T. Bernstein (Princeton University), Luca Di Carlo (Princeton University), David Schwab (Princeton University)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出通过权重耦合实现模型压缩的软量化方法，无需显式量化，诱导权重分布离散化，混合精度且仅需两个超参数，比后训练量化效果更优，属于高效大模型训练与推理中的压缩技术。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21219' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing</h3>
<p><strong>Authors:</strong> Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 将神经网络转换为逻辑流提升边缘CPU推理效率，属于高效大模型推理的边缘设备优化
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22151' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy</h3>
<p><strong>Authors:</strong> Jinhao Zhang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Wenlong Xia, Haoming Song, Youmin Gong, Jie Me</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出轻量级3D扩散visuomotor策略PocketDP3，通过Diffusion Mixer替代传统重参数U-Net解码器，将模型参数压缩至现有方法的1%以下，支持两步推理加速，直接针对模型效率优化，符合高效大模型训练与推理的核心目标。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22018' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ECSEL: Explainable Classification via Signomial Equation Learning</h3>
<p><strong>Authors:</strong> Adia Lumadjeng, Ilker Birbil, Erman Acar</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 生成可解释的符号方程作为分类器，支持全局与局部解释，属于深度学习可解释性研究
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21789' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution</h3>
<p><strong>Authors:</strong> Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出低秩影响函数解决大规模训练数据归因的可扩展性问题，提升数据归因效率，属于深度学习可解释性中的关键技术
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21929' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning</h3>
<p><strong>Authors:</strong> Hanmo Chen, Guangtao Lyu, Chenghao Xu, Jiexi Yan, Xu Yang, Cheng Deng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出PST学习框架，用Shapley-Taylor分解实现细粒度运动-语言对齐，属于深度学习可解释性中的Shapley值应用，提升跨模态检索精度。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21904' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</h3>
<p><strong>Authors:</strong> Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出VI-Probe探究VLMs的感知与记忆机制，揭示响应规律，属于深度学习可解释性中的模型行为分析，指导模型优化。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.22150' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Grounding and Enhancing Informativeness and Utility in Dataset Distillation</h3>
<p><strong>Authors:</strong> Shaobo Wang, Yantai Yang, Guo Chen, Peiru Li, Kaixin Li, Yufa Zhou, Zhaorun Chen, Linfeng Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 基于博弈论利用Shapley值最大化数据集蒸馏的信息性，结合梯度范数优化效用，在ImageNet-1K上比SOTA提升6.1%，为数据集蒸馏提供了理论框架与有效方法
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21296' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models</h3>
<p><strong>Authors:</strong> Konstantinos P. Panousis, Diego Marcos</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究稀疏概念瓶颈模型的灵活性与可解释性权衡，提出Clarity指标和评估框架，深化了可解释性模型的理解
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21944' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> GeoRC: A Benchmark for Geolocation Reasoning Chains</h3>
<p><strong>Authors:</strong> Mohit Talreja, Joshua Diao, Jim Thannikary James, Radu Casapu, Tejas Santanam, Ethan Mendes, Alan Ritter, Wei Xu, James Hays</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLMs）地理定位推理的可解释性问题，构建GeoRC基准，评估推理链的可审计性与视觉属性提取能力，属于深度学习可解释性的推理链研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21278' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification</h3>
<p><strong>Authors:</strong> Akash Pandey (Northwestern University), Payal Mohapatra (Northwestern University), Wei Chen (Northwestern University), Qi Zhu (Northwestern University), Sinan Keten (Northwestern University)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出符号-线性分解的可解释框架，结合原始时间序列和符号抽象，线性编码时间片段贡献，解决传统方法参考状态敏感和忽略序列依赖的问题，提升可解释性和分类性能，属于深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21289' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models</h3>
<p><strong>Authors:</strong> Sidney Bender, Marco Morik</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出扩散自动编码器生成可解释反事实样本，用于基础模型shortcut learning缓解，属于大模型新技术中的扩散模型研究
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion</h3>
<p><strong>Authors:</strong> Pu Cao, Yiyang Ma, Feng Zhou, Xuedan Yin, Qing Song, Lu Yang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对可控扩散中自动编码器的权衡问题（重建保真度与生成友好性），分析gFID与重建指标对可控性的影响，揭示ImageNet-centric评估与可控扩散需求的 gap，属于大模型新技术的可控扩散研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21633' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</h3>
<p><strong>Authors:</strong> Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出流匹配的classifier-free guidance改进方法，通过流形投影与Anderson加速提升生成质量，属于大模型新技术中的流模型研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21892' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks</h3>
<p><strong>Authors:</strong> Luwei Sun (Tsinghua University), Dongrui Shen (Tsinghua University), Jianfe Li (Tsinghua University), Yulong Zhao (Tsinghua University), Han Feng (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文从比率函数近似角度理论分析扩散模型，用SignReLU网络推导L^p近似界和KL风险，给出扩散模型的泛化保证，对大模型新技术中的扩散模型理论理解有重要贡献。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21242' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models</h3>
<p><strong>Authors:</strong> Ahmad Aghapour, Erhan Bayraktar, Ziqing Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出扩散模型的无维度收敛分析和损失自适应调度，提升扩散模型的采样效率和质量，属于大模型新技术中的扩散模型创新
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21943' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Making Foundation Models Probabilistic via Singular Value Ensembles</h3>
<p><strong>Authors:</strong> Mehmet Ozgur Turkoglu, Dominik J. M\"uhlematter, Alexander Becker, Konrad Schindler, Helge Aasen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出奇异值集成让基础模型概率化，提升不确定性量化能力，属于大模型新技术中的概率化模型创新
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22068' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models</h3>
<p><strong>Authors:</strong> Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 利用视频扩散模型辅助零样本视频修复，解决时间一致性问题，属于大模型新技术中的扩散模型应用，训练-free且实用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21922' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models</h3>
<p><strong>Authors:</strong> Archer Wang, Emile Anand, Yilun Du, Marin Soljačić</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出判别器驱动的扩散模型实现无监督分解与重组，提升生成一致性与多样性，属于大模型新技术中的扩散模型应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22057' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Creative Image Generation with Diffusion Model</h3>
<p><strong>Authors:</strong> Kunpeng Song, Ahmed Elgammal</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于CLIP嵌入概率的创造性扩散模型，提升生成新颖性，属于大模型新技术中的扩散模型应用，对创造性生成有价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22125' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</h3>
<p><strong>Authors:</strong> Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出物理启发的扩散模型实现全图像重新照明，提升真实场景泛化性与物理 plausibility，属于大模型新技术中的扩散模型应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22135' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Memorization Control in Diffusion Models from Denoising-centric Perspective</h3>
<p><strong>Authors:</strong> Thuy Phuong Vu, Mai Viet Hoang Do, Minhhuy Le, Dinh-Cuong Hoang, Phan Xuan Tan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 从去噪视角分析扩散模型的记忆问题，提出时间步采样策略平衡记忆与泛化，在图像和1D信号生成任务验证了有效性，为扩散模型的记忆控制提供了新视角
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21348' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Revisiting Diffusion Model Predictions Through Dimensionality</h3>
<p><strong>Authors:</strong> Qing Jin, Chaoyang Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 理论分析扩散模型预测目标与维度的关系，提出k-Diff自动学习最优预测参数，在 latent-space 与 pixel-space 图像生成任务中超过固定目标基线，为扩散模型优化提供了新视角
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21419' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Information Filtering via Variational Regularization for Robot Manipulation</h3>
<p><strong>Authors:</strong> Jinhao Zhang, Wenlong Xia, Yaojia Wang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Haoming Song, Youmin Gong, Jie Me</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对扩散visuomotor策略的冗余特征问题，提出变分正则化模块VR，通过时间步条件高斯和KL正则化构建自适应信息瓶颈，属于扩散模型的新技术优化，符合大模型新技术方向。
Score: 6
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21926' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation</h3>
<p><strong>Authors:</strong> Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan, Xuanwei Ye, Houhua Li, Xiaoming Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出DGNav框架解决视觉语言导航的粒度刚性问题，提升导航效率与精度，属于多模态智能体的导航研究。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.21751' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Causal World Modeling for Robot Control</h3>
<p><strong>Authors:</strong> Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出LingBot-VA融合视频世界模型与视觉语言预训练，提升机器人长horizon操作性能，属于多模态智能体的世界建模研究。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.21998' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving</h3>
<p><strong>Authors:</strong> Linhan Wang, Zichong Yang, Chen Bai, Guoxiang Zhang, Xiaotong Liu, Xiaoyin Zheng, Xiao-Xiao Long, Chang-Tien Lu, Cheng Lu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Drive-JEPA融合视频JEPA与多模态轨迹蒸馏，实现端到端驾驶，属于多模态智能体的自动驾驶研究，性能SOTA。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22032' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Understanding Multimodal Complementarity for Single-Frame Action Anticipation</h3>
<p><strong>Authors:</strong> Manuel Benavent-Lledo, Konstantinos Bacharidis, Konstantinos Papoutsakis, Antonis Argyros, Jose Garcia-Rodriguez</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究单帧动作预测的多模态互补性，提出AAG+框架超过视频-based方法，属于多模态智能体的动作anticipation研究。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22039' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>