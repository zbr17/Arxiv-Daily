# ArXiv 每日推荐 - 2026-01-30

> 更新于北京时间：2026-01-30 13:22:32
> 已自动阅读了 360 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：189833

## 深度学习理论

### [Score: 9.0/10] When Gradient Optimization Is Not Enough: $\dagger$ Dispersive and Anchoring Geometric Regularizer for Multimodal Learning
- **Authors:** Zixuan Xia, Hao Wang, Pengcheng Weng, Yanyu Qian, Yangxin Xu, William Dan, Fei Wang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21670](https://arxiv.org/abs/2601.21670)
- **Reason:** 提出几何正则化框架解决多模态学习的表示坍缩与跨模态不一致，属于深度学习理论中的表示学习与正则化研究。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources
- **Authors:** Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22054](https://arxiv.org/abs/2601.22054)
- **Reason:** 提出scalable的metric深度预训练框架，首次展示深度估计缩放规律，属于深度学习理论中的表示学习与缩放研究，应用广泛。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] Why Adam Works Better with β₁ = β₂: The Missing Gradient Scale Invariance Principle
- **Authors:** Alberto Fernández-Hernández, Cristian Pérez-Corral, Jose I. Mestre, Manuel F. Dolz, Enrique S. Quintana-Ortí
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21739](https://arxiv.org/abs/2601.21739)
- **Reason:** 证明Adam的β₁=β₂时具有梯度尺度不变性，解释其性能优势，属于深度学习理论中的优化器研究
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer
- **Authors:** Chenrui Xu, Wenjing Yan, Ying-Jun Angela Zhang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21750](https://arxiv.org/abs/2601.21750)
- **Reason:** 提出Fisher结构的动量正交优化器，结合几何预条件与方差 reduction，属于深度学习理论中的优化器研究
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] Just Noticeable Difference Modeling for Deep Visual Features
- **Authors:** Rui Zhao, Wenrui Li, Lin Zhu, Yajing Zheng, Weisi Lin
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21933](https://arxiv.org/abs/2601.21933)
- **Reason:** 提出FeatJND预测深度特征的可察觉扰动，属于深度学习理论中的特征质量控制研究，对特征压缩与量化有应用价值。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Deep Models, Shallow Alignment: Uncovering the Granularity Mismatch in Neural Decoding
- **Authors:** Yang Du, Siyuan Dai, Yonghao Song, Paul M. Thompson, Haoteng Tang, Liang Zhan
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21948](https://arxiv.org/abs/2601.21948)
- **Reason:** 提出Shallow Alignment用中间层表示对齐神经信号，解决粒度不匹配问题，属于深度学习理论中的表示对齐研究，提升神经解码性能。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation
- **Authors:** Li Zhang, Pengtao Xie
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22061](https://arxiv.org/abs/2601.22061)
- **Reason:** 用双级优化对齐YOLO与SAM解决目标mismatch问题，属于深度学习理论中的优化与对齐研究，提升实例分割鲁棒性。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] A Theory of Universal Agnostic Learning
- **Authors:** Steve Hanneke (Carnegie Mellon University), Shay Moran (Hebrew University of Jerusalem)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.20961](https://arxiv.org/abs/2601.20961)
- **Reason:** 该论文提出了agnostic setting下二元分类的最优通用速率理论，扩展了可实现情况的学习理论，识别了决定概念类最优速率的组合结构，对深度学习理论中的学习理论分支有重要基础贡献。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] The Depth Delusion: Why Transformers Should Be Wider, Not Deeper
- **Authors:** Md Muhtasim Munif Fahim (University of Chicago), Md Rezaul Karim (University of Chicago)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.20994](https://arxiv.org/abs/2601.20994)
- **Reason:** 论文提出架构条件缩放定律，发现Transformer的最优宽度增长速度远快于深度，验证了“宽胜于深”的结论，对Transformer架构设计和缩放策略有重要指导意义，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity
- **Authors:** Mutian Yang, Zisen Zhan, Yutong Chen, Haolin Li, Kaiwen Wang, Kaili Zheng, Yuguang Wang, Qi Wang, Jiandong Gao, Ji Wu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21577](https://arxiv.org/abs/2601.21577)
- **Reason:** 提出梯度相似性框架解释灾难性遗忘，分析冲突与协作神经元，属于深度学习理论中的优化与遗忘机制研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Dynamics Reveals Structure: Challenging the Linear Propagation Assumption
- **Authors:** Hoyeon Chang, Bálint Mucsányi, Seong Joon Oh
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21601](https://arxiv.org/abs/2601.21601)
- **Reason:** 挑战神经网络线性传播假设，分析关系代数操作的传播限制，属于深度学习理论中的网络机制研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Identifiable Equivariant Networks are Layerwise Equivariant
- **Authors:** Vahid Shahverdi, Giovanni Luca Marchetti, Georg Bökman, Kathlén Kohn
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21645](https://arxiv.org/abs/2601.21645)
- **Reason:** 证明可识别等变网络的层间等变性，属于深度学习理论中的网络结构研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Can Local Learning Match Self-Supervised Backpropagation?
- **Authors:** Wu S. Zihan, Ariane Delrocq, Wulfram Gerstner, Guillaume Bellec
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21683](https://arxiv.org/abs/2601.21683)
- **Reason:** 比较局部学习与自监督反向传播性能，提出局部学习改进变体，属于深度学习理论中的学习机制研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts
- **Authors:** Qinglun Li, Anke Tang, Miao Zhang, Mengzhu Wang, Quanjun Yin, Li Shen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21690](https://arxiv.org/abs/2601.21690)
- **Reason:** 提出模型合并的泛化理论框架，分析异质专家合并性能，属于深度学习理论中的模型合并研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics
- **Authors:** Mohamed Elgaar, Hadi Amiri
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21698](https://arxiv.org/abs/2601.21698)
- **Reason:** 分析LLM预训练的课程学习动态，比较不同课程轨迹，属于深度学习理论中的预训练机制研究
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions
- **Authors:** Jinhang Chai, Xuyuan Liu, Elynn Chen, Yujun Yan
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21873](https://arxiv.org/abs/2601.21873)
- **Reason:** 研究迁移学习中结构化矩阵估计的理论框架，提出锚定交替投影估计器并建立确定性误差界，对深度学习迁移学习的理论发展有价值
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Optimistic Transfer under Task Shift via Bellman Alignment
- **Authors:** Jinhang Chai, Enpei Zhang, Elynn Chen, Yujun Yan
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21924](https://arxiv.org/abs/2601.21924)
- **Reason:** 提出基于Bellman对齐的强化学习迁移框架，解决任务偏移下的迁移问题，建立 regret 界，属于深度学习理论中的迁移学习理论进展
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization
- **Authors:** Chuanyang Zheng, Jiankai Sun, Yihang Gao, Chi Wang, Yuehao Wang, Jing Xiong, Liliang Ren, Bo Peng, Qingmei Wang, Xiaoran Shang, Mac Schwager, Anderson Schneider, Yuriy Nevmyvaka, Xiaodong Liu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22095](https://arxiv.org/abs/2601.22095)
- **Reason:** 用测地线优化统一Pre-Norm和Post-Norm，提升Transformer性能，属于深度学习理论中归一化层与网络架构的创新
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training
- **Authors:** Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22137](https://arxiv.org/abs/2601.22137)
- **Reason:** 提出矩阵函数的自适应计算框架加速神经网络训练，属于深度学习理论中的训练优化进展
Score: 8
Field: 深度学习理论

### [Score: 7.0/10] WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models
- **Authors:** Rishi Upadhyay, Howard Zhang, Jim Solomon, Ayush Agrawal, Pranay Boreddy, Shruti Satya Narayana, Yunhao Ba, Alex Wong, Celso M de Melo, Achuta Kadambi
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21282](https://arxiv.org/abs/2601.21282)
- **Reason:** 针对世界模型的物理一致性评估问题，提出解纠缠的WorldBench基准，分离评估单个物理概念，揭示模型的物理推理缺陷，属于深度学习理论的模型评估研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Variance & Greediness: A comparative study of metric-learning losses
- **Authors:** Donghuo Zeng, Hao Niu, Zhi Li, Masato Taya
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21450](https://arxiv.org/abs/2601.21450)
- **Reason:** 针对度量学习损失的效果差异问题，提出方差（intra-/inter-class）与贪婪性（active ratio等）的诊断框架，比较7种损失的性能与适用场景，属于深度学习理论的损失函数研究。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Monotone Optimisation with Learned Projections
- **Authors:** Ahmed Rashwan (University of Oxford), Keith Briggs (University of Oxford), Chris Budd (University of Oxford), Lisa Kreusser (University of Oxford)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.20983](https://arxiv.org/abs/2601.20983)
- **Reason:** 论文提出了结构化神经架构HM-RI网络，将学习模型整合到单调优化算法中，解决了传统方法需显式函数的问题，通过单调和齐次性约束提升投影估计效率，对深度学习理论中的网络架构设计有启发。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization
- **Authors:** Ruicheng Ao, Hongyu Chen, Haoyang Liu, David Simchi-Levi, Will Wei Sun
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21470](https://arxiv.org/abs/2601.21470)
- **Reason:** 结合PPI与SVRG改进半监督优化的收敛性，推导了收敛界并验证了在 mean estimation 与 MNIST 上的性能提升，为半监督学习提供了理论与算法支持
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Fast and Geometrically Grounded Lorentz Neural Networks
- **Authors:** Robert van der Klis, Ricardo Chávez Torres, Max van Spengler, Yuhui Ding, Thomas Hofmann, Pascal Mettes
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21529](https://arxiv.org/abs/2601.21529)
- **Reason:** 改进Lorentz线性层解决输出范数缩放问题，提出高效的双曲神经网络架构，为几何深度学习提供了更符合理论的模型
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Dependence of Equilibrium Propagation Training Success on Network Architecture
- **Authors:** Qingshan Wang, Clara C. Wanjura, Florian Marquardt
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21945](https://arxiv.org/abs/2601.21945)
- **Reason:** 研究平衡传播训练成功与网络架构的关系，发现局部连接网络可达到 dense 网络性能，为网络架构设计提供理论依据
Score: 7
Field: 深度学习理论

## 原生多模态大模型

### [Score: 9.0/10] CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models
- **Authors:** Junming Huang, Weiwei Xu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21798](https://arxiv.org/abs/2601.21798)
- **Reason:** 提出首个用多模态大模型实现3D内容描述与生成的框架，融合Mixture-of-Transformer与3D VAE，突破原生多模态大模型的3D处理能力。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods
- **Authors:** Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21821](https://arxiv.org/abs/2601.21821)
- **Reason:** Closing the Multimodal Reasoning Gap via Open Data-Centric Methods
Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu
Published: 2026-01-30
Link: https://arxiv.org/abs/2601.21821
Reason: 构建1.8M样本的多模态推理数据集，微调模型达到SOTA，属于原生多模态大模型的推理能力研究，提升参数效率。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models
- **Authors:** Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22060](https://arxiv.org/abs/2601.22060)
- **Reason:** 提出多模态深度研究的MLLM，实现多轮多尺度视觉文本搜索，性能超过现有模型与闭源框架，属于原生多模态大模型的深度推理研究。
Score: 9
Field: 原生多模态大模型

### [Score: 8.0/10] Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs
- **Authors:** Haochen Zhang, Animesh Sinha, Felix Juefei-Xu, Haoyu Ma, Kunpeng Li, Zhipeng Fan, Meng Dong, Xiaoliang Dai, Tingbo Hou, Peizhao Zhang, Zecheng He
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.20911](https://arxiv.org/abs/2601.20911)
- **Reason:** 针对多轮对话式图像生成的非马尔可夫问题，提出历史条件的多模态大语言模型（MLLMs）框架，通过数据构建、token级缓存等策略提升多轮一致性与指令遵循性，属于原生多模态大模型的核心研究方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought
- **Authors:** Yu Huo, Siyu Zhang, Kun Zeng, Haoyue Liu, Owen Lee, Junlin Chen, Yuquan Lu, Yifu Guo, Yaodong Liang, Xiaoying Tang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21081](https://arxiv.org/abs/2601.21081)
- **Reason:** 针对多模态生成的结构约束问题，提出视觉Chain-of-Thought框架SoT，通过交错生成文本计划与中间状态提升组件计数、属性绑定等能力，属于原生多模态大模型的 compositional generation研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation
- **Authors:** Zihan Su, Hongyang Wei, Kangrui Cen, Yong Wang, Guanhua Chen, Chun Yuan, Xiangxiang Chu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21406](https://arxiv.org/abs/2601.21406)
- **Reason:** 针对统一多模态模型（UMMs）的理解-生成协同问题，提出多表示生成框架UniMRG，通过生成像素、深度等表示提升视觉理解与生成能力，属于原生多模态大模型的协同优化研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models
- **Authors:** Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21639](https://arxiv.org/abs/2601.21639)
- **Reason:** 提出首个端到端的holistic OCR方法，统一文本中心与视觉中心OCR，涉及多模态大模型跨域训练与融合，对原生多模态大模型的多模态数据处理有价值。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] ChartE$^{3}$: A Comprehensive Benchmark for End-to-End Chart Editing
- **Authors:** Shuo Li, Jiajun Sun, Zhekai Wang, Xiaoran Fan, Hui Li, Dingwen Yang, Zhiheng Xi, Yijun Wang, Zifei Shan, Tao Gui, Qi Zhang, Xuanjing Huang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21694](https://arxiv.org/abs/2601.21694)
- **Reason:** 提出首个端到端图表编辑基准，评估多模态大模型的细粒度控制与全局一致性，支撑原生多模态大模型的端到端编辑能力研究。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing
- **Authors:** Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21957](https://arxiv.org/abs/2601.21957)
- **Reason:** 提出0.9B多任务VLM提升野外文档解析鲁棒性，属于原生多模态大模型的文档理解研究，性能SOTA且开源。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers
- **Authors:** John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22127](https://arxiv.org/abs/2601.22127)
- **Reason:** 提出音频驱动的说话头视频编辑框架，实现transcript-based修改，属于原生多模态大模型的视频编辑研究，解决现有编辑gap。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching
- **Authors:** Li Ju, Mayank Nautiyal, Andreas Hellander, Ekta Vats, Prashant Singh
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21662](https://arxiv.org/abs/2601.21662)
- **Reason:** 提出Riemannian流匹配方法量化预训练VLMs的认知不确定性，属于原生多模态大模型研究
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models
- **Authors:** Chenyu Huang, Peng Ye, Xudong Tan, Jinhan Mu, Shenghe Zheng, Li Shen, Tao Chen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21187](https://arxiv.org/abs/2601.21187)
- **Reason:** 针对视觉语言模型（VLMs）推理能力增强的粗粒度合并问题，提出子空间级模型合并框架FRISM，通过SVD分解与自蒸馏提升推理能力同时保留视觉性能，属于原生多模态大模型的推理增强研究。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] HiFi-Mesh: High-Fidelity Efficient 3D Mesh Generation via Compact Autoregressive Dependence
- **Authors:** Yanfeng Li, Tao Tan, Qingquan Gao, Zhiwen Cao, Xiaohong liu, Yue Sun
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21314](https://arxiv.org/abs/2601.21314)
- **Reason:** 针对高保真3D网格生成的效率瓶颈，提出Latent Autoregressive Network（LANE）与AdaGraph策略，提升生成序列长度与推理速度，属于原生多模态大模型的3D生成研究。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning
- **Authors:** Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21716](https://arxiv.org/abs/2601.21716)
- **Reason:** 提出通用字符图像动画框架，将运动条件视为上下文学习问题，融合外观与运动模态，属于原生多模态大模型的多模态生成研究。
Score: 7
Field: 原生多模态大模型

### [Score: 7.0/10] RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation
- **Authors:** Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin, Yuan Liu, Sibei Yang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22094](https://arxiv.org/abs/2601.22094)
- **Reason:** 提出3D资产参考的扩散模型，融合多视图RGB与点云，提升生成一致性，属于原生多模态大模型的3D-2D生成研究。
Score: 7
Field: 原生多模态大模型

## 大模型安全与对齐

### [Score: 9.0/10] TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention
- **Authors:** Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang, Zifeng Cheng, Fei Shen, Tat-Seng Chua
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21900](https://arxiv.org/abs/2601.21900)
- **Reason:** 提出路径级干预框架断开非法语义传播，提升大模型对抗鲁棒性，属于大模型安全与对齐的防御研究，性能优于SOTA。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher
- **Authors:** Yisheng Zhong (University of California, Berkeley), Zhengbang Yang (University of California, Berkeley), Zhuangdi Zhu (University of California, Berkeley)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21283](https://arxiv.org/abs/2601.21283)
- **Reason:** 论文提出蒸馏式LLM遗忘方法，结合上下文教师模型和学生模型蒸馏，解决现有方法计算重或易受攻击的问题，提升遗忘效果和数据效率，属于大模型安全与对齐中的关键技术。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Shaping capabilities with token-level data filtering
- **Authors:** Neil Rathi, Alec Radford
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21571](https://arxiv.org/abs/2601.21571)
- **Reason:** 通过token级数据过滤在预训练时移除不需要的能力（如医疗），提升模型安全性，在大模型上验证了有效性与鲁棒性，为预训练阶段的能力塑造提供了实用方法
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities
- **Authors:** Tien Dang, The-Hai Nguyen, Dinh Mai Phuong, Nguyen Minh Phuong, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21702](https://arxiv.org/abs/2601.21702)
- **Reason:** 研究机器遗忘的可控行为，发现遗忘可增强模型能力，属于大模型安全与对齐中的unlearning研究
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] LoRA and Privacy: When Random Projections Help (and When They Don't)
- **Authors:** Yaxi Hu, Johanna Düngler, Bernhard Schölkopf, Amartya Sanyal
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21719](https://arxiv.org/abs/2601.21719)
- **Reason:** 分析LoRA的隐私特性，提出噪声变体增强隐私，属于大模型安全与对齐中的隐私研究
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models
- **Authors:** Yejin Kim, Dongjun Hwang, Sungmin Cha, Junsuk Choe
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21794](https://arxiv.org/abs/2601.21794)
- **Reason:** 提出无训练的知识向量削弱方法，实现LVLM的高效unlearning，属于大模型安全与对齐中的unlearning研究
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning
- **Authors:** Chengyi Cai, Zesheng Ye, Peike Li, Bo Han, Jianzhong Qi, Feng Liu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22020](https://arxiv.org/abs/2601.22020)
- **Reason:** 提出视觉引导的关键token正则化用于多模态大模型遗忘，提升遗忘效果并保持响应一致性，属于大模型安全与对齐的模型遗忘研究
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning
- **Authors:** Haoran Tang, Rajiv Khanna
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22028](https://arxiv.org/abs/2601.22028)
- **Reason:** 提出对比表征塑造用于LLM遗忘，减少遗忘-保留纠缠，提升遗忘的有效性和安全性，属于大模型安全与对齐的表征层面研究
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] BadDet+: Robust Backdoor Attacks for Object Detection
- **Authors:** Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21066](https://arxiv.org/abs/2601.21066)
- **Reason:** 针对目标检测模型的后门攻击弱点，提出BadDet+框架，通过对数屏障惩罚实现位置/尺度不变性与物理鲁棒性，揭露目标检测模型的安全漏洞，属于大模型安全与对齐的研究范畴。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models
- **Authors:** Alvi Md Ishmam, Najibul Haque Sarker, Zaber Ibn Abdul Hakim, Chris Thomas
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21220](https://arxiv.org/abs/2601.21220)
- **Reason:** 针对多图像多模态大模型（MLLMs）的黑盒攻击问题，提出LAMP方法，通过注意力约束与跨图像传染约束生成通用对抗扰动，提升攻击成功率，属于大模型安全与对齐的对抗攻击研究。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Factored Causal Representation Learning for Robust Reward Modeling in RLHF
- **Authors:** Yupei Yang, Lin Yang, Wanxi Deng, Lin Qu, Fan Feng, Biwei Huang, Shikui Tu, Lei Xu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21350](https://arxiv.org/abs/2601.21350)
- **Reason:** 通过因果分解将上下文嵌入分为因果与非因果因子，提升RLHF奖励模型的鲁棒性，缓解长度偏差与奉承偏差等奖励黑客问题，改善对齐效果
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] Representation Unlearning: Forgetting through Information Compression
- **Authors:** Antonio Almudévar, Alfonso Ortega
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21564](https://arxiv.org/abs/2601.21564)
- **Reason:** 提出表示遗忘框架，通过信息压缩去除特定数据的影响，提升隐私与鲁棒性，比参数级方法更高效可靠，为模型遗忘提供了新范式
Score: 7
Field: 大模型安全与对齐

## 高效大模型训练与推理

### [Score: 9.0/10] ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation
- **Authors:** Zihao Huang, Jundong Zhou, Xingwei Qu, Qiyang Min, Ge Zhang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21420](https://arxiv.org/abs/2601.21420)
- **Reason:** 动态合并语义相似token为概念表示，实现token级计算分配，在语言、长上下文与多模态任务上优于标准MoE，同时减少注意力计算与KV缓存，是高效大模型的关键创新
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening
- **Authors:** Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21590](https://arxiv.org/abs/2601.21590)
- **Reason:** 提出无需训练的幂采样方法，减少LLM推理延迟10倍，属于高效大模型推理研究
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] HeRo-Q: A General Framework for Stable Low Bit Quantization via Hessian Conditioning
- **Authors:** Jinhao Zhang, Yunquan Zhang, Zicheng yan, Boyang Zhang, Jun Sun, Daning Cheng
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21626](https://arxiv.org/abs/2601.21626)
- **Reason:** 提出Hessian条件的低比特量化框架，解决“低误差高损失”问题，属于高效大模型压缩研究
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold
- **Authors:** Luca Benfenati, Matteo Risso, Andrea Vannozzi, Ahmet Caner Yüzüğüler, Lukas Cavigelli, Enrico Macii, Daniele Jahier Pagliari, Alessio Burrello
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21686](https://arxiv.org/abs/2601.21686)
- **Reason:** 提出Stiefel流形上的KV缓存低秩近似，优化LLM推理内存瓶颈，属于高效大模型推理研究
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training
- **Authors:** Xinwei Qiang, Hongmin Chen, Shixuan Sun, Jingwen Leng, Xin Liu, Minyi Guo
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21824](https://arxiv.org/abs/2601.21824)
- **Reason:** 提出确定性注意力调度策略，提高LLM训练吞吐量与可重复性，属于高效大模型训练研究
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Where Do the Joules Go? Diagnosing Inference Energy Consumption
- **Authors:** Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22076](https://arxiv.org/abs/2601.22076)
- **Reason:** 大规模测量生成式AI推理的时间与能耗，提出诊断框架，为推理能耗优化提供关键 insights，属于高效大模型推理的重要研究
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference
- **Authors:** Ziming Dong, Hardik Sharma, Evan O'Toole, Jaya Prakash Champati, Kui Wu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22132](https://arxiv.org/abs/2601.22132)
- **Reason:** 提出LLM提示引导SLM推理，大幅降低推理成本同时保持精度，属于高效大模型推理的成本优化关键技术
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.0/10] PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models
- **Authors:** Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21238](https://arxiv.org/abs/2601.21238)
- **Reason:** 针对自回归视觉生成模型的量化挑战（通道异常值、token动态激活等），提出后训练量化框架PTQ4ARVG，通过Gain-Projected Scaling等策略提升量化效率与性能，属于高效大模型训练与推理的量化技术研究。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention
- **Authors:** Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Ao Sun, Ziqi Yuan, Hao Zhou, Fandong Meng, Zhiyuan Liu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21444](https://arxiv.org/abs/2601.21444)
- **Reason:** 针对长视频理解的预填充阶段效率瓶颈，提出序列并行近似注意力框架Spava，通过分布式计算与负载均衡提升推理速度，属于高效大模型训练与推理的长视频优化研究。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Bi-Anchor Interpolation Solver for Accelerating Generative Modeling
- **Authors:** Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21542](https://arxiv.org/abs/2601.21542)
- **Reason:** 针对生成模型的ODE求解延迟问题，提出双锚插值求解器BA-solver，通过SideNet近似双向速度与锚点集成提升生成效率（10 NFEs达到100+ NFEs性能），属于高效大模型训练与推理的生成加速研究。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion
- **Authors:** Hanmo Chen, Chenghao Xu, Xu Yang, Xuan Chen, Cheng Deng
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21896](https://arxiv.org/abs/2601.21896)
- **Reason:** 提出PaFu-KV缓存策略，通过显著性估计保留重要token，提升自回归视频扩散的推理效率与质量，属于高效大模型训练与推理的缓存优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts
- **Authors:** Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21349](https://arxiv.org/abs/2601.21349)
- **Reason:** 提出低秩空间与Lipschitz控制的MoE路由框架，提升路由稳定性与专家专业化，在语言和视觉MoE任务上持续改进性能，属于高效大模型训练的关键优化
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving
- **Authors:** Chendong Song, Meixuan Wang, Hang Zhou, Hong Liang, Yuan Lyu, Zixi Chen, Yuwei Fan, Zijie Zhou
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21351](https://arxiv.org/abs/2601.21351)
- **Reason:** 推导了分拆LLM服务中Attention/FFN的最优比例，通过概率 workload 模型与仿真验证，提升吞吐量并减少空闲时间，为LLM推理优化提供了理论指导
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] L³: Large Lookup Layers
- **Authors:** Albert Tseng, Christopher De Sa
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21461](https://arxiv.org/abs/2601.21461)
- **Reason:** 将嵌入表扩展到解码器层，通过静态token路由与信息论分配优化，比dense和MoE模型在语言建模与下游任务上更优，为稀疏大模型提供了高效架构
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)
- **Authors:** Sagi Meir, Tommer D. Keidar, Noam Levi, Shlomi Reuveni, Barak Hirshberg
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21522](https://arxiv.org/abs/2601.21522)
- **Reason:** 提出ReD方法优化固定预算下LLM的推理覆盖，减少计算与成本，在HumanEval上验证了有效性，为LLM推理效率提升提供了实用方法
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking
- **Authors:** Yiming Wang, Zhuosheng Zhang, Rui Wang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21619](https://arxiv.org/abs/2601.21619)
- **Reason:** 提出T2方法优化并行思维的样本级并行度，减少计算成本，属于高效大模型推理研究
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models
- **Authors:** Stanislav Budzinskiy, Marian Gloser, Tolunay Yilmaz, Ying Hong Tham, Yuanyi Lin, Wenyi Fang, Fan Wu, Philipp Petersen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21623](https://arxiv.org/abs/2601.21623)
- **Reason:** 提出混合精度推理策略，提高Transformer推理准确性与效率，属于高效大模型推理研究
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Effective LoRA Adapter Routing using Task Representations
- **Authors:** Akash Dhasade, Anne-Marie Kermarrec, Igor Pavlovic, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21795](https://arxiv.org/abs/2601.21795)
- **Reason:** 提出任务表示的LoRA适配器路由，优化参数高效微调，属于高效大模型训练研究
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Rate-Distortion Optimization for Transformer Inference
- **Authors:** Anderson de Andrade, Alon Harell, Ivan V. Bajić
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22002](https://arxiv.org/abs/2601.22002)
- **Reason:** 提出Transformer推理的率失真优化框架，平衡比特率与精度，提升推理效率，属于高效大模型推理的重要进展
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Value-Based Pre-Training with Downstream Feedback
- **Authors:** Shuqi Ke, Giulia Fanti
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22108](https://arxiv.org/abs/2601.22108)
- **Reason:** 提出价值-based预训练框架，用下游任务反馈引导预训练，提升模型推理性能和效率，属于高效大模型训练的预训练优化
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.0/10] Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval
- **Authors:** Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21193](https://arxiv.org/abs/2601.21193)
- **Reason:** 针对文本到视频检索的效率瓶颈，提出生成式召回+密集重排框架GRDR，通过多视图语义ID与共享码本降低存储并加速推理，属于高效大模型训练与推理的检索效率优化研究。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Soft Quantization: Model Compression Via Weight Coupling
- **Authors:** Daniel T. Bernstein (Princeton University), Luca Di Carlo (Princeton University), David Schwab (Princeton University)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21219](https://arxiv.org/abs/2601.21219)
- **Reason:** 论文提出通过权重耦合实现模型压缩的软量化方法，无需显式量化，诱导权重分布离散化，混合精度且仅需两个超参数，比后训练量化效果更优，属于高效大模型训练与推理中的压缩技术。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing
- **Authors:** Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22151](https://arxiv.org/abs/2601.22151)
- **Reason:** 将神经网络转换为逻辑流提升边缘CPU推理效率，属于高效大模型推理的边缘设备优化
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy
- **Authors:** Jinhao Zhang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Wenlong Xia, Haoming Song, Youmin Gong, Jie Me
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22018](https://arxiv.org/abs/2601.22018)
- **Reason:** 提出轻量级3D扩散visuomotor策略PocketDP3，通过Diffusion Mixer替代传统重参数U-Net解码器，将模型参数压缩至现有方法的1%以下，支持两步推理加速，直接针对模型效率优化，符合高效大模型训练与推理的核心目标。
Score: 7
Field: 高效大模型训练与推理

## 深度学习可解释性

### [Score: 9.0/10] ECSEL: Explainable Classification via Signomial Equation Learning
- **Authors:** Adia Lumadjeng, Ilker Birbil, Erman Acar
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21789](https://arxiv.org/abs/2601.21789)
- **Reason:** 生成可解释的符号方程作为分类器，支持全局与局部解释，属于深度学习可解释性研究
Score: 9
Field: 深度学习可解释性

### [Score: 9.0/10] LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution
- **Authors:** Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21929](https://arxiv.org/abs/2601.21929)
- **Reason:** 提出低秩影响函数解决大规模训练数据归因的可扩展性问题，提升数据归因效率，属于深度学习可解释性中的关键技术
Score: 9
Field: 深度学习可解释性

### [Score: 8.0/10] Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning
- **Authors:** Hanmo Chen, Guangtao Lyu, Chenghao Xu, Jiexi Yan, Xu Yang, Cheng Deng
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21904](https://arxiv.org/abs/2601.21904)
- **Reason:** 提出PST学习框架，用Shapley-Taylor分解实现细粒度运动-语言对齐，属于深度学习可解释性中的Shapley值应用，提升跨模态检索精度。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions
- **Authors:** Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22150](https://arxiv.org/abs/2601.22150)
- **Reason:** 提出VI-Probe探究VLMs的感知与记忆机制，揭示响应规律，属于深度学习可解释性中的模型行为分析，指导模型优化。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Grounding and Enhancing Informativeness and Utility in Dataset Distillation
- **Authors:** Shaobo Wang, Yantai Yang, Guo Chen, Peiru Li, Kaixin Li, Yufa Zhou, Zhaorun Chen, Linfeng Zhang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21296](https://arxiv.org/abs/2601.21296)
- **Reason:** 基于博弈论利用Shapley值最大化数据集蒸馏的信息性，结合梯度范数优化效用，在ImageNet-1K上比SOTA提升6.1%，为数据集蒸馏提供了理论框架与有效方法
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models
- **Authors:** Konstantinos P. Panousis, Diego Marcos
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21944](https://arxiv.org/abs/2601.21944)
- **Reason:** 研究稀疏概念瓶颈模型的灵活性与可解释性权衡，提出Clarity指标和评估框架，深化了可解释性模型的理解
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] GeoRC: A Benchmark for Geolocation Reasoning Chains
- **Authors:** Mohit Talreja, Joshua Diao, Jim Thannikary James, Radu Casapu, Tejas Santanam, Ethan Mendes, Alan Ritter, Wei Xu, James Hays
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21278](https://arxiv.org/abs/2601.21278)
- **Reason:** 针对视觉语言模型（VLMs）地理定位推理的可解释性问题，构建GeoRC基准，评估推理链的可审计性与视觉属性提取能力，属于深度学习可解释性的推理链研究。
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification
- **Authors:** Akash Pandey (Northwestern University), Payal Mohapatra (Northwestern University), Wei Chen (Northwestern University), Qi Zhu (Northwestern University), Sinan Keten (Northwestern University)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21289](https://arxiv.org/abs/2601.21289)
- **Reason:** 论文提出符号-线性分解的可解释框架，结合原始时间序列和符号抽象，线性编码时间片段贡献，解决传统方法参考状态敏感和忽略序列依赖的问题，提升可解释性和分类性能，属于深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性

## 大模型新技术

### [Score: 9.0/10] Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models
- **Authors:** Sidney Bender, Marco Morik
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21851](https://arxiv.org/abs/2601.21851)
- **Reason:** 提出扩散自动编码器生成可解释反事实样本，用于基础模型shortcut learning缓解，属于大模型新技术中的扩散模型研究
Score: 9
Field: 大模型新技术

### [Score: 8.0/10] A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion
- **Authors:** Pu Cao, Yiyang Ma, Feng Zhou, Xuedan Yin, Qing Song, Lu Yang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21633](https://arxiv.org/abs/2601.21633)
- **Reason:** 针对可控扩散中自动编码器的权衡问题（重建保真度与生成友好性），分析gFID与重建指标对可控性的影响，揭示ImageNet-centric评估与可控扩散需求的 gap，属于大模型新技术的可控扩散研究。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Improving Classifier-Free Guidance of Flow Matching via Manifold Projection
- **Authors:** Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21892](https://arxiv.org/abs/2601.21892)
- **Reason:** 提出流匹配的classifier-free guidance改进方法，通过流形投影与Anderson加速提升生成质量，属于大模型新技术中的流模型研究。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks
- **Authors:** Luwei Sun (Tsinghua University), Dongrui Shen (Tsinghua University), Jianfe Li (Tsinghua University), Yulong Zhao (Tsinghua University), Han Feng (Tsinghua University)
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21242](https://arxiv.org/abs/2601.21242)
- **Reason:** 论文从比率函数近似角度理论分析扩散模型，用SignReLU网络推导L^p近似界和KL风险，给出扩散模型的泛化保证，对大模型新技术中的扩散模型理论理解有重要贡献。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models
- **Authors:** Ahmad Aghapour, Erhan Bayraktar, Ziqing Zhang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21943](https://arxiv.org/abs/2601.21943)
- **Reason:** 提出扩散模型的无维度收敛分析和损失自适应调度，提升扩散模型的采样效率和质量，属于大模型新技术中的扩散模型创新
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Making Foundation Models Probabilistic via Singular Value Ensembles
- **Authors:** Mehmet Ozgur Turkoglu, Dominik J. M\"uhlematter, Alexander Becker, Konrad Schindler, Helge Aasen
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22068](https://arxiv.org/abs/2601.22068)
- **Reason:** 提出奇异值集成让基础模型概率化，提升不确定性量化能力，属于大模型新技术中的概率化模型创新
Score: 8
Field: 大模型新技术

### [Score: 7.0/10] Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models
- **Authors:** Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21922](https://arxiv.org/abs/2601.21922)
- **Reason:** 利用视频扩散模型辅助零样本视频修复，解决时间一致性问题，属于大模型新技术中的扩散模型应用，训练-free且实用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models
- **Authors:** Archer Wang, Emile Anand, Yilun Du, Marin Soljačić
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22057](https://arxiv.org/abs/2601.22057)
- **Reason:** 提出判别器驱动的扩散模型实现无监督分解与重组，提升生成一致性与多样性，属于大模型新技术中的扩散模型应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Creative Image Generation with Diffusion Model
- **Authors:** Kunpeng Song, Ahmed Elgammal
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22125](https://arxiv.org/abs/2601.22125)
- **Reason:** 提出基于CLIP嵌入概率的创造性扩散模型，提升生成新颖性，属于大模型新技术中的扩散模型应用，对创造性生成有价值。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] PI-Light: Physics-Inspired Diffusion for Full-Image Relighting
- **Authors:** Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22135](https://arxiv.org/abs/2601.22135)
- **Reason:** 提出物理启发的扩散模型实现全图像重新照明，提升真实场景泛化性与物理 plausibility，属于大模型新技术中的扩散模型应用。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Memorization Control in Diffusion Models from Denoising-centric Perspective
- **Authors:** Thuy Phuong Vu, Mai Viet Hoang Do, Minhhuy Le, Dinh-Cuong Hoang, Phan Xuan Tan
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21348](https://arxiv.org/abs/2601.21348)
- **Reason:** 从去噪视角分析扩散模型的记忆问题，提出时间步采样策略平衡记忆与泛化，在图像和1D信号生成任务验证了有效性，为扩散模型的记忆控制提供了新视角
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Revisiting Diffusion Model Predictions Through Dimensionality
- **Authors:** Qing Jin, Chaoyang Wang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21419](https://arxiv.org/abs/2601.21419)
- **Reason:** 理论分析扩散模型预测目标与维度的关系，提出k-Diff自动学习最优预测参数，在 latent-space 与 pixel-space 图像生成任务中超过固定目标基线，为扩散模型优化提供了新视角
Score: 7
Field: 大模型新技术

### [Score: 6.0/10] Information Filtering via Variational Regularization for Robot Manipulation
- **Authors:** Jinhao Zhang, Wenlong Xia, Yaojia Wang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Haoming Song, Youmin Gong, Jie Me
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21926](https://arxiv.org/abs/2601.21926)
- **Reason:** 针对扩散visuomotor策略的冗余特征问题，提出变分正则化模块VR，通过时间步条件高斯和KL正则化构建自适应信息瓶颈，属于扩散模型的新技术优化，符合大模型新技术方向。
Score: 6
Field: 大模型新技术

## 多模态智能体

### [Score: 8.0/10] Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation
- **Authors:** Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan, Xuanwei Ye, Houhua Li, Xiaoming Wang
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21751](https://arxiv.org/abs/2601.21751)
- **Reason:** 提出DGNav框架解决视觉语言导航的粒度刚性问题，提升导航效率与精度，属于多模态智能体的导航研究。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Causal World Modeling for Robot Control
- **Authors:** Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.21998](https://arxiv.org/abs/2601.21998)
- **Reason:** 提出LingBot-VA融合视频世界模型与视觉语言预训练，提升机器人长horizon操作性能，属于多模态智能体的世界建模研究。
Score: 8
Field: 多模态智能体

### [Score: 8.0/10] Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving
- **Authors:** Linhan Wang, Zichong Yang, Chen Bai, Guoxiang Zhang, Xiaotong Liu, Xiaoyin Zheng, Xiao-Xiao Long, Chang-Tien Lu, Cheng Lu
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22032](https://arxiv.org/abs/2601.22032)
- **Reason:** 提出Drive-JEPA融合视频JEPA与多模态轨迹蒸馏，实现端到端驾驶，属于多模态智能体的自动驾驶研究，性能SOTA。
Score: 8
Field: 多模态智能体

### [Score: 7.0/10] Understanding Multimodal Complementarity for Single-Frame Action Anticipation
- **Authors:** Manuel Benavent-Lledo, Konstantinos Bacharidis, Konstantinos Papoutsakis, Antonis Argyros, Jose Garcia-Rodriguez
- **Published:** 2026-01-30
- **Link:** [https://arxiv.org/abs/2601.22039](https://arxiv.org/abs/2601.22039)
- **Reason:** 研究单帧动作预测的多模态互补性，提出AAG+框架超过视频-based方法，属于多模态智能体的动作anticipation研究。
Score: 7
Field: 多模态智能体

