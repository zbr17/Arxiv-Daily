<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-01-31</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>原生多模态大模型</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-01-31</h1>
<div class='meta-info'><p>更新于北京时间：2026-01-31 13:14:34</p>
<p>已自动阅读了 360 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：188882</p>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</h3>
<p><strong>Authors:</strong> Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Closing the Multimodal Reasoning Gap via Open Data-Centric Methods
Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu
Published: 2026-01-30
Link: https://arxiv.org/abs/2601.21821
Reason: 构建大规模多模态推理数据集并训练模型，提升多模态推理能力，属于原生多模态大模型中的推理方向，数据-centric方法显著缩小了推理差距。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21821' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出多模态深度研究MLLM，实现多轮多源视觉文本搜索，属于原生多模态大模型中的深度推理方向，性能超过现有模型。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> UEval: A Benchmark for Unified Multimodal Generation</h3>
<p><strong>Authors:</strong> Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 构建统一多模态生成的基准，评估图像与文本生成模型，属于原生多模态大模型中的基准测试方向，推动了多模态生成的评估标准化。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22155' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs</h3>
<p><strong>Authors:</strong> Haochen Zhang, Animesh Sinha, Felix Juefei-Xu, Haoyu Ma, Kunpeng Li, Zhipeng Fan, Meng Dong, Xiaoliang Dai, Tingbo Hou, Peizhao Zhang, Zecheng He</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对多轮对话图像生成的非马尔可夫问题，提出历史条件MLLM框架，改进多模态模型的多轮一致性与指令遵循能力，属于原生多模态大模型的重要研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20911' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought</h3>
<p><strong>Authors:</strong> Yu Huo, Siyu Zhang, Kun Zeng, Haoyue Liu, Owen Lee, Junlin Chen, Yuquan Lu, Yifu Guo, Yaodong Liang, Xiaoying Tang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出视觉CoT框架SoT，通过逐步形状组装提升多模态生成的组合结构能力，解决生成模型的组件数量、属性绑定等问题，属于原生多模态大模型的创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21081' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models</h3>
<p><strong>Authors:</strong> Chenyu Huang, Peng Ye, Xudong Tan, Jinhan Mu, Shenghe Zheng, Li Shen, Tao Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出子空间级模型融合方法，将推理能力注入VLMs，平衡推理与视觉能力，改进多模态模型的推理性能，属于原生多模态大模型的研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21187' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</h3>
<p><strong>Authors:</strong> Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于时空上下文学习的通用角色图像动画框架，解决身份保持与运动一致性问题，属于原生多模态大模型中的图像生成方向，提升了角色动画的泛化能力。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21716' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models</h3>
<p><strong>Authors:</strong> Junming Huang, Weiwei Xu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出多模态大模型用于3D内容的描述与生成，融合视觉语言与3D表示，属于原生多模态大模型中的3D内容处理方向，扩展了大模型的3D能力。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21798' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</h3>
<p><strong>Authors:</strong> Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin, Yuan Liu, Sibei Yang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出3D资产参考的扩散模型，生成与3D资产一致的图像，属于原生多模态大模型中的3D-2D对齐方向，扩展了多模态生成的能力。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22094' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</h3>
<p><strong>Authors:</strong> John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出音频驱动的对话头视频编辑，融合音频、视频与语言，属于原生多模态大模型中的多模态编辑方向，实现了精准的视频修改。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models</h3>
<p><strong>Authors:</strong> Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li, Beibei Dong, Jing Dong</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对统一多模态模型的“认知 gap”问题，提出内源性重提示机制及SEER训练框架，通过强化学习激活模型潜在评估能力并优化生成推理，在多模态任务上有效提升性能，直接对应原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.20305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing</h3>
<p><strong>Authors:</strong> Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出多任务VLM用于野生文档解析，融合视觉与语言处理，属于原生多模态大模型中的文档处理方向，提升了OCR的鲁棒性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21957' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning</h3>
<p><strong>Authors:</strong> Hang Ni, Weijia Zhang, Fei Wang, Zezhi Shao, Hao Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出多模态LLM的细粒度对齐与解纠缠交互，结合时间序列与可视化图，属于原生多模态大模型中的时间序列理解优化，显著提升了跨模态推理性能。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.21436' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> One-step Latent-free Image Generation with Pixel Mean Flows</h3>
<p><strong>Authors:</strong> Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出单步无潜在图像生成的像素MeanFlow，提升生成效率与质量，属于大模型新技术中的流模型方向，实现了流模型的突破。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22158' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion</h3>
<p><strong>Authors:</strong> Pu Cao, Yiyang Ma, Feng Zhou, Xuedan Yin, Qing Song, Lu Yang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 分析自编码器在可控扩散中的权衡，揭示ImageNet-centric评估与可控扩散需求的差距，提供模型选择的实践指导，属于大模型新技术（diffusion相关）的研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21633' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</h3>
<p><strong>Authors:</strong> Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 改进流匹配的分类器-free引导，通过流形投影提升生成稳定性与质量，属于大模型新技术中的流匹配方向，对扩散/流模型的可控生成有重要贡献。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21892' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models</h3>
<p><strong>Authors:</strong> Archer Wang, Emile Anand, Yilun Du, Marin Soljačić</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出鉴别器驱动的扩散模型用于无监督分解与重组，提升生成的可控性与多样性，属于大模型新技术中的扩散模型扩展方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22057' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</h3>
<p><strong>Authors:</strong> Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出物理启发的扩散模型用于全图像重新照明，提升生成的物理合理性，属于大模型新技术中的物理引导扩散方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22135' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Revisiting Diffusion Model Predictions Through Dimensionality</h3>
<p><strong>Authors:</strong> Qing Jin, Chaoyang Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文从维度视角重新审视扩散模型的预测目标，提出k-Diff自动学习最优预测参数，属于大模型新技术中扩散模型的理论与实践改进，显著提升了生成性能。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21419' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models</h3>
<p><strong>Authors:</strong> Ahmad Aghapour, Erhan Bayraktar, Ziqing Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究扩散模型的无维度收敛性，提出基于熵的损失自适应调度方法，改善采样质量并降低计算成本，属于大模型新技术中扩散模型的理论和方法创新
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21943' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution</h3>
<p><strong>Authors:</strong> Zhengbo Jiao, Hongyu Xian, Qinglong Wang, Yunpu Ma, Zhebo Wang, Zifan Zhang, Dezhang Kong, Meng Han</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Policy of Thoughts框架，将LLM推理转化为实例内在线优化过程，通过Group Relative Policy Optimization更新LoRA适配器，显著提升复杂推理性能（4B模型超过GPT-4o），对应大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.20379' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Creative Image Generation with Diffusion Model</h3>
<p><strong>Authors:</strong> Kunpeng Song, Ahmed Elgammal</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于扩散模型的创意图像生成，通过低概率区域生成新颖内容，属于大模型新技术中的扩散模型创意应用方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22125' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Memorization Control in Diffusion Models from Denoising-centric Perspective</h3>
<p><strong>Authors:</strong> Thuy Phuong Vu, Mai Viet Hoang Do, Minhhuy Le, Dinh-Cuong Hoang, Phan Xuan Tan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文从去噪视角研究扩散模型的记忆控制，通过调整时间步采样策略平衡记忆与泛化，属于大模型新技术中扩散模型的关键改进，对扩散模型的记忆管理有理论与实践价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.21348' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering</h3>
<p><strong>Authors:</strong> Dongxuan Zhu, Ly Tran Ho Khanh, Andy Yat-Ming Cheung, Man-Chung Yue, Viet Anh Nguyen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出STARS方法，在Stiefel流形上优化激活引导方向，实现训练-free的推理时生成多样性提升，属于大模型新技术中的生成模型优化方向
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22010' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Depth Delusion: Why Transformers Should Be Wider, Not Deeper</h3>
<p><strong>Authors:</strong> Md Muhtasim Munif Fahim (University of Dhaka), Md Rezaul Karim (University of Dhaka)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Derives architecture-conditioned scaling laws for transformers, showing width should grow faster than depth to avoid "Depth Delusion"—directly addresses network architecture, a high-priority subfield of deep learning theory.
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20994' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Why Adam Works Better with β₁ = β₂: The Missing Gradient Scale Invariance Principle</h3>
<p><strong>Authors:</strong> Alberto Fern\'andez-Hern\'andez, Cristian P\'erez-Corral, Jose I. Mestre, Manuel F. Dolz, Enrique S. Quintana-Ort\'i</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 揭示Adam优化器中β₁=β₂时的梯度尺度不变性原理，从理论上解释其性能提升机制，对深度学习理论中优化器的设计与参数选择有重要指导意义
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21739' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Vision KAN: Towards an Attention-Free Backbone for Vision with Kolmogorov-Arnold Networks</h3>
<p><strong>Authors:</strong> Zhuoqin Yang, Jiansong Zhang, Xiaoling Luo, Xu Wu, Zheng Lu, Linlin Shen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于KAN的无注意力视觉骨干网络，解决注意力机制的二次复杂度问题，属于深度学习理论（网络架构）的创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21541' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Monotone Optimisation with Learned Projections</h3>
<p><strong>Authors:</strong> Ahmed Rashwan (University of Oxford), Keith Briggs (University of Oxford), Chris Budd (University of Oxford), Lisa Kreusser (University of Oxford)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Introduces HM-RI networks—structured neural architectures enforcing monotonicity/homogeneity—for integrating learned models into monotone optimisation. Combines deep learning architecture design with optimization, core to deep learning theory.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20983' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity</h3>
<p><strong>Authors:</strong> Mutian Yang, Zisen Zhan, Yutong Chen, Haolin Li, Kaiwen Wang, Kaili Zheng, Yuguang Wang, Qi Wang, Jiandong Gao, Ji Wu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 从梯度相似性角度建立理论框架解释灾难性遗忘机制，提出协作神经学习方法，对深度学习理论中持续学习的优化机制研究有重要价值
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21577' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer</h3>
<p><strong>Authors:</strong> Chenrui Xu, Wenjing Yan, Ying-Jun Angela Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 结合Fisher信息几何提出结构化动量正交优化器，实现对损失 landscape几何结构的自适应预处理，对深度学习理论中优化器的泛化设计有贡献
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21750' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions</h3>
<p><strong>Authors:</strong> Jinhang Chai, Xuyuan Liu, Elynn Chen, Yujun Yan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究结构化矩阵估计的迁移学习框架，提出锚定交替投影估计器并建立确定性误差边界，为迁移学习中的表示增长问题提供理论解决方案，符合深度学习理论方向
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21873' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimistic Transfer under Task Shift via Bellman Alignment</h3>
<p><strong>Authors:</strong> Jinhang Chai, Enpei Zhang, Elynn Chen, Yujun Yan</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对强化学习中的任务迁移问题，提出基于Bellman对齐的重加权目标方法，建立了依赖任务偏移复杂度的regret界，推动了迁移强化学习的理论发展，符合深度学习理论方向
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21924' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization</h3>
<p><strong>Authors:</strong> Chuanyang Zheng, Jiankai Sun, Yihang Gao, Chi Wang, Yuehao Wang, Jing Xiong, Liliang Ren, Bo Peng, Qingmei Wang, Xiaoran Shang, Mac Schwager, Anderson Schneider, Yuriy Nevmyvaka, Xiaodong Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 用流形优化中的测地线更新统一Pre-Norm和Post-Norm，提升Transformer模型性能且不增加计算成本，属于深度学习理论中的网络架构方向
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22095' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Value-Based Pre-Training with Downstream Feedback</h3>
<p><strong>Authors:</strong> Shuqi Ke, Giulia Fanti</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出V-Pretraining方法，用下游任务反馈引导预训练任务选择，在不使用下游标签的情况下提升模型推理与视觉任务性能，属于深度学习理论中的预训练方向
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22108' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</h3>
<p><strong>Authors:</strong> Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出PRISM框架自适应计算矩阵函数（如平方根、逆根），加速Shampoo和Muon优化器的训练，属于深度学习理论中的优化器方向
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22137' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Variance & Greediness: A comparative study of metric-learning losses</h3>
<p><strong>Authors:</strong> Donghuo Zeng, Hao Niu, Zhi Li, Masato Taya</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 比较七种度量学习损失的方差和贪心性，分析其对嵌入几何与优化动态的影响，提供损失选择的实践指导，属于深度学习理论的研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21450' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> When Gradient Optimization Is Not Enough: $\dagger$ Dispersive and Anchoring Geometric Regularizer for Multimodal Learning</h3>
<p><strong>Authors:</strong> Zixuan Xia, Hao Wang, Pengcheng Weng, Yanyu Qian, Yangxin Xu, William Dan, Fei Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出针对多模态学习的几何正则化框架，解决模态内表示坍塌与跨模态不一致问题，属于深度学习理论中的表示学习方向，对多模态模型的表示几何优化有重要贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21670' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Just Noticeable Difference Modeling for Deep Visual Features</h3>
<p><strong>Authors:</strong> Rui Zhao, Wenrui Li, Lin Zhu, Yajing Zheng, Weisi Lin</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出深度视觉特征的JND建模，用于特征质量控制与资源约束下的优化，属于深度学习理论中的特征表示方向，对特征学习的实用性有贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21933' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Finetune-Informed Pretraining Boosts Downstream Performance</h3>
<p><strong>Authors:</strong> Atik Faysal, Mohammad Rostami, Reihaneh Gh. Roshan, Nikhil Muralidhar, Huaxia Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出微调感知的预训练策略，优化预训练与微调的对齐，属于深度学习理论中的pretraining-finetuning策略方向，提升了下游任务性能。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20884' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Faster Predictive Coding Networks via Better Initialization</h3>
<p><strong>Authors:</strong> Luca Pinchetti, Simon Frieder, Thomas Lukasiewicz, Tommaso Salvatori</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出预测编码网络的初始化方法，提升收敛速度与性能，属于深度学习理论中的初始化策略方向，对神经科学启发的模型有贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20895' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A Theory of Universal Agnostic Learning</h3>
<p><strong>Authors:</strong> Steve Hanneke (Carnegie Mellon University), Shay Moran (Weizmann Institute of Science)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Provides a complete theory of optimal universal rates for binary classification in the agnostic setting, extending realizable-case theory and identifying key combinatorial structures governing convergence—directly relevant to deep learning theory's focus on learning fundamentals.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.20961' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations</h3>
<p><strong>Authors:</strong> Henry Kvinge (University of Minnesota), Andrew Aguilar (University of Minnesota), Nayda Farnsworth (University of Minnesota), Grace O'Brien (University of Minnesota), Robert Jasper (University of Minnesota), Sarah Scullen (University of Minnesota), Helen Jenne (University of Minnesota)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Investigates whether narrow neural networks capture abstract group-theoretic structures when trained on group operations—relevant to deep learning theory's focus on what models learn and representation capacity.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21150' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Model-Free Neural State Estimation in Nonlinear Dynamical Systems: A Comparative Study of Neural Architectures and Classical Filters</h3>
<p><strong>Authors:</strong> Zhuochen Liu (University of California, Berkeley), Hans Walker (University of California, Berkeley), Rahul Jain (University of California, Berkeley)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Compares Transformer, state-space neural networks, and recurrent architectures with classical filters for state estimation—evaluates network architecture performance in nonlinear systems, relevant to deep learning theory.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21266' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Partial Feedback Online Learning</h3>
<p><strong>Authors:</strong> Shihao Shao, Cong Fang, Zhouchen Lin, Dacheng Tao</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文研究部分反馈在线学习的minimax遗憾，提出PFLdim与PMSdim维度，属于深度学习理论中的在线学习理论，完整刻画了确定性与随机学习者的性能边界。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21462' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fast and Geometrically Grounded Lorentz Neural Networks</h3>
<p><strong>Authors:</strong> Robert van der Klis, Ricardo Chávez Torres, Max van Spengler, Yuhui Ding, Thomas Hofmann, Pascal Mettes</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文改进Lorentz线性层的几何属性，解决输出范数的对数缩放问题，属于深度学习理论中的双曲神经网络优化，提升了计算效率与几何一致性。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21529' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dependence of Equilibrium Propagation Training Success on Network Architecture</h3>
<p><strong>Authors:</strong> Qingshan Wang, Clara C. Wanjura, Florian Marquardt</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究网络架构（如局部连接 lattice）对平衡传播训练性能的影响，为神经形态计算中的架构设计提供理论依据，属于深度学习理论中的网络架构方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21945' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models</h3>
<p><strong>Authors:</strong> Chuan-Shen Hu (University of California, Los Angeles)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Uses sheaf theory and topological data analysis to model feature diffusion/aggregation in graph neural networks—provides a theoretical framework for analyzing attention mechanisms, a key part of deep learning theory.
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.21207' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher</h3>
<p><strong>Authors:</strong> Yisheng Zhong (Tsinghua University), Zhengbang Yang (Tsinghua University), Zhuangdi Zhu (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Combines distillation with contextualized teachers for efficient LLM unlearning—addresses both efficient training (distillation) and LLM safety/alignment (unlearning), a high-priority area.
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention</h3>
<p><strong>Authors:</strong> Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang, Zifeng Cheng, Fei Shen, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出路径级干预框架，切断有害语义的因果传播，提升大模型安全鲁棒性，属于大模型安全与对齐中的安全增强方向，解决了局部干预的局限性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21900' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges</h3>
<p><strong>Authors:</strong> Chen Feng, Minghe Shen, Ananth Balashankar, Carsten Gerner-Beuerle, Miguel R. D. Rodrigues</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出鲁棒的LLM统计评估方法，处理不完美法官的噪声，属于大模型安全与对齐中的评估方向，提升了LLM评估的可靠性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20913' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Factored Causal Representation Learning for Robust Reward Modeling in RLHF</h3>
<p><strong>Authors:</strong> Yupei Yang, Lin Yang, Wanxi Deng, Lin Qu, Fan Feng, Biwei Huang, Shikui Tu, Lei Xu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文用因果分解表示学习将奖励模型嵌入分解为因果与非因果因素，改进RLHF的奖励模型鲁棒性，属于大模型安全与对齐中的奖励建模关键优化，有效解决了奖励 hacking 问题。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21350' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shaping capabilities with token-level data filtering</h3>
<p><strong>Authors:</strong> Neil Rathi, Alec Radford</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文通过token级数据过滤在预训练阶段塑造模型能力，去除医疗等不需要的能力，属于大模型安全与对齐中的预训练控制，高效且鲁棒。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21571' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Yejin Kim, Dongjun Hwang, Sungmin Cha, Junsuk Choe</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出无训练的大视觉语言模型遗忘方法，通过削弱知识向量去除特定数据影响，平衡遗忘效果与计算效率，属于大模型安全与对齐中的隐私保护技术
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21794' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning</h3>
<p><strong>Authors:</strong> Chengyi Cai, Zesheng Ye, Peike Li, Bo Han, Jianzhong Qi, Feng Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对多模态大模型的遗忘问题，提出视觉引导的关键token正则化方法，有效移除私有信息同时保持性能与连贯性，属于大模型安全与对齐方向
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning</h3>
<p><strong>Authors:</strong> Haoran Tang, Rajiv Khanna</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 通过对比学习塑造表征空间实现LLM遗忘，减少遗忘-保留知识的纠缠，理论与实验结合验证有效性，属于大模型安全与对齐方向
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22028' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> StepShield: When, Not Whether to Intervene on Rogue Agents</h3>
<p><strong>Authors:</strong> Gloria Felicia (University of Virginia), Michael Eniolade (University of the Cumberlands), Jinfeng He (Cornell University), Zitha Sasindran (Indian Institute of Science Bangalore), Hemant Kumar (University of Arizona), Milan Hussain Angati (California State University Northridge), Sandeep Bandarupalli (University of Cincinnati)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出StepShield基准评估智能体安全的早期干预，量化干预时间与成本，为智能体安全系统的设计提供关键指标，属于大模型安全与对齐方向
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22136' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> BadDet+: Robust Backdoor Attacks for Object Detection</h3>
<p><strong>Authors:</strong> Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对目标检测的后门攻击，提出BadDet+框架解决现有方法的物理鲁棒性与假设不切实际问题，揭示目标检测模型的安全漏洞，属于大模型安全与对齐的研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21066' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models</h3>
<p><strong>Authors:</strong> Alvi Md Ishmam, Najibul Haque Sarker, Zaber Ibn Abdul Hakim, Chris Thomas</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对多图像MLLMs的黑盒通用对抗扰动方法，提出注意力约束与跨图像传染约束，提升攻击成功率，属于大模型安全与对齐的研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21220' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Representation Unlearning: Forgetting through Information Compression</h3>
<p><strong>Authors:</strong> Antonio Almudévar, Alfonso Ortega</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文通过表示空间的信息压缩实现模型遗忘，属于大模型安全与对齐中的机器非学习，解决了参数修改的不稳定问题，提升了遗忘的可靠性与效率。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.21564' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies</h3>
<p><strong>Authors:</strong> Gray Cox</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出多模型对话框架，结合和平研究传统（如利益协商、冲突转化）测试AI对齐策略，评估了Claude、Gemini、GPT-4o的对齐能力并生成emergent insights，对应大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.20604' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> HeRo-Q: A General Framework for Stable Low Bit Quantization via Hessian Conditioning</h3>
<p><strong>Authors:</strong> Jinhao Zhang Yunquan Zhang, Zicheng yan, Boyang Zhang, Jun Sun, Daning Cheng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对低比特量化的“低误差、高损失”悖论，提出基于Hessian条件的鲁棒量化框架，显著提升LLM低比特量化的稳定性与性能，属于高效大模型训练与推理中的压缩技术
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21626' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</h3>
<p><strong>Authors:</strong> Ziming Dong, Hardik Sharma, Evan O'Toole, Jaya Prakash Champati, Kui Wu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Shepherding框架，用LLM生成短hint引导SLM推理，实现42-94%的成本降低同时保持精度，是高效大模型推理的重要创新
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22132' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models</h3>
<p><strong>Authors:</strong> Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出针对自回归视觉生成模型的后训练量化方法，解决通道级异常值、令牌级动态激活等问题，提升推理效率，属于高效大模型训练与推理。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21238' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention</h3>
<p><strong>Authors:</strong> Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Ao Sun, Ziqi Yuan, Hao Zhou, Fandong Meng, Zhiyuan Liu</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出Spava框架，通过序列并行近似注意力加速长视频理解，解决长视频推理的计算瓶颈，属于高效大模型训练与推理。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21444' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Bi-Anchor Interpolation Solver for Accelerating Generative Modeling</h3>
<p><strong>Authors:</strong> Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出BA-solver，通过双锚点插值加速流匹配模型的ODE求解，减少推理延迟同时保持性能，属于高效大模型训练与推理。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21542' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Soft Quantization: Model Compression Via Weight Coupling</h3>
<p><strong>Authors:</strong> Daniel T. Bernstein (Princeton University), Luca Di Carlo (Princeton University), David Schwab (Princeton University)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Proposes weight coupling during training to induce mixed-precision weight discretization, outperforming post-training quantization—directly addresses model compression, a core subfield of efficient large model training and inference.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21219' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts</h3>
<p><strong>Authors:</strong> Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文提出低秩与Lipschitz控制的MoE路由框架，改进了MoE的路由稳定性与专家 specialization，属于高效大模型训练与推理中的MoE架构优化，显著提升了模型性能与效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21349' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation</h3>
<p><strong>Authors:</strong> Zihao Huang, Jundong Zhou, Xingwei Qu, Qiyang Min, Ge Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文通过自适应token到概念的压缩实现MoE的隐式计算分配，减少Attention计算与KV缓存，属于高效大模型训练与推理中的MoE优化，提升了多任务性能与效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21420' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> L³: Large Lookup Layers</h3>
<p><strong>Authors:</strong> Albert Tseng, Christopher De Sa</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文将嵌入表推广到decoder层，提出Large Lookup Layer实现稀疏性，属于高效大模型训练与推理中的架构优化，性能优于dense与MoE模型。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21461' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</h3>
<p><strong>Authors:</strong> Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出无需训练和验证器的LLM推理方法，通过分布锐化提升推理性能并降低延迟，属于高效大模型推理的关键优化技术
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21590' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold</h3>
<p><strong>Authors:</strong> Luca Benfenati, Matteo Risso, Andrea Vannozzi, Ahmet Caner Y\"uz\"ug\"uler, Lukas Cavigelli, Enrico Macii, Daniele Jahier Pagliari, Alessio Burrello</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对KV缓存的内存瓶颈问题，提出Stiefel流形上的低秩近似学习方法，直接优化解码器层输出重建误差，有效提升LLM推理的内存效率
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21686' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rate-Distortion Optimization for Transformer Inference</h3>
<p><strong>Authors:</strong> Anderson de Andrade, Alon Harell, Ivan V. Bajić</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出率失真优化框架用于Transformer推理的表示压缩，平衡比特率与精度，显著降低计算与内存成本，符合高效大模型训练与推理方向
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22002' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Making Foundation Models Probabilistic via Singular Value Ensembles</h3>
<p><strong>Authors:</strong> Mehmet Ozgur Turkoglu, Dominik J. M\"uhlematter, Alexander Becker, Konrad Schindler, Helge Aasen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出奇异值集成（SVE）实现基础模型的参数高效不确定性估计，仅增加1%参数达到显式集成的效果，显著提升模型校准度，符合高效大模型训练与推理方向
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22068' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning</h3>
<p><strong>Authors:</strong> Zhenxuan Fan, Jie Cao, Yang Dai, Zheqi Lv, Wenqiao Zhang, Zhongle Xie, Peng LU, Beng Chin Ooi</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出双粒度CoT压缩框架，结合语义抽象与token剪枝解决CoT高延迟问题，在MATH-500上用30.7%更少token取得7.6个百分点性能提升，对应高效大模型训练与推理的high compression方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20467' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</h3>
<p><strong>Authors:</strong> Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出GRDR框架，通过生成式召回和 dense reranking 提升文本到视频检索的效率，解决现有方法的语义模糊与跨模态错位问题，属于高效大模型训练与推理。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21193' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HiFi-Mesh: High-Fidelity Efficient 3D Mesh Generation via Compact Autoregressive Dependence</h3>
<p><strong>Authors:</strong> Yanfeng Li, Tao Tan, Qingquan Gao, Zhiwen Cao, Xiaohong liu, Yue Sun</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出LANE框架，通过紧凑自回归依赖提升3D mesh生成效率，解决现有方法的资源利用率低与推理慢问题，属于高效大模型训练与推理。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21314' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion</h3>
<p><strong>Authors:</strong> Hanmo Chen, Chenghao Xu, Xu Yang, Xuan Chen, Cheng Deng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于显著性估计的KV缓存策略，优化自回归视频扩散的推理效率与质量，属于高效大模型训练与推理中的推理优化方向，降低了内存占用。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21896' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization</h3>
<p><strong>Authors:</strong> Rongzheng Wang, Yihong Huang, Muquan Li, Jiakai Li, Di Liang, Bob Simons, Pei Ke, Shuang Liang, Ke Qin</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出动态感知优化生成高效求解器，提升LLM驱动的启发式设计效率，属于高效大模型训练与推理中的求解器优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20868' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Is Parameter Isolation Better for Prompt-Based Continual Learning?</h3>
<p><strong>Authors:</strong> Jiangyang Li, Chenhao Ding, Songlin Dong, Qiang Wang, Jianchao Zhao, Yuhang He, Yihong Gong</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究prompt-based持续学习的参数隔离，提出共享prompt池策略，属于高效大模型训练与推理中的持续学习方向，提升了参数利用率。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.20894' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving</h3>
<p><strong>Authors:</strong> Chendong Song, Meixuan Wang, Hang Zhou, Hong Liang, Yuan Lyu, Zixi Chen, Yuwei Fan, Zijie Zhou</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文通过理论推导分拆LLM服务中Attention与FFN的最优比例，最大化系统吞吐量，属于高效大模型训练与推理中的推理优化，提供了理论指导与实验验证。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.21351' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Where Do the Joules Go? Diagnosing Inference Energy Consumption</h3>
<p><strong>Authors:</strong> Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 大规模测量生成式AI模型的推理能耗，分析不同任务、模型和GPU的能耗差异，为高效推理的硬件与算法优化提供关键依据，符合高效大模型训练与推理方向
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22076' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy</h3>
<p><strong>Authors:</strong> Jinhao Zhang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Wenlong Xia, Haoming Song, Youmin Gong, Jie Me</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对3D视觉运动扩散政策存在的模型冗余问题，提出轻量级Diffusion Mixer替代厚重U-Net解码器，实现模型参数压缩（仅为现有方法1%以下）与两步高效推理，在多个机器人基准上达SOTA，契合高效大模型训练与推理对压缩和实时部署的需求。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22018' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models</h3>
<p><strong>Authors:</strong> Sidney Bender, Marco Morik</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 结合扩散自编码器与可分解字典学习，实现高效梯度-free的counterfactual生成，用于缓解基础模型的shortcut learning，属于深度学习可解释性中基于反事实的解释方法
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution</h3>
<p><strong>Authors:</strong> Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出低秩影响函数解决大规模训练数据归因的存储和计算瓶颈，在保持归因质量的同时实现20倍存储 reduction和查询加速，是深度学习可解释性中训练数据归因方向的重要突破
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21929' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models</h3>
<p><strong>Authors:</strong> Rishi Upadhyay, Howard Zhang, Jim Solomon, Ayush Agrawal, Pranay Boreddy, Shruti Satya Narayana, Yunhao Ba, Alex Wong, Celso M de Melo, Achuta Kadambi</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出WorldBench基准，诊断世界模型的物理推理能力，解决现有基准的概念纠缠问题，属于深度学习可解释性的研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning</h3>
<p><strong>Authors:</strong> Hanmo Chen, Guangtao Lyu, Chenghao Xu, Jiexi Yan, Xu Yang, Cheng Deng</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 利用金字塔Shapley-Taylor学习实现细粒度运动-语言检索，属于深度学习可解释性中的Shapley值扩展应用，提升了跨模态对齐的精细度。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21904' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification</h3>
<p><strong>Authors:</strong> Akash Pandey (University of Illinois at Urbana-Champaign), Payal Mohapatra (University of Illinois at Urbana-Champaign), Wei Chen (University of Illinois at Urbana-Champaign), Qi Zhu (University of Illinois at Urbana-Champaign), Sinan Keten (University of Illinois at Urbana-Champaign)</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> Proposes a symbolic-linear decomposition framework for interpretable time series classification, providing white-box explanations of temporal segment importance—directly aligns with deep learning explainability.
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21289' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Grounding and Enhancing Informativeness and Utility in Dataset Distillation</h3>
<p><strong>Authors:</strong> Shaobo Wang, Yantai Yang, Guo Chen, Peiru Li, Kaixin Li, Yufa Zhou, Zhaorun Chen, Linfeng Zhang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 论文将Shapley Value归因用于数据集蒸馏的信息性最大化，结合梯度范数优化效用，解决了数据集蒸馏中信息性与效用的平衡问题，对深度学习可解释性中的Shapley Value应用及数据集蒸馏的理论与实践有重要改进。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21296' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ECSEL: Explainable Classification via Signomial Equation Learning</h3>
<p><strong>Authors:</strong> Adia Lumadjeng, Ilker Birbil, Erman Acar</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出基于符号方程的可解释分类框架，直接学习闭合形式的解释性表达式，在保持分类性能的同时实现全局与局部可解释性，属于深度学习可解释性的重要进展
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21789' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models</h3>
<p><strong>Authors:</strong> Konstantinos P. Panousis, Diego Marcos</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 研究稀疏概念瓶颈模型中灵活性与可解释性的权衡，提出Clarity评估框架，对可解释性模型的设计与优化有重要指导意义，符合深度学习可解释性方向
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21944' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> GeoRC: A Benchmark for Geolocation Reasoning Chains</h3>
<p><strong>Authors:</strong> Mohit Talreja, Joshua Diao, Jim Thannikary James, Radu Casapu, Tejas Santanam, Ethan Mendes, Alan Ritter, Wei Xu, James Hays</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 提出地理定位推理链基准，评估VLM的推理可解释性，揭示VLM在细粒度视觉属性提取中的不足，属于深度学习可解释性的研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.21278' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution</h3>
<p><strong>Authors:</strong> Le Zhang, Yixiong Xiao, Xinjiang Lu, Jingjia Cao, Yusai Zhao, Jingbo Zhou, Lang An, Zikan Feng, Wanxiang Sha, Yu Shi, Congxi Xiao, Jian Xiong, Yankai Zhang, Hua Wu, Haifeng Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对GUI代理的通用任务执行需求，提出数据构建管道（开源+自动合成）和 decoupled训练范式（SFT+GRPO），基于MoE backbone在ScreenSpot-V2等基准取得SOTA，直接对应多模态智能体方向的GUI Agent任务。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.20380' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation</h3>
<p><strong>Authors:</strong> Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan, Xuanwei Ye, Houhua Li, Xiaoming Wang</p>
<p><strong>Published:</strong> 2026-01-30</p>
<p><strong>Reason:</strong> 针对视觉语言导航的粒度刚性问题，提出动态拓扑感知框架，提升导航效率与精度，属于多模态智能体中的视觉语言导航方向，优化了导航中的空间推理。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.21751' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>