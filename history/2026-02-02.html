<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-02</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>多模态智能体</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型新技术</a>
<a href='#' >原生多模态大模型</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-02</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-02 13:35:05</p>
<p>已自动阅读了 345 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：178909</p>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 10.0/10]</span> Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution</h3>
<p><strong>Authors:</strong> Hongze Mi (), Yibo Feng (), WenJie Lu (), Song Cao (), Jinyuan Li (), Yanming Li (), Xuelin Zhang (), Haotian Luo (), Songyang Peng (), He Cui (), Tengfei Tian (), Jun Fang (), Hua Chai (), Naiqiang Tan ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出无需训练的达尔文记忆系统，通过生存竞争机制动态调整GUI Agent记忆，解决跨应用长horizon任务的上下文限制与记忆污染问题，实验显示成功率先18%，完全匹配GUI Agent方向。
Score: 10
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22528' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 10.0/10]</span> Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training</h3>
<p><strong>Authors:</strong> Linjia Kang (), Zhimin Wang (), Yongkang Zhang (), Duo Wu (), Jinghe Wang (), Ming Ma (), Haopeng Yan (), Zhi Wang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出自适应难度的数据生成框架MobileGen，生成与GUI代理能力匹配的交互轨迹，实验显示代理性能提升1.57倍，完全匹配移动GUI Agent方向。
Score: 10
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22781' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning</h3>
<p><strong>Authors:</strong> Xiangyu Zeng, Zhiqiu Zhang, Yuhan Zhu, Xinhao Li, Zikang Wang, Changlian Ma, Qingyu Zhang, Zizheng Huang, Kun Ouyang, Tianxiang Jiang, Ziang Yan, Yi Wang, Hongjie Zhang, Yali Wang, Limin Wang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对长视频多跳推理中的证据查找问题，提出支持迭代发现salient视觉线索、细粒度关键段检查和自适应终止的框架，构建Seeker-173K工具交互轨迹数据集，提升长视频多跳推理能力，符合多模态智能体的研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.23224' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents</h3>
<p><strong>Authors:</strong> Zehong Wang (), Fang Wu (), Hongru Wang (), Xiangru Tang (), Bolian Li (), Zhenfei Yin (), Yijun Ma (), Yiyang Li (), Weixiang Sun (), Xiusi Chen (), Yanfang Ye ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 分析LLM代理长horizon规划失败的原因，提出FLARE方法引入未来感知前瞻与奖励估计，实验显示LLaMA-8B+FLARE超过GPT-4o。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22311' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents</h3>
<p><strong>Authors:</strong> Jiaxuan Gao (), Jiaao Chen (), Chuyi He (), Wei-Chen Wang (), Shusheng Xu (), Hanrui Wang (), Di Jin (), Yi Wu ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出自进化合成数据与验证奖励RL的后训练框架，提升多轮工具使用代理性能，实验在tau^2-bench上Airline任务达到73.0% pass^1。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22607' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference</h3>
<p><strong>Authors:</strong> Emilien Bir\'e (), Mar\'ia Santos (), Kai Yuan ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出Q函数动作排序方法，在WebVoyager上提升Qwen2.5-VL-7B成功率从38.8%到55.7%，提升智能体动作选择效果。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22701' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement</h3>
<p><strong>Authors:</strong> Libin Qiu (), Zhirong Gao (), Junfu Chen (), Yuhang Ye (), Weizhi Huang (), Xiaobo Xue (), Wenkai Qiu (), Shuo Tang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出从轨迹中提取可复用expertise的AutoRefine框架，提升LLM代理持续性能，实验在ALFWorld达到98.4% pass^1。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2601.22758' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Symmetry Breaking in Transformers for Efficient and Interpretable Training</h3>
<p><strong>Authors:</strong> Eva Silverstein, Daniel Kunin, Vasudev Shyam</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对Transformer注意力机制的冗余旋转自由度问题，提出对称性-breaking协议，通过batchwise采样的偏差插入偏好方向，提升简单优化器性能，同时增强注意力头的语义可解释性，符合深度学习理论中优化器与网络架构的研究方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22257' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Riemannian Lyapunov Optimizer: A Unified Framework for Optimization</h3>
<p><strong>Authors:</strong> Yixuan Wang, Omkar Sudhir Patil, Warren E. Dixon</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 从控制理论角度提出优化器统一框架，将优化视为Riemannian参数流形上的离散控制动态系统，通过构建Lyapunov函数保证收敛，统一经典优化器，符合深度学习理论中优化器的研究方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22284' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mano: Restriking Manifold Optimization for LLM Training</h3>
<p><strong>Authors:</strong> Yufei Gu, Zeke Xie</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 将流形优化应用于LLM训练，提出Mano优化器：通过投影动量到切空间并约束在Oblique流形上，解决AdamW忽略结构信息与Muon丢失曲率的问题，显著提升训练效率与性能，属于深度学习理论中的优化器研究。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23000' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> YuriiFormer: A Suite of Nesterov-Accelerated Transformers</h3>
<p><strong>Authors:</strong> Aleksandr Zimin (), Yury Polyanskiy (), Philippe Rigollet ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 将Transformer层解释为优化算法迭代，提出Nesterov加速的Transformer架构，实验在TinyStories和OpenWebText上超过nanoGPT，理论与实践结合。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23236' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning</h3>
<p><strong>Authors:</strong> Jian Shi, Michael Birsak, Wenqing Cui, Zhenyu Li, Peter Wonka</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 研究视觉Transformer中位置嵌入（PEs）作为几何先验的作用，通过 token级诊断分析揭示PEs对多视图几何一致性和空间推理的影响，深化了对Transformer架构空间建模能力的理解。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22231' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Exact closed-form Gaussian moments of residual layers</h3>
<p><strong>Authors:</strong> Simon Kuang (), Xinfan Lin ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文推导了残差层的高斯矩精确闭式解，解决了残差网络中高斯分布传播的理论问题，对深度学习理论中的网络架构（残差层）和优化（矩匹配）有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22307' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks</h3>
<p><strong>Authors:</strong> Puyu Wang (), Junyu Zhou (), Philipp Liznerski (), Marius Kloft ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文推导了KAN网络的梯度下降优化、泛化和差分隐私边界，对深度学习理论中的网络架构和优化有重要理论贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22409' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stabilizing Transformer Training Through Consensus</h3>
<p><strong>Authors:</strong> Shyam Venkatasubramanian, Sean Moushegian, Michael Lin, Mir Park, Ankit Singhal, Connor Lee</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对Transformer训练中学习率过指定导致的不稳定性，提出consensus机制替代注意力，通过图形模型分析证明其稳定性，实证显示在文本、DNA、蛋白质模态上提升学习率范围的稳定性，为Transformer训练稳定性提供新方法。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22614' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse Attention as Compact Kernel Regression</h3>
<p><strong>Authors:</strong> Saul Santos, Nuno Gonçalves, Daniel C. McNamee, André F. T Martins</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 建立稀疏注意力与紧支撑核回归的统一联系，证明ReLU、sparsemax等注意力对应Epanechnikov等核，解释稀疏性的核设计根源。实验显示核基稀疏注意力在语言建模等任务上性能 competitive，为注意力机制设计提供理论框架。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22766' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Perplexity Cannot Always Tell Right from Wrong</h3>
<p><strong>Authors:</strong> Petar Veličković, Federico Barbero, Christos Perivolaropoulos, Simon Osindero, Razvan Pascanu</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 从理论上证明Perplexity作为模型评估指标的局限性：准确模型必然存在低Perplexity但错误的预测，且Perplexity不一定选择更准确的模型，属于深度学习理论中的模型评估理论研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22950' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients</h3>
<p><strong>Authors:</strong> Cheng Ge, Caitlyn Heqi Yin, Hao Liang, Jiawei Zhang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 从局部曲率角度分析GRPO中标准差归一化的必要性：归一化实现自适应梯度，证明其收敛率优于未归一化的REINFORCE，且收敛增益与prompt内奖励方差相关，属于深度学习理论中的梯度优化研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23135' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning</h3>
<p><strong>Authors:</strong> \.Ilker I\c{s}{\i}k (), Wenchao Li ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出符号不变的Transformer机制，通过并行嵌入流与聚合注意力解决开放词汇学习中符号泛化问题，理论上保证重命名不变性，实验验证在开放词汇任务上的性能增益。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23169' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Execute Graph Algorithms Exactly with Graph Neural Networks</h3>
<p><strong>Authors:</strong> Muhammad Fetrat Qharabagh (), Artur Back de Luca (), George Giapitzakis (), Kimon Fountoulakis ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 通过Neural Tangent Kernel理论证明GNN可精确学习图算法（如BFS、DFS），提出MLP集合作为GNN更新函数的方法，推进GNN对算法任务的理论理解与应用。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23207' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Agile Reinforcement Learning through Separable Neural Architecture</h3>
<p><strong>Authors:</strong> Rajib Mostakim (), Reza T. Batley (), Sourav Saha ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出基于样条的可分离神经网络SPAN，解决RL中MLP的参数低效问题，实验在离散、连续与离线RL任务上提升30-50%样本效率。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23225' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training</h3>
<p><strong>Authors:</strong> Ruijie Zhang (), Yequan Zhao (), Ziyue Liu (), Zhengyang Wang (), Dongyang Li (), Yupeng Su (), Sijia Liu (), Zheng Zhang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 扩展Muon优化器的层-wise正交化为张量化正交化TEON，理论证明收敛性更优，实验在GPT和LLaMA模型上提升预训练与验证困惑度。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23261' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Is Hierarchical Quantization Essential for Optimal Reconstruction?</h3>
<p><strong>Authors:</strong> Shirin Reyhanian, Laurenz Wiskott</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 探讨VQ-VAE中分层量化的必要性，通过控制变量实验验证单级模型在缓解码本崩溃后可达到分层模型的重建性能，挑战了分层量化更优的传统认知，对生成模型架构设计有启发。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22244' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Hierarchical Shift Mixing -- Beyond Dense Attention in Transformers</h3>
<p><strong>Authors:</strong> Robert Forchheimer</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出Hierarchical Shift Mixing框架替代Transformer的 dense attention，通过分层分配token交互降低计算复杂度（线性时间），同时保持性能接近dense attention，属于Transformer架构的理论改进。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.22852' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference</h3>
<p><strong>Authors:</strong> Yizhi Liu</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 分析熵正则化结构推理中的“过早模式崩溃”问题，提出自适应退火调度算法（Efficient PH-ASC），通过监测推理稳定性调整冷却速度，解决传统指数冷却的动态不匹配问题，属于深度学习理论中的结构化推理优化研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23039' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Regularisation in neural networks: a survey and empirical analysis of approaches</h3>
<p><strong>Authors:</strong> Christiaan P. Opperman, Anna S. Bosman, Katherine M. Malan</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 对神经网络正则化方法进行全面综述与实证分析，涵盖数据、架构、训练、损失函数四大类策略，揭示正则化的有效性依赖数据集特性（如数值数据集受益于正则化项，图像数据集受益于 batch normalisation），属于深度学习理论中的正则化研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23131' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem</h3>
<p><strong>Authors:</strong> Tianming Qiu (), Christos Sonis (), Hao Shen ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 利用隐函数定理建立数据空间与隐式神经表示权重空间的严格映射，理论分析hypernetwork的语义编码能力，为权重空间学习提供新视角。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2601.23181' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment</h3>
<p><strong>Authors:</strong> Yavuz Bakman (), Duygu Nur Yaldiz (), Salman Avestimehr (), Sai Praneeth Karimireddy ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文指出黑盒评估无法保证大模型更新后的对齐，揭示了大模型对齐的核心限制，对大模型安全与对齐的理论和实践有重要指导意义。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22313' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models</h3>
<p><strong>Authors:</strong> Shi Fu (), Yingjie Wang (), Shengchao Hu (), Peng Wang (), Dacheng Tao ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 首次给出自奖励语言模型的理论保证，证明迭代更新能指数衰减初始模型依赖，解释自奖励对齐的机制，对自监督对齐有理论突破。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22513' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> THINKSAFE: Self-Generated Safety Alignment for Reasoning Models</h3>
<p><strong>Authors:</strong> Seanie Lee (), Sangwoo Park (), Yumin Choi (), Gyeongman Kim (), Minki Kang (), Jihun Yun (), Dongmin Park (), Jongho Park (), Sung Ju Hwang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出无需外部教师的自生成安全对齐框架，在提升推理模型安全性的同时保留推理能力，对大模型安全与对齐有实践价值
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.23143' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Jailbreaks on Vision Language Model via Multimodal Reasoning</h3>
<p><strong>Authors:</strong> Aarush Noheria, Yuguang Yao</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出基于后训练Chain-of-Thought（CoT）提示和ReAct驱动自适应加噪的越狱框架，针对视觉语言模型（VLMs）的安全过滤漏洞，提升攻击成功率同时保持文本和视觉自然性，对VLM安全对齐研究有重要参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22398' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework</h3>
<p><strong>Authors:</strong> Shiyu Liu, Xinyi Wen, Zhibin Lan, Ante Wang, Jinsong Su</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对大视觉语言模型（LVLMs）过度依赖语言先验导致的目标幻觉问题，提出无训练自验证框架，通过验证候选caption的目标存在性减少幻觉，显著提升图像caption的可靠性（如LLaVA-v1.5-7B的CHAIRI metric提升65.6%）。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22451' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples</h3>
<p><strong>Authors:</strong> Hsiang Hsu (), Pradeep Niroula (), Zichang He (), Ivan Brugere (), Freddy Lecue (), Chun-Fu Chen ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文揭示了机器遗忘中扰动样本导致的残留知识问题，提出缓解策略，对大模型安全与对齐中的隐私保护有重要贡献。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Machine Unlearning in Low-Dimensional Feature Subspace</h3>
<p><strong>Authors:</strong> Kun Fang (), Qinghua Tao (), Junxu Liu (), Yaxin Xiao (), Qingqing Ye (), Jian Sun (), Haibo Hu ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文提出低维特征子空间的机器遗忘方法，减少计算开销并保护隐私，对大模型安全与对齐中的隐私保护有实际应用价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22456' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Clipping-Free Policy Optimization for Large Language Models</h3>
<p><strong>Authors:</strong> Ömer Veysel Çağatan, Barış Akgün, Gözde Gül Şahin, Xuandong Zhao</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出CFPO替代强化学习微调中的clip机制，基于总变差 divergence 约束的凸二次 penalty，实现处处可微的目标函数。在对齐任务中减少verbosity exploitation和能力退化，推理性能 competitive，是LLM对齐训练的有效优化器。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22801' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> dgMARK: Decoding-Guided Watermarking for Diffusion Language Models</h3>
<p><strong>Authors:</strong> Pyo Min Hong, Albert No</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对扩散语言模型的生成顺序敏感性，提出解码引导的水印方法，通过控制unmasking顺序嵌入水印，提升生成内容的可检测性与抗编辑鲁棒性，属于大模型安全与对齐的水印技术研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22985' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CATTO: Balancing Preferences and Confidence in Language Models</h3>
<p><strong>Authors:</strong> Nisarg Parikh, Kunjal Panchal, Ananya Sai, Pannaga Shivaswamy, Andrew Lan</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出CATTO目标，结合偏好优化与置信度校准，在不损失任务 accuracy 的前提下，降低in-distribution与out-of-distribution的预期校准误差（ECE），属于大模型安全与对齐的校准研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.23096' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data</h3>
<p><strong>Authors:</strong> Eugenia Iofinova, Dan Alistarh</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出Behemoth全合成数据基准，研究LLM的unlearning效果与训练数据分布的关系，发现限制更新秩可提升unlearning有效性，为真实LLM的模型编辑提供理论指导，属于大模型安全与对齐的unlearning研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.23153' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling</h3>
<p><strong>Authors:</strong> Mingqian Feng (), Xiaodong Liu (), Weiwei Yang (), Chenliang Xu (), Christopher White (), Jianfeng Gao ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出SABER方法，从少量样本准确估计大规模对抗风险，揭示模型在并行攻击下的风险放大问题，降低安全评估成本。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22636' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UCPO: Uncertainty-Aware Policy Optimization</h3>
<p><strong>Authors:</strong> Xianzhou Zeng (), Jing Huang (), Chunmei Xie (), Gongrui Nan (), Siye Chen (), Mengyu Lu (), Weiqi Xiong (), Qixuan Zhou (), Junhao Zhang (), Qiang Zhu (), Yadong Li (), Xingzhong Xu ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出不确定性感知的政策优化框架UCPO，解决RL中的优势偏差与奖励黑客问题，提升LLM在数学推理中的可靠性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22648' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Real-Time Aligned Reward Model beyond Semantics</h3>
<p><strong>Authors:</strong> Zixuan Huang (), Xin Xia (), Yuxi Ren (), Jianbin Zheng (), Xuefeng Xiao (), Hongyan Xie (), Li Huaqiu (), Songshi Liang (), Zhongxiang Dai (), Fuzhen Zhuang (), Jianxin Li (), Yikun Ban (), Deqing Wang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出实时对齐的奖励模型R2M，利用政策反馈调整奖励模型，解决RLHF中的奖励过拟合问题。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22664' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks</h3>
<p><strong>Authors:</strong> Nathaniel Mitrani Hadida (), Sassan Bhanji (), Cameron Tice (), Puria Radmard ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 研究输出监督下CoT混淆的泛化问题，指出当前 penalize 有害生成的做法可能降低LLM可监控性，对大模型安全与对齐有警示意义
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.23086' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization</h3>
<p><strong>Authors:</strong> Hui Lu (), Yi Yu (), Yiming Yang (), Chenyu Yi (), Xueyi Ke (), Qixing Zhang (), Bingquan Shen (), Alex Kot (), Xudong Jiang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出针对闭源多模态大模型的通用目标对抗攻击方法，提升了攻击的通用性和迁移性，对大模型安全研究有重要意义
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.23179' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding</h3>
<p><strong>Authors:</strong> Yuansheng Gao, Jinman Zhao, Tong Zhang, Xingguo Xu, Han Bao, Zonghui Wang, Wenzhi Chen</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出时空语义对比解码策略，通过构造破坏时空一致性和语义关联的负特征，在推理时抑制视频大模型的幻觉生成，同时保持模型的视频理解和推理能力，针对视频大模型的可靠性问题。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models</h3>
<p><strong>Authors:</strong> Enyi Shi, Pengyang Shao, Yanxin Zhang, Chenhang Cui, Jiayi Lyu, Xu Xie, Xiaobo Xia, Fei Shen, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 构建多语言多模态的安全基准Lingua-SafetyBench，包含10万+有害图像-文本对（覆盖10种语言），评估视觉语言大模型（VLLMs）在不同语言和模态下的安全性能，为多语言VLM安全对齐提供关键工具。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22737' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing</h3>
<p><strong>Authors:</strong> Yilong Huang, Songze Li</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出FaceDefense框架，通过扩散损失增强对抗样本的防御效果，结合定向属性编辑恢复扰动导致的失真，实现 imperceptibility（视觉自然性）和防御效果的平衡，针对扩散人脸交换的安全问题（如恶意伪造）。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22744' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs</h3>
<p><strong>Authors:</strong> Youxu Shi, Suorong Yang, Dong Liu</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对Vision Language Models（VLMs）的幻觉与安全问题，提出One-shot Steering with Generative Anchor（OSGA）框架，通过方差-based数据选择和对比目标学习通用steering向量，推理时无需修改模型参数即可缓解幻觉并增强安全，契合大模型安全与对齐的研究方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.23041' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution</h3>
<p><strong>Authors:</strong> Khush Patel (), Siva Surendira (), Jithin George (), Shreyas Kapale ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出共识驱动的分解执行框架，通过任务分解、多代理采样与共识投票提升LLM系统可靠性，达到六西格玛标准，对企业级部署有价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22290' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?</h3>
<p><strong>Authors:</strong> Ala N. Tak (), Amin Banayeeanzade (), Anahita Bolourani (), Fatemeh Bahrani (), Ashutosh Chaubey (), Sai Praneeth Karimireddy (), Norbert Schwarz (), Jonathan Gratch ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 评估LLM在理性选择任务上与人类判断的一致性，分析情感引导对LLM理性的影响，为大模型对齐人类偏好提供参考。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2601.22329' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use</h3>
<p><strong>Authors:</strong> Julien Delavande (), Regis Pierrard (), Sasha Luccioni ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文分析了LLM推理中的量化、批处理和服务策略对能耗的影响，提出系统级优化方法，对高效大模型推理有重要实践价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22362' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation</h3>
<p><strong>Authors:</strong> Pingzhi Tang, Ruijie Zhou, Fanxu Meng, Wenjie Pei, Muhan Zhang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出LoRDS框架，通过连续低秩分解打破块结构限制，实现LLM量化与适配的统一。在Llama3-8B上3bit量化比NormalFloat提升27% accuracy，4bit PEFT比QLoRA提升9.6%，且无推理 overhead，解决了量化与适配的兼容性问题。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22716' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation</h3>
<p><strong>Authors:</strong> Andrei Panferov, Erik Schultheis, Soroush Tabesh, Dan Alistarh</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出MS-EDEN无偏量化方法与Quartet II低精度训练方案，解决NVFP4格式下的梯度估计误差问题，实现LLM的端到端量化预训练，显著提升训练效率与硬件利用率，是高效大模型训练的重要突破。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22813' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation</h3>
<p><strong>Authors:</strong> Muqing Liu, Chongjie Si, Yuheng Jia</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对LoRA固定秩的局限性，提出熵引导的灵活低秩适应框架，支持秩剪枝与扩展（全局预算内），并通过零影响初始化保证稳定性，显著提升参数高效微调的性能，属于高效大模型训练的关键改进。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training</h3>
<p><strong>Authors:</strong> Powei Chang, Jinpeng Zhang, Bowen Chen, Chenyu Wang, Chenlu Guo, Yixing Zhang, Yukang Gao, JianXiang Xiang, Yue Gao, Chaoqun Sun, Yiyi Chen, Dongying Kong</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出SPICE数据选择方法，通过子模信息最大化与冲突惩罚选择高信息低冲突数据，用10%的训练数据达到全数据训练的性能，显著降低训练成本，属于高效大模型训练的数据效率研究。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.23155' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification</h3>
<p><strong>Authors:</strong> Haoyun Jiang (), Junqi He (), Feng Hong (), Xinlong Yang (), Jianwei Zhang (), Zheng Li (), Zhengyang Zhuge (), Zhiyong Chen (), Bo Han (), Junyang Lin (), Jiangchao Yao ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对speculative decoding的验证成本瓶颈，提出三元框架TriSpec，通过轻量代理验证减少目标模型调用，实验显示比标准SD提升35%速度且保持精度，直接解决大模型推理效率问题。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.23180' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration</h3>
<p><strong>Authors:</strong> Hanxun Yu, Wentong Li, Xuan Qu, Song Wang, Junbo Chen, Jianke Zhu</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出VisionTrim框架，通过全局-局部主导视觉token选择（DVTS）和文本引导视觉补充（TGVC）优化视觉token，实现无训练的多模态大语言模型（MLLM）加速，提升高分辨率和视频场景的计算效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22674' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs</h3>
<p><strong>Authors:</strong> Yanlong Chen, Amirhossein Habibian, Luca Benini, Yawei Li</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出GRACE框架，结合知识蒸馏和量化感知训练（QAT）优化视觉语言模型（VLMs）的部署效率，INT4模型性能接近FP16（如LLaVA-1.5-7B的SQA得分从66.8提升至70.1），同时提升3倍吞吐量和减少54%内存占用。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22709' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization</h3>
<p><strong>Authors:</strong> Sai Sanjeet (), Ian Colbert (), Pablo Monteagudo-Lago (), Giuseppe Franco (), Yaman Umuroglu (), Nicholas J. Fraser ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文通过块旋转和排列优化提升后训练量化的精度，减少大模型推理开销，对高效大模型推理（高压缩）有实际应用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22347' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism</h3>
<p><strong>Authors:</strong> Thalaiyasingam Ajanthan (), Sameera Ramasinghe (), Gil Avraham (), Hadi Mohaghegh Dolatabadi (), Chamin P Hewa Koneputugodage (), Violetta Shevchenko (), Yan Zuo (), Alexander Long ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文提出异步优化用于数据和流水线并行，提升大模型训练效率，对高效大模型训练有实际应用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22442' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EUGens: Efficient, Unified, and General Dense Layers</h3>
<p><strong>Authors:</strong> Sang Min Kim, Byeongchan Kim, Arijit Sehanobish, Somnath Basu Roy Chowdhury, Rahul Kidambi, Dongseok Shim, Avinava Dubey, Snigdha Chaturvedi, Min-hwan Oh, Krzysztof Choromanski</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对全连接层的计算与参数瓶颈，提出EUGens框架，通过随机特征和输入范数依赖实现线性时间推理，结合知识迁移技术适配预训练模型。实证显示在Transformer和MLP中提升推理速度27%、内存效率30%，对大模型高效部署具有重要价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22563' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation</h3>
<p><strong>Authors:</strong> Haonan He, Jingqi Ye, Minglei Li, Zhengbo Wang, Tao Chen, Lei Bai, Peng Ye</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 系统性研究LoRA变体，从rank、优化动态等4个维度分类，统一理论框架分析低秩更新 dynamics，开发LoRAFactory代码base并开展标准化实验，揭示LoRA对学习率的敏感性等关键发现，为参数高效微调提供全面指导。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22708' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Float8@2bits: Entropy Coding Enables Data-Free Model Compression</h3>
<p><strong>Authors:</strong> Patrick Putzky, Martin Genzel, Mattes Mollenhauer, Sebastian Schulze, Thomas Wollmann, Stefan Dietzel</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出EntQuant框架，通过熵编码 decouple 数值精度与存储成本，实现数据无关的极端模型压缩（2bits），30分钟内压缩70B模型，解决低比特率下功能崩溃问题，兼顾速度与 fidelity，对大模型压缩部署意义重大。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22787' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models</h3>
<p><strong>Authors:</strong> Pit Neitemeier, Alessio Serra, Jiaze Li, Sascha Wirges, Lukas Balles, Jan Hendrik Metzen</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对分层序列模型的边界放置问题提出量化指标与优化方法，通过引导边界向高预测难度位置对齐，提升自回归建模的效率与准确性，属于高效大模型训练与推理的核心方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22805' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA</h3>
<p><strong>Authors:</strong> Zhan Fa, Yue Duan, Jian Zhang, Lei Qi, Wanqi Yang, Yinghuan Shi</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 将单LoRA模块重构为可分解的Rank-1专家池，通过稀疏组合与正交正则化实现视觉语言模型的高效持续学习，减少参数更新与任务干扰，属于参数高效微调的关键改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22828' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding</h3>
<p><strong>Authors:</strong> Zhanglu Yan, Kaiwen Tang, Zixuan Zhu, Zhenyu Bai, Qianhui Liu, Weng-Fai Wong</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出模拟稀疏脉冲Transformer架构，通过masked TTFS编码减少 spike 移动开销，并结合忆阻突触单元消除权重访问成本，显著提升能量效率，属于硬件-算法协同优化的高效大模型研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22876' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models</h3>
<p><strong>Authors:</strong> Yangyan Li</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出MoVE机制，通过全局可学习价值嵌入库扩展自回归模型的参数记忆，解耦模型容量与计算成本（无需加深/加宽网络），实现“内存密集型”模型的高效训练，属于高效大模型训练的核心创新。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22887' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning</h3>
<p><strong>Authors:</strong> Arvind Mahankali, Kaiyue Wen, Tengyu Ma</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出Divide-and-Conquer CoT，通过RL训练模型将长CoT分解为并行子任务，减少推理延迟（最长路径长度降低35-40%），同时保持 accuracy，属于高效大模型推理的加速研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.23027' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Anytime Safe PAC Efficient Reasoning</h3>
<p><strong>Authors:</strong> Chengyao Yu (), Hao Zeng (), Youxin Zhu (), Jianguo Huang (), Huajun Zeng (), Bingyi Jing ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出B-PAC推理框架，通过逆倾向得分估计与测试超鞅动态调整路由阈值，实现随时安全且高效的在线推理。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22446' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR</h3>
<p><strong>Authors:</strong> Hao Yi (), Yulan Hu (), Xin Li (), Sheng Ouyang (), Lizhong Ding (), Yong Liu ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出不确定性一致性的查询选择方法，用30%数据达到全量RLVR性能，降低标注成本，提升训练效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22595' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models</h3>
<p><strong>Authors:</strong> Hongxi Yan (), Qingjie Liu (), Yunhong Wang ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出熵引导的动态截断方法EntroCut，减少40%token使用同时保持精度，引入EPR度量效率-性能权衡，提升小模型推理效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22617' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learnable Permutation for Structured Sparsity on Transformer Models</h3>
<p><strong>Authors:</strong> Zekai Li, Ji Liu, Guanchen Li, Yixing Xu, Ziqiong Liu, Xuanwu Yin, Dong Li, Emad Barsoum</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出端到端可学习排列框架，通过可微分二分匹配求解器优化权重排列，提升Transformer的结构化稀疏性能，属于高效大模型训练的结构化稀疏研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.22980' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> High-quality generation of dynamic game content via small language models: A proof of concept</h3>
<p><strong>Authors:</strong> Morten I. K. Munk (), Arturo Valdivia (), Paolo Burelli ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 探索小语言模型在动态游戏内容生成中的应用，解决大模型的成本和离线问题，属于高效大模型训练与推理方向的实践
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2601.23206' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features</h3>
<p><strong>Authors:</strong> Yiting Liu (), Zhi-Hong Deng ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文提出权重基解释框架，无需激活数据即可理解稀疏自编码器特征，对深度学习可解释性中的白盒解释有重要贡献。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.22447' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations</h3>
<p><strong>Authors:</strong> Joao Fonseca, Julia Stoyanovich</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出ExplainerPFN，基于TabPFN预训练的表格基础模型，实现零样本Shapley值估计（无需模型访问或参考解释），性能优于依赖2-10个SHAP示例的少样本方法，属于深度学习可解释性的关键突破。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.23068' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models</h3>
<p><strong>Authors:</strong> Mingyu Lu, Soham Gadgil, Chris Lin, Chanwoo Kim, Su-In Lee</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对T2I模型的贡献者归因问题，提出训练-free的Shapley值近似框架，通过预训练模型和梯度提升树优化效用函数，避免重复训练的高计算成本，契合深度学习可解释性中Shapley value的研究方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.22276' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Probing the Trajectories of Reasoning Traces in Large Language Models</h3>
<p><strong>Authors:</strong> Marthe Ballon, Brecht Verbeken, Vincent Ginis, Andres Algaba</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出轨迹探测协议，通过截断推理轨迹并注入模型测量答案分布，分析LLM推理过程的准确性、决策承诺演化与内容相关性，揭示推理轨迹的内部机制，属于深度学习可解释性的推理过程研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.23163' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Quantifying Model Uniqueness in Heterogeneous AI Ecosystems</h3>
<p><strong>Authors:</strong> Lei You ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 探讨异构AI生态系统中模型唯一性的量化方法，指出Shapley值等博弈论方法无法检测冗余，与深度学习可解释性中的Shapley值应用高度相关
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.22977' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success</h3>
<p><strong>Authors:</strong> Luca Zhou, Bo Zhao, Rose Yu, Emanuele Rodolà</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对模型合并的成功因素问题，用可解释的pairwise指标（如子空间重叠、梯度对齐）预测合并性能，发现方法特定“指纹”和通用先决条件，符合深度学习可解释性的研究方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2601.22285' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FOCUS: DLLMs Know How to Tame Their Compute Bound</h3>
<p><strong>Authors:</strong> Kaihua Liang (), Xin Tan (), An Zhong (), Hong Xu (), Marco Canini ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对Diffusion LLM的推理效率问题，提出动态聚焦可解码token的FOCUS框架，提升3.52倍吞吐量，属于大模型新技术中的diffusion LLM方向。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.23278' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VMonarch: Efficient Video Diffusion Transformers with Structured Attention</h3>
<p><strong>Authors:</strong> Cheng Liang, Haoxian Chen, Liang Hou, Qi Fan, Gangshan Wu, Xin Tao, Limin Wang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出VMonarch注意力机制，利用Monarch矩阵优化视频扩散Transformer的时空注意力复杂度（子二次复杂度），显著提升长视频生成效率，属于扩散大模型的高效化技术创新。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22275' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity</h3>
<p><strong>Authors:</strong> Jianhao Huang (), Baharan Mirzasoleiman ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 论文通过k-parity insights调整掩码扩散语言模型的隐式正则化，提升泛化能力，对大模型新技术（diffusion LLM）有重要贡献。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22450' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation</h3>
<p><strong>Authors:</strong> Youngjoong Kim, Duhoe Kim, Woosung Kim, Jaesik Park</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 对consistency models的训练稳定性进行流图分析，揭示退化解的成因，提出自蒸馏策略稳定优化，无需预训练扩散模型即可扩展到扩散政策学习，解决了consistency训练的核心问题，对大模型新技术（生成模型）有重要贡献。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22679' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector</h3>
<p><strong>Authors:</strong> Wenqiang Zu, Shenghao Xie, Bo Lei, Lei Ma</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出无训练的表示引导方案，通过表示对齐投影仪向扩散模型中间采样步骤注入语义锚点，缓解早期去噪的语义漂移，显著提升图像生成的语义一致性和保真度（如REPA-XL/2的FID从5.9降至3.3），属于扩散模型的新技术。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22468' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation</h3>
<p><strong>Authors:</strong> Xin Jiang, Jingwen Chen, Yehao Li, Yingwei Pan, Kezhou Chen, Zechao Li, Ting Yao, Tao Mei</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 基于视觉自回归（VAR）模型提出DreamVAR框架，通过预填充主体特征序列简化自回归依赖，并结合强化学习提升主体驱动图像生成的一致性和质量，属于自回归大模型的生成技术创新。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.22507' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants</h3>
<p><strong>Authors:</strong> Santanu Subhash Rathod, Pietro Liò, Xiao Zhang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出SplineFlow，用B样条插值改进流匹配算法，解决线性插值无法捕捉复杂动力学的问题，同时保证多边际约束，提升动态系统生成的准确性与稳定性，属于大模型新技术中的流模型研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.23072' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Manifold-Aware Perturbations for Constrained Generative Modeling</h3>
<p><strong>Authors:</strong> Katherine Keegan, Lars Ruthotto</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 提出流形感知扰动方法，通过约束扰动在数据流形的切空间内，改进约束生成模型的数据分布恢复与采样稳定性，解决传统扰动易引入分布外噪声的问题，属于大模型新技术中的约束生成研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2601.23151' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> NativeTok: Native Visual Tokenization for Improved Image Generation</h3>
<p><strong>Authors:</strong> Bin Wu, Mengqi Huang, Weinan Jia, Zhendong Mao</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对VQ-based图像生成中tokenization与生成模型的依赖约束不匹配问题，提出原生视觉tokenization框架，通过Meta Image Transformer和Mixture of Causal Expert Transformer嵌入token序列的关系约束，解决无序分布导致的生成偏差，提升图像生成效果，契合原生多模态大模型中tokenizer与图像生成的研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22837' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation</h3>
<p><strong>Authors:</strong> Hun Chang, Byunghee Cha, Jong Chul Ye</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 结合预训练Vision Foundation Model（DINO）与球型自动编码器，提出Hierarchical Convolutional Patch Embedding和Cosine Similarity Alignment解决高保真图像重建的高频细节丢失问题，并用Riemannian Flow Matching在球型latent流形上训练Diffusion Transformer，提升生成的语义对齐与效率，符合原生多模态大模型中图像生成与理解的方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22904' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization</h3>
<p><strong>Authors:</strong> Luca Della Libera (), Cem Subakan (), Mirco Ravanelli ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对语音tokenization的固定帧率问题，提出动态字符对齐的DyCAST方法，减少token数量同时保持语音合成质量与下游性能，对多模态大模型的tokenizer设计有价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.23174' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Alignment among Language, Vision and Action Representations</h3>
<p><strong>Authors:</strong> Nicola Milano (), Stefano Nolfi ()</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 研究语言、视觉、动作三种模态表示的对齐问题，发现跨模态存在共享的语义结构，对原生多模态大模型的表示学习具有理论意义
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22948' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation</h3>
<p><strong>Authors:</strong> Hongyang Du, Junjie Ye, Xiaoyan Cong, Runhao Li, Jingcheng Ni, Aman Agarwal, Zeqi Zhou, Zekun Li, Randall Balestriero, Yue Wang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对视频扩散模型的3D一致性问题，利用几何基础模型提取偏好信号，通过Direct Preference Optimization引导模型，提升视频生成的时空稳定性与物理合理性，契合原生多模态大模型与大模型新技术的方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.23286' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport</h3>
<p><strong>Authors:</strong> Yilong Zuo, Xunkai Li, Zhihan Zhang, Qiangqiang Dai, Ronghua Li, Guoren Wang</p>
<p><strong>Published:</strong> 2026-02-02</p>
<p><strong>Reason:</strong> 针对多模态属性图的结构-语义冲突问题，提出基于非平衡最优传输的正则化框架，通过Fused Gromov-Wasserstein距离引导跨模态结构一致性，提升多模态图模型的表示学习效果，属于原生多模态大模型的结构对齐研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2601.22856' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>