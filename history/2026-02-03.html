<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-03</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >多模态智能体</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型安全与对齐</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-03</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-03 13:57:31</p>
<p>已自动阅读了 931 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：507342</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models</h3>
<p><strong>Authors:</strong> Dhruv Parikh, Haoyang Fan, Rajgopal Kannan, Viktor Prasanna</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 融合视觉编码器显著性与跨模态注意力，减少视觉语言模型（VLMs）的冗余token，在保持精度的同时提升推理效率，属于高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00946' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers</h3>
<p><strong>Authors:</strong> Haopeng Li, Shitong Shao, Wenliang Zhong, Zikai Zhou, Lichen Bai, Hui Xiong, Zeke Xie</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出分段稀疏注意力机制，对扩散Transformer的关键块精确计算、非关键块泰勒近似，在保持生成质量的同时实现1.2-2.57倍加速，属于高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01077' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model</h3>
<p><strong>Authors:</strong> Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, Haoji Hu (Alibaba Group, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 发现MLLM解码层注意力的冗余性，提出Lazy Attention与Q Cache跨层共享注意力模式，减少35%KV缓存使用并提升1.5倍吞吐量，性能损失仅1%，是高效大模型训练与推理的突破性成果。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning</h3>
<p><strong>Authors:</strong> Zhishen Sun, Sizhe Dang, Guang Dai, Haishan Ye</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出ESSAM框架，将进化策略与Sharpness-Aware Maximization结合，大幅降低LLM微调的内存使用（比PPO少18×），同时保持推理性能。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01003' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models</h3>
<p><strong>Authors:</strong> Xin Nie, Haicheng Zhang, Liang Dong, Beining Feng, Jinhong Weng, Guiling Sun</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出搜索-free、硬件友好的混合精度量化框架SFMP，解决现有量化方法的效率瓶颈，在保持性能的同时显著降低内存占用和计算开销，是高效大模型推理的关键优化。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01027' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training</h3>
<p><strong>Authors:</strong> Yunjie Pan, Yongyi Yang, Hanmei Yang, Scott Mahlke</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出自适应子字节混合精度训练框架SNIP，通过量化损失影响的 metrics 指导层级精度优化，实验验证可在不同规模模型上显著降低FLOPs并保持质量，属于高效大模型训练与推理的关键方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01410' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> A State-Transition Framework for Efficient LLM Reasoning</h3>
<p><strong>Authors:</strong> Liang Zhang, Yu Zhao, Longyue Wang, Tianqi Shi, Weihua Luo, Kaifu Zhang, Jinsong Su</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将LLM推理建模为状态转移过程，用线性注意力估计推理状态，将注意力复杂度从二次降至线性，显著提升推理效率，解决高效大模型推理的核心问题。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01198' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection</h3>
<p><strong>Authors:</strong> Jongseok Park, Sunga Kim, Alvin Cheung, Ion Stoica</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出支点截断的高效Top-k/Top-p算法，在GPU上实现2倍吞吐量和50%内存节省，解决大模型推理中采样步骤的性能瓶颈。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01518' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models</h3>
<p><strong>Authors:</strong> Xuliang Wang, Yuetao Chen, Maochan Zhen, Fang Liu, Xinzhou Zheng, Xingwu Liu, Hong Xu, Ming Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 重构投机采样草稿模型计算路径，解耦模型容量与推理成本，显著提升解码吞吐量（2.6x），是高效推理的重要优化
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01762' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models</h3>
<p><strong>Authors:</strong> Pengcheng Zheng, Chaoning Zhang, Jiarong Mo, GuoHui Li, Jiaquan Zhang, Jiahao Zhang, Sihan Cao, Sheng Zheng, Caiyan Qin, Guoqing Wang, Yang Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对大 multimodal 模型压缩中低秩分解与量化耦合导致的重建误差问题，提出联合傅里叶近似的压缩方法，利用傅里叶变换的去相关和共轭对称性质实现更紧凑准确的权重表示，有效提升压缩效果，为多模态大模型的高效部署提供了新方案。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00135' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Samyak Jha, Junho Kim</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于注意力贡献的视觉token剪枝与FFN近似框架，通过区分低贡献“概率Dump”和高贡献“结构锚点”实现精准剪枝，同时对FFN进行线性近似减少冗余计算，有效平衡了大视觉语言模型的推理效率与性能。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00247' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding</h3>
<p><strong>Authors:</strong> Yujia Tong, Tian Zhang, Yunyang Wan, Kaiwei Lin, Jingling Yuan, Chuang Hu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出熵引导的自适应投机解码方法，在不损失输出质量的情况下显著加速VLM推理，属于高效大模型训练与推理中的推理加速方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00523' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware</h3>
<p><strong>Authors:</strong> Brandon Leblanc, Charalambos Poullis</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 通过蒸馏将大型3D基础模型压缩到可在单工作站训练的小模型，参数减少9倍、推理加速5倍，解决3D模型的计算壁垒，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00865' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models</h3>
<p><strong>Authors:</strong> Guangshuo Qin, Zhiteng Li, Zheng Chen, Weihang Zhang, Linghe Kong, Yulun Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对混合专家（MoE）视觉语言模型提出模态自适应量化，平衡跨模态差异与专家异质性，提升压缩效率与精度，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01037' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution</h3>
<p><strong>Authors:</strong> Xun Zhang, Kaicheng Yang, Hongliang Lu, Haotong Qin, Yong Guo, Yulun Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对扩散Transformer的超分辨率模型提出细节保持量化，通过分层SVD与变分感知混合精度，在压缩模型的同时保留纹理细节，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01273' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Token Pruning for In-Context Generation in Diffusion Transformers</h3>
<p><strong>Authors:</strong> Junqing Lin, Xingyu Zheng, Pei Cheng, Bin Fu, Jingwei Sun, Guangzhong Sun</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出ToPi框架用于Diffusion Transformers的in-context生成token剪枝，解决in-context生成的计算瓶颈，属于高效大模型推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01609' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation</h3>
<p><strong>Authors:</strong> Xiao Liang, Yunzhu Zhang, Linchao Zhu (Shanghai Jiao Tong University, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出渐进式蒸馏框架，通过教师模型引导学生模型用更大步长采样，将视频扩散采样步骤从48缩减至6同时保持质量，优于现有蒸馏方法，是高效大模型训练与推理的重要成果。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01814' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space</h3>
<p><strong>Authors:</strong> FSVideo Team, Qingyu Chen, Zhiyuan Fang, Haibin Huang, Xinwei Huang, Tong Jin, Minxuan Lin, Bo Liu, Celong Liu, Chongyang Ma, Xing Mei, Xiaohui Shen, Yaojie Shen, Fuwen Tan, Angtian Wang, Xiao Yang, Yiding Yang, Jiamin Yuan, Lingxi Zhang, Yuxin Zhang (Alibaba Group, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出高压缩 latent 空间的视频扩散模型，结合高效视频autoencoder与扩散transformer，实现10倍速度提升且保持生成质量，属于高效大模型训练与推理的重要成果。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02092' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos</h3>
<p><strong>Authors:</strong> Yangyi Cao, Yuanhang Li, Lan Chen, Qi Mao (Chinese universities/institutions)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 采用分治策略与Velocity Blend、Attention Sink模块解决分钟级视频编辑的效率与一致性难题，显著优于现有方法，是高效大模型训练与推理在长视频任务中的重要应用。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02123' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Block removal for large language models through constrained binary optimization</h3>
<p><strong>Authors:</strong> David Jansen, Roman Rausch, David Montero, Roman Orus</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Formulates LLM block removal as a constrained binary optimization problem, improving compression and performance, relevant to efficient LLM training.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00161' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation</h3>
<p><strong>Authors:</strong> Aaron R. Flouro, Shawn P. Chadwick</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes SparseKD, a post-training method combining SVD pruning and self-referential distillation for LLM compression, relevant to efficient LLM training.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00372' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models</h3>
<p><strong>Authors:</strong> Jiarui Zhang, Yuchen Yang, Ran Yan, Zhiyu Mei, Liyuan Zhang, Daifeng Li, Wei Fu, Jiaxuan Gao, Shusheng Xu, Yi Wu, Binhang Yuan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出动态树注意力框架解决RL训练LLM的计算低效问题，通过前缀共享减少重复计算，显著提升训练吞吐量，属于高效大模型训练的核心方向
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00482' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation</h3>
<p><strong>Authors:</strong> Hao Gu, Mao-Lin Luo, Zi-Hao Zhou, Han-Chen Zhang, Min-Ling Zhang, Tong Wei</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 发现低秩持续适应中的谱不平衡导致遗忘问题，提出约束优化方法缓解遗忘，属于参数高效微调的关键改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00722' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision</h3>
<p><strong>Authors:</strong> Yihao Xue, Allan Zhang, Jianhao Huang, Amit Sahai, Baharan Mirzasoleiman</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 发现训练时推理长度的缩放能提升OOD泛化性能，属于高效大模型训练中的策略优化，为鲁棒性提升提供新方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00927' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SALAAD: Sparse And Low-Rank Adaptation via ADMM</h3>
<p><strong>Authors:</strong> Hao Ma, Melis Ilayda Bal, Liang Zhang, Bingcong Li, Niao He, Melanie Zeilinger, Michael Muehlebach</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出ADMM-based稀疏低秩适应方法，实现模型容量的灵活控制，属于参数高效微调的关键技术，提升训练效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00942' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Spectral Flattening of Quantized Embeddings</h3>
<p><strong>Authors:</strong> Junlin Huang, Wenyi Fang, Zhenheng Tang, Yuxin Wang, Xueze Kang, Yang Zheng, Bo Li, Xiaowen Chu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究量化嵌入的光谱平坦化问题，揭示低比特优化的稳定条件，属于模型压缩的理论基础，对高效训练有重要指导意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00969' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents</h3>
<p><strong>Authors:</strong> Hyesung Jeon, Hyeongju Ha, Jae-Joon Kim</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文针对多LoRA智能体提出KV缓存共享框架LRAgent，通过分解缓存为共享基组件和适配器依赖组件，显著降低内存和计算开销，是高效大模型训练与推理的重要创新。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01053' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching</h3>
<p><strong>Authors:</strong> Tianhao Miao, Zhongyuan Bao, Lejun Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出Lotus方法，通过随机低秩梯度投影与自适应子空间切换，解决GaLore等方法的SVD计算瓶颈，显著降低训练时间与内存占用，是高效大模型训练的关键优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01233' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> P-EAGLE: Parallel-Drafting EAGLE with Scalable Training</h3>
<p><strong>Authors:</strong> Mude Hui, Xin Huang, Jaime Campos Salas, Yue Sun, Nathan Pemberton, Xiang Song, Ashish Khetan, George Karypis</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将EAGLE从自回归扩展为并行多token预测，通过共享隐状态与序列分割提升训练 scalability 并降低推理延迟，属于高效大模型训练与推理中的推理优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01469' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> You Need an Encoder for Native Position-Independent Caching</h3>
<p><strong>Authors:</strong> Shiju Zhao, Junhao Hu, Jiaqi Zheng, Guihai Chen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于encoder的位置无关KV缓存方法COMB，通过重新引入encoder解决传统缓存的位置依赖问题，实验验证可显著提升推理效率，属于高效大模型训练与推理中的缓存优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01519' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning</h3>
<p><strong>Authors:</strong> Haoran Zhao, Soyeon Caren Han, Eduard Hovy</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析低秩LoRA的初始化敏感性问题，提出Gap-Init方法对齐模态间隙方向以稳定rank-1训练，属于高效大模型训练与推理中的参数高效微调方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01522' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models</h3>
<p><strong>Authors:</strong> Sergii Kozyrev (Minima AI, Inc.), Davyd Maiboroda (Minima AI, Inc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出张量网络压缩 pipeline Minima，通过敏感度预测与混合张量分解降低大模型内存占用，实验验证可结合投机解码提升部署效率，属于高效大模型训练与推理中的高压缩方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01613' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling</h3>
<p><strong>Authors:</strong> Runsong Zhao (), Shilei Liu (), Jiwei Tang (), Langming Liu (), Haibin Chen (), Weidong Zhang (), Yujin Yuan (), Tong Xiao (), Jingbo Zhu (), Wenbo Su (), Bo Zheng ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出协作记忆Transformer，通过双内存系统处理长上下文，实现线性时间复杂度和恒定内存占用，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01766' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs</h3>
<p><strong>Authors:</strong> Meng Li, Peisong Wang, Yuantian Shao, Qinghao Hu, Hongjian Fang, Yifan Zhang, Zhihui Wei, Jian Cheng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出针对LLM的块内PCA结构剪枝框架，解决现有PCA剪枝的额外参数和激活分布破坏问题，通过利用Transformer模块结构特性设计近似PCA方法，提升压缩性能，属于高效大模型训练与推理的关键方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01975' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Limits of Layer Pruning for Generative Reasoning in LLMs</h3>
<p><strong>Authors:</strong> Safal Shrestha, Anubhav Shrestha, Aadim Nepal, Minwu Kim, Keith Ross</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 系统研究层剪枝对LLM生成推理的限制，发现多步推理任务对深度减少敏感，并提出基于自生成响应的微调策略缓解性能下降，对高效大模型的剪枝设计有指导意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01997' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs</h3>
<p><strong>Authors:</strong> Yoonjun Cho, Dongjae Jeon, Soeun Kim, Moongyu Jeon, Albert No</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出结构化残差重建框架，通过保留权重的顶部奇异子空间并平衡量化误差重建的秩预算，提升LLM量化性能，支持量化参数高效微调，属于高效大模型训练与推理的量化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02001' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dissecting Outlier Dynamics in LLM NVFP4 Pretraining</h3>
<p><strong>Authors:</strong> Peijie Dong, Ruibo Fan, Yuechen Tao, Di Mou, Wenhu Hu, Zhenheng Tang, Yinghao Yu, Jiamang Wang, Wenbo Su, Guodong Yang, Liping Zhang, Xiaowen Chu, Baochun Li, Bo Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析LLM NVFP4预训练中的异常值动态，发现异常值集中在特定架构组件且随训练演化，并提出Hot-Channel Patch在线补偿机制，提升低精度训练性能，属于高效大模型训练与推理。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02047' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling</h3>
<p><strong>Authors:</strong> Zisheng Ye, Xiaoyu He, Maoyuan Song, Guoliang Qiu, Chao Liao, Chen Wu, Yonggang Sun, Zhichun Li, Xiaoru Xie, Yuanyong Luo, Hu Liu, Pinyan Lu, Heng Liao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对Transformer的Softmax瓶颈，提出基于HiF8格式和块感知精度调整的低精度方案，减少数据移动带宽和EXP2单元面积，提升推理效率，属于高效大模型训练与推理中的推理优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02071' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Two-Stage Grid Optimization for Group-wise Quantization of LLMs</h3>
<p><strong>Authors:</strong> Junhan Kim, Gukryeol Lee, Seungwoo Son, Jeewook Kim, Yongkweon Jeon</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出两阶段组量化优化框架，通过初始化阶段最小化组内重建损失和精调阶段最小化层间重建损失，提升LLM低比特量化性能，属于高效大模型训练与推理中的量化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02126' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization</h3>
<p><strong>Authors:</strong> Yuli Zhou, Qingxuan Chen, Luca Benini, Guolei Sun, Yawei Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出向量化重新参数化的自适应rounding方法（VQRound），通过紧凑码本最小化元素级最坏情况误差，提升LLM量化性能，属于高效大模型训练与推理中的量化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02151' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs</h3>
<p><strong>Authors:</strong> Weikang Meng, Liangyu Huo, Yadan Luo, Jiawen Guan, Jingyi Zhang, Yingjian Li, Zheng Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出intra-layer混合注意力线性化框架，通过自显著性得分选择显著token进行稀疏软max注意力，其余token用线性注意力总结，提升LLM推理效率，属于高效大模型训练与推理中的推理优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02180' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models</h3>
<p><strong>Authors:</strong> Hao Wang, Hao Gu, Hongming Piao, Kaixiong Gong, Yuxiao Ye, Xiangyu Yue, Sirui Han, Yike Guo, Dapeng Wu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出CurioSFT方法在监督微调阶段保持模型探索能力，通过自适应自蒸馏提升后续RL阶段性能，有效优化大模型训练流程，属于高效训练的重要改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02244' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ReasonCACHE: Teaching LLMs To Reason Without Weight Updates</h3>
<p><strong>Authors:</strong> Sharut Gupta, Phillip Isola, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja, Mark Ibrahim, Mohammad Pezeshki</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出ReasonCACHE通过Prefix Tuning将演示蒸馏到固定缓存中，无需权重更新即可让LLM学习推理，显著提升大模型训练效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02366' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Cross-Modal Memory Compression for Efficient Multi-Agent Debate</h3>
<p><strong>Authors:</strong> Jing Wu, Yue Sun, Tianpei Xie, Suiyao Chen, Jingyuan Bao, Yaopengxiao Xu, Gaoyuan Du, Inseok Heo, Alexander Gutfraind, Xin Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将文本辩论历史压缩为图像表示，减少92%的token使用，显著提升多智能体辩论的推理效率，直接关联高效大模型推理的压缩需求。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00454' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design</h3>
<p><strong>Authors:</strong> Wei Zeng, Xuchen Li, Ruili Feng, Zhen Liu, Fengwei An, Jian Zhao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 通过硬件-算法协同设计，突破生成式模型的“内存墙”，实现高分辨率（720×480）实时生成，属于高效大模型训练与推理的硬件协同优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00608' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models</h3>
<p><strong>Authors:</strong> Yuting Huang, Leilei Ding, Zhipeng Tang, Zenghuan Zhu, Jiajun Deng, Xinrui Lin, Shuo Liu, Haojie Ren, Jianmin Ji, Yanyong Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出无训练的自适应剪枝框架EcoVLA，针对视觉-语言-动作（VLA）模型的环境动态调整稀疏模式，减少推理延迟（最高2.18倍加速），属于高效大模型训练与推理的剪枝优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00780' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement</h3>
<p><strong>Authors:</strong> Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou, Xiaoyue Ma, Jianing Li, Yao Zhu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出动态单样本策略优化（DoPR），减少推理LLM的RL训练开销（近一个数量级），属于高效大模型训练与推理的强化学习优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00815' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Output Critique: Self-Correction via Task Distillation</h3>
<p><strong>Authors:</strong> Hossein A. Rahmani, Mengting Wan, Pei Zhou, Longqi Yang, Nick Craswell, Emine Yilmaz, Sujay Kumar Jauhar</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出任务蒸馏的自纠正框架SELF-THOUGHT，通过任务抽象引导解决方案优化，提升小模型的自纠正能力，属于高效大模型训练与推理的自纠正优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00871' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models</h3>
<p><strong>Authors:</strong> Katrina Brown, Aneesh Muppidi, Rana Shahout</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用轻量级预测器估计查询最优推理长度，动态分配token预算，在相同成本下提升准确率，解决高效大模型推理的计算-准确率权衡问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01237' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression</h3>
<p><strong>Authors:</strong> Aryan Sood, Tanvi Sharma, Vansh Agrawal</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出LASER-KV框架优化KV缓存压缩，克服贪心偏差，Babilong基准上128k上下文下性能超现有方法10%
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02199' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DISK: Dynamic Inference SKipping for World Models</h3>
<p><strong>Authors:</strong> Anugunj Naman, Gaibo Zhang, Ayushman Singh, Yaguang Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出动态推理跳过方法，通过双分支控制器协调视频和轨迹扩散模型，在保持性能的同时提升推理速度，属于高效大模型训练与推理中的推理加速方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00440' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression</h3>
<p><strong>Authors:</strong> Yangzhi Ma, Bojun Liu, Wenting Liao, Dong Liu, Zhu Li, Li Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出分层点基潜表示，压缩动态高斯splatting以提升传输效率，属于高效大模型训练与推理中的压缩方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching</h3>
<p><strong>Authors:</strong> Divya Jyoti Bajpai, Shubham Agarwal, Apoorv Saxena, Kuldeep Kulkarni, Subrata Mitra, Manjesh Kumar Hanawal</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出零成本推测流匹配框架，通过轨迹预测加速流模型推理，解决流模型慢推理问题，有理论分析和多任务实验验证，属于高效大模型推理方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01329' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Yinchao Ma, Qiang Zhou, Zhibin Wang, Xianing Chen, Hanqing Yang, Jun Song, Bo Zheng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于RL的贡献感知token压缩框架，解决视频理解的计算低效问题，属于高效大模型训练与推理中的token压缩方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01649' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models</h3>
<p><strong>Authors:</strong> Arthur Negrão, Pedro Silva, Vander L. S. Freitas, Gladston Moreira, Eduardo Luz</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes Benford-Quant, a Benford's Law-inspired non-uniform quantizer for LLMs, improving compression in few-bit regimes, relevant to efficient LLM training.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00165' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints</h3>
<p><strong>Authors:</strong> Evan Chen, Wenzhi Fang, Shiqiang Wang, Christopher Brinton</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes DA-GRPO for joint continual learning of local LLMs and cloud offloading, addressing resource constraints, relevant to efficient LLM training.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00166' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference</h3>
<p><strong>Authors:</strong> Nikhil Gopal, Kostis Kaffes</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes Harvest, a peer-to-peer GPU caching framework for LLM inference, reducing data movement and improving throughput, relevant to efficient LLM training/inference.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00328' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity</h3>
<p><strong>Authors:</strong> Aayush Gautam, Mukul Gagrani, Junyoung Park, Mingu Lee, Chiris Lott, Narasimha Reddy</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes FastForward to accelerate LLM prefill via predictive FFN sparsity, improving inference speed, relevant to efficient LLM training/inference.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00397' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA</h3>
<p><strong>Authors:</strong> Xiaoyu Wang, Xiaotian Li, Zhixiang Zhou, Chen Li, Yong Liu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对去中心化联邦学习中LoRA微调的稳定性问题，提出TAD-LoRA框架，提升高效微调的鲁棒性，属于高效大模型训练的重要方向
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00451' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models</h3>
<p><strong>Authors:</strong> Apurba Prasad Padhy, Fernando Camacho, Saibal Mukhopadhyay</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于渐近脉冲响应能量的SSM状态剪枝方法，减少状态维度同时保持性能，属于高效大模型训练的重要技术
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00534' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations</h3>
<p><strong>Authors:</strong> Theologos Anthimopoulos, Milad Kokhazadeh, Vasilios Kelefouras, Benjamin Himpel, Georgios Keramidas</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对RISC-V架构优化DNN的Tensor Train分解，通过设计空间探索排除低效分解形状，并结合编译器优化提升边缘设备的DNN部署效率，属于高效大模型训练与推理。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01996' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits</h3>
<p><strong>Authors:</strong> Seoungbin Bae, Junyoung Son, Dabeen Lee</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于用户重试行为的LLM路由和调度算法，通过上下文排队bandit模型学习用户偏好，提升LLM服务的效率和稳定性，属于高效大模型训练与推理中的LLM infra方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02061' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> An Empirical Study of World Model Quantization</h3>
<p><strong>Authors:</strong> Zhongqian Fu, Tianyi Zhao, Kai Han, Hang Zhou, Xinghao Chen, Yunhe Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 系统研究世界模型的量化效果，发现分组权重量化可稳定低比特rollout、激活量化粒度影响不一致等特性，揭示量化对规划任务的影响，属于高效大模型训练与推理中的量化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02110' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence</h3>
<p><strong>Authors:</strong> Qizhen Zhang, Ankush Garg, Jakob Foerster, Niladri Chatterji, Kshitiz Malik, Mike Lewis</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 系统研究噪声数据对LLM预训练损失发散的影响，分析噪声类型、数量和模型规模的作用，为大模型预训练稳定性优化提供实证依据。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02400' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE</h3>
<p><strong>Authors:</strong> Yuanteng Chen, Peisong Wang, Nanxin Zeng, Yuantian Shao, Gang Li, Jing Liu, Jian Cheng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Expert-Sample方法在Fine-grained MoE测试时保持高置信度选择并注入尾部随机性，提升多样本生成性能，属于高效大模型推理改进。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02443' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Conflict-Aware Client Selection for Multi-Server Federated Learning</h3>
<p><strong>Authors:</strong> Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang, Xia Du, Zihan Fang, Zhaolu Kang, Dianxin Luan, Shunzhi Zhu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出RL-CRP框架解决多服务器联邦学习的客户端选择冲突，提升训练效率，属于高效大模型训练与推理的改进。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02458' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning</h3>
<p><strong>Authors:</strong> Qifan Yu, Xinyu Ma, Zhijian Zhuo, Minrui Wang, Deyi Liu, Shiyi Zhan, Yiyuan Ma, Liang Xiang, Xingyan Bin, Di He</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出SPARKLING框架解决宽度渐进学习中的激活不稳定与梯度对称问题，通过RMS-scale一致性与对称打破提升训练稳定性，减少训练成本。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02472' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning More from Less: Unlocking Internal Representations for Benchmark Compression</h3>
<p><strong>Authors:</strong> Yueqi Zhang, Jin Hu, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yiwei Li, Jiayi Shi, Chuyi Tan, Ji Zhang, Boyuan Pan, Yao Hu, Kan Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 利用模型内部表示构建核心集，实现基准测试压缩，仅需10个源模型即可精确估计性能，属于高效大模型训练与推理的基准测试优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00710' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models</h3>
<p><strong>Authors:</strong> Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang, Weijie Liu, Clive Bai, Kai Yang, Yangkun Chen, Saiyong Yang, Xiangyang Ji</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用小型预测模型引导大模型RL后训练，优化prompt选择提升训练效率，多基准实验验证效率与性能双提升
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01970' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing</h3>
<p><strong>Authors:</strong> Mika Okamoto, Ansel Kaplan Erol, Glenn Matlin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出BELLA框架，通过技能轮廓分析优化LLM路由，平衡性能与成本，为大模型部署提供可解释的成本-性能权衡方案
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02386' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Toward Cognitive Supersensing in Multimodal Large Language Model</h3>
<p><strong>Authors:</strong> Boyi Li, Yifan Shen, Yuanzhe Liu, Yifan Xu, Jiateng Liu, Xinzhuo Li, Zhengyuan Li, Jingyuan Zhu, Yunhan Zhong, Fangzhou Lan, Jianguo Cao, James M. Rehg, Heng Ji, Ismini Lourentzou, Xu Cao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出认知超感知范式，通过视觉意象潜在头增强MLLM的认知推理能力，解决感知与认知的 gap，属于原生多模态大模型的认知理解方向。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01541' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings</h3>
<p><strong>Authors:</strong> Yifei Shao, Kun Zhou, Ziming Xu, Mohammad Atif Quamar, Shibo Hao, Zhen Wang, Zhiting Hu, Biwei Huang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出模态混合CoT，通过潜在嵌入 interleaving文本与视觉步骤，解决纯文本CoT在视觉密集任务中的不足，直接针对原生多模态大模型的推理能力提升。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</h3>
<p><strong>Authors:</strong> Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, Jiaqi Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出UniReason框架，将图像生成与编辑整合为知识规划+自反思的统一流程，提升推理密集型任务的性能，属于原生多模态大模型的推理架构研究
Score: 8.5
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02437' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs</h3>
<p><strong>Authors:</strong> Beidi Zhao, Wenlong Deng, Xinting Liao, Yushu Li, Nazim Shaikh, Yao Nie, Xiaoxiao Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对Retrieval-Augmented多模态模型中检索文本抑制视觉注意力的问题，提出MAD-RAG训练-free干预方法，通过双问题构建与注意力混合保留视觉证据，有效提升模型在知识型VQA任务中的表现，解决了RAG在多模态场景中的关键缺陷。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00344' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Jingrui Zhang, Feng Liang, Yong Zhang, Wei Wang, Runhao Zeng, Xiping Hu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出稀疏shortcut架构，高效融合多模态特征以提升MLLM的跨模态理解能力，属于原生多模态大模型中的架构优化方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00505' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DuoGen: Towards General Purpose Interleaved Multimodal Generation</h3>
<p><strong>Authors:</strong> Min Shi, Xiaohui Zeng, Jiannan Huang, Yin Cui, Francesco Ferroni, Jialuo Li, Shubham Pachori, Zhaoshuo Li, Yogesh Balaji, Haoxiang Wang, Tsung-Yi Lin, Xiao Fu, Yue Zhao, Chieh-Yun Chen, Ming-Yu Liu, Humphrey Shi</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出通用interleaved多模态生成框架，结合MLLM和扩散模型实现灵活的多模态内容生成，属于原生多模态大模型中的多模态生成方向创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00508' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning</h3>
<p><strong>Authors:</strong> Wenhao Li, Xianjing Meng, Qiangchang Wang, Zhongyi Han, Zhibin Wu, Yilong Yin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出双级语义构建与强化学习门控注意力，融合视觉与语言的高低层语义，有效提升少样本学习的跨模态对齐精度，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00795' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval</h3>
<p><strong>Authors:</strong> Tong Wang, Yunhan Zhao, Shu Kong</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 利用大语言模型生成“心理图像”解决零-shot组合图像检索，通过构建合成空间匹配多模态查询，训练-free且性能优于现有方法，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00813' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning</h3>
<p><strong>Authors:</strong> Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen, Menglu Han, Wenhao Chen, Yanxiang Huang, Hao Fei, Mong-Li Lee, Wynne Hsu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 引入心理理论（ToM）引导多模态情感推理，提出基准数据集与推理链，提升情感理解的深度与一致性，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00971' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis</h3>
<p><strong>Authors:</strong> Matej Suchanek, Klara Janouskova, Ondrej Vasatko, Jiri Matas</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用Fukunaga-Koontz线性判别分析改进CLIP的监督适应，提升视觉语言特征的类分离性与压缩效率，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment</h3>
<p><strong>Authors:</strong> Lancheng Gao, Ziheng Jia, Zixuan Xing, Wei Sun, Huiyu Duan, Guangtao Zhai, Xiongkuo Min</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 构建大规模图像情感理解数据集，提出多阶段大模型框架，支持情感的多维度分析与细粒度评估，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01173' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ObjEmbed: Towards Universal Multimodal Object Embeddings</h3>
<p><strong>Authors:</strong> Shenghao Fu, Yukun Su, Fengyun Rao, Jing Lyu, Xiaohua Xie, Wei-Shi Zheng (South China University of Technology, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出面向通用多模态对象嵌入的MLLM模型，解决细粒度图像-文本对齐难题，生成区域与全局互补嵌入以支持视觉 grounding等多任务，在18个基准数据集上表现出强语义辨别能力，对原生多模态大模型的细粒度交互研究具有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01753' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Yu Zeng, Wenxuan Huang, Zhen Fang, Shuang Chen, Yufan Shen, Yishuo Cai, Xiaoman Wang, Zhenfei Yin, Lin Chen, Zehui Chen, Shiting Huang, Yiming Zhao, Yao Hu, Philip Torr (University of Oxford), Wanli Ouyang (University of Sydney), Shaosheng Cao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对多模态大语言模型的视觉-文本搜索能力构建基准VDR-Bench，解决现有基准非视觉搜索中心和场景理想化的问题，对原生多模态大模型的评估具有重要参考价值
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02185' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Show, Don't Tell: Morphing Latent Reasoning into Image Generation</h3>
<p><strong>Authors:</strong> Harold Haodong Chen, Xinxiang Yin, Wen-Jie Shu, Hongfei Zhang, Zixin Zhang, Chenfei Liao, Litao Guo, Qifeng Chen (The Hong Kong University of Science and Technology), Ying-Cong Chen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出LatentMorph框架，将隐式潜在推理融入文本到图像生成，解决显式推理的低效和信息丢失问题，有效提升生成模型的推理能力与细化效果，属于原生多模态大模型的生成推理研究
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02227' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unified Personalized Reward Model for Vision Generation</h3>
<p><strong>Authors:</strong> Yibin Wang, Yuhang Zang, Feng Han, Jiazi Bu, Yujie Zhou, Cheng Jin, Jiaqi Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出UnifiedReward-Flex个性化奖励模型，解决现有奖励模型的“一刀切”问题，结合上下文自适应推理提升视觉生成与人类偏好的对齐度，属于原生多模态大模型的奖励机制研究
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02380' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation</h3>
<p><strong>Authors:</strong> Yebin Yang, Huaijin Wu, Fu Guo, Lin Yao, Xiaohan Qin, Jingzhi Wang, Debing Zhang, Junchi Yan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出token嵌入作为新的缩放轴，解耦模型容量与计算量，属于原生多模态大模型中的tokenizer相关研究，完善了tokenizer的缩放规律。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00800' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization</h3>
<p><strong>Authors:</strong> Haochen You, Heng Zhang, Hongyang He, Yuqi Li, Baojing Liu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出GRIT-VQ框架，改进可微分向量量化的代码本利用与梯度稳定性，向量量化是原生多模态大模型tokenizer的核心技术，对提升多模态表示学习效率有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01140' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs</h3>
<p><strong>Authors:</strong> Lv Tang, Tianyi Zheng, Bo Li, Xingyu Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 基于信息瓶颈原理提出InfoTok机制，优化统一多模态大模型的视觉tokenization以平衡压缩与任务相关性，属于原生多模态大模型中的tokenizer研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01554' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning</h3>
<p><strong>Authors:</strong> Zhen-Hao Xie, Jun-Tao Tang, Yu-Cheng Shi, Han-Jia Ye, De-Chuan Zhan, Da-Wei Zhou</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对多模态持续指令调优中的路由器漂移和专家漂移问题，提出稳定的混合专家框架，通过分解路由动态和曲率感知缩放等机制提升多模态大模型的持续学习能力，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01990' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models</h3>
<p><strong>Authors:</strong> Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia, Weili Guan, Zeke Xie, Shuo Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 实证分析视觉语言模型（VLM）中的任务干扰层，提出训练无关的层旁路方法提升性能，直接关联原生多模态大模型的层优化问题。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01167' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling</h3>
<p><strong>Authors:</strong> Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang, Tiejun Zhao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用漫画作为多模态推理中间表示，平衡图像静态性与视频动态性，时间/因果推理性能超图像，效率高于视频
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02453' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating</h3>
<p><strong>Authors:</strong> Hang Wu, Tongqing Chen, Jiasen Wang, Xiaotao Li, Lu Fang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出双系统架构StreamVLA，通过Lock-and-Gated机制优化Vision-Language-Action（VLA）模型的推理-动作循环，减少冗余计算并提升目标稳定性，属于原生多模态大模型的优化研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01100' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models</h3>
<p><strong>Authors:</strong> Shuanghao Bai, Jing Lyu, Wanqi Zhou, Zhe Li, Dakai Wang, Lei Xing, Xiaoguang Zhao, Pengwei Wang, Zhongyuan Wang, Cheng Chi, Badong Chen, Shanghang Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出LaRA-VLA框架，将多模态Chain-of-Thought（CoT）推理融入连续潜在表示，优化VLA模型的推理效率与动作导向控制，属于原生多模态大模型的推理机制研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01166' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization</h3>
<p><strong>Authors:</strong> Zhenpeng Huang, Jiaqi Li, Zihan Jia, Xinhao Li, Desen Meng, Lingxue Song, Xi Chen, Liang Li, Limin Wang (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出两阶段直接偏好优化框架，利用锚定线索和自推理让短上下文VLM处理长视频，无需长视频标注，显著提升长视频理解性能，属于原生多模态大模型的长视频对齐研究
Score: 7.5
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02341' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Personalized Image Generation via Human-in-the-loop Bayesian Optimization</h3>
<p><strong>Authors:</strong> Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出MultiBO方法，利用人类多选择反馈优化扩散模型，有效缩小个性化图像生成的“想象 gap”，属于原生多模态大模型的个性化生成研究
Score: 7.5
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02388' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs</h3>
<p><strong>Authors:</strong> Baiqi Li, Kangyi Zhao, Ce Zhang, Chancharik Mitra, Jean de Dieu Nyandwi, Gedas Bertasius</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对视频LLM时空理解能力评估不足的问题，提出TimeBlind基准，通过最小对范式评估模型对原子事件、事件属性及依赖关系的理解，暴露了前沿模型依赖静态视觉捷径的缺陷，为多模态模型的时空推理能力提升提供了关键诊断工具。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00288' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting</h3>
<p><strong>Authors:</strong> Xin Zhang, Shen Chen, Jiale Zhou, Lei Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出两阶段框架，结合文本布局推理和高斯splatting生成高保真全景3D场景，属于原生多模态大模型中的文本驱动图像生成方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00463' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment</h3>
<p><strong>Authors:</strong> Tianyi Zhang, Antoine Simoulin, Kai Li, Sana Lakdawala, Shiqing Yu, Arpit Mittal, Hongyu Fu, Yu Lin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出多层面视觉语言对齐框架，提升开放词汇目标检测性能，属于原生多模态大模型中的视觉语言对齐方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00531' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment</h3>
<p><strong>Authors:</strong> Lukas Kuhn, Giuseppe Serra, Florian Buettner</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出无需负采样的非对比视觉语言学习框架，提升模态对齐性能，属于原生多模态大模型中的模态学习方向创新。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00653' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning</h3>
<p><strong>Authors:</strong> Zihao Zhao, Shengting Cao, Muchao Ye</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出反射-aware学习框架，通过反射导向的思维链（CoT）提升多模态大模型对视频异常的深度推理能力，符合原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01004' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Videos to Conversations: Egocentric Instructions for Task Assistance</h3>
<p><strong>Authors:</strong> Lavisha Aggarwal, Vikas Bahirwani, Andrea Colaco</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将第一视角教学视频转化为多模态对话数据集，支持大模型的任务辅助训练，提供可扩展的多模态数据资源，符合原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01038' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment</h3>
<p><strong>Authors:</strong> Yunchuan Ma, Laiyun Qing, Guorong Li, Yuqing Liu, Yuankai Qi, Qingming Huang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 结合文本细化与对齐，融合视觉与语言特征，提升点监督时序动作定位的精度，符合原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01257' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination</h3>
<p><strong>Authors:</strong> Yuanshuai Li (), Yuping Yan (), Jirui Han (), Fei Ming (), Lingjuan Lv (), Yaochu Jin ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对多模态大模型的幻觉问题，利用隐式奖励引导内部筛选，解决模态冲突，符合原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01769' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance</h3>
<p><strong>Authors:</strong> Wei Yang, Hong Xie, Tao Tan, Xin Li, Defu Lian, Enhong Chen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 基于层电导量化VLM任务相似性，解决视觉语言模型的迁移性与选择问题，关联原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.01346' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation</h3>
<p><strong>Authors:</strong> Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay, Marcelo H. Ang Jr., Haiyue Zhu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出力蒸馏视觉-语言-动作（VLA）模型，通过力蒸馏模块将学习到的力令牌注入预训练VLM，解决无物理力传感器的接触-rich操作问题，实验验证蒸馏力令牌效果优于直接传感器测量，属于原生多模态大模型的扩展与优化，提升了多模态模型在接触操作中的鲁棒性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02142' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation</h3>
<p><strong>Authors:</strong> Jun He, Junyan Ye, Zilong Huang, Dongzhi Jiang, Chenjue Zhang, Leqi Zhu, Renrui Zhang, Xiang Zhang, Weijia Li (Chinese universities/institutions)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出模拟人类“思考-研究-创造”范式的agentic框架，通过多模态证据检索与推理工具解决生成模型的意图理解与知识推理缺陷，构建Mind-Bench基准并显著提升Qwen-Image等模型性能，是多模态智能体在生成任务中的重要突破。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.01756' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration</h3>
<p><strong>Authors:</strong> Qingni Wang, Yue Fan, Xin Eric Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对GUI grounding模型提出不确定性校准框架，提升预测可靠性，ScreenSpot-Pro上系统精度较Gemini-only提升5.38%
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02419' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generative Visual Code Mobile World Models</h3>
<p><strong>Authors:</strong> Woosung Koh, Sungjun Han, Segyu Lee, Se-Young Yun, Jamin Shin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于可渲染代码的移动GUI世界模型gWorld，通过生成web代码替代直接像素生成，提升GUI Agent的视觉 fidelity 与政策性能，属于多模态智能体中的GUI Agent方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.01576' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?</h3>
<p><strong>Authors:</strong> Weizheng Gu, Chengze Li, Zhuohao Yu, Mengyuan Sun, Zhibang Yang, Wei Wang, Hongrui Jia, Shikun Zhang, Wei Ye</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析轨迹SFT训练的Agent对接口的依赖问题，提出PIPE协议通过最小化接口改写诊断接口 shortcutting，属于多模态智能体中的Agent泛化性研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.01611' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</h3>
<p><strong>Authors:</strong> Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出RLAnything动态优化RL系统的环境、策略与奖励模型，通过闭环优化提升agent性能，属于多模态智能体的重要改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02488' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents</h3>
<p><strong>Authors:</strong> Zhisheng Chen, Tingyu Wu, Zijie Zhou, Zhengwei Xie, Ziyan Weng, Yingwei Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出无训练的极化潜在图记忆框架，解决多模态智能体中文本中心化通信的语义损失问题，支持可验证推理，直接针对多模态智能体的核心挑战。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00415' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dual Latent Memory for Visual Multi-agent System</h3>
<p><strong>Authors:</strong> Xinlei Yu, Chengming Xu, Zhangquan Chen, Bo Yin, Cheng Yang, Yongbo He, Yihao Hu, Jiangning Zhang, Cheng Tan, Xiaobin Hu, Shuicheng Yan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对视觉多智能体系统的“缩放墙”问题，提出双潜在记忆框架，解决文本通信的语义损失，提升可扩展性，属于多模态智能体的核心改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00471' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> World Models as an Intermediary between Agents and the Real World</h3>
<p><strong>Authors:</strong> Sherry Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出将世界模型作为智能体与现实世界的中介，解决高成本领域的样本低效问题，属于多模态智能体的环境交互优化方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00785' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward</h3>
<p><strong>Authors:</strong> Senkang Hu, Yong Dai, Yuzhi Zhao, Yihang Tao, Yu Guo, Zhengru Fang, Sam Tak Wu Kwong, Yuguang Fang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于合成语义信息增益的奖励框架，优化智能体的检索式推理，提升准确率（最高5.4%），属于多模态智能体的推理能力改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00845' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents</h3>
<p><strong>Authors:</strong> Zergham Ahmed, Kazuki Irie, Joshua B. Tenenbaum, Christopher J. Bates, Samuel J. Gershman</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出TheoryCoder-2，通过LLM的上下文学习主动学习抽象，提升程序合成智能体的分层规划能力和样本效率，属于多模态智能体的规划能力改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00929' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts</h3>
<p><strong>Authors:</strong> Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 融合多 grounding 专家构建模仿人类经验的web代理，Online-Mind2Web上达开源最优性能，逼近闭源模型
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02468' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MapDream: Task-Driven Map Learning for Vision-Language Navigation</h3>
<p><strong>Authors:</strong> Guoxin Lian, Shuo Wang, Yucheng Wang, Yongcai Wang, Maiyue Chen, Kaihui Wang, Bo Zhang, Zhizhong Su, Deying Li, Zhaoxin Fan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出任务驱动的地图学习框架，提升视觉-语言导航性能，R2R-CE和RxR-CE上达单目模型最优
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00222' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction</h3>
<p><strong>Authors:</strong> Chaoqun Cui, Jing Huang, Shijing Wang, Liming Zheng, Qingchao Kong, Zhixiong Zeng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对GUI Agent验证的痛点，提出Agentic Interactive Verification框架VAGEN，通过主动交互解决部分可观测性问题，提升GUI Agent的评估准确性，直接对应多模态智能体中的GUI Agent研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00575' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding</h3>
<p><strong>Authors:</strong> Jiahe Wu, Bing Cao, Qilong Wang, Qinghua Hu, Dongdong Li, Pengfei Zhu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出视觉模态思维链引导的强化学习框架，增强MLLM的多模态grounding能力，属于多模态智能体的基础感知与推理方向。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00504' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds</h3>
<p><strong>Authors:</strong> Xianzhe Fan, Shengliang Deng, Xiaoyang Wu, Yuxiang Lu, Zhuoling Li, Mi Yan, Yujia Zhang, Zhizheng Zhang, He Wang, Hengshuang Zhao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将3D点云融入视觉-语言-动作（VLA）模型，通过多源点云数据增强空间理解与鲁棒性，解决2D输入的空间限制问题，属于多模态智能体方向。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00807' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs</h3>
<p><strong>Authors:</strong> Daniel Yezid Guarnizo Orjuela, Leonardo Scappatura, Veronica Di Gennaro, Riccardo Andrea Izzo, Gianluca Bardaro, Matteo Matteucci</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 引入 corruption restoration transformer（CRT）修复视觉输入，提升视觉-语言-动作（VLA）模型对传感器噪声的鲁棒性，属于多模态智能体方向。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.01158' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome</h3>
<p><strong>Authors:</strong> Jiaqi Xu, Tao Huang, Kai Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 在VirtualHome环境下评估LLMs的具身智能任务，提出结构化自洽性策略提升性能，属于多模态智能体的具身评估与改进。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00611' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Persuasion Propagation in LLM Agents</h3>
<p><strong>Authors:</strong> Hyejun Jeong, Amir Houmansadr, Shlomo Zilberstein, Eugene Bagdasarian</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究用户说服对LLM智能体任务行为的影响，发现信念预填充会降低搜索效率，属于多模态智能体的行为机制分析。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act</h3>
<p><strong>Authors:</strong> Pengyuan Guo, Zhonghao Mai, Zhengtong Xu, Kaidi Zhang, Heng Zhang, Zichen Miao, Arash Ajoudani, Zachary Kingston, Qiang Qiu, Yu She</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出模型无关的真实世界机器人Agent平台AgenticLab，结合VLMs的感知与推理能力，基准测试揭示了现有VLM-based manipulation管道的真实环境缺陷，属于多模态智能体的平台与评估研究。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.01662' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SSI-DM: Singularity Skipping Inversion of Diffusion Models</h3>
<p><strong>Authors:</strong> Chen Min, Enze Jiang, Jishen Peng, Zheng Ma</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出奇点跳过的扩散模型反转方法，解决反转过程中的病态问题，生成符合高斯分布的噪声，有效提升扩散模型的编辑能力，属于大模型新技术中的扩散模型核心问题研究
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02193' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</h3>
<p><strong>Authors:</strong> Zehong Ma, Ruihan Xu, Shiliang Zhang (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出PixelGen像素扩散框架，利用感知损失解决高维像素流形的优化难题，性能超过主流潜在扩散模型，属于大模型新技术中的扩散模型架构研究
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02493' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode</h3>
<p><strong>Authors:</strong> Zeyuan He, Yupeng Chen, Lang Lin, Yihan Wang, Shenxu Chang, Eric Sommerlade, Philip Torr, Junchi Yu, Adel Bibi, Jialin Yu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Analyzes the safety of Diffusion LLMs, identifying a context nesting failure mode, relevant to large model new technologies (diffusion LLM) and safety.
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00388' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion</h3>
<p><strong>Authors:</strong> Guinan Chen, Xunpeng Huang, Ying Sun, Shijin Wang, Yanyong Zhang, Chao Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 建立masked扩散与连续高斯过程的对偶性，提出MCD框架实现16×推理加速，属于diffusion LLM的理论与方法突破。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00792' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation</h3>
<p><strong>Authors:</strong> Hongzhou Zhu, Min Zhao, Guande He, Hang Su (Tsinghua University), Chongxuan Li (Tsinghua University), Jun Zhu (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出因果强制的自回归扩散蒸馏方法，桥接双向扩散模型与自回归学生模型的架构鸿沟，显著提升实时交互视频生成性能，属于大模型新技术中的扩散模型蒸馏研究
Score: 8.5
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02214' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models</h3>
<p><strong>Authors:</strong> Alicja Polowczyk, Agnieszka Polowczyk, Piotr Borycki, Joanna Waczyńska, Jacek Tabor, Przemysław Spurek</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对flow matching模型的 artifacts问题，提出训练-free的轨迹校正方法，在不修改模型权重的情况下提升生成质量，属于大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00883' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks</h3>
<p><strong>Authors:</strong> Bohan Zeng, Kaixin Zhu, Daili Hua, Bozhou Li, Chengzhuo Tong, Yuran Wang, Xinyi Huang, Yifan Dai, Zixiang Zhang, Yifan Yang, Zhou Liu, Hao Liang, Xiaochen Ma, Ruichuan An, Tianyi Bai, Hongcheng Gao, Junbo Niu, Yang Shi, Xinlong Chen, Yue Ding, Minglei Shi, Kai Zeng, Yiwen Tang, Yuanxing Zhang, Pengfei Wan, Xintao Wang, Wentao Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析世界模型研究的碎片化问题，提出统一设计规范，为世界模型研究提供结构化视角，属于大模型新技术中的世界模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01630' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention</h3>
<p><strong>Authors:</strong> Dvir Samuel, Issar Tzachor, Matan Levy, Micahel Green, Gal Chechik, Rami Ben-Ari (Tel Aviv University, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对自回归视频扩散的注意力计算瓶颈，提出TempCache缓存压缩、AnnCA/AnnSA稀疏注意力等模块，实现5-10倍推理加速且保持视觉质量，属于大模型新技术中扩散模型高效推理的关键进展。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01801' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly</h3>
<p><strong>Authors:</strong> Hengchang Liu, Zhao Yang, Bing Su</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 揭示扩散语言模型（DLMs）的隐式最优填充长度能力，提出CAL方法无需训练即可提升填充性能，属于大模型新技术（diffusion LLM）的重要进展
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00476' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs</h3>
<p><strong>Authors:</strong> Hao Mark Chen, Zhiwen Mo, Royson Lee, Qianzhou Wang, Da Li, Shell Xu Hu, Wayne Luk, Timothy Hospedales, Hongxiang Fan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出动态专家共享技术解决MoE扩散LLM的内存瓶颈，提升并行推理效率，属于diffusion LLM的高效推理创新。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00879' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective</h3>
<p><strong>Authors:</strong> Zhichao Chen, Hao Wang, Fangyikang Wang, Licheng Pan, Zhengnan Li, Yunfei Teng, Haoxuan Li, Zhouchen Lin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文从近端递归视角分析扩散模型在时间序列填补中的问题，提出SPIRIT框架改进性能，扩散模型是大模型新技术的重要方向，扩展了其在序列数据中的应用。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01182' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> High-accuracy sampling for diffusion models and log-concave distributions</h3>
<p><strong>Authors:</strong> Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出扩散模型的高精度采样算法，实现polylog(1/δ)步数的δ误差，理论证明其效率，是扩散模型领域的重要理论进展，提升了扩散模型的采样质量与效率。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01338' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning</h3>
<p><strong>Authors:</strong> Shangzhe Li, Xuchao Zhang, Chetan Bansal, Weitong Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文将自对弈微调与对抗模仿学习联系，提出基于χ²-divergence的自对弈算法，解决现有方法的稳定性问题，实验验证其在多任务上的优势，是大模型自监督训练的重要进展。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01357' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems</h3>
<p><strong>Authors:</strong> Yiming Yang, Xiaoyuan Cheng, Yi He, Kaiyu Li, Wenxuan Yuan, Zhuo Sun</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析扩散后验采样的稳定性和鲁棒性，提出鲁棒扩散后验采样方法解决似然失配问题，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02045' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unifying Masked Diffusion Models with Various Generation Orders and Beyond</h3>
<p><strong>Authors:</strong> Chunsan Hong, Sanghyun Lee, Jong Chul Ye</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出统一的掩码扩散模型框架（OeMDM），支持多种生成顺序，并扩展为可学习顺序的LoMDM，实现上下文依赖的生成顺序，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02112' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning with Autoregressive-Diffusion Collaborative Thoughts</h3>
<p><strong>Authors:</strong> Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 结合自回归与扩散模型进行联合推理生成，解决自回归模型空间接地不足和扩散模型逻辑控制缺失问题，属于大模型新技术中的扩散LLM方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01608' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SIDiffAgent: Self-Improving Diffusion Agent</h3>
<p><strong>Authors:</strong> Shivank Garg, Ayush Singh, Gaurav Kumar Nayak</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 基于Qwen系列模型构建训练-free扩散代理框架，自主优化 prompt 工程与生成质量，GenAIBench上VQA得分0.884超现有方法
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02051' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV</h3>
<p><strong>Authors:</strong> Zhihao Chen, Yiyuan Ge, Ziyang Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 结合Kolmogorov-Arnold Networks（KAN）与Receptance Weighted Key Value（RWKV）改进3D Flow Matching的backbone，减少模型参数并提升效率，属于大模型新技术中的扩散模型优化方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01115' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Bridging Degradation Discrimination and Generation for Universal Image Restoration</h3>
<p><strong>Authors:</strong> JiaKui Hu, Zhengjian Yao, Lujia Jin, Yanye Lu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出桥接退化判别与生成的扩散模型，提升通用图像恢复性能，属于大模型新技术中的扩散模型创新方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00579' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation</h3>
<p><strong>Authors:</strong> Mingwei Li, Hehe Fan, Yi Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将扩散模型与DINOv3的密集视觉语义结合，解决透明物体法线估计的难点，提升结构细节保留能力，属于大模型新技术方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.00839' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner</h3>
<p><strong>Authors:</strong> Haoqiang Kang (), Yizhe Zhang (), Nikki Lijing Kuang (), Yi-An Ma (), Lianhui Qin ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将潜在扩散模型引入强化学习以解决LLM推理中的多样性 collapse问题，通过扩散引导的 latent空间探索提升推理多样性，符合大模型新技术中的diffusion LLM方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01705' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models</h3>
<p><strong>Authors:</strong> Jinbin Bai (), Yixuan Li (), Yuchen Zhu (), Yi Xin (), Qingyu Shi (), Aosong Feng (), Xiaohong Liu (), Molei Tao (), Jianru Xue (), Xiangtai Li (), Ming-Hsuan Yang ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对离散扩散语言模型提出高效测试时缩放方法，提升推理效率和质量，符合大模型新技术中的diffusion LLM方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01842' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models</h3>
<p><strong>Authors:</strong> Ziwei Luo (), Ziqi Jin (), Lei Wang (), Lidong Bing (), Thomas B. Sch\"on ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出自奖励SMC方法，通过多粒子扩散过程提升掩码扩散语言模型的采样质量，符合大模型新技术中的diffusion LLM方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01849' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Logic-Guided Vector Fields for Constrained Generative Modeling</h3>
<p><strong>Authors:</strong> Ali Baheri</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将符号逻辑注入流匹配生成模型，通过训练时逻辑损失和推理时约束梯度调整解决约束生成问题，属于大模型新技术中的神经符号结合生成方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02009' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics</h3>
<p><strong>Authors:</strong> Nima Shoghi, Yuxuan Liu, Yuning Shen, Rob Brekelmans, Pan Li, Quanquan Gu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出可扩展的SE(3)扩散模型（STAR-MD），通过因果扩散Transformer捕捉时空依赖，实现长时蛋白质动力学模拟，属于大模型新技术中的扩散模型方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02128' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unlocking the Duality between Flow and Field Matching</h3>
<p><strong>Authors:</strong> Daniil Shlenskii, Alexander Varlamov, Nazar Buzun, Alexander Korotin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 揭示条件流匹配（CFM）与交互场匹配（IFM）的对偶性，为生成模型的统一框架提供理论基础，属于大模型新技术的重要进展。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02261' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Expanding the Capabilities of Reinforcement Learning via Text Feedback</h3>
<p><strong>Authors:</strong> Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出用文本反馈增强RL能力，通过自蒸馏与反馈建模提升性能，属于大模型新技术中的RL与文本结合研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02482' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks</h3>
<p><strong>Authors:</strong> Sumit Yadav</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Analyzes the relationship between representation geometry (effective dimension) and generalization across image and NLP models, contributing to deep learning theory.
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00130' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</h3>
<p><strong>Authors:</strong> Franz A. Heinsen, Leo Kozachkov</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes a constant-cost self-attention mechanism using symmetry-aware Taylor approximation, reducing compute and memory for Transformers, relevant to deep learning theory (network architecture).
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00294' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Depth, Not Data: An Analysis of Hessian Spectral Bifurcation</h3>
<p><strong>Authors:</strong> Shenyang Deng, Boyao Liao, Zhuoli Ouyang, Tianyu Pang, Yaoqing Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 证明深度线性网络的Hessian谱分叉由网络深度而非数据不平衡导致，为优化景观与网络架构的关系提供理论依据，属于深度学习理论的关键成果
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00545' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities</h3>
<p><strong>Authors:</strong> Marco Nurisso, Pierrick Leroy, Giovanni Petri, Francesco Vaccarino</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究ReLU网络学习空间的拓扑和几何性质，揭示网络架构相关的连通性与奇点规律，对理解训练动态有重要理论意义。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00693' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases</h3>
<p><strong>Authors:</strong> Zixiao Wang, Yifei Shen, Huishuai Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出OLion优化器，结合谱控制与ℓ∞隐式偏置，理论证明其收敛性，并在GPT-2、Llama等大模型预训练中验证优于AdamW等基线，是优化器设计领域的重要突破。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01105' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SimpleGPT: Improving GPT via A Simple Normalization Strategy</h3>
<p><strong>Authors:</strong> Marco Chen, Xianbiao Qi, Yelin He, Jiaquan Ye, Rong Xiao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文通过二阶几何分析提出SimpleNorm归一化策略，解决Transformer训练中的激活 scale不稳定问题，在1B-8B参数模型上验证其提升训练稳定性与性能，是大模型架构优化的重要成果。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01212' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Dispelling the Curse of Singularities in Neural Network Optimizations</h3>
<p><strong>Authors:</strong> Hengjie Cao, Mengyi Chen, Yifeng Yang, Fang Dong, Ruijun Huang, Anrui Chen, Jixian Zhou, Mingzhi Dong, Yujiang Wang, Dongsheng Li, Wenyi Fang, Yuanyi Lin, Fan Wu, Li Shang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文研究神经网络优化中的奇点问题，提出PSS方法平滑权重矩阵的奇异谱，解决训练不稳定性，实验验证其提升训练效率与泛化性能，是深度学习理论中优化稳定性的核心突破。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01308' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Phase Transitions for Feature Learning in Neural Networks</h3>
<p><strong>Authors:</strong> Andrea Montanari, Zihao Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析两层神经网络特征学习的相变现象，推导梯度下降动力学的阈值δ_NN，深入探究网络架构与训练算法的理论关系，属于深度学习理论的核心研究内容。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01434' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers</h3>
<p><strong>Authors:</strong> Ionut-Vlad Modoranu, Philip Zmushko, Erik Schultheis, Mher Safaryan, Dan Alistarh</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出更快的Shampoo优化器实现，通过批处理块预处理提升GPU利用率，并引入Newton-DB迭代等高效逆根求解方法，解决Shampoo计算慢的问题，属于深度学习理论中的优化器方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02016' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Backpropagation as Physical Relaxation: Exact Gradients in Finite Time</h3>
<p><strong>Authors:</strong> Antonino Emanuele Scurria</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将反向传播解释为物理系统的有限时间弛豫过程，建立连续物理动态与离散反向传播的精确对应，为深度学习理论提供新的物理视角。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02281' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Multi-Head Attention Is a Multi-Player Game</h3>
<p><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundar</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将多头注意力形式化为潜在博弈，证明无政府价格（PoA）边界，并提出GAME-LoRA减少幻觉，属于深度学习理论中的注意力机制分析。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00861' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</h3>
<p><strong>Authors:</strong> Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Identity Bridge数据正则化方法，理论证明单 transformer 层可打破反转诅咒，1B模型微调后反转任务成功率达40%
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02470' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval</h3>
<p><strong>Authors:</strong> Zizheng Zhang, Yuyang Liao, Chen Chen, Jian He, Dun Wu, Qianjin Yu, Yanqin Gao, Jin Yang, Kailai Zhang, Eng Siong Chng, Xionghu Zhong</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出TextBFGS方法，将拟牛顿优化应用于离散文本，通过梯度算子检索近似逆Hessian，提升离散文本优化的收敛速度，属于深度学习理论中的优化器研究
Score: 8.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00059' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning</h3>
<p><strong>Authors:</strong> Brady Steele</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出LoRA抗标签噪声的理论框架，分析其容量限制与噪声抵抗机制，为参数高效微调的鲁棒性研究提供理论基础，属于深度学习理论中的参数高效微调研究
Score: 8.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00084' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> 3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting</h3>
<p><strong>Authors:</strong> Roger Hsiao, Yuchen Fang, Xiangru Huang, Ruilong Li, Hesam Rabeti, Zan Gojcic, Javad Lavaei, James Demmel, Sophia Shao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出适用于3D高斯splatting的二阶优化器，采用Hutchinson方法近似Hessian对角线并结合参数级信任域技术，解决了现有二阶方法的复杂度问题，属于深度学习理论中的优化器方向创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00395' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection</h3>
<p><strong>Authors:</strong> Kunal Mahatha, Ali Bahri, Pierre Marza, Sahar Dastani, Maria Vakalopoulou, Stergios Christodoulidis, Jose Dolz, Christian Desrosiers</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 改进视觉状态空间模型（SSMs）的空间感知能力，通过多方向扫描与遍历选择解决因果结构导致的空间关系丢失问题，属于深度学习理论方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00904' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas</h3>
<p><strong>Authors:</strong> Christoffer Koo Øhrstrøm, Rafael I. Cabral Muchacho, Yifei Dong, Filippos Moumtzidellis, Ronja Güldenring, Florian T. Pokorny, Lazaros Nalpantidis</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于抛物线的视觉位置编码PaPE，解决视觉Transformer位置编码的模态适配问题，通过多数据集验证有效性，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01418' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Sparse Visual Representations via Spatial-Semantic Factorization</h3>
<p><strong>Authors:</strong> Theodore Zhengde Zhao, Sid Kiblawi, Jianwei Yang, Naoto Usuyama, Reuben Tan, Noel C Codella, Tristan Naumann, Hoifung Poon, Mu Wei (Microsoft, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出空间-语义分解框架解决自监督学习中语义理解与图像重建的冲突，用少量稀疏token同时支持高保真重建与语义任务，属于深度学习理论中表示学习的重要进展。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism</h3>
<p><strong>Authors:</strong> Ming-Yao Ho, Cheng-Kai Wang, You-Teng Lin, Hung-Hsuan Chen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出SCPL方法，解耦反向传播为多个短梯度流，显著提升神经网络训练的吞吐量，属于深度学习理论中的训练架构优化研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00062' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective</h3>
<p><strong>Authors:</strong> Qiyao Liang, Jinyeop Song, Yizhou Liu, Jeff Gore, Ila Fiete, Risto Miikkulainen, Xin Qiu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Explores the role of dimensionality in LLM fine-tuning, analyzing reward landscapes and optimization dynamics, contributing to deep learning theory.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00170' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors</h3>
<p><strong>Authors:</strong> Arian Khorasani, Nathaniel Chen, Yug D Oswal, Akshat Santhana Gopalan, Egemen Kolemen, Ravid Shwartz-Ziv</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Uses normalizing flows as oracles to study scaling laws, generalization, and active learning, contributing to deep learning theory.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00315' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Convergent World Representations and Divergent Tasks</h3>
<p><strong>Authors:</strong> Core Francisco Park</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究多任务训练下的世界表示收敛性，揭示任务对表示几何的影响，为多任务学习的理论理解提供可控证据，属于深度学习理论的重要研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00533' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs</h3>
<p><strong>Authors:</strong> Tushaar Gangavarapu, Jiping Li, Christopher Vattheuer, Zhangyang Wang, Baharan Mirzasoleiman</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 揭示数据分布对优化器泛化的引导作用，通过调整数据分布减少简单性偏差，提升LLM泛化性能，属于深度学习理论（优化与泛化）的重要进展
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00576' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization</h3>
<p><strong>Authors:</strong> Taesun Yeom, Taehyeok Ha, Jaeho Lee</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究特征学习强度对泛化的影响，发现最优特征学习强度的存在，解释了过对齐与过拟合的权衡关系，属于深度学习理论的核心问题。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00827' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments</h3>
<p><strong>Authors:</strong> Fuxin Wang, Amr Alazali, Yiqiao Zhong</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文通过合成实验系统研究自回归训练中Chain-of-Thought推理不忠实性的形成机制，涉及训练动态、简单性偏差等深度学习理论核心问题，为理解模型推理过程的训练根源提供了关键 insights。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01017' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Superposition unifies power-law training dynamics</h3>
<p><strong>Authors:</strong> Zixin Jessie Chen, Hao Chen, Yizhou Liu, Jeff Gore</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文通过教师-学生框架揭示特征叠加在幂律训练动态中的核心作用，证明叠加瓶颈可诱导通用幂律指数，为理解大模型训练动态的理论机制提供了统一视角。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01045' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse</h3>
<p><strong>Authors:</strong> Perry Dong, Kuo-Han Hung, Alexander Swerdlow, Dorsa Sadigh, Chelsea Finn</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对Transformer在RL值函数中的注意力崩溃问题，提出TQL方法通过控制注意力熵稳定训练并提升性能，属于深度学习理论中网络架构缩放与训练稳定性的重要研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01439' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts</h3>
<p><strong>Authors:</strong> Viet Nguyen, Tuan Minh Pham, Thinh Cao, Tan Dinh, Huy Nguyen, Nhat Ho, Alessandro Rinaldo</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用分层混合专家模型解释门控注意力的统计特性，证明其样本效率优势并分析门控位置的影响，属于深度学习理论中注意力机制的理论分析方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01468' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability</h3>
<p><strong>Authors:</strong> Eric Regis, Sinho Chewi</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Rod Flow连续时间模型解释梯度下降在稳定边缘的动力学行为，理论证明其对临界锐度阈值的预测能力，属于深度学习理论中训练算法的理论研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01480' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Effect of Mini-Batch Noise on the Implicit Bias of Adam</h3>
<p><strong>Authors:</strong> Matias D. Cattaneo (), Boris Shigida ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究Adam优化器的隐式偏差，分析mini-batch噪声对优化器记忆隐式偏差的影响，提出理论框架并通过实验验证，符合深度学习理论中optimizer的研究方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01642' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration</h3>
<p><strong>Authors:</strong> Lianhai Ren (), Yucheng Ding (), Xiao Liu (), Qianxiao Li (), Peng Cheng (), Yeyun Gong ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对LLM训练不稳定问题，提出MSign优化器通过定期恢复权重矩阵的稳定秩来防止训练崩溃，符合深度学习理论中的optimizer方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01734' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention</h3>
<p><strong>Authors:</strong> Xiaowei Ye (), Xiaoyu He (), Chao Liao (), Chen Wu (), Pinyan Lu ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 理论证明混合线性-全注意力的表达能力层次，首次建立线性注意力与全注意力的表达能力分离，符合深度学习理论中的network architecture方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01763' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> State Rank Dynamics in Linear Attention LLMs</h3>
<p><strong>Authors:</strong> Ao Sun, Hongtao Zhang, Heng Zhou, Yixuan Ma, Yiran Qin, Tongrui Su, Yan Liu, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究线性注意力LLM的状态秩动态，发现低秩头对推理至关重要，提出联合秩-范数剪枝策略，提升效率，属于
[PAPER_START]
Title: Spectral Superposition: A Theory of Feature Geometry
Authors: Georgi Ivanov, Narmeen Oozeer, Shivam Raval, Tasana Pejovic, Shriyash Upadhyay, Amir Abdullah
Published: 2026-02-03
Link: https://arxiv.org/abs/2602.02224
Reason: 提出谱叠加理论分析神经网络特征的几何结构，结合算子理论推进特征交互的全局分析，为深度学习理论中特征表示的几何理解提供新视角。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02195' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Statistical Learning Theory in Lean 4: Empirical Processes from Scratch</h3>
<p><strong>Authors:</strong> Yuanhe Zhang, Jason D. Lee, Fanghui Liu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 首次用Lean 4形式化统计学习理论，包括高斯浓度、Dudley熵积分定理等，为深度学习理论提供严格的形式化基础。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02285' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Transformers learn factored representations</h3>
<p><strong>Authors:</strong> Adam Shai, Loren Amdahl-Culleton, Casper L. Christensen, Henry R. Bigelow, Fernando E. Rosas, Alexander B. Boyd, Eric A. Alt, Kyle J. Ray, Paul M. Riechers</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 证明Transformer通过因式分解学习表示，将世界分解为正交子空间中的部分，为Transformer的表示学习理论提供实证与理论支持。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02385' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Poly-attention: a general scheme for higher-order self-attention</h3>
<p><strong>Authors:</strong> Sayak Chakrabarti, Toniann Pitassi, Josh Alman</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Poly-attention框架统一高阶自注意力机制，分析其计算复杂度与表示能力，为Transformer注意力机制设计提供理论指导。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02422' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation</h3>
<p><strong>Authors:</strong> Seo Taek Kong, R. Srikant</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 推导非线性随机近似的有限样本Wasserstein误差界与浓度不等式，为深度学习中的优化算法提供理论保证。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02445' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic</h3>
<p><strong>Authors:</strong> Yani Zhang, Helmut B\"olcskei</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 利用多值逻辑完全识别深层ReLU神经网络的功能等价类，为深度学习理论中的网络可识别性提供新方法。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00266' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Geometric Analysis of Token Selection in Multi-Head Attention</h3>
<p><strong>Authors:</strong> Timur Mudarisov, Mikhal Burtsev, Tatiana Petrova, Radu State</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 从几何角度解析多头注意力的token选择机制，提出Precision/Recall等量化指标，结合理论推导与多模型实验验证注意力的结构特性
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01893' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Emergent Analogical Reasoning in Transformers</h3>
<p><strong>Authors:</strong> Gouki Minegishi, Jingyuan Feng, Hiroki Furuta, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用范畴论函子形式化类比推理，揭示Transformer中类比推理的涌现机制（几何对齐+函子应用），理论与实验结合验证
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01992' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation</h3>
<p><strong>Authors:</strong> Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari, Mert D. Pesé</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出非参数网络架构，采用自适应高斯-傅里叶位置编码实现3D分类与分割，属于深度学习理论中的网络架构方向创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00542' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition</h3>
<p><strong>Authors:</strong> Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong, Wei Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于对称正定（SPD）流形的二阶几何统计框架，捕捉视觉表示的几何稳定性，提升零-shot场景下的place recognition鲁棒性，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00841' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025</h3>
<p><strong>Authors:</strong> Phu-Hoa Pham, Chi-Nguyen Tran, Dao Sy Duy Minh, Nguyen Lam Phu Quy, Huynh Trung Kiet</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究模型复杂度对视觉鲁棒性与神经对齐的影响，通过简单架构实现高鲁棒性，通过深层模型实现神经对齐，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00982' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification</h3>
<p><strong>Authors:</strong> Ying Shu, Pujian Zhan, Huiqi Yang, Hehe Fan, Youfang Lin, Kai Lv</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出双正则化双向Transformer，融合视觉基础模型的局部纹理与视觉语言模型的全局语义，提升行人重识别的特征判别性，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01059' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion</h3>
<p><strong>Authors:</strong> Chunming He, Rihan Zhang, Fengyang Xiao, Dingming Zhang, Zhiwen Cao, Sina Farsiu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 结合课程选择与反课程促进，优化上下文纠缠内容的分割模型，提升特征表示的可靠性与泛化性，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01183' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting</h3>
<p><strong>Authors:</strong> Hao Chen, Tao Han, Jie Zhang, Song Guo, Fenghua Ling, Lei Bai</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出高效多尺度Transformer，通过单卷积提取多尺度特征，提升天气预测的长上下文建模能力与推理速度，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01194' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Gauss-Newton Natural Gradient Descent for Shape Learning</h3>
<p><strong>Authors:</strong> James King, Arturs Berzins, Siddhartha Mishra, Marius Zeinhofer</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes Gauss-Newton natural gradient descent for shape learning, addressing ill-conditioning and optimization mismatch, relevant to deep learning theory (optimizer).
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00099' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models</h3>
<p><strong>Authors:</strong> Chen Liu, Xingzhi Sun, Xi Xiao, Alexandre Van Tassel, Ke Xu, Kristof Reimann, Danqi Liao, Mark Gerstein, Tianyang Wang, Xiao Wang, Smita Krishnaswamy</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Identifies embedding condensation in small LLMs and proposes dispersion loss to mitigate it, improving generalization, relevant to deep learning theory.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00217' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Momentum and Nonlinear Damping for Neural Network Training</h3>
<p><strong>Authors:</strong> Aikaterini Karoni, Rajit Rajpal, Benedict Leimkuhler, Gabriel Stoltz</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes adaptive momentum with cubic damping for neural network training, improving stability and convergence, relevant to deep learning theory (optimizer).
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00334' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Softmax Linear Attention: Reclaiming Global Competition</h3>
<p><strong>Authors:</strong> Mingwei Xu (), Xuan Lin (), Xinnan Guo (), Wanqing Xu (), Wanyun Cui ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 改进线性注意力机制，通过head-level的softmax恢复全局竞争能力，提升线性注意力的表达能力，符合深度学习理论中的network architecture方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01744' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It</h3>
<p><strong>Authors:</strong> Yaxiang Zhang (), Yingru Li (), Jiacai Liu (), Jiawei Xu (), Ziniu Li (), Qian Liu (), Haoyuan Li ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析训练-推理不匹配的优化本质，通过学习率调度解决不稳定问题，符合深度学习理论中的training optimization方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01826' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning</h3>
<p><strong>Authors:</strong> Youqi Wu, Farzan Farnia</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 扩展冯诺依曼熵的最大熵原理到机器学习，提供游戏理论解释，并应用于核表示选择和核矩阵补全等问题，属于深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02117' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Variational Entropic Optimal Transport</h3>
<p><strong>Authors:</strong> Roman Dyachenko, Nikita Gushchin, Kirill Sokolov, Petr Mokrov, Evgeny Burnaev, Alexander Korotin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出变分熵最优传输（VarEOT），通过辅助归一化器的变分重构解决熵最优传输的计算难题，为深度学习理论中的优化方法提供新工具。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02241' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control</h3>
<p><strong>Authors:</strong> Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Sungheon Jeong, Mohsen Imani</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出HopFormer通过头特定n-hop稀疏注意力注入图结构，实现显式感受野控制，改进Graph Transformer架构设计，属于深度学习理论中的网络架构研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02268' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Supervised Learning from Structural Invariance</h3>
<p><strong>Authors:</strong> Yipeng Zhang, Hafez Ghaemi, Jungyoon Lee, Shahab Bakhtiari, Eilif B. Muller, Laurent Charlin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于结构不变性的自监督学习方法，解决一对多映射问题，为自监督学习的表示学习理论提供新视角。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02381' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Trust Region Continual Learning as an Implicit Meta-Learner</h3>
<p><strong>Authors:</strong> Zekun Wang, Anant Gupta, Christopher J. MacLellan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出信任区域持续学习框架，结合生成重放和信任区域约束实现隐式元学习，属于深度学习理论中的持续学习研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02417' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics</h3>
<p><strong>Authors:</strong> Sangwoo Shin, BumJun Kim, Kyelim Lee, Moongyu Jeon, Albert No</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 分析掩码扩散模型缓解反转诅咒的机制，结合注意力结构与训练动态，理论推导与实验验证其效果
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02133' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Flow Policy Gradients for Robot Control</h3>
<p><strong>Authors:</strong> Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong, Carmelo Sferrazza, Yi Ma, Rocky Duan, Pieter Abbeel, Guanya Shi, Karen Liu, Angjoo Kanazawa</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于流匹配的政策梯度框架，绕过似然计算以训练更具表达力的机器人控制政策，在腿式移动、人形运动跟踪和操作任务中验证了有效性，属于深度学习理论中的优化方法创新，为机器人控制的政策学习提供了新的思路。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02481' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions</h3>
<p><strong>Authors:</strong> Akram Aldroubi</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出可解释的PUNN架构，通过划分unity直接定义类概率，无需softmax，实现透明的类分配，属于深度学习可解释性的关键进展
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00511' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> EDIS: Diagnosing LLM Reasoning via Entropy Dynamics</h3>
<p><strong>Authors:</strong> Chenghua Zhu, Siyan Wu, Xiangkang Zeng, Zishan Xu, Zhaolu Kang, Yifu Guo, Yuquan Lu, Junduan Huang, Guojing Zhou</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出EDIS指标，通过熵动态分析诊断LLM推理的正确性，发现错误推理的特征模式（如突发 spike），实验验证其提升推理准确性，是可解释性领域中推理过程诊断的重要成果。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01288' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding</h3>
<p><strong>Authors:</strong> Panagiotis Koromilas, Andreas D. Demou, James Oldfield, Yannis Panagakis, Mihalis Nicolaou</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出PolySAE，通过多项式解码扩展稀疏自编码器，捕捉特征交互，提升模型的解释性与探针性能，解决线性SAE无法建模组合结构的问题，是可解释性领域的重要创新。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01322' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks</h3>
<p><strong>Authors:</strong> Jia Liang, Liangming Pan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 采用logit-lens、线性探针、注意力分析等方法，对Latent-CoT模型的分步推理机制进行深入研究，属于深度学习可解释性的核心内容（白盒解释）。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs</h3>
<p><strong>Authors:</strong> Benno Krojer, Shravan Nayak, Oscar Mañas, Vaibhav Adlakha, Desmond Elliott, Siva Reddy, Marius Mosbach</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出将VLM的潜视觉表示映射到自然语言描述的方法，解释视觉token的含义，显著提升了VLM的可解释性，属于深度学习可解释性方向的创新。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00462' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering</h3>
<p><strong>Authors:</strong> Guangtao Lyu, Xinyi Cheng, Qi Liu, Chenghao Xu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用稀疏自动编码器分析VLM神经元以揭示幻觉成因，提出对比神经元引导方法缓解幻觉，属于深度学习可解释性与大模型安全的结合，核心是可解释性分析。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00621' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Preserving Localized Patch Semantics in VLMs</h3>
<p><strong>Authors:</strong> Parsa Esmaeilkhani, Longin Jan Latecki</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 引入Logit Lens Loss约束视觉token保持局部语义，解决VLMs中视觉信息扩散导致的可解释性问题，属于深度学习可解释性核心方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01530' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ReasonEdit: Editing Vision-Language Models using Human Reasoning</h3>
<p><strong>Authors:</strong> Jiaxing Qiu, Kaihua Hou, Roxana Daneshjou, Ahmed Alaa, Thomas Hartvigsen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出ReasonEdit框架，通过存储和检索人类推理轨迹编辑视觉语言模型，显著提升编辑的泛化性与准确性，属于深度学习可解释性中的模型编辑研究
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02408' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits</h3>
<p><strong>Authors:</strong> Neha Kalibhat, Zi Wang, Prasoon Bajpai, Drew Proud, Wenjun Zeng, Been Kim (Google Research), Mani Malek</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出通过原子概念编辑学习模型行为的可验证“宪法”，提升模型的可解释性与可控性，属于深度学习可解释性中的模型行为理解研究
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00092' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity</h3>
<p><strong>Authors:</strong> Jordan Levy, Paul Saves, Moncef Garouani, Nicolas Verstaevel, Benoit Gaudou</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Uses Shapley Additive Explanations to analyze anomaly detection algorithms, measuring complementarity via attribution profiles, directly relevant to deep learning explainability (Shapley value).
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00208' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> In-Run Data Shapley for Adam Optimizer</h3>
<p><strong>Authors:</strong> Meng Ding, Zeqing Zhang, Di Wang, Lijie Hu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes Adam-aware In-Run Data Shapley to estimate data contributions dynamically, addressing optimizer-dependent attribution, relevant to deep learning explainability (Shapley value).
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00329' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes</h3>
<p><strong>Authors:</strong> Maryam Maghsoudi, Ayushi Mishra</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文通过机械可解释性方法研究脑-语模型的跨模态表示机制，发现层特定紧凑子空间介导跨模态转移，属于深度学习可解释性中的white-box解释，为多模态模型理解提供了重要视角。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01247' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> An Odd Estimator for Shapley Values</h3>
<p><strong>Authors:</strong> Fabian Fumagalli, Landon Butler, Justin Singh Kang, Kannan Ramchandran, R. Teal Witter</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对Shapley值的高效近似问题展开研究，提出OddSHAP方法，通过分离集合函数的奇数分量并结合傅里叶基与代理模型提升估计精度，直接贴合深度学习可解释性中Shapley value这一核心方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01399' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks</h3>
<p><strong>Authors:</strong> Donald Ye</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出梯度-因果 gap概念，系统分析梯度重要性在复杂任务中的失效原因及对模型剪枝的影响，属于深度学习可解释性中梯度-based解释方法的局限性研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01442' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs</h3>
<p><strong>Authors:</strong> Wenbo Pan (), Zhichao Liu (), Xianlong Wang (), Haining Yu (), Xiaohua Jia ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出FlashTrace方法，解决LLM长上下文推理的多token归因问题，提升可解释性的效率和忠实度，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01914' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance</h3>
<p><strong>Authors:</strong> Hyunsuk Chung, Caren Han, Yerin Choi, Seungyeon Ji, Jinwoo Kim, Eun-Jung Holden, Kyungreem Han</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出指令条件的参数高效自适应框架，通过特征组对齐LoRA模块和指令条件门控控制模型对内部特征的依赖，实现对核心/虚假特征的选择性放大/抑制，属于深度学习可解释性中的特征依赖控制方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs</h3>
<p><strong>Authors:</strong> Liyan Xu, Mo Yu, Fandong Meng, Jie Zhou</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 通过Tele-Lens探测方法研究LLM的CoT潜在规划能力，发现其短视特性（仅增量过渡无全局规划），并验证小部分CoT位置可代表整个路径的不确定性，属于深度学习可解释性中的CoT解释方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02103' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Interpretable Tabular Foundation Models via In-Context Kernel Regression</h3>
<p><strong>Authors:</strong> Ratmir Miftachov, Bruno Charron, Simon Valentin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出KernelICL框架，通过核回归（高斯、点积等）增强表格基础模型的可解释性，实现透明的样本加权预测，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02162' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Supervised sparse auto-encoders as unconstrained feature models for semantic composition</h3>
<p><strong>Authors:</strong> Ouns El Harzli, Hugo Wallner, Yoonsoo Nam, Haixuan Xavier Tao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出有监督稀疏自编码器（SAE），解决SAE的L1惩罚非光滑和语义对齐问题，支持特征级干预，属于深度学习可解释性的机械可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00924' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)</h3>
<p><strong>Authors:</strong> Zeinab Dehghani</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 扩展SMILE框架至生成式AI，提供token级归因和热图解释，解决生成式模型的可解释性问题，直接关联深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01206' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient</h3>
<p><strong>Authors:</strong> Changming Li, Kaixing Zhang, Haoyun Xu, Yingdong Shi, Zheng Zhang, Kaitao Song, Kan Ren</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出IPG框架，通过策略梯度反向传播解释LLM推理行为，实现精准定位与推理调制，实验验证其有效性
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02313' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Structure Enables Effective Self-Localization of Errors in LLMs</h3>
<p><strong>Authors:</strong> Ankur Samanta, Akshayaa Magesh, Ayush Jain, Kavosh Asadi, Youliang Yu, Daniel Jiang, Boris Vidolov, Kaveh Hassani, Paul Sajda, Jalaj Bhandari, Yonathan Efroni</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出结构化推理提示方法，让LLM有效定位错误步骤，自纠正性能比传统CoT提升20-40%，可解释性强
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02416' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations</h3>
<p><strong>Authors:</strong> Leonidas Christodoulou, Chang Sun</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 系统研究模型不确定性对反事实解释鲁棒性的影响，发现解释对模型不确定性高度敏感，对深度学习可解释性的可靠性提升具有重要启示
Score: 7.5
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00063' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies</h3>
<p><strong>Authors:</strong> Wenjin Hou, Wei Liu, Han Hu, Xiaoxiao Sun, Serena Yeung-Levy, Hehe Fan (Stanford University, Google, etc.)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 构建VIA-Bench基准评估MLLM对视觉错觉与异常的处理能力，揭示模型感知瓶颈与CoT推理的局限性，为深度学习可解释性中模型行为理解提供重要依据。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01816' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models</h3>
<p><strong>Authors:</strong> Cristian Sbrolli, Matteo Matteucci, Toshihiko Yamasaki (Politecnico di Milano, Osaka University)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 开发自动化基准生成 pipeline 评估VLMs的组合推理能力，揭示CLIP/SigLIP等模型的普遍缺陷与visio-linguistic上下文的权衡关系，为深度学习可解释性中模型组合推理分析提供工具支持。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02043' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models</h3>
<p><strong>Authors:</strong> Joseph L. Breeden</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Argues that forecast uncertainty is key to explainability, criticizing local linear methods like LIME/SHAP, relevant to deep learning explainability.
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00179' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers</h3>
<p><strong>Authors:</strong> Ajay Jaiswal, Lauren Hannah, Han-Byul Kim, Duc Hoang, Arnav Kundu, Mehrdad Farajtabar, Minsik Cho</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> Proposes MemoryLLM to decouple FFNs as interpretable memory, improving transparency of Transformer components, relevant to deep learning explainability (white-box).
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00398' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation</h3>
<p><strong>Authors:</strong> Seonghyeon Park (), Jewon Yeom (), Jaewon Sok (), Jeongjae Park (), Heejun Kim (), Taesup Kim ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 通过知识蒸馏高效估计LLM的认知不确定性，减少幻觉，符合深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01956' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach</h3>
<p><strong>Authors:</strong> Jiancheng Tu, Wenqi Fan, Zhibin Wu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出MIP-based框架，优化非线性指标（如F1-score）的最优分类树，通过问题特定加速技术提升 scalability，属于深度学习可解释性中的可解释模型方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02173' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Interpretability in Deep Time Series Models Demands Semantic Alignment</h3>
<p><strong>Authors:</strong> Giovanni De Felice, Riccardo D'Elia, Alberto Termine, Pietro Barbiero, Giuseppe Marra, Silvia Santini</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对深度时间序列模型的黑箱问题，提出语义对齐的可解释性要求，强调推理过程与人类认知的一致性，为时间序列模型的可解释性研究提供新框架。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02239' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models</h3>
<p><strong>Authors:</strong> Shaima Ahmad Freja, Ferhat Ozgur Catak, Betul Yurdem, Chunming Rong</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> A Framework for Step-Level Reasoning Evaluation in Large Language Models
Authors: Shaima Ahmad Freja, Ferhat Ozgur Catak, Betul Yurdem, Chunming Rong
Published: 2026-02-03
Link: https://arxiv.org/abs/2602.02295
Reason: 提出EvalQReason框架通过步骤级概率分布分析评估LLM推理质量，无需人工标注，为深度学习可解释性中的推理过程评估提供新方法。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02295' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning</h3>
<p><strong>Authors:</strong> Qihao Wen, Jiahao Wang, Yang Nan, Pengfei He, Ravi Tandon, Han Xu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于嵌入扰动的不确定性量化方法，有效识别LLM推理中的错误步骤，为深度学习可解释性中的不确定性评估提供新视角。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02427' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Localizing and Correcting Errors for LLM-based Planners</h3>
<p><strong>Authors:</strong> Aditya Kumar, William W. Cohen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Localized In-Context Learning方法定位并纠正LLM规划器的错误步骤，显著提升规划有效性，属于深度学习可解释性的应用。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00276' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach</h3>
<p><strong>Authors:</strong> Martino Ciaperoni, Marzio Di Vece, Luca Pappalardo, Fosca Giannotti, Francesco Giannini</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出比较性解释框架Δ-XAI，强调通过参考模型与干预模型的对比解释LLM行为变化，为可解释性研究提供新范式
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02304' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</h3>
<p><strong>Authors:</strong> Muhammed Ustaomeroglu, Guannan Qu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 通过阻塞因果特征防止涌现错位，在不影响目标任务的情况下大幅减少错位，是大模型对齐的关键技术创新。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00767' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents</h3>
<p><strong>Authors:</strong> Pengfei He, Ash Fox, Lesly Miculicich, Stefan Friedli, Daniel Fabian, Burak Gokturk, Jiliang Tang, Chen-Yu Lee, Tomas Pfister, Long T. Le</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出多Agent框架用于LLM的安全发现和利用，通过整合安全领域知识、执行反馈和长时记忆，提升LLM安全评估能力，属于大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02164' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Self-Guard: Defending Large Reasoning Models via enhanced self-reflection</h3>
<p><strong>Authors:</strong> Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng, Zhenkai Liang, Xiang Wang, An Zhang, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出轻量级安全防御框架，通过安全导向提示和激活 steering 桥接大推理模型的“认知-合规 gap”，提升安全性能且不损失效用，属于大模型安全与对齐的核心改进。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> How RLHF Amplifies Sycophancy</h3>
<p><strong>Authors:</strong> Itai Shapira, Gerdus Benade, Ariel D. Procaccia</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究RLHF如何放大阿谀奉承行为，通过形式化分析揭示对齐过程中行为漂移的因果机制，并提出干预策略，直接关联大模型安全与对齐的核心问题。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01002' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models</h3>
<p><strong>Authors:</strong> Ignacy Kolton, Kacper Marzol, Paweł Batorski, Marcin Mazur, Paul Swoboda, Przemysław Spurek</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对扩散模型机器遗忘后的概念恢复问题，提出强化学习训练的对抗性prompt搜索方法，揭示了unlearned模型的安全漏洞，是大模型安全与对齐领域的重要评估工作。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00350' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Text is All You Need for Vision-Language Model Jailbreaking</h3>
<p><strong>Authors:</strong> Yihang Chen, Zhao Xu, Youyuan Jiang, Tianle Zheng, Cho-Jui Hsieh</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Text-DJ攻击方法，利用VLM的OCR能力将有害查询分解为文本图像以绕过安全过滤，揭示了VLM在多模态输入下的安全漏洞，是大模型安全与对齐领域的关键研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00420' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Wenbin Xing, Quanxing Zha, Lizheng Zu, Mengran Li, Ming Li, Junchi Yan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出对比解码框架，通过强化学习缓解VLLM的组合幻觉问题，属于大模型安全与对齐中的幻觉问题研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00559' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance</h3>
<p><strong>Authors:</strong> Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong, Shiyu Tang, Shuai Liu, Shaokang Yang, Cheng Yang, Hayden Kwok-Hay So, Ngai Wong</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 利用历史解码信息引导残差修正，抑制视觉语言模型的幻觉生成，提升视觉 grounding 精度，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01047' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models</h3>
<p><strong>Authors:</strong> Zhiqi Zhang, Xinhao Zhong, Yi Sun, Shuoyang Sun, Bin Chen, Shu-Tao Xia, Xuan Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对flow matching模型提出训练-free的差异向量擦除，精确移除目标概念（如NSFW内容）而不影响其他语义，提升生成模型的安全性，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01089' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons</h3>
<p><strong>Authors:</strong> Xianhui Zhang, Chengyu Xie, Linxia Zhu, Yonghui Yang, Weixiang Zhao, Zifeng Cheng, Cong Wang, Fei Shen, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对LLM跨语言安全对齐问题，发现并验证跨语言共享安全神经元的因果作用，提出神经元导向训练策略增强非高资源语言安全性，属于大模型安全与对齐核心方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning</h3>
<p><strong>Authors:</strong> Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu, Ye Tian, Wendong Wang (Chinese universities/institutions)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出训练-free的线索追踪插件，通过问题到视觉线索的传播路径定位任务相关特征，有效抑制MLLM的幻觉问题，对大模型安全与对齐中的幻觉治理具有重要意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02004' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models</h3>
<p><strong>Authors:</strong> Shuozhe Li, Jincheng Cao, Bodun Hu, Aryan Mokhtari, Leqi Liu, Amy Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出CARE-RFT方法，通过置信锚定平衡大模型的推理性能与信任度（校准、幻觉），解决强化微调的“性能-信任”权衡问题，属于大模型安全与对齐研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00085' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference</h3>
<p><strong>Authors:</strong> Vikram Krishnamurthy</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文将LLM形式化为带注意力的高维非线性自回归模型，涵盖预训练、对齐（如RLHF、DPO）和推理的数学框架，对大模型对齐的理论分析有价值
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00426' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Building Non-Fine-Tunable Foundation Models</h3>
<p><strong>Authors:</strong> Ziyao Wang, Nizhang Li, Pingzhi Li, Guoheng Sun, Tianlong Chen, Ang Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出PMP框架构建不可微调的基础模型，通过私有掩码和稀疏子网络防止未授权微调，解决大模型安全与对齐中的未授权适应问题
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00446' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains</h3>
<p><strong>Authors:</strong> Luca Viano, Ruida Zhou, Yifan Sun, Mahdi Namazifar, Volkan Cevher, Shoham Sabach, Mohammad Ghavamzadeh</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出结合rating信息的DPO算法，提升对齐效果并证明理论增益，属于大模型对齐的核心方法改进
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00603' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Provably Protecting Fine-Tuned LLMs from Training Data Extraction</h3>
<p><strong>Authors:</strong> Tom Segal, Asaf Shabtai, Yuval Elovici</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出SCP-Δr算法，为微调LLM提供训练数据提取攻击的形式化隐私保证，解决现有防御方法缺乏保证或效用下降的问题，属于大模型安全与对齐的重要研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00688' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity</h3>
<p><strong>Authors:</strong> Prakhar Ganesh, Reza Shokri, Golnoosh Farnadi</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 引入prompt multiplicity框架量化LLM输出的一致性，分析幻觉的检测与缓解，为大模型安全中的幻觉问题提供新视角。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00723' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing</h3>
<p><strong>Authors:</strong> Anxin Guo, Jingwei Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 用率失真定理证明幻觉是空间最优的必然结果，为大模型安全中的幻觉问题提供理论解释，深化了对幻觉本质的理解。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00906' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Continuous-Utility Direct Preference Optimization</h3>
<p><strong>Authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zihao He, Muhammad Usman Rafique, Asad Aali, Muhammad Ali Jamshed, John M. Cioffi, Emily Fox</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出连续效用直接偏好优化，用精细评分替代二元标签，提升LLM推理质量与对齐效果，属于大模型对齐的重要方法创新。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00931' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models</h3>
<p><strong>Authors:</strong> Kaiyuan Cui, Yige Li, Yutao Wu, Xingjun Ma, Sarah Erfani, Christopher Leckie, Hanxun Huang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文针对视觉语言模型提出通用可迁移的越狱攻击框架UltraBreak，解决现有方法的模型过拟合问题，实验验证其在多模型和多目标上的优势，是大模型安全领域的重要研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01025' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models</h3>
<p><strong>Authors:</strong> Mete Erdogan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 论文提出Tangent-Space DPO方法，通过切线空间学习多目标偏好方向，实现可控的大模型偏好对齐，解决现有方法的单目标局限，是大模型对齐领域的关键进展。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01128' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models</h3>
<p><strong>Authors:</strong> Weiqing He, Xiang Li, Li Shen, Weijie Su, Qi Long</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究水marking强度与投机采样效率的权衡关系，提出机制平衡检测能力与推理效率，水marking作为大模型安全溯源的核心技术，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01428' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Nearly Optimal Active Preference Learning and Its Application to LLM Alignment</h3>
<p><strong>Authors:</strong> Yao Zhao, Kwang-Sung Jun</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出近最优主动偏好学习算法，通过问题-specific设计提升LLM对齐的样本效率，属于大模型安全与对齐中的对齐方法研究方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01581' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chance-Constrained Inference for Hallucination Risk Control in Large Language Models</h3>
<p><strong>Authors:</strong> Sreenivasan Mohandas</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 将LLM幻觉控制形式化为部署时风险约束问题，提出机会约束推理方法以概率保证限制幻觉频率，实验验证可可靠控制风险，属于大模型安全与对齐中的幻觉控制方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01637' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> $\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality</h3>
<p><strong>Authors:</strong> Pengyu Li (), Lingling Zhang (), Zhitao Gao (), Yanrui Wu (), Yuxuan Dong (), Huan Liu (), Bifan Wei (), Jun Liu ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Adversarial Gating Training与Adaptive Orthogonality结合的框架，解决LLM遗忘中的鲁棒擦除与效用保留平衡问题，属于大模型安全中的数据隐私保护方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01703' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AICD Bench: A Challenging Benchmark for AI-Generated Code Detection</h3>
<p><strong>Authors:</strong> Daniil Orel, Dilshod Azizov, Indraneil Paul, Yuxia Wang, Iryna Gurevych, Preslav Nakov</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出全面的AI生成代码检测基准，覆盖2M示例、77个模型和9种编程语言，包含鲁棒分类、模型家族归因等任务，解决现有基准的局限性，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02079' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EvoMU: Evolutionary Machine Unlearning</h3>
<p><strong>Authors:</strong> Pawel Batorski, Paul Swoboda</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出进化机器遗忘方法，通过进化搜索自动发现任务特定的遗忘损失函数，解决现有方法依赖人工设计损失的问题，属于大模型安全与对齐中的机器遗忘方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02139' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Alignment-Aware Model Adaptation via Feedback-Guided Optimization</h3>
<p><strong>Authors:</strong> Gaurav Bhatt, Aditya Chinchure, Jiawei Zhou, Leonid Sigal</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出对齐感知微调框架，通过反馈引导优化平衡任务目标与对齐目标，解决下游微调导致的模型对齐退化问题，推进大模型安全对齐研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02258' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning</h3>
<p><strong>Authors:</strong> Abhishek Mishra, Mugilan Arulvanan, Reshma Ashok, Polina Petrova, Deepesh Suranjandass, Donnie Winkelmann</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 评估不同领域微调对大模型emergent misalignment的
[PAPER_START]
Title: RobustDebias: Debiasing Language Models using Distributionally Robust Optimization
Authors: Deep Gandhi, Katyani Singh, Nidhi Hegde
Published: 2026-02-03
Link: https://arxiv.org/abs/2602.00405
Reason: 针对LLM微调过程中的偏差放大问题，提出基于分布鲁棒优化（DRO）的去偏方法，直接关联大模型安全与对齐中的偏差缓解需求。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00298' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models</h3>
<p><strong>Authors:</strong> Shule Lu, Yujing Wang, Hainan Zhang, Xiaoshan Yang, Hongwei Zheng, Yongxin Tong, Changsheng Xu, Zhiming Zheng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出基于混合奖励的联邦对齐框架MoR，解决异质VLMs的隐私保护对齐问题，提升泛化性和鲁棒性，属于大模型安全与对齐的联邦场景扩展。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00485' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees</h3>
<p><strong>Authors:</strong> Minhyuk Lee, Hyekyung Yoon, Myungjoo Kang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出仅推理阶段的prompt投影框架，在不微调的情况下抑制文本到图像生成的不安全内容，同时保持对齐，属于大模型安全与对齐的多模态场景应用。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00616' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Small-Margin Preferences Still Matter-If You Train Them Right</h3>
<p><strong>Authors:</strong> Jinlong Pang, Zhaowei Zhu, Na Di, Yichi Zhang, Yaxuan Wang, Chen Qian, Yang Liu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出MixDPO，通过难度感知的课程学习和目标路由，利用小边际偏好数据提升对齐效果，属于大模型安全与对齐的偏好优化改进。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00954' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?</h3>
<p><strong>Authors:</strong> Sidharth Pulipaka, Oliver Chen, Manas Sharma, Taaha S Bajwa, Vyas Raina, Ivaxi Sheth</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 构建PersistBench基准衡量LLM长期记忆的安全风险（跨域泄漏、记忆诱导阿谀），评估18个模型的记忆安全问题，为大模型安全提供关键实证支撑。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01146' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models</h3>
<p><strong>Authors:</strong> Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li, Zhejun Zhao, Shuaiqiang Wang, Yuchen Li, Dawei Yin</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出SAGE框架，通过动态数据选择和稳定性评分解决偏好对齐的低效与不稳定问题，提升推理模型的对齐效果，关联大模型安全与对齐。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01207' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</h3>
<p><strong>Authors:</strong> Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 通过攻击者-防御者协同进化游戏提升LLM安全鲁棒性，解决静态数据下防御滞后问题，关联大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01539' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking</h3>
<p><strong>Authors:</strong> Mohammad Beigi, Ming Jin, Junshan Zhang, Qifan Wang, Lifu Huang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对RLHF中奖励黑客问题提出对抗性奖励审计框架，主动检测并缓解奖励滥用，平衡对齐与效用，实验验证多场景有效性
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01750' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction</h3>
<p><strong>Authors:</strong> Enes Altinisik, Masoomali Fatehkia, Fatih Deniz, Nadir Durrani, Majd Hawasly, Mohammad Raza, Husrev Taha Sencar</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出自验证框架VeriFY，通过一致性判断减少LLM事实幻觉，实验显示幻觉率降低9.7-53.3%且召回率损失小
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02018' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models</h3>
<p><strong>Authors:</strong> Yingsha Xie, Tiansheng Huang, Enneng Yang, Rui Min, Wenjie Lu, Xiaochun Cao, Naiqiang Tan, Li Shen</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出DGR方法优化安全对齐数据集的分布一致性，缓解安全税（推理能力下降），实验显示推理准确率提升21.2-30.2%
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02136' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning</h3>
<p><strong>Authors:</strong> Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia, Rui Shao, Xiu Su, Shuo Yang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出INFUSE框架向VLA模型注入后门，下游微调后仍保持高攻击成功率（模拟91.0%、真实79.8%），揭示VLA模型安全风险
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00500' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models</h3>
<p><strong>Authors:</strong> Siqi Wen, Shu Yang, Shaopeng Fu, Jingfeng Zhang, Lijie Hu, Di Wang</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对视觉语言动作（VLA）模型的推理时安全风险（如被攻击触发不安全物理行为），提出概念基字典学习框架，通过构建可解释字典识别有害概念方向并干预，实验显示攻击成功率降低超70%且保持任务性能，是首个针对 embodied 系统的推理时概念安全方法，对大模型安全与对齐研究有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01834' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification</h3>
<p><strong>Authors:</strong> Rory Driscoll, Alexandros Christoforos, Chadbourne Davis</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对VLMs序列推理中的幻觉问题，提出LogicGaze基准，通过因果序列与视觉矛盾扰动评估模型的因果验证能力，暴露了前沿模型推理接地不足的问题，为提升多模态推理的可靠性提供了重要评估工具。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00292' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts</h3>
<p><strong>Authors:</strong> Songping Wang, Qinglong Liu, Yueming Lyu, Ning Li, Ziwen He, Caifeng Shan</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究视频MoE模型的对抗脆弱性，提出组件级攻击与防御方法，增强MoE鲁棒性，属于大模型安全与对齐中的鲁棒性问题。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01369' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models</h3>
<p><strong>Authors:</strong> Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对VLMs的可迁移定向攻击问题，提出语义引导的分层对齐方法，提升攻击迁移性，属于大模型安全与对齐中的对抗攻击方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization</h3>
<p><strong>Authors:</strong> Xinquan Yu, Wei Lu, Xiangyang Luo (Chinese universities/institutions)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出弱监督多模态操纵定位框架，结合隐式与显式线索解决标签噪声问题，性能接近全监督方法，对大模型安全与对齐中的操纵检测具有重要价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02175' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards</h3>
<p><strong>Authors:</strong> Ziyao Wang, Daeun Jung, Yexiao He, Guoheng Sun, Zheyu Shen, Myungjin Lee, Ang Li</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出联邦GRPO框架解决异质奖励下的LLM多目标对齐问题，兼顾个性化与全局性能，属于大模型对齐的关键问题
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00453' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sparsity-Aware Unlearning for Large Language Models</h3>
<p><strong>Authors:</strong> Yuze Wang, Yujia Tong, Ke Xu, Jingling Yuan, Jiawei Jiang, Chuang Hu</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出SAU框架解决稀疏LLM的机器遗忘问题，通过梯度掩码和重要性重分配实现有效遗忘，属于大模型安全与对齐的关键技术
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00577' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment</h3>
<p><strong>Authors:</strong> Byeonghu Na (), Hyungho Na (), Yeongmin Kim (), Suhyeon Jo (), HeeSun Bae (), Mina Kang (), Il-Chul Moon (KAIST)</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 针对RLHF框架中KL散度正则化缺乏语义感知的问题，提出基于Wasserstein距离的语义感知正则化方法，提升大模型对齐效果，符合大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01685' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning</h3>
<p><strong>Authors:</strong> Zheng Zhang (), Ao Lu (), Yuanhao Zeng (), Ziwei Shan (), Jinjin Guo (), Lufei Li (), Yexin Li (), Kan Ren ()</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 从裁判模型的推理过程中提取密集奖励，提升开放任务中的LLM推理质量，符合大模型安全与对齐中的reward设计方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01791' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts</h3>
<p><strong>Authors:</strong> Guangyi Zhang, Yunlong Cai, Guanding Yu, Osvaldo Simeone</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出半监督风险监测方法PPRM，通过合成标签与真实标签结合构建风险下界，有效检测部署模型的有害分布偏移，支撑大模型安全对齐中的部署风险防控。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02229' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization</h3>
<p><strong>Authors:</strong> Maksim Afanasyev, Illarion Iov</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出SLIME框架解决偏好优化中的目标错位问题，通过锚定项和双边际机制保持生成质量与偏好学习平衡，提升大模型对齐稳定性。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02383' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Samuel Nellessen, Tal Kachman</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出Slingshot框架通过强化学习自动发现agent-to-agent攻击向量，验证工具增强环境中的安全威胁，对大模型安全对齐有重要警示意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02395' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory</h3>
<p><strong>Authors:</strong> Junhyuk Choi, Sohhyung Park, Chanhee Cho, Hyeonchu Park, Bugeun Kim</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 采用项目反应理论（IRT）诊断LLM裁判的可靠性，解决对齐评估中的裁判一致性问题，是大模型安全与对齐的关键方法论支撑。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00521' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> R-HTN: Rebellious Online HTN Planning for Safety and Game AI</h3>
<p><strong>Authors:</strong> Hector Munoz-Avila, David W. Aha, Paola Rizzo</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出R-HTN框架，让HTN智能体在违反安全指令时停止或修改计划，解决智能体的安全行为问题，属于大模型安全与对齐的智能体规划安全方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00951' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Building Better Deception Probes Using Targeted Instruction Pairs</h3>
<p><strong>Authors:</strong> Vikram Natarajan, Devina Jain, Shivam Arora, Satvik Golechha, Joseph Bloom</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 研究目标指令对构建欺骗探针的作用，揭示指令对捕获欺骗意图的重要性，解决大模型安全中的欺骗检测问题。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron</h3>
<p><strong>Authors:</strong> Sicheng Shen, Mingyang Lv, Han Shen, Jialin Wu, Binghao Wang, Zhou Yang, Guobin Shen, Dongcheng Zhao, Feifei Zhao, Yi Zeng</p>
<p><strong>Published:</strong> 2026-02-03</p>
<p><strong>Reason:</strong> 提出轻量级对齐方法，用单神经元门控平衡模型能力与安全，训练开销低且泛化性好，适用于大模型安全部署
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02027' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>