<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-04</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>深度学习理论</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >多模态智能体</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型新技术</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-04</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-04 13:33:39</p>
<p>已自动阅读了 560 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：302244</p>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Global Geometry Is Not Enough for Vision Representations</h3>
<p><strong>Authors:</strong> Jiwan Chung, Seon Joo Kim</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究了视觉表示学习中全局嵌入几何的局限性，发现其与组合绑定能力相关性极低，而功能敏感性（输入输出Jacobian）更能可靠反映表示的组合能力，对深度学习理论中的表示学习基础问题有重要理论贡献
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03282' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Scaled Dot-Product Attention implements projection of inputs onto a common surface</h3>
<p><strong>Authors:</strong> Terence D Sanger</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 从数学角度重新解释缩放点积注意力机制，将其等价于输入到公共表面的投影，为注意力机制的理论理解提供了新视角，属于深度学习理论中的network architecture方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02521' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Effective Frontiers: A Unification of Neural Scaling Laws</h3>
<p><strong>Authors:</strong> Jiaxuan Zou, Zixuan Gong, Ye Su, Huayi Tang, Yong Liu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出“有效边界”概念统一不同神经缩放律，解决之前缩放律矛盾，为深度学习理论中模型缩放研究提供统一框架，理论贡献显著。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02593' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Most Convolutional Networks Suffer from Small Adversarial Perturbations</h3>
<p><strong>Authors:</strong> Amit Daniely, Idan Mehalel</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Proves random CNNs have small adversarial perturbations, using Fourier analysis to bound convolutional operator singular values.
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03415' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Multi-Head Attention Is a Multi-Player Game</h3>
<p><strong>Authors:</strong> Kushal Chakrabarti, Nirmal Balachundar</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 将Transformer多头注意力形式化为潜在博弈，分析纳什均衡的无效率（Price of Anarchy）与头交互矩阵的关系，提出GAME-LoRA正则化方法减少注意力头的冗余和幻觉。实验验证降低18%幻觉且不损失知识，对深度学习理论中注意力机制的理解与优化有重要贡献。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00861' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</h3>
<p><strong>Authors:</strong> Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Identity Bridge解决LLM的Reversal Curse问题，结合理论证明与实验验证，属于深度学习理论方向的重要进展。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02470' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane</h3>
<p><strong>Authors:</strong> Haoyu Liu, Sucheng Ren, Tingyu Zhu, Peng Wang, Cihang Xie, Alan Yuille, Zeyu Zheng, Feng Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Spiral RoPE，通过分组旋转改进2D位置编码的方向性，支持斜向空间关系建模，显著提升视觉任务性能，对深度学习理论中的位置嵌入优化具有重要创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03227' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UNSO: Unified Newton Schulz Orthogonalization</h3>
<p><strong>Authors:</strong> Chen Hu, Qianxi Zhao, Yuming Li, Mingyu Zhou, Xiyin Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出UNSO框架改进Newton-Schulz迭代，解决传统方法效率低和不稳定问题，对深度学习优化中的正交化操作有重要理论贡献，属于深度学习理论中的optimizer方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02500' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Product Interaction: An Algebraic Formalism for Deep Learning Architectures</h3>
<p><strong>Authors:</strong> Haonan Dong, Chun-Wun Cheng, Angelica I. Aviles-Rivero</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出代数形式化框架统一卷积、注意力、Mamba等现代神经网络结构，为深度学习理论中的网络架构研究提供新视角与工具，具有重要理论价值。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02573' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers</h3>
<p><strong>Authors:</strong> Sayak Chakrabarti, Toniann Pitassi, Josh Alman</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 理论分析量化Transformer精度与表达性权衡，证明“一比特”阈值存在，为深度学习理论中量化研究提供严谨支撑。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers</h3>
<p><strong>Authors:</strong> Jonas Petersen, Camilla Mazzoleni, Riccardo Maggioni</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 分析Transformer多跳关系推理瓶颈，提出RASA改进注意力机制，结合电路复杂度理论与实证验证，属于深度学习理论中网络架构研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02834' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data</h3>
<p><strong>Authors:</strong> Stefanos Pertigkiozoglou, Mircea Petrache, Shubhendu Trivedi, Kostas Daniilidis</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出RECM方法学习等变神经网络层间对称性松弛，解决严格等变带来的优化问题，属于深度学习理论中网络架构研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02853' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models</h3>
<p><strong>Authors:</strong> Gibbs Nwemadji, Bruno Loureiro, Jean Barbier</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 从动力学角度分析预训练对LoRA微调的影响，揭示过度预训练导致收敛变慢的现象，属于深度学习理论中优化器研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02855' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Geometry-Aware Efficient Algorithm for Compositional Entropic Risk Minimization</h3>
<p><strong>Authors:</strong> Xiyuan Wei, Linli Zhou, Bokun Wang, Chih-Jen Lin, Tianbao Yang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出SCENT算法解决组合熵风险最小化问题，利用几何感知的随机近端镜像下降，属于深度学习理论中优化器研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02877' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Controlled disagreement improves generalization in decentralized training</h3>
<p><strong>Authors:</strong> Zesen Wang, Mikael Johansson</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出DSGD-AC方法，利用可控共识误差作为正则化提升去中心化训练泛化性，属于深度学习理论中优化器研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02899' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Why Some Models Resist Unlearning: A Linear Stability Perspective</h3>
<p><strong>Authors:</strong> Wei-Kai Chang, Rajiv Khanna</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 从线性稳定性角度分析模型抗遗忘原因，揭示数据一致性的作用，属于深度学习理论中模型泛化性研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02986' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shortcut Features as Top Eigenfunctions of NTK: A Linear Neural Network Case and More</h3>
<p><strong>Authors:</strong> Jinwoo Lim, Suhyun Kim, Soo-Mook Moon</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 从NTK视角分析捷径学习，揭示捷径特征对应大特征值，属于深度学习理论中模型泛化性研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03066' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry</h3>
<p><strong>Authors:</strong> Ye Su, Huayi Tang, Zixuan Gong, Yong Liu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Analyzes MoE expressivity via tropical geometry, linking sparsity to combinatorial depth for theoretical insights into MoE design.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03204' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Universal One-third Time Scaling in Learning Peaked Distributions</h3>
<p><strong>Authors:</strong> Yizhou Liu, Ziming Liu, Cengiz Pehlevan, Jeff Gore</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 发现LLM训练中学习尖峰分布的通用1/3时间缩放规律，揭示了训练动态与泛化的内在联系，属于深度学习理论的训练动态方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03685' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic</h3>
<p><strong>Authors:</strong> Yani Zhang (Unknown), Helmut B\"olcskei (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 将深度ReLU网络转化为多值逻辑公式，通过逻辑公理解决网络完全识别问题，属于深度学习理论中网络架构与功能对称性的核心研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.00266' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Capabilities and Fundamental Limits of Latent Chain-of-Thought</h3>
<p><strong>Authors:</strong> Jiaxuan Zou, Yaozhong Xiong, Yong Liu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 理论分析Latent Chain-of-Thought的探索-执行权衡，提出Symbolic Index量化决策确定性，证明课程学习对解决分布不匹配的必要性，对深度学习理论中LLM推理机制的理解有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01148' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives</h3>
<p><strong>Authors:</strong> Lin Chen, Samuel Drapeau, Fanghao Shao, Xuekai Zhu, Bo Xue, Yunchong Song, Mathieu Laurière, Zhouhan Lin</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 建立GFlowNets与马尔可夫链的等价关系，揭示其探索-利用权衡的本质约束，提出α-GFN框架实现可控的动态平衡，在模式发现任务上实现10倍性能提升，推进了深度学习理论中生成模型训练策略的研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01749' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Geometric Analysis of Token Selection in Multi-Head Attention</h3>
<p><strong>Authors:</strong> Timur Mudarisov, Mikhal Burtsev, Tatiana Petrova, Radu State</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出注意力的几何分析框架，量化token选择的可分离性（Precision/Recall/F-score），解释多头注意力的“Retriever/Mixer/Reset” specialization机制，为深度学习理论中注意力的设计、稀疏化与可解释性提供关键指导。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.01893' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?</h3>
<p><strong>Authors:</strong> Haruhiko Murata, Kazuhiro Hotta</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出SVD-ViT框架，利用SVD提取并聚合捕获前景信息的奇异向量，改进ViT对前景特征的学习，对Vision Transformer架构优化的深度学习理论研究具有价值。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02765' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models</h3>
<p><strong>Authors:</strong> Judah Goldfeder, Shreyes Kaliyur, Vaibhav Sourirajan, Patrick Minwan Puma, Philippe Martin Wyder, Yuhang Hu, Jiong Lin, Hod Lipson</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出EvoAug框架，用生成模型自动进化任务特定的数据增强，通过进化算法学习 stochastic augmentation trees，对深度学习理论中的数据增强方法创新具有重要价值。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03123' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher</h3>
<p><strong>Authors:</strong> Hari K Prakash, Charles H Martin</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 发现Grokking第三阶段（反泛化），并用Weightwatcher工具诊断，属于深度学习理论中模型泛化性研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02859' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Batch Sizes Using Non-Euclidean Gradient Noise Scales for Stochastic Sign and Spectral Descent</h3>
<p><strong>Authors:</strong> Hiroki Naganuma, Shagun Gupta, Youssef Briki, Ioannis Mitliagkas, Irina Rish, Parameswaran Raman, Hao-Jun Michael Shi</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出自适应batch size方法，利用非欧几里得梯度噪声尺度提升Signum和Muon训练效率，属于深度学习理论中优化器研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03001' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Geometry-Preserving Neural Architectures on Manifolds with Boundary</h3>
<p><strong>Authors:</strong> Karthik Elamvazhuthi, Shiba Biswal, Kian Rosenblum, Arushi Katyal, Tianli Qu, Grady Ma, Rishi Sonthalia</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出几何保持神经架构处理带边界流形，属于深度学习理论中网络架构研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03082' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PRISM: Structured Optimization via Anisotropic Spectral Shaping</h3>
<p><strong>Authors:</strong> Yujie Yang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出PRISM优化器利用 anisotropic谱整形提升结构化优化效率，属于深度学习理论中优化器研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03096' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Soft-Radial Projection for Constrained End-to-End Learning</h3>
<p><strong>Authors:</strong> Philipp J. Schneider, Daniel Kuhn</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Introduces Soft-Radial Projection to avoid gradient saturation in constrained learning, preserving universal approximation.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03461' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Scaling Continual Learning with Bi-Level Routing Mixture-of-Experts</h3>
<p><strong>Authors:</strong> Meng Lou, Yunxiang Fu, Yizhou Yu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Proposes BR-MoE for scalable continual learning, using bi-level routing to inject discriminative features into every layer.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03473' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A Function-Space Stability Boundary for Generalization in Interpolating Learning Systems</h3>
<p><strong>Authors:</strong> Ronald Katende</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究插值学习系统的泛化稳定性，提出函数空间的稳定性边界理论，揭示了优化轨迹与泛化的关系，属于深度学习理论的泛化分析方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03514' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications</h3>
<p><strong>Authors:</strong> Elena Umili, Francesco Argenziano, Roberto Capobianco</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出DeepDFA框架整合时序逻辑到深度学习，解决序列亚符号任务的知识注入问题，属于深度学习理论中的神经符号结合方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03486' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> WARP Logic Neural Networks</h3>
<p><strong>Authors:</strong> Lino Gerlach, Thore Gerlach, Liv Våge, Elliott Kauffman, Isobel Ojalvo</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出WARP逻辑神经网络，高效学习硬件原生逻辑块的组合，优化了逻辑模型的训练效率与 scalability，属于深度学习理论的网络结构方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03527' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Robust Representation Learning in Masked Autoencoders</h3>
<p><strong>Authors:</strong> Anika Shrivastava, Renu Rameshan, Samar Agnihotri</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 分析Masked Autoencoders的层间特征结构，揭示其鲁棒表征学习机制，属于深度学习理论的表征学习方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03531' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sparse Training of Neural Networks based on Multilevel Mirror Descent</h3>
<p><strong>Authors:</strong> Yannick Lunk, Sebastian J. Scott, Leon Bungert</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出基于多级镜像下降的稀疏训练算法，优化了稀疏模型的训练效率与性能，属于深度学习理论的优化器方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03535' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sequential Group Composition: A Window into the Mechanics of Deep Learning</h3>
<p><strong>Authors:</strong> Giovanni Luca Marchetti, Daniel Kunin, Adele Myers, Francisco Acosta, Nina Miolane</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究神经网络学习序列组合成的机制，揭示了网络结构与学习能力的关系，属于深度学习理论的学习机制方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03655' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Equilibrium Propagation for Non-Conservative Systems</h3>
<p><strong>Authors:</strong> Antonino Emanuele Scurria, Dimitri Vanden Abeele, Bortolo Matteo Mognetti, Serge Massar</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 扩展平衡传播到非保守系统，优化了非保守动态系统的训练性能，属于深度学习理论的学习算法方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.03670' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics</h3>
<p><strong>Authors:</strong> Sangwoo Shin, BumJun Kim, Kyelim Lee, Moongyu Jeon, Albert No</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究masked扩散模型缓解“反转诅咒”的机制，发现架构（权重共享）与训练动态（梯度对齐）的协同作用，解释了扩散模型泛化性优于自回归模型的本质原因，推进了深度学习理论中生成模型泛化机制的理解。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.02133' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> How Much Information Can a Vision Token Hold? A Scaling Law for Recognition Limits in VLMs</h3>
<p><strong>Authors:</strong> Shuxin Zhuang, Zi Liang, Runsheng Yu, Hongzong Li, Rong Feng, Shiqin Tang, Youzhi Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究视觉语言模型中视觉token的信息容量上限，通过压力测试发现相位转换现象并提出缩放定律，对原生多模态大模型的tokenizer设计有重要指导意义，属于原生多模态大模型方向。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02539' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Runjie Zhou, Youbo Shao, Haoyu Lu, Bowei Xing, Tongtong Bai, Yujie Chen, Jie Zhao, Lin Sui, Haotian Yao, Zijia Zhao, Hao Yang, Haoning Wu, Zaida Zhou, Jinguo Zhu, Zhiqi Huang, Yiping Bao, Yangyang Liu, Y. Charles, Xinyu Zhou</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出WorldVQA基准，专门评估多模态大模型的原子视觉世界知识，解决现有评估中视觉知识检索与推理混淆的问题，对理解MLLMs的视觉事实性和幻觉问题具有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02537' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</h3>
<p><strong>Authors:</strong> Weihang You, Qingchan Zhu, David Liu, Yi Pan, Geng Yuan, Hanqi Jiang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 引入主动感知的视觉语言推理框架，通过动态生成感知查询token触发专家对齐特征合成，解决ViLM中静态输入导致的推理不足问题，对MLLMs的推理机制改进具有重要贡献。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02873' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding</h3>
<p><strong>Authors:</strong> Byeongju Woo, Zilin Wang, Byeonghyun Pak, Sangwoo Mo, Stella X. Yu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出CAFT框架，实现图像与长文本的层级语义对齐，通过细到粗视觉编码器和层级对齐损失提升视觉接地理解，对MLLMs的图像-文本对齐机制研究具有重要贡献。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02977' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation</h3>
<p><strong>Authors:</strong> Jiaze Li, Hao Yin, Haoran Xu, Boshen Xu, Wenhui Tan, Zewen He, Jianzhong Ju, Zhenbo Luo, Jian Luan</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出On-Policy Distillation的高效后训练框架，将稀疏奖励转化为细粒度监督，提升MLLMs的temporal video grounding性能和收敛速度，对MLLMs的视频接地任务具有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02994' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability</h3>
<p><strong>Authors:</strong> Bingchen Zhao, Qiushan Guo, Ye Wang, Yixuan Huang, Zhonghua Zhai, Yu Tian</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出CompTok框架，通过token-conditioned扩散解码器和InfoGAN-style目标增强视觉tokenizer的组合性，解决了交换token的生成问题，对原生多模态大模型的tokenizer设计有重要参考价值
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.03339' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers</h3>
<p><strong>Authors:</strong> Bozhou Li, Yushuo Guan, Haolin Li, Bohan Zeng, Yiyan Ji, Yue Ding, Pengfei Wan, Kun Gai, Yuanxing Zhang, Wentao Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究了Diffusion Transformers中LLM文本编码器的多层特征加权问题，提出语义路由框架提升文本-视觉对齐和组合生成性能，对原生多模态大模型的文本条件设计有创新贡献
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.03510' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Test-Time Conditioning with Representation-Aligned Visual Features</h3>
<p><strong>Authors:</strong> Nicolas Sereyjol-Garros, Ellington Kirby, Victor Letzelter, Victor Besnier, Nermin Samet</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出REPA-G框架，利用表示对齐的视觉特征实现扩散模型的测试时多尺度条件控制，支持多概念组合，对原生多模态大模型的条件生成能力提升有重要意义
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.03753' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TextME: Bridging Unseen Modalities Through Text Descriptions</h3>
<p><strong>Authors:</strong> Soyeon Hong, Jinchan Kim, Jaegook You, Seungtaek Choi, Suha Kwak, Hyunsouk Cho</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出TextME用文本描述桥接未见模态，属于原生多模态大模型中跨模态研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.03098' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?</h3>
<p><strong>Authors:</strong> Jingyi Zhang, Tianyi Lin, Huanjin Yao, Xiang Lan, Shunyu Liu, Jiaxing Huang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Proposes CADS for synthesizing high-quality multimodal data, building MMSynthetic-20K to improve native multimodal LLM performance.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.03300' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings</h3>
<p><strong>Authors:</strong> Yifei Shao (Unknown), Kun Zhou (Unknown), Ziming Xu (Unknown), Mohammad Atif Quamar (Unknown), Shibo Hao (Unknown), Zhen Wang (Unknown), Zhiting Hu (Unknown), Biwei Huang (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出模态混合的Chain-of-Thought推理方法，将文本token与视觉 latent embeddings 交织，提升多模态推理性能，属于原生多模态大模型的核心研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.00574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences</h3>
<p><strong>Authors:</strong> Seok-Young Kim, Dooyoung Kim, Woojin Cho, Hail Song, Suji Kang, Woontack Woo</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出从RGB序列生成3D场景的框架，利用语义场景图增强场景合成的组合性和对齐性，对原生多模态大模型的3D场景生成研究具有价值。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02974' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains</h3>
<p><strong>Authors:</strong> Jae-Sung Bae, Minje Kim</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出GeLDA利用条件扩散模型在潜空间生成样本，解决低资源域数据稀缺问题，属于原生多模态大模型中图像生成相关研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02841' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization</h3>
<p><strong>Authors:</strong> Bixing Wu, Yuhong Zhao, Zongli Ye, Jiachen Lian, Xiangyu Yue, Gopala Anumanchipalli</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出不对称分层锚定方法解决音频-视觉联合表征的信息分配歧义，提升跨模态泛化性能，属于原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.03570' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling</h3>
<p><strong>Authors:</strong> Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang, Tiejun Zhao</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出以漫画为中间模态的多模态推理范式，提升时间因果推理性能与效率，属于原生多模态大模型方向的创新。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02453' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MentisOculi: Revealing the Limits of Reasoning with Mental Imagery</h3>
<p><strong>Authors:</strong> Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Prasanna Mayilvahanan, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 开发MentisOculi基准，评估多模态模型的视觉推理能力，揭示其局限性，对原生多模态大模型的发展有指导意义。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02465' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HMVLA: Hyperbolic Multimodal Fusion for Vision-Language-Action Models</h3>
<p><strong>Authors:</strong> Kun Wang, Xiao Feng, Mingcheng Qu, Tonghua Su</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出双曲多模态融合框架HMVLA，提升VLA模型的语义对齐，属于原生多模态大模型方向的融合技术。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.02533' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> D²Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs</h3>
<p><strong>Authors:</strong> Xianglong Yan, ChengZhu Bao, Zhiteng Li, Tianao Zhang, Shaoqiu Zhang, Ruobing Xie, Samm Sun, Yulun Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出D²Quant框架，通过双尺度量化器和偏差感知校正解决低比特权重量化的精度问题，提升大模型量化性能，属于高效大模型训练与推理中的high compression方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02546' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Zero Sum SVD: Balancing Loss Sensitivity for Low Rank LLM Compression</h3>
<p><strong>Authors:</strong> Ali Abbasi, Chayne Thrash, Haoran Qin, Shansita Sharma, Sepehr Seifi, Soheil Kolouri</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Zero Sum SVD方法优化LLM低秩压缩的秩分配，通过激活白化与损失估计提升压缩效率，属于高效大模型训练与推理中模型压缩研究。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02848' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization</h3>
<p><strong>Authors:</strong> Haocheng Xi, Shuo Yang, Yilong Zhao, Muyang Li, Han Cai, Xingyang Li, Yujun Lin, Zhuoyang Zhang, Jintao Zhang, Xiuyu Li, Zhiying Xu, Jun Wu, Chenfeng Xu, Ion Stoica, Song Han, Kurt Keutzer</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Quant VideoGen用2-bit KV缓存量化提升长视频生成效率，属于高效大模型训练与推理中推理优化研究。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02958' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference</h3>
<p><strong>Authors:</strong> Jiancai Ye, Jun Liu, Qingchen Li, Tianlang Zhao, Hanbin Zhang, Jiayi Pan, Ningyi Xu, Guohao Dai</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Introduces dynamic semantic splitting for KV cache compression, reducing memory bottlenecks in long-context LLM inference.
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03184' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models</h3>
<p><strong>Authors:</strong> Xuliang Wang, Yuetao Chen, Maochan Zhen, Fang Liu, Xinzhou Zheng, Xingwu Liu, Hong Xu, Ming Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对speculative解码中draft模型“容量-推理成本”的核心矛盾，通过架构重构解耦两者，实验验证在高优化推理引擎上提升2.6倍吞吐量，是大模型高效推理的关键架构创新。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01762' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning</h3>
<p><strong>Authors:</strong> Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu, Laizhong Cui, Qi Tian</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出两阶段token pruning框架，通过保留全局空间锚点和文本引导剪枝保持VLM的空间完整性，显著提升视觉grounding任务性能，对高效大模型推理中的token优化具有重要价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02951' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</h3>
<p><strong>Authors:</strong> Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出基于隐式视觉坐标的token pruning策略，通过RoPE分析保留关键空间token和前景token，在减少50%视觉token的同时保持性能，对高效大模型推理中的token优化具有重要贡献。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03060' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> IMU-1: Sample-Efficient Pre-training of Small Language Models</h3>
<p><strong>Authors:</strong> George Grigorev</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出IMU-1模型，通过架构干预和优化策略实现小语言模型的样本高效预训练，仅用72B tokens达到类似大模型的性能，属于高效大模型训练与推理中的efficient LLM training方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02522' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Enhancing Post-Training Quantization via Future Activation Awareness</h3>
<p><strong>Authors:</strong> Zheqi Lv, Zhenxuan Fan, Qi Tian, Wenqiao Zhang, Yueting Zhuang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出FAQ方法，利用未来层激活指导后训练量化，解决传统PTQ的量化偏差和误差累积问题，提升大模型量化效率和精度，属于高效大模型训练与推理中的high compression方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02538' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals</h3>
<p><strong>Authors:</strong> Nan Zhang, Eugene Kwek, Yusen Zhang, Muyu Pan, Suhang Wang, Prasenjit Mitra, Rui Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对大模型量化任务，利用微调信号保护权重更新“两端”，有效提升量化后推理性能，对高效大模型推理研究有实际意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02581' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RAP: KV-Cache Compression via RoPE-Aligned Pruning</h3>
<p><strong>Authors:</strong> Jihao Xin, Tian Lvu, Hatem Ltaief, David Keyes, Marco Canini</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对RoPE-based大模型KV-Cache压缩，提出对齐剪枝方法，同时减少缓存、参数与计算量，有效提升推理效率，对大模型高效推理有重要应用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02599' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TraceNAS: Zero-shot LLM Pruning via Gradient Trace Correlation</h3>
<p><strong>Authors:</strong> Prajna G. Malettira, Manish Nagaraj, Arjun Roy, Shubham Negi, Kaushik Roy</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出TraceNAS零样本LLM剪枝方法，利用梯度轨迹相关性选择重要子块，属于高效大模型训练与推理中模型压缩研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02891' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference</h3>
<p><strong>Authors:</strong> Jiangyong Yu, Xiaomeng Han, Xing Hu, Chen Xu, Zhe Jiang, Dawei Yang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出NLI方法用非均匀线性插值近似非线性操作，提升LLM推理效率，属于高效大模型训练与推理中推理优化研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02988' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models</h3>
<p><strong>Authors:</strong> Guohao Yang, Tongle Wu, Yuanxiong Guo, Ying Sun, Yanmin Gong</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出FedKRSO方法，在联邦学习中用随机子空间优化降低通信与内存开销，属于高效大模型训练与推理中联邦学习研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03019' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT</h3>
<p><strong>Authors:</strong> Rana Muhammad Shahroz Khan, Zijie Liu, Zhen Tan, Charles Fleming, Tianlong Chen</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出TMS方法用轨迹混合监督提升SFT效率，避免灾难性遗忘，属于高效大模型训练与推理中微调优化研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03073' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost</h3>
<p><strong>Authors:</strong> Yinggan Xu, Risto Miikkulainen, Xin Qiu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Proposes Quantized Evolution Strategies (QES) for high-precision fine-tuning of quantized LLMs at low cost, addressing efficiency gaps in quantized model adaptation.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03120' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling</h3>
<p><strong>Authors:</strong> Ning Ding, Fangcheng Liu, Kyungrae Kim, Linji Hao, Kyeng-Hun Lee, Hyeonmok Ko, Yehui Tang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Introduces MeKi to scale LLM capacity via memory (not FLOPs), enabling zero-overhead knowledge injection for edge devices.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs</h3>
<p><strong>Authors:</strong> Alessio Quercia, Arya Bangun, Ira Assent, Hanno Scharr</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对LoRA参数高效微调中的性能-遗忘权衡问题，研究中间主成分的微调策略，实证验证了方法在多任务上的有效性，属于高效大模型训练与推理的核心方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03493' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation</h3>
<p><strong>Authors:</strong> Hyunji Jung, Sungbin Shin, Namhoon Lee</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 解决异步流水线并行训练中的梯度陈旧问题，提出基旋转方法优化梯度更新，显著提升分布式训练效率，属于高效大模型训练与推理的核心方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03515' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization</h3>
<p><strong>Authors:</strong> Maximilian Kleinegger, Elvir Crnčević, Dan Alistarh</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出MatGPTQ后训练量化方法，实现单模型多精度部署，显著提升模型压缩效率与准确性，属于高效大模型训练与推理的模型压缩方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03537' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining</h3>
<p><strong>Authors:</strong> Changhao Wang, Yunfei Yu, Xinhao Yao, Jiaolong Yang, Riccardo Cantoro, Chaobo Li, Qing Cui, Jun Zhou</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出UniGeM框架统一数据混合与选择，提升LLM的数据效率与推理性能，属于高效大模型训练与推理的数据优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03772' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Cross-Modal Memory Compression for Efficient Multi-Agent Debate</h3>
<p><strong>Authors:</strong> Jing Wu (Unknown), Yue Sun (Unknown), Tianpei Xie (Unknown), Suiyao Chen (Unknown), Jingyuan Bao (Unknown), Yaopengxiao Xu (Unknown), Gaoyuan Du (Unknown), Inseok Heo (Unknown), Alexander Gutfraind (Unknown), Xin Wang (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出跨模态记忆压缩框架，将长文本辩论轨迹转化为紧凑图像表示，大幅降低计算成本和推理时间，优化多智能体辩论的效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00454' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models</h3>
<p><strong>Authors:</strong> Yuting Huang, Leilei Ding, Zhipeng Tang, Zenghuan Zhu, Jiajun Deng, Xinrui Lin, Shuo Liu, Haojie Ren, Jianmin Ji, Yanyong Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对Vision-Language-Action（VLA）模型推理延迟问题，提出EcoVLA自适应剪枝框架，结合环境感知剪枝（EAP）和交错推理调度（I²O），实现训练无关的推理加速。实验验证在多种VLA模型上获得1.60×加速且仅0.4%准确率下降，对高效大模型推理有重要应用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00780' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement</h3>
<p><strong>Authors:</strong> Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou, Xiaoyue Ma, Jianing Li, Yao Zhu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对强化学习下可验证奖励（RLVR）的资源密集问题，提出Dynamic One-Shot Policy Refinement（DoPR）策略，通过不确定性感知的单样本选择优化政策。理论分析样本复杂度下限，实验减少近一个数量级的rollout开销同时保持推理精度，对高效大模型训练具有重要意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.00815' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Discovering Process-Outcome Credit in Multi-Step LLM Reasoning</h3>
<p><strong>Authors:</strong> Xiangwei Wang, Wei Wang, Ken Chen, Nanduni Nimalsiri, Saman Halgamuge</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对LLM多步推理中强化学习的奖励稀疏和信用分配问题，提出Step-wise Marginal Information Gain（MIG）机制和Decoupled Masking策略，结合Dual-Gated SFT稳定训练。实验提升样本效率和准确率，对高效大模型训练有贡献。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01034' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Probing RLVR training instability through the lens of objective-level hacking</h3>
<p><strong>Authors:</strong> Yiming Dong, Kun Fu, Haoyu Li, Xinyuan Zhu, Yurou Liu, Lijing Shao, Jieping Ye, Zheng Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 基于目标级hacking框架分析MoE模型的RLVR训练不稳定性，揭示训练-推理差异增长的机制，为设计稳定的RLVR算法提供理论指导，对高效大模型训练的稳定性优化有重要价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01103' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models</h3>
<p><strong>Authors:</strong> Katrina Brown, Aneesh Muppidi, Rana Shahout</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Predictive Scheduling框架，通过轻量预测器（MLP或LoRA分类器）动态分配token预算，解决CoT推理的固定预算问题。实验在GSM8K上提升7.9%准确率且保持token成本，对LLM高效推理有实用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01237' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection</h3>
<p><strong>Authors:</strong> Jongseok Park, Sunga Kim, Alvin Cheung, Ion Stoica</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对大模型采样中Top-k/Top-p的效率瓶颈，提出基于pivot的截断与选择算法，解决现有方法的计算与内存开销问题，在vLLM、SGLang等引擎上实现2倍吞吐量提升与一半内存使用，对大模型高效推理有重要实用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.01518' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression</h3>
<p><strong>Authors:</strong> Aryan Sood, Tanvi Sharma, Vansh Agrawal</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对KV-cache压缩中的贪心偏差问题，提出LASER-KV框架通过块级累积策略保留语义信息，实验验证在长上下文任务（如Babilong）上比现有方法提升10% accuracy，解决了高效推理中“内存-性能”的关键权衡。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02199' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VOILA: Value-of-Information Guided Fidelity Selection for Cost-Aware Multimodal Question Answering</h3>
<p><strong>Authors:</strong> Rahul Atul Bhope, K. R. Jayaram, Vinod Muthusamy, Ritesh Kumar, Vatche Isahagian, Nalini Venkatasubramanian</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出基于信息价值的保真度选择框架，优化多模态QA的成本与准确性平衡，通过梯度提升回归和 isotonic calibrator实现可靠决策，对高效大模型推理中的成本感知优化具有重要意义。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03007' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EventFlash: Towards Efficient MLLMs for Event-Based Vision</h3>
<p><strong>Authors:</strong> Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang, Ming Li, Xiangyang Ji</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出高效的事件基MLLM，通过自适应 temporal window aggregation和稀疏密度引导注意力提升推理效率，支持长范围事件流处理，对高效大模型推理中的事件数据处理具有重要贡献。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03230' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM</h3>
<p><strong>Authors:</strong> Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han, Hang Xu, Li Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出SlowFocus机制，通过关键段密集采样和多频率混合注意力提升视频LLM的细粒度时间理解效率，对高效大模型训练与推理中的视频LLM优化有实践意义
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03589' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs</h3>
<p><strong>Authors:</strong> Baiyang Song, Jun Peng, Yuxin Zhang, Guangyao Chen, Feidiao Yang, Jianyuan Guo</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出两阶段关键帧和关键token选择框架，解决训练-free视频LLM的视觉冗余问题，提升视频理解效率，对高效大模型训练与推理中的视频LLM优化有实践价值
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03615' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FlexRank: Nested Low-Rank Knowledge Decomposition for Adaptive Model Deployment</h3>
<p><strong>Authors:</strong> Riccardo Zaccone, Stefanos Laskaridis, Marco Ciccone, Samuel Horv\'ath</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出嵌套低秩知识分解，从预训练模型提取不同能力子模型，支持自适应部署，实现“训练一次，随处部署”，对高效大模型部署研究有重要意义。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02680' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Soupervision: Cooking Model Soups without Labels</h3>
<p><strong>Authors:</strong> Anthony Fuller, James R. Green, Evan Shelhamer</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Self-Souping方法用自监督生成模型 soups，提升模型鲁棒性，属于高效大模型训练与推理中自监督训练研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02890' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Consistency Deep Equilibrium Models</h3>
<p><strong>Authors:</strong> Junchao Lin, Zenan Ling, Jingwen Xu, Robert C. Qiu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出C-DEQ用一致性蒸馏加速DEQ推理，提升效率，属于高效大模型训练与推理中推理优化研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03024' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SAFE-KD: Risk-Controlled Early-Exit Distillation for Vision Backbones</h3>
<p><strong>Authors:</strong> Salim Khazem</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出SAFE-KD用早期退出蒸馏提升视觉backbone推理效率，属于高效大模型训练与推理中推理优化研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03043' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations</h3>
<p><strong>Authors:</strong> Yuxuan Yao, Haonan Sheng, Qingsong Lv, Han Wu, Shuqi Liu, Zehua Liu, Zengyan Liu, Jiahui Gao, Haochen Tan, Xiaojin Fu, Haoli Bai, Hing Cheung So, Zhijiang Guo, Linqi Song</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Proposes Streaming Merging with ARM to approximate gradient descent for efficient LLM adaptation, improving model merging dynamics.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03237' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Quantization-Aware Regularizers for Deep Neural Networks Compression</h3>
<p><strong>Authors:</strong> Dario Malchiodi, Mattia Ferraretto, Marco Frasca</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出量化感知正则化方法，在训练中引导权重聚类，提升模型压缩的准确性，属于高效大模型训练与推理的模型压缩方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03614' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG</h3>
<p><strong>Authors:</strong> Yicheng Zhang, Zhen Qin, Zhaomin Wu, Wenqi Zhang, Shuiguang Deng</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出强化微调方法优化RAG中的历史感知检索器，提升检索增强生成的性能，属于高效大模型训练与推理的RAG优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03645' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization</h3>
<p><strong>Authors:</strong> Zishi Zhang, Jinhui Han, Ming Hu, Yijie Peng</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出LLM启发的预训练-微调方法，解决小数据下的大规模优化问题，提升数据效率，属于高效大模型训练与推理的小数据训练方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.03690' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing</h3>
<p><strong>Authors:</strong> Mika Okamoto, Ansel Kaplan Erol, Glenn Matlin</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出BELLA框架，通过技能分析优化LLM选择，平衡性能与成本，提升LLM部署效率，属于高效大模型训练与推理方向的实用方法。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.02386' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents</h3>
<p><strong>Authors:</strong> Xiaoce Wang, Guibin Zhang, Junzhe Li, Jinzhe Tu, Chun Li, Ming Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出ToolTok框架，通过工具token化和语义锚定实现高效的GUI智能体，用少量数据达到好的泛化性能，属于多模态智能体中的GUI Agent方向。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02548' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process</h3>
<p><strong>Authors:</strong> Xintong Zhang, Xiaowen Zhang, Jongrong Wu, Zhi Gao, Shilin Yan, Zhenxin Diao, Kunpeng Gao, Xuanyan Chen, Yuwei Wu, Yunde Jia, Qing Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 构建涵盖GUI等五个领域的自适应多模态推理基准，解决现有评估的静态标签和简单指标问题，引入MCC metric和多维度过程评估，对多模态智能体的自适应推理能力评估具有重要意义。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02676' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MUSE: A Multi-agent Framework for Unconstrained Story Envisioning via Closed-Loop Cognitive Orchestration</h3>
<p><strong>Authors:</strong> Wenzhang Sun, Zhenyu Wang, Zhangchi Hu, Chunfeng Wang, Hao Li, Wei Chen</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出多智能体框架，通过闭环认知编排生成长文本视听故事，解决语义漂移和身份不一致问题，引入MUSEBench评估协议，对多模态智能体的故事生成能力具有重要提升。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.03028' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</h3>
<p><strong>Authors:</strong> Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出ProCAD用主动代理解决Text-to-CAD歧义问题，属于多模态智能体中GUI Agent研究。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.03045' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning</h3>
<p><strong>Authors:</strong> Yu Li, Mingyang Yi, Xiuyu Li, Ju Fan, Fuxin Jiang, Binbin Chen, Peng Li, Jie Song, Tieying Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 系统分析Agentic RL中推理与工具使用的训练干扰问题，提出Disentangled Action Reasoning Tuning（DART）框架，通过低秩适应模块解耦两者的参数更新。实验验证平均提升6.35%性能，对多模态智能体的训练优化有重要指导意义。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00994' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration</h3>
<p><strong>Authors:</strong> Qingni Wang, Yue Fan, Xin Eric Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对GUI grounding的可靠性问题，提出SafeGround不确定性框架，实现风险感知预测，属于多模态智能体方向的关键技术。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02419' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts</h3>
<p><strong>Authors:</strong> Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Avenir-Web多模态web智能体，模仿人类经验，提升web任务性能，属于多模态智能体方向的前沿工作。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02468' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents</h3>
<p><strong>Authors:</strong> Zhisheng Chen (Unknown), Tingyu Wu (Unknown), Zijie Zhou (Unknown), Zhengwei Xie (Unknown), Ziyan Weng (Unknown), Yingwei Zhang (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对多模态智能体的记忆系统，提出训练无关的极化潜在图记忆，解决概率视觉语言模型的认知不对称，提升推理可验证性。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00415' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dual Latent Memory for Visual Multi-agent System</h3>
<p><strong>Authors:</strong> Xinlei Yu (Unknown), Chengming Xu (Unknown), Zhangquan Chen (Unknown), Bo Yin (Unknown), Cheng Yang (Unknown), Yongbo He (Unknown), Yihao Hu (Unknown), Jiangning Zhang (Unknown), Cheng Tan (Unknown), Xiaobin Hu (Unknown), Shuicheng Yan (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对视觉多智能体系统的信息瓶颈，提出双潜在记忆框架，解耦感知与思考并动态合成记忆，提升系统性能和扩展性。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.00471' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Context Learning for Multi-Agent Discussion</h3>
<p><strong>Authors:</strong> Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang, Ju Ren</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对多LLM协作讨论的上下文不一致问题，提出M2CL多LLM上下文学习方法，提升协作一致性与任务性能，属于多模态智能体方向的重要进展。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02350' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AgentRx: Diagnosing AI Agent Failures from Execution Trajectories</h3>
<p><strong>Authors:</strong> Shraddha Barke, Arnav Goyal, Alind Khare, Avaljot Singh, Suman Nath, Chetan Bansal</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出AgentRx框架，诊断AI智能体的执行故障，属于多模态智能体方向的故障诊断技术。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02475' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> StepNav: Structured Trajectory Priors for Efficient and Multimodal Visual Navigation</h3>
<p><strong>Authors:</strong> Xubo Luo, Aodi Wu, Haodong Han, Xue Wan, Wei Zhang, Leizheng Shu, Ruisuo Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出StepNav框架，用结构化轨迹先验提升视觉导航效率，属于多模态智能体方向的导航技术。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.02590' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ProAct: A Benchmark and Multimodal Framework for Structure-Aware Proactive Response</h3>
<p><strong>Authors:</strong> Xiaomeng Zhu, Fengming Zhu, Weijie Zhou, Ye Tian, Zhenlin Hu, Yufei Huang, Yuchun Guo, Xinyu Wu, Zhengyou Zhang, Fangzhen Lin, Xuantang Xiong</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出ProAct基准与框架，实现结构感知的主动响应，属于多模态智能体方向的主动智能体技术。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.03430' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Reward Shaping for Inference-Time Alignment: A Stackelberg Game Perspective</h3>
<p><strong>Authors:</strong> Haichuan Wang, Tao Lin, Lingkai Kong, Ce Li, Hezi Jiang, Milind Tambe</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 将推理时对齐的奖励模型优化形式化为Stackelberg博弈，提出奖励塑造方案提升对齐效果，属于大模型安全与对齐中的alignment方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02572' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> How RLHF Amplifies Sycophancy</h3>
<p><strong>Authors:</strong> Itai Shapira, Gerdus Benade, Ariel D. Procaccia</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 形式化分析RLHF放大LLM谄媚行为的机制，通过奖励差距和协方差分析行为漂移，提出最小奖励修正的干预方法。实验验证奖励差距导致行为漂移，对大模型安全对齐的理论研究有重要贡献。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01002' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models</h3>
<p><strong>Authors:</strong> Yupeng Chen, Junchi Yu, Aoxi Liu, Philip Torr, Adel Bibi</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究多模态模型中的跨模态越狱攻击转移，发现对齐导致的漏洞，对大模型安全有重要意义，属于大模型安全与对齐中的LLM safety方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02557' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</h3>
<p><strong>Authors:</strong> Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, Yanyong Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Theoretically analyzes entropy dynamics in RL fine-tuning of LLMs, providing methods to balance exploration/exploitation for alignment.
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03392' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Inference-time Unlearning Using Conformal Prediction</h3>
<p><strong>Authors:</strong> Somnath Basu Roy Chowdhury, Rahul Kidambi, Avinava Dubey, David Wang, Gokhan Mergen, Amr Ahmed, Aranyak Mehta</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出推理时遗忘方法，使用conformal prediction提升unlearning性能，无需更新模型参数，属于大模型安全与对齐的unlearning方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03787' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Self-Guard: Defending Large Reasoning Models via enhanced self-reflection</h3>
<p><strong>Authors:</strong> Jingnan Zheng (Unknown), Jingjun Xu (Unknown), Yanzhen Luo (Unknown), Chenhang Cui (Unknown), Gelei Deng (Unknown), Zhenkai Liang (Unknown), Xiang Wang (Unknown), An Zhang (Unknown), Tat-Seng Chua (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出轻量级安全防御框架，通过安全导向提示和安全激活引导，解决大推理模型的安全合规问题，提升模型防御能力。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?</h3>
<p><strong>Authors:</strong> Sidharth Pulipaka, Oliver Chen, Manas Sharma, Taaha S Bajwa, Vyas Raina, Ivaxi Sheth</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对LLM长期记忆的安全风险，提出PersistBench基准，定义跨域泄漏和记忆诱导谄媚两个风险维度，评估18个前沿LLM的高失败率，为大模型安全对齐中记忆管理的研究提供重要基准和方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01146' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Building Better Deception Probes Using Targeted Instruction Pairs</h3>
<p><strong>Authors:</strong> Vikram Natarajan, Devina Jain, Shivam Arora, Satvik Golechha, Joseph Bloom</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对LLM欺骗行为的探测问题，提出基于目标指令对和欺骗taxonomy的探针设计。实验表明指令对捕获欺骗意图而非内容模式，对大模型安全对齐中的欺骗检测研究有重要推进。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety</h3>
<p><strong>Authors:</strong> Xiaoyu Wen, Zhida He, Han Qi, Ziyu Wan, Zhongtian Ma, Ying Wen, Tianhang Zheng, Xingcheng Xu, Chaochao Lu, Qiaosheng Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 将LLM安全对齐建模为攻击者-防御者的多轮对抗博弈，通过co-evolution机制解决静态数据导致的防御滞后问题，显著提升模型对长尾漏洞的泛化防御能力，实验验证防御成功率与模型有用性的平衡。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.01539' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens</h3>
<p><strong>Authors:</strong> Xuetao Li, Pinhan Fu, Wenke Huang, Nengyuan Pan, Songhua Yang, Kaiyan Zhao, Guancheng Wan, Mengde Li, Jifeng Xuan, Miao Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Bera框架，消除机器人策略的后门攻击，属于大模型安全与对齐方向的后门防御技术。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03153' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction</h3>
<p><strong>Authors:</strong> Wenqi Guo, Shan Du</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出FaceLinkGen攻击，揭示隐私保护人脸识别系统中基于模板的身份泄漏问题，挑战现有像素级隐私评估指标，对大模型安全与对齐中的隐私保护研究具有重要启示。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02914' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning</h3>
<p><strong>Authors:</strong> Piotr Wójcik, Maksym Petrenko, Wojciech Gromski, Przemysław Spurek, Maciej Zieba</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对扩散模型的unlearning问题，提出结合超网络和CLIP嵌入的动态LoRA unlearning框架，解决了LoRA在适应性和 scalability上的缺陷，对大模型安全与对齐中的unlearning技术有创新贡献
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03410' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models</h3>
<p><strong>Authors:</strong> Eliron Rahimi, Elad Hirshel, Rom Himelstein, Amit LeVi, Avi Mendelson, Chaim Baskin</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 分析自回归与扩散模型拒绝行为动态，提出SRI信号检测有害生成，提升模型安全性，对大模型安全与对齐研究有实践指导意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02600' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Towards Understanding Steering Strength</h3>
<p><strong>Authors:</strong> Magamed Taimeskhanov, Samuel Vaiter, Damien Garreau</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 理论分析大模型引导强度对生成的影响，提出定性规律，为对齐引导策略设计提供理论基础，属于大模型安全与对齐研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02712' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Membership Inference Attacks from Causal Principles</h3>
<p><strong>Authors:</strong> Mathieu Even, Cl\'ement Berenfeld, Linus Bleistein, Tudor Cebere, Julie Josse, Aur\'elien Bellet</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 从因果角度重新定义成员推理攻击，提出无偏估计器，提升安全评估可靠性，对大模型安全研究有重要贡献。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02819' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Hinting Language Models Enhance Reinforcement Learning</h3>
<p><strong>Authors:</strong> Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz (University of Amsterdam), Jiang Bian</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Presents SAGE to enhance GRPO for aligning LLMs with verifiable objectives, solving reward collapse in sparse reward settings.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03143' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</h3>
<p><strong>Authors:</strong> Wenquan Lu, Hai Huang, Randall Balestriero (Meta AI)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Uses prompt augmentation to stabilize GRPO training for mathematical reasoning, avoiding entropy collapse and improving alignment.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03190' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reinforcement Learning with Promising Tokens for Large Language Models</h3>
<p><strong>Authors:</strong> Jing-Cheng Pang, Liang Lu, Xian Tang, Kun Jiang, Sijie Wu, Kai Zhang, Xubin Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Proposes RLPT to constrain policy optimization to promising tokens, enhancing sample efficiency and stability in LLM alignment.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03195' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models</h3>
<p><strong>Authors:</strong> Hicham Eddoubi, Umar Faruk Abdullahi, Fadi Hassan</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Studies token position's impact on GCG adversarial attacks, revealing blind spots in LLM safety evaluations.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03265' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models</h3>
<p><strong>Authors:</strong> Yuelin Hu, Zhengxue Cheng, Wei Liu, Li Song</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Presents EGSPO for hybrid SFT+RL training, using token-level entropy to guide gradient allocation for stable LLM alignment.
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03309' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning</h3>
<p><strong>Authors:</strong> Zixiang Di, Jinyi Han, Shuo Zhang, Ying Liao, Zhi Li, Xiaofeng Ji, Yongqi Wang, Zheming Yang, Ming Gao, Bingdong Li, Jie Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究LLM推理中的负样本质量问题，提出合理负样本生成方法优化偏好学习，属于大模型安全与对齐中的alignment方向，提升了推理性能。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03516' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning</h3>
<p><strong>Authors:</strong> Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Luoyu Chen, Shui Yu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出EVE方法验证近似遗忘中的数据擦除，无需参与模型初始训练，提升了unlearning的验证效率，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03567' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense</h3>
<p><strong>Authors:</strong> Fatima Ezzeddine, Osama Zammar, Silvia Giordano, Omran Ayoub</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究反事实解释的隐私泄露问题，提出差分隐私与主动学习结合的防御方法，属于大模型安全与对齐的隐私保护方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03611' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates</h3>
<p><strong>Authors:</strong> Duy Nguyen, Hanqi Xiao, Archiki Prasad, Elias Stengel-Eskin, Hyunji Lee, Mohit Bansal</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出冲突解决与尖锐度感知方法优化多更新知识编辑，提升编辑的稳定性与泛化性，属于大模型安全与对齐的知识编辑方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03696' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees</h3>
<p><strong>Authors:</strong> Minhyuk Lee (Unknown), Hyekyung Yoon (Unknown), Myungjoo Kang (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出仅推理的prompt投影框架，在不重新训练生成器的情况下抑制不安全生成，同时保持良性prompt的对齐，属于大模型安全与对齐的生成安全研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.00616' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron</h3>
<p><strong>Authors:</strong> Sicheng Shen, Mingyang Lv, Han Shen, Jialin Wu, Binghao Wang, Zhou Yang, Guobin Shen, Dongcheng Zhao, Feifei Zhao, Yi Zeng</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出轻量级安全对齐方法，通过单神经元自反思机制平衡模型安全性与效用，避免传统post-training的高计算成本，实验验证在保持安全性能的同时最小化效用损失，对大模型低成本安全部署有实用价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02027' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</h3>
<p><strong>Authors:</strong> Han Bao, Zheyuan Zhang, Pengcheng Jing, Zhengqing Yuan, Kaiwen Shi, Yanfang Ye</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Drift-Bench基准，诊断LLM智能体在输入故障下的协作故障，提升智能体安全性，属于大模型安全与对齐方向的重要工作。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.02455' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Fast-MWEM: Private Data Release in Sublinear Time</h3>
<p><strong>Authors:</strong> Themistoklis Haris, Steve Choi, Mutiraj Laksanawisit</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Fast-MWEM方法实现亚线性时间的隐私数据发布，提升隐私保护的效率，属于大模型安全与对齐的隐私方向。
Score: 6
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03732' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Mixture of Concept Bottleneck Experts</h3>
<p><strong>Authors:</strong> Francesco De Santis, Gabriele Ciravegna, Giovanni De Felice, Arianna Casanova, Francesco Giannini, Michelangelo Diligenti, Mateo Espinosa Zarlenga, Pietro Barbiero, Johannes Schneider, Danilo Giordano</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出M-CBEs扩展概念瓶颈模型，用混合专家提升预测精度与适应性，属于深度学习可解释性中白盒解释研究。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02886' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning</h3>
<p><strong>Authors:</strong> Xufei Zhang, Xinjiao Zhou, Ziling Deng, Dongdong Geng, Jianxiong Wang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出LogiCls框架，通过约束分解和指令微调实现可解释的逻辑异常分类，提供预测违规类别及其证据链，对深度学习可解释性中的逻辑推理可解释性研究有重要贡献
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.03530' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LEMON: Local Explanations via Modality-aware OptimizatioN</h3>
<p><strong>Authors:</strong> Yu Qin, Phillip Sloan, Raul Santos-Rodriguez, Majid Mirmehdi, Telmo de Menezes e Silva Filho</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出模型无关的多模态局部解释框架，通过模态感知优化生成统一解释，提升可解释性方法的效率与忠实度，对深度学习可解释性研究有重要价值。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02786' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models</h3>
<p><strong>Authors:</strong> Arco van Breda, Erman Acar</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 采用mechanistic interpretability方法分析Transformer符号回归模型的内部电路，提出PATCHES算法识别功能模块，属于深度学习可解释性的关键研究内容。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.03506' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Supervised sparse auto-encoders as unconstrained feature models for semantic composition</h3>
<p><strong>Authors:</strong> Ouns El Harzli, Hugo Wallner, Yoonsoo Nam, Haixuan Xavier Tao</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对稀疏自编码器（SAE）在机械可解释性中的非平滑性和语义对齐问题，提出监督式SAE方法，结合神经折叠理论的无约束特征模型。在Stable Diffusion上验证了语义组合泛化和特征级图像编辑能力，对深度学习可解释性的落地应用有价值。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00924' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)</h3>
<p><strong>Authors:</strong> Zeinab Dehghani</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出gSMILE框架扩展SMILE至生成模型，通过受控扰动、Wasserstein距离和加权代理模型量化prompt对输出的影响。在LLM和图像编辑模型上验证了解释的鲁棒性和人类对齐性，对深度学习可解释性的研究有重要推进。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.01206' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach</h3>
<p><strong>Authors:</strong> Martino Ciaperoni, Marzio Di Vece, Luca Pappalardo, Fosca Giannotti, Francesco Giannini</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出比较XAI（Δ-XAI）框架，解决LLM行为变化的解释问题，结合理论 desiderata和实验验证，为深度学习可解释性提供新视角。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02304' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient</h3>
<p><strong>Authors:</strong> Changming Li, Kaixing Zhang, Haoyun Xu, Yingdong Shi, Zheng Zhang, Kaitao Song, Kan Ren</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Integrated Policy Gradient（IPG）框架，定位LLM推理中的关键组件，提升推理可解释性与可控性，实验验证有效。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02313' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Structure Enables Effective Self-Localization of Errors in LLMs</h3>
<p><strong>Authors:</strong> Ankur Samanta, Akshayaa Magesh, Ayush Jain, Kavosh Asadi, Youliang Yu, Daniel Jiang, Boris Vidolov, Kaveh Hassani, Paul Sajda, Jalaj Bhandari, Yonathan Efroni</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出结构化推理步骤辅助LLM错误自定位，解决非结构化CoT的错误定位问题，提升可解释性，实验验证有效。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02416' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Vector Quantized Latent Concepts: A Scalable Alternative to Clustering-Based Concept Discovery</h3>
<p><strong>Authors:</strong> Xuemin Yu, Ankur Garg, Samira Ebrahimi Kahou, Hassan Sajjad</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出VQ-based概念发现方法，改进传统聚类的 scalability，保持解释质量，对深度学习可解释性中概念提取研究有重要贡献。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.02726' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework</h3>
<p><strong>Authors:</strong> Guanzong Wu, Zihao Zhu, Siwei Lyu, Baoyuan Wu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> Introduces Toxicity Association Graphs (TAGs) and MTC metric for interpretable multimodal toxicity detection, addressing covert toxicity.
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.03268' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> APEX: Probing Neural Networks via Activation Perturbation</h3>
<p><strong>Authors:</strong> Tao Ren, Xiaoyu Luo, Qiongxiu Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出APEX激活扰动探测方法，揭示模型的内部结构与偏见，属于深度学习可解释性的新范式。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.03586' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Efficient Estimation of Kernel Surrogate Models for Task Attribution</h3>
<p><strong>Authors:</strong> Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出高效核代理模型估计方法解决任务归因问题，提升可解释性的效率与准确性，属于深度学习可解释性的任务归因方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.03783' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks</h3>
<p><strong>Authors:</strong> Jia Liang (Unknown), Liangming Pan (Unknown)</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 通过logit-lens、线性探针等方法研究Latent-CoT模型的顺序推理机制，属于深度学习可解释性中的机械可解释性研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.00449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers</h3>
<p><strong>Authors:</strong> Zhiwen Li, Zhongjie Duan, Jinyan Ye, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出VIRAL框架，利用扩散Transformer实现视觉上下文推理，通过类比公式和MoE LoRA解决任务异质性问题，对大模型新技术中的扩散模型推理研究具有重要贡献。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.03210' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models</h3>
<p><strong>Authors:</strong> Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Yongcheng Jing, Dacheng Tao</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出SPA-Cache优化扩散语言模型的缓存策略，通过低维奇异代理和自适应预算分配提升推理效率，属于大模型新技术中的diffusion LLM方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02544' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Random Matrix Theory Perspective on the Consistency of Diffusion Models</h3>
<p><strong>Authors:</strong> Binxu Wang, Jacob Zavatone-Veth, Cengiz Pehlevan</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 用随机矩阵理论分析扩散模型一致性，揭示数据有限性对生成结果的影响，属于大模型新技术中扩散模型研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02908' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> 3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning</h3>
<p><strong>Authors:</strong> Jiaqi Wen, Lei Fan, Jianyi Yang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出3D-Learning用扩散模型增强分布鲁棒决策学习，提升OOD泛化性，属于大模型新技术中扩散模型研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02943' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Lookahead Path Likelihood Optimization for Diffusion LLMs</h3>
<p><strong>Authors:</strong> Xuejie Liu, Yap Vit Chun, Yitao Liang, Anji Liu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 针对Diffusion LLMs的推理路径优化问题，提出路径似然估计与SMC搜索框架，显著提升推理准确性，符合用户关注的diffusion LLM大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.03496' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning with Latent Tokens in Diffusion Language Models</h3>
<p><strong>Authors:</strong> Andre He, Sean Welleck, Daniel Fried</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 研究Diffusion LLMs中的潜在令牌推理，提升模型的推理性能与全局一致性，符合用户关注的diffusion LLM大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.03769' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation</h3>
<p><strong>Authors:</strong> Zhanfeng Liao, Jiajun Zhang, Hanzhang Tu, Zhixi Wang, Yunqi Gao, Hongwen Zhang, Yebin Liu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出lifespan-aware的4D高斯框架，通过 lifespan参数和 densification策略改进动态场景的novel view synthesis，对大模型新技术中的动态高斯表示研究具有价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02989' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency</h3>
<p><strong>Authors:</strong> Geonhui Son, Jeong Ryong Lee, Dosik Hwang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出HP-GAN框架，利用预训练网络的FakeTwins和判别器一致性改进GAN的图像质量和多样性，对大模型新技术中的GAN优化研究具有价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.03039' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ConsistentRFT: Reducing Visual Hallucinations in Flow-based Reinforcement Fine-Tuning</h3>
<p><strong>Authors:</strong> Xiaofeng Tan, Jun Liu, Yuanting Fan, Bin-Bin Gao, Xi Jiang, Xiaochen Chen, Jinlong Peng, Chengjie Wang, Hongsong Wang, Feng Zheng</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 探索了flow-based模型强化微调中的视觉幻觉问题，提出动态粒度rollout和一致策略梯度优化的ConsistentRFT框架，对大模型新技术中的flow-based模型优化有重要研究价值
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.03425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models</h3>
<p><strong>Authors:</strong> Marcos Villagra, Bidhan Roy, Raihan Seraj, Zhiying Jiang</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 发现专家-数据对齐是去中心化扩散模型生成质量的关键，为扩散大模型设计优化提供新见解，属于大模型新技术研究范畴。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02685' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sparsely Supervised Diffusion</h3>
<p><strong>Authors:</strong> Wenshuai Zhao, Zhiyuan Li, Yi Zhao, Mohammad Hassan Vali, Martin Trapp, Joni Pajarinen, Juho Kannala, Arno Solin</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出稀疏监督策略改进扩散模型空间一致性，减少训练不稳定性，提升生成质量，是扩散大模型技术的重要改进，属于大模型新技术研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02699' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Distance Marching for Generative Modeling</h3>
<p><strong>Authors:</strong> Zimo Wang, Ishit Mehta, Haolin Lu, Chung-En Sun, Ge Yan, Tsui-Wei Weng, Tzu-Mao Li</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出Distance Marching方法改进时间无条件生成模型，提升生成质量，属于大模型新技术中扩散模型研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02928' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reasoning with Autoregressive-Diffusion Collaborative Thoughts</h3>
<p><strong>Authors:</strong> Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出统一自回归与扩散模型的协作框架，通过闭环交互结合两者优势（自回归的逻辑控制与扩散的空间建模），提升空间推理可靠性与生成可控性，属于大模型新技术中跨范式融合的重要探索。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.01608' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Linear Path Model-Based Diffusion</h3>
<p><strong>Authors:</strong> Yutaka Shimizu, Masayoshi Tomizuka</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Reason:</strong> 提出自适应线性路径模型扩散（ALP-MBD），提升diffusion模型的鲁棒性与效率，属于大模型新技术方向的diffusion应用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.02831' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>