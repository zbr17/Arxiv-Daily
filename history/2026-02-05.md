# ArXiv 每日推荐 - 2026-02-05

> 更新于北京时间：2026-02-05 13:25:47
> 已自动阅读了 287 篇最新的论文。
> 使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：149089

## 深度学习可解释性

### [Score: 10.0/10] Stroke Lesions as a Rosetta Stone for Language Model Interpretability
- **Authors:** Julius Fridriksson (University of South Carolina, ALLT.AI, LLC), Roger D. Newman-Norlund (University of South Carolina, ALLT.AI, LLC), Saeed Ahmadi (University of South Carolina), Regan Willis (University of South Carolina, Department of Computer Science and Engineering), Nadra Salman (University of South Carolina, Linguistics Program), Kalil Warren (University of South Carolina, Linguistics Program), Xiang Guan (University of South Carolina, Department of Computer Science and Engineering), Yong Yang (University of South Carolina, Department of Computer Science and Engineering), Srihari Nelakuditi (University of South Carolina, Department of Computer Science and Engineering), Rutvik Desai (Department of Psychology, University of South Carolina), Leonardo Bonilha (Department of Neurology, USC School of Medicine), Jeff Charney (ALLT.AI, LLC, MKHSTRY, LLC), Chris Rorden (Department of Psychology, University of South Carolina)
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04074](https://arxiv.org/abs/2602.04074)
- **Reason:** 利用中风患者的病灶-症状映射作为外部参考评估LLM扰动效果，为可解释性提供神经科学验证，方法新颖且实证支持强。
Score: 10
Field: 深度学习可解释性

### [Score: 9.0/10] Axiomatic Foundations of Counterfactual Explanations
- **Authors:** Leila Amgoud, Martin Cooper
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04028](https://arxiv.org/abs/2602.04028)
- **Reason:** 系统研究反事实解释的公理框架，分类不同类型的反事实解释，属于深度学习可解释性中的white-box explanation方向，理论价值高。
Score: 9
Field: 深度学习可解释性

### [Score: 8.0/10] When Chains of Thought Don't Matter: Causal Bypass in Large Language Models
- **Authors:** Anish Sathyanarayanan, Aditya Nagarsekar, Aarush Rathore
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03994](https://arxiv.org/abs/2602.03994)
- **Reason:** 揭示CoT的因果旁路问题，为CoT的忠实性评估提供诊断框架，属于深度学习可解释性的关键问题。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration
- **Authors:** Sudipto Ghosh, Sujoy Nath, Sunny Manchanda, Tanmoy Chakraborty
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04291](https://arxiv.org/abs/2602.04291)
- **Reason:** 研究多专家系统编排策略的可解释性，解耦专家交互的因果重要性与涌现结构，对多专家系统的可解释性分析有重要贡献。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Decomposing Query-Key Feature Interactions Using Contrastive Covariances
- **Authors:** Andrew Lee, Yonatan Belinkov, Fernanda Viégas, Martin Wattenberg
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04752](https://arxiv.org/abs/2602.04752)
- **Reason:** 提出对比协方差方法分解Transformer的query-key交互，识别低秩可解释组件，提升注意力机制的可解释性，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性

### [Score: 8.0/10] Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning
- **Authors:** Zidi Xiong, Shan Chen, Himabindu Lakkaraju
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03978](https://arxiv.org/abs/2602.03978)
- **Reason:** 研究RLVR训练中CoT的可监控性（即是否忠实反映内部计算），属于深度学习可解释性中的white-box explanation方向，对理解LLM的推理过程有价值。
Score: 8
Field: 深度学习可解释性

### [Score: 7.0/10] Counterfactual Explanations for Hypergraph Neural Networks
- **Authors:** Fabiano Veglianti, Lorenzo Antonelli, Gabriele Tolomei
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04360](https://arxiv.org/abs/2602.04360)
- **Reason:** 提出超图神经网络的反事实解释方法，生成简洁且结构有意义的解释，解决超图模型的可解释性问题，属于深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性

### [Score: 7.0/10] Identifying Intervenable and Interpretable Features via Orthogonality Regularization
- **Authors:** Moritz Miller, Florent Draye, Bernhard Schölkopf
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04718](https://arxiv.org/abs/2602.04718)
- **Reason:** 通过正交正则化识别可干预和可解释的特征，提升模型的因果干预能力与可解释性，属于深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性

## 高效大模型训练与推理

### [Score: 10.0/10] Learning to Reason in 13 Parameters
- **Authors:** John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04118](https://arxiv.org/abs/2602.04118)
- **Reason:** 提出TinyLoRA，用13个参数训练LLM推理能力，在GSM8K达91%准确率，为高效训练的参数效率提供突破性结果。
Score: 10
Field: 高效大模型训练与推理

### [Score: 9.0/10] Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models
- **Authors:** Yichen Xu, Yuyang Liang, Shan Dai, Tianyang Hu, Tsz Nam Chan, Chenhao Ma
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04019](https://arxiv.org/abs/2602.04019)
- **Reason:** 针对PEFT的层选择问题提出Layer Card诊断工具，优化层选择以提升效率，属于高效大模型训练的核心问题。
Score: 9
Field: 高效大模型训练与推理

### [Score: 9.0/10] BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models
- **Authors:** Junyu Chen, Jungang Li, Jing Xiong, Wenjie Wang, Qingyao Yang, He Xiao, Zhen Li, Taiqiang Wu, Mengzhao Chen, Zhen Peng, Chaofan Tao, Long Shi, Hongxia Yang, Ngai Wong
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04163](https://arxiv.org/abs/2602.04163)
- **Reason:** 提出BPDQ量化方法，优化LLM低比特量化，在2比特下保持高保真，为高效推理部署提供关键技术。
Score: 9
Field: 高效大模型训练与推理

### [Score: 8.5/10] PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective
- **Authors:** Haokui Zhang, Congyang Ou, Dawei Yan, Peng Wang, Qingsen Yan, Ying Li, Rong Xiao, Chunhua Shen
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04657](https://arxiv.org/abs/2602.04657)
- **Reason:** 从推理目标出发，通过层局部代理损失生成token级梯度显著性，结合NMS选择重要视觉token，实现VLM的训练-free加速，保持97.2%原性能的同时获得2.67倍prefill加速，属于高效大模型训练与推理的关键优化。
Score: 8.5
Field: 高效大模型训练与推理

### [Score: 8.0/10] Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers
- **Authors:** Peihao Xiang (Unknown), Kaida Wu (Unknown), Ou Bai (Unknown)
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03918](https://arxiv.org/abs/2602.03918)
- **Reason:** 提出基于信息熵的无数据块重要性估计方法，用于掩码自监督ViT的块剪枝，解决了大模型部署的资源约束问题，实验验证在VideoMAE上剪枝91.7%块仍保持 competitive性能，属于高效大模型训练与推理方向的关键技术。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration
- **Authors:** Zekun Li, Ning Wang, Tongxin Bai, Changwang Mei, Peisong Wang, Shuang Qiu, Jian Cheng
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04361](https://arxiv.org/abs/2602.04361)
- **Reason:** 提出训练-free的稀疏注意力框架，利用视觉自回归模型的注意力特性动态预测稀疏模式，实现高分辨率图像生成的推理加速，同时保持高频细节，属于高效大模型训练与推理的核心方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention
- **Authors:** Chengtao Lv, Yumeng Shi, Yushi Huang, Ruihao Gong, Shen Ren, Wenya Wang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04789](https://arxiv.org/abs/2602.04789)
- **Reason:** 针对自回归视频扩散模型的稀疏注意力加速，提出Chunk-Aware Growth和Hierarchical Sparse Attention，实现1.2-1.3倍端到端加速，结合量化和LightVAE后达到2.3倍加速，属于高效大模型训练与推理的视频扩散优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] SpecMD: A Comprehensive Study On Speculative Expert Prefetching
- **Authors:** Duc Hoang, Ajay Jaiswal, Mohammad Samragh, Minsik Cho
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03921](https://arxiv.org/abs/2602.03921)
- **Reason:** 开发SpecMD框架基准测试MoE缓存策略，提出Least-Stale策略利用MoE可预测的专家访问模式，减少碰撞 miss 85倍，提升缓存命中率和推理速度，属于高效大模型训练与推理的MoE优化。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Topology-Aware Revival for Efficient Sparse Training
- **Authors:** Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04166](https://arxiv.org/abs/2602.04166)
- **Reason:** 提出TAR稀疏训练方法，通过拓扑感知恢复剪枝连接，提升静态稀疏模型的鲁棒性，属于高效大模型训练的稀疏优化方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] LoRDO: Distributed Low-Rank Optimization with Infrequent Communication
- **Authors:** Andrej Jovanović, Alex Iacob, Mher Safaryan, Ionut-Vlad Modoranu, Lorenzo Sani, William F. Shen, Xinchi Qiu, Dan Alistarh, Nicholas D. Lane
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04396](https://arxiv.org/abs/2602.04396)
- **Reason:** 提出分布式低秩优化框架，通过不频繁通信和低秩投影提升大模型训练效率，减少通信开销，属于高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] The Key to State Reduction in Linear Attention: A Rank-based Perspective
- **Authors:** Philipp Nazari, T. Konstantin Rusch
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04852](https://arxiv.org/abs/2602.04852)
- **Reason:** 分析线性注意力的低秩状态，提出结构化剪枝减少查询和键矩阵，提高推理效率，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism
- **Authors:** Chenwei Cui, Rockwell Jackson, Benjamin Joseph Herrera, Ana María Tárano, Hannah Kerner
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04870](https://arxiv.org/abs/2602.04870)
- **Reason:** 解决MoE的通信成本和负载不平衡问题，提出Head Parallel，训练更快，符合高效大模型训练方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent
- **Authors:** Yinyi Luo, Yiqiao Jin, Weichen Yu, Mengqi Zhang, Srijan Kumar, Xiaoxiao Li, Weijie Xu, Xin Chen, Jindong Wang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03955](https://arxiv.org/abs/2602.03955)
- **Reason:** 将多智能体的推理能力蒸馏到单LLM，保持效率同时提升性能，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Interfaze: The Future of AI is built on Task-Specific Small Models
- **Authors:** Harsha Vardhan Khurdula, Vineet Agarwal, Yoeven D Khemlani
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04101](https://arxiv.org/abs/2602.04101)
- **Reason:** 用小模型处理感知任务（OCR、ASR等），大模型处理distilled context，减少大模型的计算量，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 8.0/10] Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning
- **Authors:** Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04284](https://arxiv.org/abs/2602.04284)
- **Reason:** 训练LLM Agent自适应省略冗余的思考和观察，提高推理效率，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理

### [Score: 7.5/10] When LLaVA Meets Objects: Token Composition for Vision-Language-Models
- **Authors:** Soumya Jahagirdar, Walid Bousselham, Anna Kukleva, Hilde Kuehne
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04864](https://arxiv.org/abs/2602.04864)
- **Reason:** 组合mask-based对象表示、全局token和局部patch token，实现VLM的视觉token压缩，测试时可灵活减少对象token数量而不 retrain，属于高效大模型训练与推理的VLM优化。
Score: 7.5
Field: 高效大模型训练与推理

### [Score: 7.5/10] Laminating Representation Autoencoders for Efficient Diffusion
- **Authors:** Ram\'on Calvo-Gonz\'alez, Fran\c{c}ois Fleuret
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04873](https://arxiv.org/abs/2602.04873)
- **Reason:** 提出FlatDINO variational autoencoder压缩SSL patch特征，将扩散模型的序列长度减少8倍，实现扩散模型的训练和推理加速，属于高效大模型训练与推理的扩散模型优化。
Score: 7.5
Field: 高效大模型训练与推理

### [Score: 7.0/10] Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation
- **Authors:** Sayan Biswas, Davide Frey, Romaric Gaudel, Nirupam Gupta, Anne-Marie Kermarrec, Dimitri Leréverend, Rafael Pires, Rishi Sharma, François Taïani, Martijn de Vos
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04352](https://arxiv.org/abs/2602.04352)
- **Reason:** 提出模型碎片化的去中心化学习框架，通过分解模型为片段并独立传播，提升去中心化学习的性能与效率，属于高效大模型训练与推理方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning
- **Authors:** Yuxi Guo, Paul Sheridan
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04491](https://arxiv.org/abs/2602.04491)
- **Reason:** 提出基于梯度矩阵norm的注意力头剪枝方法，动态计算头重要性，提升模型压缩效率，属于高效大模型训练与推理中的高压缩方向。
Score: 7
Field: 高效大模型训练与推理

### [Score: 7.0/10] Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure
- **Authors:** Shuhui Qu
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03975](https://arxiv.org/abs/2602.03975)
- **Reason:** 提出自适应分配验证计算，减少冗余调用，提高推理效率，符合高效大模型训练与推理方向。
Score: 7
Field: 高效大模型训练与推理

## 原生多模态大模型

### [Score: 9.0/10] VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents
- **Authors:** Feng Wang (Unknown), Yichun Shi (Unknown), Ceyuan Yang (Unknown), Qiushan Guo (Unknown), Jingxiang Sun (Unknown), Alan Yuille (Unknown), Peng Wang (Unknown)
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04202](https://arxiv.org/abs/2602.04202)
- **Reason:** 提出解耦空间-时间表示的视频tokenizer，将视频token复杂度从帧计数与单帧token数的乘积降为和，在视频理解和文本到视频生成任务上优于基线，是原生多模态大模型中视频tokenization的关键改进。
Score: 9
Field: 原生多模态大模型

### [Score: 9.0/10] Group Contrastive Learning for Weakly Paired Multimodal Data
- **Authors:** Aditya Gorla, Hugues Van Assel, Jan-Christian Huetter, Heming Yao, Kyunghyun Cho, Aviv Regev, Russell Littman
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04021](https://arxiv.org/abs/2602.04021)
- **Reason:** 针对弱配对多模态数据提出GroupCLIP框架，解决多模态表示学习的弱配对挑战，作者包括权威学者Kyunghyun Cho。
Score: 9
Field: 原生多模态大模型

### [Score: 8.0/10] Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
- **Authors:** Sicheng Liu, Xunkai Li, Daohan Su, Ru Zhang, Hongchao Qin, Ronghua Li, Guoren Wang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04116](https://arxiv.org/abs/2602.04116)
- **Reason:** 提出分治策略的多模态图基础模型，解决多模态图的交互与对齐问题，属于原生多模态大模型的新兴方向。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] Training Data Efficiency in Multimodal Process Reward Models
- **Authors:** Jinyuan Li, Chengsong Huang, Langlin Huang, Shaoyang Xu, Haolin Liu, Wenxuan Zhang, Jiaxin Huang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04145](https://arxiv.org/abs/2602.04145)
- **Reason:** 研究多模态过程奖励模型的数据效率，提出BIS策略筛选有效数据，提升训练效率，属于原生多模态大模型的实用化关键。
Score: 8
Field: 原生多模态大模型

### [Score: 8.0/10] OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows
- **Authors:** Ruiting Dai, Zheyu Wang, Haoyu Yang, Yihan Liu, Chengzhi Wang, Zekun Zhang, Zishan Huang, Jiaman Cen, Lisi Mo
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04144](https://arxiv.org/abs/2602.04144)
- **Reason:** 解决多模态数据缺失的生成问题，采用粗到细的agentic workflow，属于原生多模态大模型方向，对多模态生成的鲁棒性有价值。
Score: 8
Field: 原生多模态大模型

### [Score: 7.0/10] EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets
- **Authors:** Jiapeng Liu, Liang Li, Bing Li, Peng Fu, Xiyan Gao, Chengyang Fang, Xiaoshuai Hao, Can Ma
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04365](https://arxiv.org/abs/2602.04365)
- **Reason:** 提出基于熵增益最大化的子集选择方法，用于探测多模态大模型（Chart Understanding的MLLM）的能力增益，提升数据集迭代优化效率，属于原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型

## 大模型安全与对齐

### [Score: 9.0/10] KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing
- **Authors:** Siyu Jiang (Unknown), Feiyang Chen (Unknown), Xiaojin Zhang (Unknown), Kun He (Unknown)
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04268](https://arxiv.org/abs/2602.04268)
- **Reason:** 提出注意力熵引导的KV缓存自适应平滑方法，无需重新训练即可减少多模态大模型的幻觉问题，实验验证显著降低幻觉率同时提升性能，属于大模型安全与对齐方向的实用技术。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning
- **Authors:** Zeming Wei, Qiaosheng Zhang, Xia Hu, Xingcheng Xu
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04224](https://arxiv.org/abs/2602.04224)
- **Reason:** 提出RAPO框架，通过风险感知偏好优化提升LLM安全推理泛化能力，有效防御 jailbreak攻击，属于大模型安全与对齐的关键问题。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] Rethinking the Trust Region in LLM Reinforcement Learning
- **Authors:** Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04879](https://arxiv.org/abs/2602.04879)
- **Reason:** 指出PPO的ratio clipping不适合LLM的大词汇量，提出DPPO用政策 divergence约束，提高训练稳定性和对齐性，符合大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐

### [Score: 9.0/10] Steering LLMs via Scalable Interactive Oversight
- **Authors:** Enyu Zhou, Zhiheng Xi, Long Ma, Zhihao Zhang, Shihan Dou, Zhikai Lei, Guoteng Wang, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04210](https://arxiv.org/abs/2602.04210)
- **Reason:** 将复杂意图分解为递归决策树，通过交互式监督引导LLM，增强人类对LLM的控制，符合大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐

### [Score: 8.0/10] RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models
- **Authors:** Jiacheng Liang, Yuhui Wang, Tanqiu Jiang, Ting Wang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04448](https://arxiv.org/abs/2602.04448)
- **Reason:** 提出路由感知的专家级对齐框架，解决MoE模型的安全对齐问题，通过修复安全关键专家提升模型安全性，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐

### [Score: 8.0/10] SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF
- **Authors:** Dipan Maity
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04651](https://arxiv.org/abs/2602.04651)
- **Reason:** 提出熵感知的预测控制的RLHF方法，通过双软最小评论家与自适应KL调节提升对齐微调的稳定性，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐

### [Score: 7.0/10] MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems
- **Authors:** Jonathan Nöther, Adish Singla, Goran Radanovic
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04431](https://arxiv.org/abs/2602.04431)
- **Reason:** 提出博弈论方法设计安全的多智能体系统，通过Meta-Agent与Meta-Adversary的对抗优化提升系统安全性，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐

### [Score: 7.0/10] From Data to Behavior: Predicting Unintended Model Behaviors Before Training
- **Authors:** Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04735](https://arxiv.org/abs/2602.04735)
- **Reason:** 提出轻量级方法预测LLM训练前的非预期行为，通过数据特征注入揭示潜在偏见与安全风险，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐

## 深度学习理论

### [Score: 9.0/10] Rational ANOVA Networks
- **Authors:** Jusheng Zhang, Ningyuan Liu, Qinhan Lyu, Jing Yang, Keze Wang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04006](https://arxiv.org/abs/2602.04006)
- **Reason:** 提出基于ANOVA分解与有理近似的网络架构，解决传统模型的效率与稳定性问题，为深度学习理论中的网络设计提供新范式。
Score: 9
Field: 深度学习理论

### [Score: 9.0/10] From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers
- **Authors:** Ibrahim Albool, Malak Gamal El-Din, Salma Elmalaki, Yasser Shoukry
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04264](https://arxiv.org/abs/2602.04264)
- **Reason:** 提出Deep Bernstein Networks，用Bernstein多项式替代残差层，解决梯度消失与死神经元问题，有理论证明且效果优于传统残差网络。
Score: 9
Field: 深度学习理论

### [Score: 8.0/10] TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions
- **Authors:** Ali Bayeh (Unknown), Samira Sadaoui (Unknown), Malek Mouhoub (Unknown)
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03879](https://arxiv.org/abs/2602.03879)
- **Reason:** 针对Kolmogorov-Arnold Networks（KAN）的架构改进，提出用截断幂函数替代B样条基，在保持表达能力的同时提升准确性、训练效率和可解释性，实验验证了在计算机视觉任务上优于其他KAN变体，属于深度学习理论中网络架构方向的重要改进。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Partial Ring Scan: Revisiting Scan Order in Vision State Space Models
- **Authors:** Yi-Kuan Hsieh (Unknown), Jun-Wei Hsieh (Unknown), Xin li (Unknown), Ming-Ching Chang (Unknown), Yu-Chee Tseng (Unknown)
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04170](https://arxiv.org/abs/2602.04170)
- **Reason:** 针对视觉状态空间模型（Vision SSM）的扫描顺序问题，提出旋转鲁棒的Partial Ring Scan，改进空间邻接性和几何变换鲁棒性，实验在ImageNet上实现更高 accuracy和 throughput，属于深度学习理论中网络架构设计的重要创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression
- **Authors:** Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Shui Yu
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03906](https://arxiv.org/abs/2602.03906)
- **Reason:** 从信息几何视角重新设计信息瓶颈，结合分布级Fisher-Rao差异和几何级Jacobian-Frobenius项，实现更好的预测精度与压缩比权衡，属于深度学习理论的信息瓶颈改进。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Representation Geometry as a Diagnostic for Out-of-Distribution Robustness
- **Authors:** Ali Zia, Farid Hazratian
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03951](https://arxiv.org/abs/2602.03951)
- **Reason:** 提出基于表示几何的分布外鲁棒性诊断框架，通过谱复杂度与曲率度量识别模型鲁棒性，为深度学习理论中的鲁棒性评估提供新工具。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Non-linear PCA via Evolution Strategies: a Novel Objective Function
- **Authors:** Thomas Uriot, Elise Chung
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03967](https://arxiv.org/abs/2602.03967)
- **Reason:** 提出进化策略优化的非线性PCA框架，解决Kernel PCA的可解释性与超参数问题，属于深度学习理论中的表示学习创新。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Principles of Lipschitz continuity in neural networks
- **Authors:** R\'ois\'in Luo
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04078](https://arxiv.org/abs/2602.04078)
- **Reason:** 系统研究神经网络的Lipschitz连续性原理，分析其对鲁棒性与泛化的影响，为深度学习理论中的模型稳定性提供基础框架。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] MirrorLA: Reflecting Feature Map for Vision Linear Attention
- **Authors:** Weikang Meng, Liangyu Huo, Yadan Luo, Yaowei Wang, Yingjian Li, Zheng Zhang
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04346](https://arxiv.org/abs/2602.04346)
- **Reason:** 提出反射特征图的视觉线性注意力框架，解决线性注意力因非负约束导致的性能低下问题，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning
- **Authors:** Rui Yuan, Mykola Khandoga, Vinay Kumar Sankarapu
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04380](https://arxiv.org/abs/2602.04380)
- **Reason:** 提出用灵活的Bregman散度代替KL散度的策略优化方法，提升LLM推理性能，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL
- **Authors:** Lunjun Zhang, Jimmy Ba
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04417](https://arxiv.org/abs/2602.04417)
- **Reason:** 提出EMA锚点和Top-k KL的强化学习方法，提升LLM的推理和智能体行为性能，解决强化学习的稳定性问题，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions
- **Authors:** Dmitry Yarotsky, Eugene Golikov, Yaroslav Gusev
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04548](https://arxiv.org/abs/2602.04548)
- **Reason:** 开发梯度流的图展开框架，分析大学习问题的缩放机制并推导显式解，揭示不同学习阶段的特性，属于深度学习理论方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] Delving into Muon and Beyond: Deep Analysis and Extensions
- **Authors:** Xianbiao Qi, Marco Chen, Jiaquan Ye, Yelin He, Rong Xiao
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04669](https://arxiv.org/abs/2602.04669)
- **Reason:** 深入分析Muon优化器的机制，提出基于谱变换的扩展变体，揭示Muon作为谱归一化的本质，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论

### [Score: 8.0/10] It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task
- **Authors:** Hannah Pinson
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04832](https://arxiv.org/abs/2602.04832)
- **Reason:** 分析单隐藏层ReLU网络的神经元学习动态，解释彩票假设，属于深度学习理论中的优化和网络架构方向，对理解梯度下降的容量调整有价值。
Score: 8
Field: 深度学习理论

### [Score: 7.5/10] Online Vector Quantized Attention
- **Authors:** Nick Alonso, Tomas Figliolia, Beren Millidge
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.03922](https://arxiv.org/abs/2602.03922)
- **Reason:** 提出在线矢量量化注意力层，通过稀疏内存更新增加内存容量，实现线性计算和常量内存，平衡效率与长上下文处理，属于深度学习理论的注意力架构改进。
Score: 7.5
Field: 深度学习理论

### [Score: 7.0/10] Continual Learning through Control Minimization
- **Authors:** Sander de Haan, Yassine Taoudi-Benchekroun, Pau Vilimelis Aceituno, Benjamin F. Grewe
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04542](https://arxiv.org/abs/2602.04542)
- **Reason:** 将持续学习重新表述为控制问题，通过控制最小化平衡可塑性与稳定性，推导持续自然梯度，属于深度学习理论方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Finding Structure in Continual Learning
- **Authors:** Pourya Shamsolmoali, Masoumeh Zareapoor
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04555](https://arxiv.org/abs/2602.04555)
- **Reason:** 用Douglas-Rachford Splitting重新表述持续学习目标，通过协商可塑性与稳定性目标发现学习结构，提升持续学习性能，属于深度学习理论方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations
- **Authors:** Maya Janvier, Julien Salomon, Etienne Meunier
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04608](https://arxiv.org/abs/2602.04608)
- **Reason:** 提出雅可比正则化稳定神经微分方程的长期积分，解决长时序模拟的稳定性问题，属于深度学习理论中的网络架构方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations
- **Authors:** Hang Yu, Yu-Hu Yan, Peng Zhao
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04761](https://arxiv.org/abs/2602.04761)
- **Reason:** 研究bandit反馈下的梯度变化在线学习，改进维度依赖性，属于深度学习理论中的优化方向，对optimizer的理论改进有价值。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] From independent patches to coordinated attention: Controlling information flow in vision transformers
- **Authors:** Kieran A. Murphy
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04784](https://arxiv.org/abs/2602.04784)
- **Reason:** 提出通过变分信息瓶颈控制视觉Transformer的注意力信息流动，属于网络架构的改进，符合深度学习理论中的network architecture方向。
Score: 7
Field: 深度学习理论

### [Score: 7.0/10] Fluid Representations in Reasoning Models
- **Authors:** Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04843](https://arxiv.org/abs/2602.04843)
- **Reason:** 分析推理模型在推理过程中逐渐改进的内部表示，属于深度学习理论中的network architecture和表示学习方向，对理解推理模型的工作机制有价值。
Score: 7
Field: 深度学习理论

## 大模型新技术

### [Score: 9.0/10] Generative Modeling via Drifting
- **Authors:** Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04770](https://arxiv.org/abs/2602.04770)
- **Reason:** 提出Drifting Models新生成范式，单步推理在ImageNet上达到SOTA，属于大模型新技术中的生成模型突破，作者团队影响力高。
Score: 9
Field: 大模型新技术

### [Score: 8.0/10] CoRe: Context-Robust Remasking for Diffusion Language Models
- **Authors:** Kevin Zhai, Sabbir Mollah, Zhenyi Wang, Mubarak Shah
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04096](https://arxiv.org/abs/2602.04096)
- **Reason:** 针对扩散语言模型的上下文刚性问题提出CoRe推理框架，提升推理稳定性，属于大模型新技术中的diffusion LLM优化。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Generative Neural Operators through Diffusion Last Layer
- **Authors:** Sungwon Park, Anthony Zhou, Hongjoong Kim, Amir Barati Farimani
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04139](https://arxiv.org/abs/2602.04139)
- **Reason:** 提出扩散最后层，为神经算子添加轻量概率头，提升 stochastic PDE的不确定性量化，属于大模型新技术中的diffusion应用创新。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design
- **Authors:** Jaemoo Choi, Yuchen Zhu, Wei Guo, Petr Molodyk, Bo Yuan, Jinbin Bai, Yi Xin, Molei Tao, Yongxin Chen
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04663](https://arxiv.org/abs/2602.04663)
- **Reason:** 重新思考扩散模型的强化学习设计空间，强调似然估计的重要性，通过ELBO-based似然估计提升优化效率与稳定性，属于diffusion LLM相关的大模型新技术。
Score: 8
Field: 大模型新技术

### [Score: 8.0/10] Dynamical Regimes of Multimodal Diffusion Models
- **Authors:** Emil Albrychiewicz, Andrés Franco Valiente, Li-Ching Chen
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04780](https://arxiv.org/abs/2602.04780)
- **Reason:** 分析多模态扩散模型的动态机制，解释同步间隙等现象，对diffusion LLM的多模态生成有理论指导，符合大模型新技术方向。
Score: 8
Field: 大模型新技术

### [Score: 7.0/10] UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching
- **Authors:** Kou Misaki, Takuya Akiba
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04344](https://arxiv.org/abs/2602.04344)
- **Reason:** 针对掩码扩散语言模型提出确定性动作分支的测试时缩放框架，通过蒙特卡洛树搜索优化生成路径，提升复杂任务性能，属于diffusion LLM相关的大模型新技术。
Score: 7
Field: 大模型新技术

### [Score: 7.0/10] Theory of Speciation Transitions in Diffusion Models with General Class Structure
- **Authors:** Beatrice Achilli, Marco Benedetti, Giulio Biroli, Marc Mézard
- **Published:** 2026-02-05
- **Link:** [https://arxiv.org/abs/2602.04404](https://arxiv.org/abs/2602.04404)
- **Reason:** 发展扩散模型的物种形成转变理论，适用于任意类结构的目标分布，推导物种形成时间的通用准则，属于diffusion LLM相关的大模型新技术。
Score: 7
Field: 大模型新技术

