<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-06</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-06</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-06 13:26:13</p>
<p>已自动阅读了 329 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：175403</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs</h3>
<p><strong>Authors:</strong> Qi Li, Yanzhe Zhao, Yongxin Zhou, Yameng Wang, Yandong Yang, Yuanjia Zhou, Jue Wang, Zuojian Wang, Jinxiang Liu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出视觉token压缩的多模态嵌入框架，通过多阶段训练提升效率和性能，属于高效大模型训练与推理的token高效方向，性能优于现有方法。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05275' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Regularized Calibration with Successive Rounding for Post-Training Quantization</h3>
<p><strong>Authors:</strong> Seohyeon Cha, Huancheng Chen, Dongjun Kim, Haoran Zhang, Kevin Chan, Gustavo de Veciana, Haris Vikalo</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出正则化校准与逐次舍入改进后训练量化，提升LLM低比特推理性能，属高效推理关键技术。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05902' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Orthogonal Model Merging</h3>
<p><strong>Authors:</strong> Sihan Yang, Kexuan Shi, Weiyang Liu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出正交群流形上的模型合并方法，保留权重几何结构，提升多任务适应效率与性能。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05943' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Layer-wise LoRA fine-tuning: a similarity metric approach</h3>
<p><strong>Authors:</strong> Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Lucas Pellicer, Rosimeire Pereira Costa, Edson Bollis, Anna Helena Reali Costa, Artur Jordao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出分层LoRA微调，通过相似性度量选关键层，减少训练参数同时保持性能，属参数高效训练技术。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05988' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fast-SAM3D: 3Dfy Anything in Images but Faster</h3>
<p><strong>Authors:</strong> Weilun Feng, Mingqiang Wu, Zhiliang Chen, Chuanguang Yang, Haotong Qin, Yuqi Li, Xiaokun Liu, Guoxin Fan, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出SAM3D的训练无关加速框架，通过异质性感知机制提升推理速度，属于高效大模型训练与推理的模型加速方向，速度提升2.67×且保真度损失小。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05293' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion</h3>
<p><strong>Authors:</strong> Zhuokun Chen, Jianfei Cai, Bohan Zhuang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出块扩散的注意力缓存机制，复用稳定的跨块注意力输出，属于高效大模型训练与推理的长上下文生成方向，提升token吞吐量和减少注意力时间。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dataset Distillation via Relative Distribution Matching and Cognitive Heritage</h3>
<p><strong>Authors:</strong> Qianxin Xia, Jiawei Du, Yuhan Zhang, Jielei Wang, Guoming Lu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出数据集蒸馏框架，通过统计流匹配和分类器继承减少计算和内存，属于高效大模型训练与推理的数据集高效方向，性能优于基线。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05391' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching</h3>
<p><strong>Authors:</strong> Chang Zou, Changlin Li, Yang Li, Patrol Li, Jianbing Wu, Xiao He, Songtao Liu, Zhao Zhong, Kailin Huang, Linfeng Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出视频扩散Transformer的加速框架，结合蒸馏和可学习特征缓存，属于高效大模型训练与推理的视频生成加速方向，提升速度且保持质量。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression</h3>
<p><strong>Authors:</strong> Kangjie Zhang, Wenxuan Huang, Xin Zhou, Boxiang Zhou, Dejia Song, Yuan Xie, Baochang Zhang, Lizhuang Ma, Nemo Chen, Xu Tang, Yao Hu, Shaohui Lin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对CLIP模型高资源消耗问题，提出CLIP-Map框架，通过结构化矩阵映射和对角继承初始化实现参数高效压缩，在高压缩比下保持特征表示能力，实验优于现有选择基压缩方法，属于高效大模型训练与推理（高压缩）方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05909' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog</h3>
<p><strong>Authors:</strong> Yiran Zhao, Shengyang Zhou, Zijian Wu, Tongyan Hu, Yuhui Xu, Rengan Dou, Kenji Kawaguchi, Shafiq Joty, Junnan Li, Michael Qizhe Shieh</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出逐步压缩法（Prune-Tune Loop），通过多轮剪枝与微调逐步减少LLM参数，在保持推理性能的前提下提升训练与推理效率，属于高效大模型训练与推理的重要方法。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.04919' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation</h3>
<p><strong>Authors:</strong> Junhan Kim, Yeo Jeong Park, Seungwoo Son, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出TurboBoA，通过联合量化多输出通道与误差补偿，实现更快的精确注意力感知量化，无需反向传播，显著提升LLM量化效率，属于高效大模型训练与推理的关键技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.04929' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning</h3>
<p><strong>Authors:</strong> Yu-Ang Lee, Ching-Yun Ko, Pin-Yu Chen, Mi-Yen Yeh</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 系统研究LoRA的学习率影响，发现适当调优学习率后，vanilla LoRA的性能与改进型LoRA相当，为高效大模型微调提供了简洁有效的方案。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.04998' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference</h3>
<p><strong>Authors:</strong> Jiyoung Park, Hankyu Jang, Changseok Song, Wookeun Jung</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出TIDE框架整合在线draft自适应与异构集群，零开销复用目标模型隐状态，提升speculative decoding吞吐量1.15倍
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05145' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoSA: Compressed Sensing-Based Adaptation of Large Language Models</h3>
<p><strong>Authors:</strong> Songtao Wei, Yi Li, Bohan Zhang, Zhichun Guo, Ying Huang, Yuede Ji, Miao Yin, Guanpeng Li, Bingzhe Li</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出CoSA方法替代低秩分解，通过压缩感知实现LLM高效自适应，在10项任务中匹配或超越LoRA、PiSSA等SOTA
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05148' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs</h3>
<p><strong>Authors:</strong> Wentao Ni, Kangqi Zhang, Zhongming Yu, Oren Nelson, Mingu Lee, Hong Cai, Fatih Porikli, Jongryool Kim, Zhijian Liu, Jishen Zhao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出Double-P分层top-p稀疏注意力，通过簇级粗选与令牌级细选优化长上下文注意力计算，实现1.3x推理加速与近零精度损失
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05191' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ZeroS: Zero-Sum Linear Attention for Efficient Transformers</h3>
<p><strong>Authors:</strong> Jiecheng Lu, Xu Han, Yan Sun, Viresh Pati, Yubin Kim, Siddhartha Somani, Shihao Yang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出ZeroS零和线性注意力，移除常数项并重新加权残差，实现O(N)复杂度与softmax注意力相当的性能
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05230' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers</h3>
<p><strong>Authors:</strong> Boxiang Zhang, Baijian Yang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出CORP无微调结构化剪枝方法，通过闭形式岭回归保留ViT表示能力，在DeiT-Huge上剪枝50%结构仍保持82.8% Top-1 accuracy
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05243' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction</h3>
<p><strong>Authors:</strong> David Alejandro Trejo Pizzo</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出HGF框架，通过1.58-bit三元骨干与低秩校正流稳定低精度LLM，恢复55% FP16性能差距，仅增加12-15%内存开销
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05269' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging</h3>
<p><strong>Authors:</strong> Yayuan Li, Ze Peng, Jian Zhang, Jintao Guo, Yue Duan, Yinghuan Shi</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出SVC方法校准模型合并的奇异值，解决共享知识的过度积累问题，将Task Arithmetic性能提升13.0%
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05536' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Shiva-DiT: Residual-Based Differentiable Top-k Selection for Efficient Diffusion Transformers</h3>
<p><strong>Authors:</strong> Jiaji Zhang, Hailiang Zhao, Guoxuan Zhu, Ruichao Sun, Jiaju Wu, Xinkui Zhao, Hanlin Tang, Weiyi Lu, Kan Liu, Tao Lan, Lin Qu, Shuiguang Deng</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出残差可微分Top-k选择方法优化Diffusion Transformers的推理效率，属于高效大模型训练与推理方向，有效降低了diffusion模型的计算开销并保持生成质量。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05605' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CSRv2: Unlocking Ultra-Sparse Embeddings</h3>
<p><strong>Authors:</strong> Lixuan Guo, Yifei Wang, Tiansheng Wen, Yifan Wang, Aosong Feng, Bo Chen, Stefanie Jegelka, Chenyu You</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出CSRv2框架实现超稀疏嵌入，有效降低嵌入的计算与存储开销，属于高效大模型训练与推理方向，对大模型的轻量化与边缘部署有实用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance</h3>
<p><strong>Authors:</strong> Xiandong Zou, Jianshu Li, Jing Huang, Pan Zhou</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 重新设计推测解码的草稿训练策略，从token似然转向序列接受度优化，属于高效大模型训练与推理方向，显著提升了大模型的推理效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05774' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures</h3>
<p><strong>Authors:</strong> Chuqin Zhou, Xiaoyue Ling, Yunuo Chen, Jincheng Dai, Guo Lu, Wenjun Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出超低比特率图像压缩框架，结合扩散模型的显式语义和隐式纹理，属于高效大模型训练与推理的高压缩方向，性能优于DiffC等基线。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05213' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning</h3>
<p><strong>Authors:</strong> Enwei Tong, Yuanchao Bai, Yao Zhu, Junjun Jiang, Xianming Liu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 模拟人类视觉感知的聚焦-扫描-细化策略，提出FSR框架实现高效视觉token剪枝，解决现有方法在高压缩下局部证据与全局上下文平衡的问题，提升VLMs的推理效率和 accuracy-efficiency trade-off，实验优于现有SOTA剪枝方法，属于高效大模型训练与推理方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05809' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MambaVF: State Space Model for Efficient Video Fusion</h3>
<p><strong>Authors:</strong> Zixiang Zhao, Yukun Cui, Lilun Deng, Haowen Bai, Haotong Qin, Tao Feng, Konrad Schindler</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对视频融合依赖光流估计导致的高计算开销问题，提出MambaVF框架，基于状态空间模型（SSM）实现线性复杂度的长程时序建模，无需显式运动估计，在多任务上取得SOTA性能，属于高效大模型训练与推理（高效推理）方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06017' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Internalizing LLM Reasoning via Discovery and Replay of Latent Actions</h3>
<p><strong>Authors:</strong> Zhenning Shi, Yijia Zhu, Junhan Shi, Xun Zhang, Lei Wang, Congcong Miao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出STIR框架，通过发现与重放潜在推理动作，将链式思维（CoT）内化为隐状态，减少推理时的token消耗，提升大模型推理效率。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.04925' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Path-Guided Flow Matching for Dataset Distillation</h3>
<p><strong>Authors:</strong> Xuhui Li, Zhengquan Luo, Xiwei Liu, Yongqiang Yu, Zhiqiang Xu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 采用路径引导的Flow Matching方法实现高效数据集蒸馏，属于高效大模型训练与推理方向，提升了数据集压缩的效率与下游任务性能，对大模型高效训练有实用价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.05616' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback</h3>
<p><strong>Authors:</strong> Xiaoxuan He, Siming Fu, Wanli Li, Zhiyuan Li, Dacheng Yin, Kang Rong, Fengyun Rao, Bo Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出自增强迭代学习框架，用少量人类反馈对齐扩散模型，属于大模型安全与对齐的对齐方向，减少标注需求且性能优于现有方法。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05380' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation</h3>
<p><strong>Authors:</strong> Igor Santos-Grueiro</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究大模型对齐的行为评估可验证性，提出规范不可区分性概念，揭示了行为评估无法完全验证潜在对齐的局限性，属于大模型安全与对齐方向，对对齐评估的理论框架有重要补充。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05656' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> $f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment</h3>
<p><strong>Authors:</strong> Rajdeep Haldar, Lantao Mei, Guang Lin, Yue Xing, Qifan Song</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出基于$f$-散度的RL算法，支持偏好与可验证奖励对齐，理论保证奖励提升，实验验证数学推理与安全性能。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05946' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation</h3>
<p><strong>Authors:</strong> Yongwoo Kim, Sungmin Cha, Hyunsoo Kim, Jaewon Lee, Donghyun Kim</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出概念擦除框架，通过unsafe-safe配对和Fisher加权适应保持语义一致性，属于大模型安全与对齐的概念移除方向，性能优于基线。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05339' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Causal Perspective for Enhancing Jailbreak Attack and Defense</h3>
<p><strong>Authors:</strong> Licheng Pan, Yunsheng Lu, Jiexi Liu, Jialing Tao, Haozhe Feng, Hui Xue, Zhixuan Chu, Kui Ren</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 从因果视角分析LLM越狱攻击的驱动因素，提出Causal Analyst框架增强攻击与防御能力，为大模型安全领域提供了可解释的因果分析方法，实验验证了其有效性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.04893' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment</h3>
<p><strong>Authors:</strong> Youngjae Cho, Jongsuk Kim, Ji-Hoon Kim</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出几何锚定偏好优化（GAPO），用动态几何锚点替代固定参考策略，解决偏好对齐中的分布不匹配问题，提升LLM对齐的鲁棒性，为大模型安全与对齐提供了有效方法。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.04909' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Private PoEtry: Private In-Context Learning via Product of Experts</h3>
<p><strong>Authors:</strong> Rob Romijnders, Mohammad Mahdi Derakhshani, Jonathan Petit, Max Welling, Christos Louizos, Yuki M. Asano</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出基于Product of Experts的私有上下文学习方法，通过组合多个专家模型实现隐私保护，提升ICL的隐私性与性能，为大模型安全（隐私）提供了有效解决方案。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05012' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks</h3>
<p><strong>Authors:</strong> William F. Shen, Xinchi Qiu, Chenxi Whitehouse, Lisa Alazraki, Shashwat Goel, Francesco Barbieri, Timon Willi, Akhil Mathur, Ilias Leontiadis</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出RRD框架优化rubric生成，解决现有方法覆盖不足、冗余等问题，显著提升LLM judge准确性与RFT奖励信号质量，对大模型对齐关键模块有突破
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05125' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification</h3>
<p><strong>Authors:</strong> Tao Huang, Rui Wang, Xiaofei Liu, Yi Qin, Li Duan, Liping Jing</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出EUQ方法量化LVLM的不确定性，检测幻觉、越狱、对抗攻击等错误行为，在多个基准中超越现有方法
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05535' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Inject: Automated Prompt Injection via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Xin Chen, Jie Zhang, Florian Tramer</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 采用强化学习实现自动prompt injection攻击，揭示大模型的prompt注入安全漏洞，属于大模型安全与对齐方向，对大模型安全防御研究有重要参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05746' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chunky Post-Training: Data Driven Failures of Generalization</h3>
<p><strong>Authors:</strong> Seoirse Murray, Allison Qi, Timothy Qian, John Schulman, Collin Burns, Sara Price</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 分析后训练数据的虚假关联导致泛化失败，提出工具定位问题，对大模型后训练安全有重要意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05910' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</h3>
<p><strong>Authors:</strong> Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出Diamond Maps通过随机流图实现高效奖励对齐，蒸馏自GLASS Flows，提升生成模型偏好适应能力。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05993' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making</h3>
<p><strong>Authors:</strong> Shutong Fan, Lan Zhang, Xiaoyong Yuan</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出对抗性解释攻击，操纵LLM解释框架影响人类信任，量化信任校准差距，属安全与对齐问题。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.04003' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing</h3>
<p><strong>Authors:</strong> Amir Asiaee, Kaveh Aryan</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究群体条件先验概率转移下的公平性，证明误差率公平性的不变性与接受率公平性的不可避免漂移，提出TAP-GPPS实现目标域公平性
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05144' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Position: Capability Control Should be a Separate Goal From Alignment</h3>
<p><strong>Authors:</strong> Shoaib Ahmed Siddiqui, Eleni Triantafillou, David Krueger, Adrian Weller</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 主张能力控制与对齐分离，提出数据、学习、系统三层控制框架，对大模型安全的理论框架有重要补充
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05164' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions</h3>
<p><strong>Authors:</strong> Yuntai Bao, Xuhong Zhang, Jintao Chen, Ge Su, Yuxiang Cai, Hao Peng, Bing Sun, Haiqin Weng, Liu Yan, Jianwei Yin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出CDAS框架，通过分布匹配与分布式干预实现忠实双向模型控制，在AxBench与安全案例中提升 steering稳定性
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05234' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Erase at the Core: Representation Unlearning for Machine Unlearning</h3>
<p><strong>Authors:</strong> Jaewon Lee, Yongwoo Kim, Donghyun Kim</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出EC框架实现表示级机器遗忘，通过多 layer对比学习与深度监督，解决现有方法的表层遗忘问题
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05375' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning</h3>
<p><strong>Authors:</strong> Zidi Xiong, Shan Chen, Himabindu Lakkaraju</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究RLVR训练中monitorability的自发提升，分析数据多样性作用，属reasoning alignment研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.03978' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Steering LLMs via Scalable Interactive Oversight</h3>
<p><strong>Authors:</strong> Enyu Zhou, Zhiheng Xi, Long Ma, Zhihao Zhang, Shihan Dou, Zhikai Lei, Guoteng Wang, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出可扩展交互式监督框架，分解复杂意图为递归决策树，提升web开发任务对齐度。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.04210' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Private Prediction via Shrinkage</h3>
<p><strong>Authors:</strong> Chao Yan</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究差分隐私的在线预测，提出收缩方法将样本复杂度从√T降低到polylog(T)，对隐私保护的理论边界有贡献
Score: 6
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.05219' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Stable Velocity: A Variance Perspective on Flow Matching</h3>
<p><strong>Authors:</strong> Donglin Yang, Yongxing Zhang, Xin Yu, Liang Hou, Xin Tao, Pengfei Wan, Xiaojuan Qi, Renjie Liao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 从方差角度优化流匹配，提出稳定速度框架加速训练和采样，属于大模型新技术的扩散模型方向，性能优于现有流匹配方法。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05435' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Logical Guidance for the Exact Composition of Diffusion Models</h3>
<p><strong>Authors:</strong> Francesco Alesiani, Jonathan Warrell, Tanja Bien, Henrik Christiansen, Matheus Ferraz, Mathias Niepert</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出LOGDIFF框架实现diffusion模型的精确逻辑引导，属于大模型新技术中的diffusion相关研究，解决了复杂逻辑约束下的diffusion模型生成问题，对diffusion模型的可控性提升有重要贡献。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05549' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces</h3>
<p><strong>Authors:</strong> Arran Carter, Sanghyeok Choi, Kirill Tamogashev, Víctor Elvira, Nikolay Malkin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究离散扩散采样器的off-policy训练与桥接，应用于图像生成隐空间，属diffusion相关大模型新技术。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05961' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</h3>
<p><strong>Authors:</strong> Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对条件flow matching中源分布设计的问题，提出学习条件依赖的源分布，解决分布坍塌和不稳定性问题，通过方差正则化和方向对齐提升模型性能，实验在文本到图像基准上取得更快收敛和更好性能，属于大模型新技术（flow matching作为diffusion替代）方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05951' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Temporal Pair Consistency for Variance-Reduced Flow Matching</h3>
<p><strong>Authors:</strong> Chika Maduabuchi, Jindong Wang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出时间对一致性（TPC）方法，通过耦合同一概率路径上的时间步预测，减少流匹配的梯度方差，提升扩散模型的采样效率与质量，属于大模型新技术的重要改进。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.04908' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models</h3>
<p><strong>Authors:</strong> Atula Tejaswi, Litu Rout, Constantine Caramanis, Sanjay Shakkottai, Sujay Sanghavi</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出EntRGi，通过熵感知的奖励引导动态调整扩散过程，解决离散扩散语言模型的梯度反馈问题，提升生成质量，属于大模型新技术的重要改进。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05000' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization</h3>
<p><strong>Authors:</strong> Kevin Han, Yuhang Zhou, Mingze Gao, Gedi Zhou, Serena Li, Abhishek Kumar, Xiangjun Fan, Weiwei Li, Lizhu Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出EBPO改进GRPO的稳定性，通过经验贝叶斯收缩平衡局部与全局统计，解决小分组方差与梯度消失问题，提升AIME等基准性能
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05165' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Disentangled Representation Learning via Flow Matching</h3>
<p><strong>Authors:</strong> Jinjin Chi, Taoping Liu, Mengtao Yin, Ximing Li, Yongcheng Jing, Dacheng Tao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出流匹配-based解纠缠表示学习框架，通过正交正则化抑制跨因子干扰，提升解纠缠分数与生成可控性
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05214' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</h3>
<p><strong>Authors:</strong> Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出ARM机制改进LLM推理的探索，通过 Prompt Perplexity与Answer Confidence平衡探索与利用，提升Pass@32指标13.9%
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05281' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective</h3>
<p><strong>Authors:</strong> Yinan Huang, Hans Hao-Hsun Hsu, Junran Wang, Bo Dai, Pan Li</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出贝叶斯滤波视角的序列流匹配，通过前验后验初始化加速采样，在预测任务中实现 competitive性能与更快采样
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.05319' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature</h3>
<p><strong>Authors:</strong> Yikuan Zhang, Ning Yang, Yuhai Tu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 深入分析SGD噪声协方差与损失景观曲率的超线性关系，纠正了以往线性假设的局限性，属于深度学习理论中的优化器与损失景观研究，为理解SGD的优化行为提供了更准确的理论依据。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05600' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers</h3>
<p><strong>Authors:</strong> Artem Riabinin, Andrey Veprikov, Arman Bolatov, Martin Tak\'a\v{c}, Aleksandr Beznosikov</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究优化器的自适应warm-up调度策略，解释了warm-up的理论来源并提出自动调整方法，属于深度学习理论中的优化器方向，对提升优化器的训练稳定性与效率有重要意义。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05813' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network</h3>
<p><strong>Authors:</strong> Junzhou Li, Manqi Zhao, Yilin Gao, Zhiheng Yu, Yin Li, Dongsheng Jiang, Li Xiao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出轻量级混合网络，结合高效卷积和门控线性注意力优化感受野，属于深度学习理论的网络架构方向，在ImageNet和下游任务中性能优于同类模型。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05262' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel</h3>
<p><strong>Authors:</strong> Jose Miguel Luna, Taha Bouhsine, Krzysztof Choromanski</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出几何感知的球面线性注意力（SLAY），基于Yat-Kernel实现线性时间注意力，解决Transformer注意力的计算瓶颈，属于深度学习理论（网络架构）的创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.04915' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Does SGD Seek Flatness or Sharpness? An Exactly Solvable Model</h3>
<p><strong>Authors:</strong> Yizhou Xu, Pierfrancesco Beneventano, Isaac Chuang, Liu Ziyin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究SGD在损失景观中的flatness-seeking行为，提出解析可解模型并通过MLP、RNN、Transformer验证，对深度学习优化理论有重要贡献
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05065' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Decoupled Orthogonal Dynamics: Regularization for Deep Network Optimizers</h3>
<p><strong>Authors:</strong> Hao Chen, Jinghui Yuan, Hanmin Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出AdamO优化器，解耦参数幅度与方向更新，解决AdamW的“径向拔河”问题，在视觉与语言任务中提升泛化与稳定性
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05136' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms</h3>
<p><strong>Authors:</strong> Feng Zhu, Robert W. Heath Jr., Aritra Mitra</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出统一收敛分析框架，简化SAG、SAGA、IAG的证明，首次给出高概率界，对优化算法的理论理解有突破
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05304' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation</h3>
<p><strong>Authors:</strong> Zhiqi Yu, Zhangquan Chen, Mengting Liu, Heye Zhang, Liangqiong Qu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对GRPO优化器的隐式优势对称性问题展开研究，揭示其在探索与难度适应上的瓶颈，属于深度学习理论中的优化器方向，对改进强化学习优化器设计有理论指导意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05548' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization</h3>
<p><strong>Authors:</strong> Aleksandar Armacki, Dragana Bajovi\'c, Du\v{s}an Jakoveti\'c, Soummya Kar, Ali H. Sayed</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 分析(clipped) SGD在非凸优化中的长期长尾衰减特性，给出更紧的理论上界，属于深度学习理论中的优化器方向，为理解SGD的长期训练行为提供了更准确的理论支撑。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05657' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Muon in Associative Memory Learning: Training Dynamics and Scaling Laws</h3>
<p><strong>Authors:</strong> Binghui Li, Kaifei Wang, Han Zhong, Pinyan Lu, Liwei Wang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究Muon优化器在联想记忆学习中的训练动态与缩放律，揭示其相比SGD的优势，属于深度学习理论中的优化器方向，对优化器的适应性与效率提升有理论指导意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05725' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting</h3>
<p><strong>Authors:</strong> Tianqi Shen, Jinji Yang, Junze He, Kunhan Gao, Ziye Ma</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出确定性框架通过模拟提升逃离非凸矩阵感知的局部极小，属于深度学习理论中的非凸优化方向，提供了可证明的局部极小逃离方法，对非凸优化问题有普遍意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05887' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Parity, Sensitivity, and Transformers</h3>
<p><strong>Authors:</strong> Alexander Kozachinskiy, Tomasz Steifer, Przemysław Wa̧łęga</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究Transformer架构对PARITY问题的计算能力，给出新构造方法与下界，深化Transformer理论理解。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05896' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training</h3>
<p><strong>Authors:</strong> Zhenghao Xu, Qin Lu, Changlong Yu, Tuo Zhao</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 研究Policy Mirror Descent的log-partition近似带来的隐式正则化，深化LLM后训练优化机制理解。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05933' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Breaking Symmetry Bottlenecks in GNN Readouts</h3>
<p><strong>Authors:</strong> Mouad Talhi, Arne Wolf, Anthea Monod</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 分析GNN readout层对称性瓶颈，提出projector-based不变readout，提升GNN表达能力。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05950' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Orthogonal Self-Attention</h3>
<p><strong>Authors:</strong> Leo Zhang, James Martens</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出正交自注意力，通过矩阵指数映射query-key为正交矩阵，改善Transformer稳定性与表达能力。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05996' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Attention Retention for Continual Learning with Vision Transformers</h3>
<p><strong>Authors:</strong> Yue Lu, Xiangyu Zhou, Shizhou Zhang, Yinghui Xing, Guoqiang Liang, Wencong Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出注意力保持框架减少持续学习中的灾难性遗忘，属于深度学习理论的持续学习方向，性能优于同类模型。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05454' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SpectraKAN: Conditioning Spectral Operators</h3>
<p><strong>Authors:</strong> Chun-Wun Cheng, Carola-Bibiane Sch\"onlieb, Angelica I. Aviles-Rivero</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出SpectraKAN改进谱神经算子，通过交叉注意力实现输入条件化谱混合，提升PDE预测性能，对算子模型的适应性有创新
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05187' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks</h3>
<p><strong>Authors:</strong> Minyoung Kim</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出大规模分数-based变分后验推断方法，提升贝叶斯深度神经网络的可扩展性，属于深度学习理论中的推断方法方向，对贝叶斯大模型的应用有推动作用。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05873' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Inverse Depth Scaling From Most Layers Being Similar</h3>
<p><strong>Authors:</strong> Yizhou Liu, Sara Kangaslahti, Ziming Liu, Jeff Gore</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 发现LLM与残差网络中深度的逆缩放律，分析相似层的ensemble效应，深化网络架构理论。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05970' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Clifford Kolmogorov-Arnold Networks</h3>
<p><strong>Authors:</strong> Matthias Wolff, Francesco Alesiani, Christof Duhme, Xiaoyi Jiang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出Clifford Kolmogorov-Arnold Networks，用于Clifford代数空间函数近似，属网络架构创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05977' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On Computation and Reinforcement Learning</h3>
<p><strong>Authors:</strong> Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 形式化计算受限的RL策略，证明更多计算能解决更复杂任务，提出可变计算架构，属RL与计算理论研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.05999' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</h3>
<p><strong>Authors:</strong> Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 分析主动推理中好奇心系数，证明自洽学习与无遗憾优化，连接主动推理与贝叶斯实验设计。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06029' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Xu Wang, Bingqing Jiang, Yu Wan, Baosong Yang, Lingpeng Kong, Difan Zou</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 采用稀疏自动编码器实现diffusion语言模型的机械可解释性，属于深度学习可解释性方向，为理解diffusion大模型的内部机制提供了有效工具。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.05859' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Axiomatic Foundations of Counterfactual Explanations</h3>
<p><strong>Authors:</strong> Leila Amgoud, Martin Cooper</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 建立反事实解释公理框架，证明不可能与表示定理，分类五种反事实类型，属可解释性理论研究。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.04028' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs</h3>
<p><strong>Authors:</strong> Tina Khezresmaeilzadeh, Jike Zhong, Konstantinos Psounis</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出VRIQ基准分析VLMs的视觉推理能力，通过诊断探针揭示感知限制，属于深度学习可解释性的模型分析方向，为改进视觉推理提供依据。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.05382' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability</h3>
<p><strong>Authors:</strong> Kingsuk Maitra</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 结合物理先验（辛几何、信号处理）提出动量注意力机制，研究Transformer的机制可解释性，揭示上下文学习的光谱特征，为深度学习可解释性提供了新颖的物理视角。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.04902' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution</h3>
<p><strong>Authors:</strong> Long Zhang, Fangwei Lin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 通过层-wise几何分析，揭示LLM上下文冲突中的“正交干扰”机制，分离幅度与方向的影响，为理解上下文学习的内在机制提供了可解释的几何视角。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.04918' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mechanisms of AI Protein Folding in ESMFold</h3>
<p><strong>Authors:</strong> Kevin Lu, Jannik Brinkmann, Stefan Huber, Aaron Mueller, Yonatan Belinkov, David Bau, Chris Wendler</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 通过反事实干预分析ESMFold折叠机制，定位关键阶段与特征，属white-box可解释性研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</h3>
<p><strong>Authors:</strong> Miranda Muqing Miao, Young-Min Cho, Lyle Ungar</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出CORAL通过正则化探针提取内部正确性信号，提升MCQA准确性与校准度，属内部激活可解释性分析。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06022' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering</h3>
<p><strong>Authors:</strong> Eitan Sprejer, Oscar Agustín Stanchi, María Victoria Carro, Denise Alejandra Mester, Iván Arcuschin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 实证研究特征引导中的性能与行为控制权衡，发现特征引导会导致模型性能下降，为理解深度学习模型的可解释性与控制方法的局限性提供了重要依据。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.04903' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models</h3>
<p><strong>Authors:</strong> Shahar Haim, Daniel C McNamee</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 通过几何分析揭示LLM从上下文处理到预测形成的深度-wise过渡，解释隐状态表示的几何结构变化，为深度学习可解释性提供了机制性 insights。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.04931' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Reliable Explanations or Random Noise? A Reliability Metric for XAI</h3>
<p><strong>Authors:</strong> Poushali Sengupta, Sabita Maharjan, Frank Eliassen, Shashi Raj Pandey, Yan Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对XAI解释的可靠性问题，提出ERI指标量化解释稳定性，构建ERI-Bench基准，对可解释性研究的实践落地有价值
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.05082' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</h3>
<p><strong>Authors:</strong> John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, Matthew Lyle Olson</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 结合稀疏自编码器（SAE）与LLM总结器分析LLM多智能体训练动态，提出Meta-Autointerp方法，揭示角色扮演、策略退化等行为
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.05183' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling</h3>
<p><strong>Authors:</strong> Shivanshu Shekhar, Uttaran Bhattacharya, Raghavendra Addanki, Mehrab Tanjim, Somdeb Sarkhel, Tong Zhang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 将视频生成模型转化为时序感知的奖励模型，通过对比学习和潜在空间扰动减少标注需求，属于原生多模态大模型的视频生成方向，性能优于VLM基线。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.05202' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching</h3>
<p><strong>Authors:</strong> Jiahao Nie, Wenbin An, Gongjie Zhang, Yicheng Xu, Yap-Peng Tan, Alex C. Kot, Shijian Lu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对Vid-LLM的时序视频接地任务，提出事件级感知框架增强语义连续性，属于原生多模态大模型的视频-语言方向，性能优于现有Vid-LLM。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.05215' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Multimodal Latent Reasoning via Hierarchical Visual Cues Injection</h3>
<p><strong>Authors:</strong> Yiming Zhang, Qiangyu Yan, Borui Jiang, Kai Han</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出多模态潜在推理框架，通过分层视觉线索注入增强MLLM的推理能力，属于原生多模态大模型的多模态推理方向，提升复杂场景理解。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.05359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows</h3>
<p><strong>Authors:</strong> Ruiting Dai, Zheyu Wang, Haoyu Yang, Yihan Liu, Chengzhi Wang, Zekun Zhang, Zishan Huang, Jiaman Cen, Lisi Mo</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出OMG-Agent通过粗到细agentic工作流解决缺失模态生成，提升多模态鲁棒性，属原生多模态技术。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.04144' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting</h3>
<p><strong>Authors:</strong> Hao Feng, Wei Shi, Ke Zhang, Xiang Fei, Lei Liao, Dingkang Yang, Yongkun Du, Xuecheng Wu, Jingqun Tang, Yang Liu, Hong Chen, Can Huang</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出文档解析的VLM框架，支持数字和拍摄文档的多任务处理，属于原生多模态大模型的文档理解方向，性能优于原Dolphin。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.05384' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification</h3>
<p><strong>Authors:</strong> Lexiang Hu, Youze Xue, Dian Li, Gang Liu, Zhouchen Lin</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对MLLM多模态嵌入仅能捕捉全局语义的局限，提出AGFF-Embed方法融合全局与细粒度感知信息，通过自适应聚合不同维度语义嵌入提升多模态理解能力，实验在MMEB和MMVP-VLM基准上取得最优性能，直接关联原生多模态大模型研究方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.05729' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training</h3>
<p><strong>Authors:</strong> Guanjie Cheng, Boyi Li, Lingyu Sun, Mengying Zhu, Yangyang Wu, Xinkui Zhao, Shuiguang Deng</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出TADS框架优化多任务多模态预训练数据选择，整合质量评估、任务相关性与分布多样性，用36%数据超越全量数据基线
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.05251' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation</h3>
<p><strong>Authors:</strong> Dekang Qi, Shuang Zeng, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Mu Xu</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 提出零样本目标导航框架，通过记忆-执行-回顾提升成功率和泛化，属于多模态智能体的embodied导航方向，性能优于现有方法。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.05467' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</h3>
<p><strong>Authors:</strong> Han Xiao, Guozhi Wang, Hao Wang, Shilong Liu, Yuxiang Chai, Yue Pan, Yufeng Zhou, Xiaoxin Chen, Yafei Wen, Hongsheng Li</p>
<p><strong>Published:</strong> 2026-02-06</p>
<p><strong>Reason:</strong> 针对移动GUI代理在线强化学习中的长任务信用分配和经验转移问题，提出UI-Mem框架，通过分层经验记忆和自进化循环提升跨任务泛化能力，实验在GUI基准上显著优于传统RL方法，直接对应多模态智能体（GUI Agent）研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.05832' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>