<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-09</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>原生多模态大模型</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >多模态智能体</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习理论</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-09</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-09 13:36:23</p>
<p>已自动阅读了 264 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：140507</p>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning</h3>
<p><strong>Authors:</strong> Bangji Yang, Ruihan Guo, Jiajun Fan, Chaoran Cheng, Ge Liu</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出训练-free的多智能体循环推理框架，通过Planner、Checker、Refiner等模块解决文本到图像的组合生成问题，开源模型性能超过Imagen4等商业模型，为原生多模态大模型的高保真生成提供了新范式。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06166' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EgoAVU: Egocentric Audio-Visual Understanding</h3>
<p><strong>Authors:</strong> Ashish Seth, Xinhao Mei, Changsheng Zhao, Varun Nagaraja, Ernie Chang, Gregory P. Meyer, Gael Le Lan, Yunyang Xiong (Microsoft), Vikas Chandra, Yangyang Shi, Dinesh Manocha, Zhipeng Cai</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对多模态大模型在以自我为中心视频中重视觉轻音频的问题，构建了EgoAVU-Instruct训练数据集与EgoAVU-Bench基准，通过微调提升模型的音频-视觉联合理解能力，在多个基准上取得显著性能提升，为原生多模态大模型的场景化应用提供了新方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06139' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ChatUMM: Robust Context Tracking for Conversational Interleaved Generation</h3>
<p><strong>Authors:</strong> Wenxun Dai, Zhiyuan Zhao, Yule Zhong, Yiji Cheng, Jianwei Zhang, Linqing Wang, Shiyi Zhang, Yunlong Lin, Runze He, Fellix Song, Wayne Zhuang, Yong Liu, Haoji Zhang, Yansong Tang, Qinglin Lu, Chunyu Wang (ByteDance)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对统一多模态模型的单轮交互限制，提出ChatUMM框架，通过多轮对话数据合成与上下文跟踪机制，提升模型的 interleaved生成能力，在视觉理解、指令编辑等任务上超过现有开源模型，为原生多模态大模型的对话式应用提供了新方法。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06442' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> NECromancer: Breathing Life into Skeletons via BVH Animation</h3>
<p><strong>Authors:</strong> Mingxi Xu, Qi Wang, Zhengyu Wen, Phong Dao Thien, Zhengyu Li, Ning Zhang, Xiaoyu He, Wei Zhao, Kehong Gong, Mingyuan Zhang (Animotion Lab)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出通用运动tokenizer NECromancer，通过本体感知骨架图编码器、拓扑无关tokenizer和大规模BVH数据集，解决跨物种骨架的运动处理问题，支持多任务（跨物种迁移、生成、检索等），实验验证高保真重建和结构解耦能力
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06548' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RAIGen: Rare Attribute Identification in Text-to-Image Generative Models</h3>
<p><strong>Authors:</strong> Silpa Vadakkeeveetil Sreelatha, Dan Wang, Serge Belongie, Muhammad Awais, Anjan Dutta (Cornell University, etc.)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出无监督稀有属性发现框架RAIGen，利用Matryoshka稀疏自编码器和 minority metric识别扩散模型中的稀有属性，支持Stable Diffusion/SDXL的审计与稀有属性放大，解决数据偏差问题
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06806' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BrokenBind: Universal Modality Exploration beyond Dataset Boundaries</h3>
<p><strong>Authors:</strong> Zhuo Huang, Runnan Chen, Bo Han, Gang Niu, Masashi Sugiyama, Tongliang Liu</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出跨数据集的模态绑定方法，利用共享模态连接不同数据集的模态，实现通用模态探索，解决传统多模态模型依赖同数据集的局限，属于原生多模态大模型研究。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06451' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization</h3>
<p><strong>Authors:</strong> Arvid E. Gollwitzer, Paridhi Latawa, David de Gruijl, Deepak A. Subramanian, Adrián Noriega de la Colina</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出质量感知tokenization方法，解决噪声语料预训练问题，属于原生多模态大模型中的tokenizer方向，在多领域验证有效性。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06394' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</h3>
<p><strong>Authors:</strong> Yuxuan Yao, Yuxuan Chen, Hui Li, Kaihui Cheng, Qipeng Guo, Yuwei Sun, Zilong Dong, Jingdong Wang, Siyu Zhu (JD.com, etc.)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出训练-free的prompt reinjection方法，将早期层prompt表示重新注入后期层，缓解SD3、FLUX.1等MMDiTs的prompt遗忘问题，提升指令遵循与生成质量
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06886' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs</h3>
<p><strong>Authors:</strong> Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究VLLMs中任务复杂度对视觉token专业化的影响，通过合成基准揭示任务复杂度与视觉压缩的关系，为训练下一代VLLMs提供关键 insights
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06914' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Same Answer, Different Representations: Hidden instability in VLMs</h3>
<p><strong>Authors:</strong> Farooq Ahmad Wani, Alessandro Suglia, Rohit Saxena, Aryo Pradipta Gema, Wai-Chung Kwan, Fazl Barez, Maria Sofia Bucarelli, Fabrizio Silvestri, Pasquale Minervini</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究视觉语言模型的内部表示不稳定性，属于原生多模态大模型方向，揭示VLM输出稳定但内部漂移的问题，对多模态模型鲁棒性有重要意义。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.06652' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions</h3>
<p><strong>Authors:</strong> Navita Goyal, Hal Daum\'e III</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 系统研究模型转向的特异性与鲁棒性，发现转向虽提升目标性能但可能降低安全鲁棒性（如易受越狱攻击），为大模型安全调控提供关键 insights。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06256' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DeDPO: Debiased Direct Preference Optimization for Diffusion Models</h3>
<p><strong>Authors:</strong> Khiem Pham, Quang Nguyen, Tung Nguyen, Jingsen Zhu, Michele Santacatterina, Dimitris Metaxas (Rutgers University), Ramin Zabih (Cornell University)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对扩散模型DPO方法依赖大量人类偏好标签的痛点，结合因果推断的去偏技术，利用合成AI反馈替代部分人类标签，提升对齐效果，性能接近全人类标签的理论上限，为大模型安全与对齐提供了高效解决方案。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06195' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Provably avoiding over-optimization in Direct Preference Optimization without knowing the data distribution</h3>
<p><strong>Authors:</strong> Adam Barla, Emanuele Nevali, Luca Viano, Volkan Cevher</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出PEPO算法，通过 disjoint数据子集的集成模型实现DPO的过优化避免，无需数据分布知识，有理论样本复杂度保证，实验验证优于DPO
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06239' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop</h3>
<p><strong>Authors:</strong> Patryk Rybak, Pawe{\l} Batorski, Paul Swoboda, Przemys{\l}aw Spurek</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 利用进化算法生成对抗性提示，评估大模型未学习的有效性，揭示未学习后仍存在隐藏知识的安全隐患，属于大模型安全与对齐研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06248' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</h3>
<p><strong>Authors:</strong> Mark Russinovich, Yanan Cai, Keegan Hines, Giorgio Severi, Blake Bullwinkel, Ahmed Salem</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出仅用一个无标签提示即可解除大模型对齐的方法，验证其在多模型（7-20B参数）上的有效性，揭示对齐的脆弱性，属于大模型安全与对齐研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06258' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Don't Break the Boundary: Continual Unlearning for OOD Detection Based on Free Energy Repulsion</h3>
<p><strong>Authors:</strong> Ningkang Peng, Kun Shao, Jingyang Mao, Linjing Qian, Xiaoqian Peng, Xichen Yang, Yanhui Gu</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出自由能排斥的持续未学习框架，将遗忘类转化为OOD样本，解决未学习与OOD检测的边界冲突，属于大模型安全与对齐研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06331' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Displacement-Resistant Extensions of DPO with Nonconvex $f$-Divergences</h3>
<p><strong>Authors:</strong> Idan Pipano, Shoham Sabach, Kavosh Asadi, Mohammad Ghavamzadeh</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 扩展DPO框架至非凸$f$-散度，解决概率位移问题，提升大模型对齐的稳定性，属于大模型安全与对齐研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06788' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Identifiability of Steering Vectors in Large Language Models</h3>
<p><strong>Authors:</strong> Sohan Venkatesh, Ashish Mahendran Kurapath</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 分析LLM中steering vectors的可识别性，指出其非可识别性及解决方法，对大模型安全控制有重要意义，属于大模型安全与对齐研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06801' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Endogenous Resistance to Activation Steering in Language Models</h3>
<p><strong>Authors:</strong> Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, Mike Vaiana, Diogo de Lucena, Judd Rosenblatt, Michael S. A. Graziano</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究LLM对激活转向的内生抗性，揭示模型内部的一致性检查机制，属于大模型安全与对齐研究
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06941' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LLM Active Alignment: A Nash Equilibrium Perspective</h3>
<p><strong>Authors:</strong> Tonghan Wang, Yuqi Pan, Xinyi Yang, Yanchen Jiang, Milind Tambe, David C. Parkes</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 从纳什均衡角度提出LLM主动对齐框架，属于大模型安全与对齐方向，解决多Agent LLM的对齐问题，实验验证社交媒体场景有效性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06836' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Is Gradient Ascent Really Necessary? Memorize to Forget for Machine Unlearning</h3>
<p><strong>Authors:</strong> Zhuo Huang, Qizhou Wang, Ziming Hong, Shanshan Ye, Bo Han, Tongliang Liu</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出“记忆-遗忘”的未学习方法，通过模型外推替代梯度上升，避免灾难性崩溃，实现高效未学习，属于大模型安全与对齐研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.06441' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Condensate Theorem: Transformers are O(n), Not $O(n^2)$</h3>
<p><strong>Authors:</strong> Jorge L. Ruiz Williams</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 证明Transformer可通过拓扑注意力（锚点+窗口+动态Top-k）实现O(n)复杂度，解决二次复杂度瓶颈，对长上下文推理具有里程碑意义。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06317' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models</h3>
<p><strong>Authors:</strong> Hyochan Chong, Dongkyu Kim, Changdong Kim, Minseop Choi</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出高效的后训练量化方法，将LLM压缩至子1位，显著提升推理效率并降低硬件需求，属于高效大模型训练与推理中的高压缩研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06694' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> When RL Meets Adaptive Speculative Training: A Unified Training-Serving System</h3>
<p><strong>Authors:</strong> Junxiong Wang, Fengxiang Bie, Jisen Li, Zhongzhu Zhou, Zelei Shao, Yubo Wang, Yinghui Liu, Qingyang Wu, Avner May, Sri Yanamandra, Yineng Zhang, Ce Zhang, Tri Dao, Percy Liang, Ben Athiwaratkun, Shuaiwen Leon Song, Chenfeng Xu, Xiaoxia Wu</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出Aurora统一系统，通过RL在线学习投机模型，提升LLM推理速度与适应性，属于高效大模型训练与推理研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06932' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning</h3>
<p><strong>Authors:</strong> Zhuoming Chen, Hongyi Liu, Yang Zhou, Haizhong Zheng, Beidi Chen</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出OBRS框架，减少rollout模型与政策的分布不匹配，提升RL for LLMs的效率与稳定性，属于高效大模型训练与推理研究
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06107' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs</h3>
<p><strong>Authors:</strong> Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang, Zexue He, Hang Hua, Konrad Schindler, Mattia Rigotti (ETH Zurich, etc.)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出模块化框架分离VLMs的视觉感知与推理，支持测试时缩放和计算优化（减少视觉token数量），在视觉推理基准上超过 monolithic 基线和视觉接地方法，提升精度与效率
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06566' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</h3>
<p><strong>Authors:</strong> Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Couto Pimentel Ramos, Luca Morreale, Mehdi Noroozi, Abhinav Mehrotra</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过蒸馏和结构优化将17B的FLUX.1-Schnell压缩到2.4B的NanoFLUX，实现移动设备上2.5秒生成512×512高质量图像，解决大模型移动端部署问题
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06879' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Compressing LLMs with MoP: Mixture of Pruners</h3>
<p><strong>Authors:</strong> Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Victor Zacarias, Leandro Giusti Mugnaini, Keith Ando Ogawa, Lucas Pellicer, Rosimeire Pereira Costa, Edson Bollis, Anna Helena Reali Costa, Artur Jordao</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出MoP框架统一深度与宽度剪枝，迭代选择剪枝分支，在LLaMA-2/3上超过现有结构化剪枝方法，实现40%压缩率下39%的延迟 reduction，提升推理效率
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models</h3>
<p><strong>Authors:</strong> Nurbek Tastan, Stefanos Laskaridis, Karthik Nandakumar, Samuel Horvath</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出MoSE架构，每个专家具有嵌套slimmable结构，支持条件计算（专家选择+宽度调整），在OpenWebText上改善精度-计算权衡，实现更连续的性能缩放
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06154' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training</h3>
<p><strong>Authors:</strong> Meghana Madhyastha, Daniel Haziza, Jesse Cai, Newsha Ardalani, Zhiqi Bu, Carole-Jean Wu (NVIDIA, etc.)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 利用2:4权重稀疏和Venom激活稀疏加速LLM预训练的FFN部分，实现1.4-1.7×端到端加速，不影响模型性能，适配A100及以上GPU
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06183' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SOCKET: SOft Collison Kernel EsTimator for Sparse Attention</h3>
<p><strong>Authors:</strong> Sahil Joshi, Agniva Chowdhury, Wyatt Bellinger, Amar Kanakamedala, Ekam Singh, Hoang Anh Duy Le, Aditya Desai, Anshumali Shrivastava</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 用软LSH替代硬LSH改进稀疏注意力的 token 选择，解决传统LSH的离散碰撞问题，提升长上下文推理效率，属于高效大模型推理研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fine-Grained Model Merging via Modular Expert Recombination</h3>
<p><strong>Authors:</strong> Haiyun Qiu, Xingyu Wu, Liang Feng, Kay Chen Tan</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出模块化专家重组的细粒度模型合并方法，通过多目标优化选择 Pareto 最优配置，无需重新训练即可整合任务特定模型，属于高效大模型训练研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06552' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models</h3>
<p><strong>Authors:</strong> Yi Chen, Wonjin Shin, Shuhong Liu, Tho Mai, Jeongmo Lee, Chuanbo Hua, Kun Wang, Jun Liu, Joo-Young Kim</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出在线结构剪枝框架，实现大模型高效推理，属于高效大模型训练与推理方向，无需预处理或再训练，在多模型类型上验证有效性。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06822' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation</h3>
<p><strong>Authors:</strong> Xiyang Zhang, Yuanhe Tian, Hongzhi Wang, Yan Song</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过梯度正交性选择训练数据，实现高效域适应，减少灾难性遗忘，提升域特定任务性能同时保持通用能力，属于高效大模型训练研究。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.06359' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings</h3>
<p><strong>Authors:</strong> Grégoire Dhimoïla, Thomas Fel, Victor Boutin, Agustin Picard</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过Iso-Energy假设与对齐的稀疏自编码器，揭示了视觉语言模型嵌入空间的几何结构（如bimodal atoms的跨模态对齐作用、unimodal atoms的模态偏差），提升了模型的可解释性与检索性能，对深度学习可解释性研究有重要贡献。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06218' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks</h3>
<p><strong>Authors:</strong> Yu Zhang, Sean Bin Yang, Arijit Khan, Cuneyt Gurcan Akcora</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出结合对抗攻击技术的图神经网络反事实解释框架，解决传统方法仅依赖边删除的局限，生成更真实的解释，属于深度学习可解释性研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06240' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning a Generative Meta-Model of LLM Activations</h3>
<p><strong>Authors:</strong> Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 用扩散模型学习LLM激活分布，提升可解释性与干预效果，属于深度学习可解释性研究
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06964' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Features to Actions: Explainability in Traditional and Agentic AI Systems</h3>
<p><strong>Authors:</strong> Sindhuja Chaduvula, Jessee Ho, Kina Kim, Aravind Narayanan, Mahshid Alinoori, Muskan Garg, Dhanesh Ramachandram, Shaina Raza</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 比较静态与智能体AI的可解释性方法，提出轨迹级解释框架，属于深度学习可解释性方向，解决Agentic系统行为诊断问题，优于传统归因方法。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06841' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification</h3>
<p><strong>Authors:</strong> Mikołaj Janusz, Adam Wróbek, Bartosz Zieliński, Dawid Rymarczyk</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过 latent向量量化实现原型稳定性与可解释性，设计高效可解释的头部，解决原型漂移问题，在ImageNet和细粒度数据集（CUB-200、Cars-196）上取得竞争性能
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06592' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DAVE: Distribution-aware Attribution via ViT Gradient Decomposition</h3>
<p><strong>Authors:</strong> Adam Wróbek, Siddhartha Gairola, Jacek Tabor, Bernt Schiele, Bartosz Zieliński, Dawid Rymarczyk (MPI Informatics, etc.)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对ViTs提出基于梯度分解的归因方法，分离局部等变稳定成分与架构诱导的artifacts，生成高质量归因图，解决现有方法的结构伪影问题
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.06613' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> POINTS-GUI-G: GUI-Grounding Journey</h3>
<p><strong>Authors:</strong> Zhongyin Zhao, Yuan Liu, Yikun Liu, Haicheng Wang, Le Tian, Xiao Zhou, Yangxiu You, Zilin Yu, Yang Yu, Jie Zhou (Microsoft Research Asia)</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 从基础模型POINTS-1.5出发，通过数据工程、训练策略优化与强化学习，提升GUI grounding能力，提出POINTS-GUI-G模型，在ScreenSpot-Pro、OSWorld-G等多个GUI基准上达到SOTA，为多模态智能体的GUI交互基础能力提供了关键解决方案。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.06391' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion</h3>
<p><strong>Authors:</strong> Longhui Ma, Di Zhao, Siwei Wang, Zhao Lv, Miao Wang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对GUI Grounding问题提出多模态融合框架，直接对应多模态智能体方向，解决现有方法数据依赖和泛化差的问题，实验验证性能优于现有方法。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.06351' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks</h3>
<p><strong>Authors:</strong> Junxian Li, Kai Liu, Leyang Chen, Weida Wang, Zhixin Wang, Jiaqi Xu, Fan Li, Renjing Pei, Linghe Kong, Yulun Zhang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出新基准PlanViz评估多模态模型在计算机使用任务中的规划能力，设计路由规划、工作 diagramming、web&UI显示三个子任务及任务自适应评分PlanScore，揭示模型在空间推理与流程理解上的局限性
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.06663' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</h3>
<p><strong>Authors:</strong> Chao Zhou, Tianyi Wei, Yiling Chen, Wenbo Zhou, Nenghai Yu</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对多条件Diffusion Transformers提出PKA框架，通过位置对齐和关键词范围注意力消除冗余，实现10×推理加速和5.1×内存节省，支持高保真多条件生成
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.06850' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters</h3>
<p><strong>Authors:</strong> Haoran Zhang, Haixuan Liu, Yong Liu, Yunzhong Qiu, Yuxuan Wang, Jianmin Wang, Mingsheng Long</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 将多模态扩散Transformer应用于时间序列预测，设计双流Transformer块捕捉跨变量与时间依赖，属于大模型新技术（扩散LLM）研究。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.06597' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improved Sampling Schedules for Discrete Diffusion Models</h3>
<p><strong>Authors:</strong> Alberto Foresti, Mustapha Bounoua, Giulio Franzese, Luca Ambrogioni, Pietro Michiardi</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 分析离散扩散的反向过程，提出基于熵和Wasserstein距离的采样调度，提升生成效率，属于大模型新技术中的diffusion LLM研究
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.06849' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models</h3>
<p><strong>Authors:</strong> Yuming Li, Qingyu Li, Chengyu Bai, Xiangyang Luo, Zeyue Xue, Wenyu Qin, Meng Wang, Yikai Wang, Shanghang Zhang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出基于熵的自适应政策优化方法，改进扩散模型的对齐效率，属于大模型新技术中的diffusion LLM研究
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.06825' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning</h3>
<p><strong>Authors:</strong> Nan Chen, Soledad Villar, Soufiane Hayou</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出μA框架刻画LoRA的学习率缩放规律，识别两种缩放 regime（秩不变/逆秩缩放），支持LoRA到全微调的学习率迁移，实验验证跨任务有效性
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06204' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization</h3>
<p><strong>Authors:</strong> Changmin Kang, Jihun Yun, Baekrok Shin, Yeseul Cho, Chulhee Yun</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 分析Muon优化器在LoRA中的谱增长特性，证明其均匀谱增长与收敛性，解决传统梯度下降的“大奇异值优先”问题，属于深度学习理论中的优化器研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06385' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Trust Regions Sell, But Who's Buying? Overlap Geometry as an Alternative Trust Region for Policy Optimization</h3>
<p><strong>Authors:</strong> Gaurish Trivedi, Alakh Sharma, Kartikey Singh Bhandari, Yash Sinha, Pratik Narang, Dhruv Kumar, Jagat Sesh Challa</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究了替代KL散度的重叠几何信任区域方法，改进政策优化的稳定性，属于深度学习理论中的优化器相关研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06627' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Pruning at Initialisation through the lens of Graphon Limit: Convergence, Expressivity, and Generalisation</h3>
<p><strong>Authors:</strong> Hoang Pham, The-Anh Ta, Long Tran-Thanh</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 连接初始化剪枝方法与图论极限理论，分析稀疏神经网络的表达性与泛化能力，属于深度学习理论中的网络架构与剪枝研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06675' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explaining Grokking in Transformers through the Lens of Inductive Bias</h3>
<p><strong>Authors:</strong> Jaisidh Singh, Diganta Misra, Antonio Orvieto</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过归纳偏置分析Transformers中的Grokking现象，揭示架构与优化设置对模型训练动态的影响，属于深度学习理论中的模型行为研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06702' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Decoupling Variance and Scale-Invariant Updates in Adaptive Gradient Descent for Unified Vector and Matrix Optimization</h3>
<p><strong>Authors:</strong> Zitao Song, Cedar Site Bai, Zhe Zhang, Brian Bullins, David F. Gleich</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出DeVA框架，桥接向量与矩阵优化的自适应方法，改进梯度下降的稳定性，属于深度学习理论中的优化器研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06880' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers</h3>
<p><strong>Authors:</strong> Ziming Liu, Sophia Sanborn, Surya Ganguli, Andreas Tolias</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过归纳偏置让Transformers学习物理定律，揭示模型架构对世界模型学习的影响，属于深度学习理论中的归纳偏置研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06923' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Emergent Low-Rank Training Dynamics in MLPs with Smooth Activations</h3>
<p><strong>Authors:</strong> Alec S. Xu, Can Yaras, Matthew Asato, Qing Qu, Laura Balzano</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 理论分析MLPs的低秩训练动态，刻画不变子空间的 emergence，实验验证低秩参数化匹配全参数模型的分类性能，为低秩训练提供理论支撑
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06208' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Sparse M\"obius Transforms for Learning Polynomials</h3>
<p><strong>Authors:</strong> Yigit Efe Erginbas, Justin Singh Kang, Elizabeth Polito, Kannan Ramchandran</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对稀疏布尔多项式学习问题，提出自适应稀疏Möbius变换算法，利用自适应组测试解决AND基的相干性挑战，属于深度学习理论中的学习理论研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06246' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Swap Regret Minimization Through Response-Based Approachability</h3>
<p><strong>Authors:</strong> Ioannis Anagnostides, Gabriele Farina, Maxwell Fishelson, Haipeng Luo, Jon Schneider</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 利用响应式可达性框架开发高效交换遗憾最小化算法，证明其在凸集上的最优性，属于深度学习理论中的优化理论研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06264' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error</h3>
<p><strong>Authors:</strong> Vinh Hoang, Sebastian Krumscheid, Holger Rauhut, Ra\'ul Tempone</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出基于尖锐度感知的对抗性净化方法，通过最小化期望重建误差引导样本到数据分布的模态区域，提升对抗鲁棒性，属于深度学习理论中的鲁棒性研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06269' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Plasticity and Stability for Post-Training Large Language Models</h3>
<p><strong>Authors:</strong> Wenwen Qiang, Ziyin Gu, Jiahuan Zhou, Jie Hu, Jingyao Wang, Changwen Zheng, Hui Xiong</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究大模型后训练的可塑性与稳定性，发现梯度冲突是性能下降的根源，提出概率冲突解决框架，提升训练稳定性，属于深度学习理论中的模型训练研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06453' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Achieving Better Local Regret Bound for Online Non-Convex Bilevel Optimization</h3>
<p><strong>Authors:</strong> Tingkai Jia, Haiguang Wang, Cheng Chen</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 针对在线非凸双层优化问题，提出更优的局部遗憾界，设计单循环算法，属于深度学习理论中的优化理论研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06457' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Can Microcanonical Langevin Dynamics Leverage Mini-Batch Gradient Noise?</h3>
<p><strong>Authors:</strong> Emanuel Sommer, Kangning Diao, Jakob Robnik, Uros Seljak, David R\"ugamer</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 分析微正则朗之万动力学对小批量梯度噪声的利用，提出预条件和自适应调谐方法，提升高维贝叶斯推理效率，属于深度学习理论中的优化器研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06500' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Refining the Information Bottleneck via Adversarial Information Separation</h3>
<p><strong>Authors:</strong> Shuai Ning, Zhenpeng Wang, Lin Wang, Bing Chen, Shuangrong Liu, Xu Wu, Jin Zhou, Bo Yang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过对抗信息分离改进信息瓶颈，分离任务相关特征与噪声，提升少数据下的泛化能力，属于深度学习理论中的表示学习研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06549' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Which Graph Shift Operator? A Spectral Answer to an Empirical Question</h3>
<p><strong>Authors:</strong> Yassine Abbahaddou</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 通过谱分析选择最优图移位算子，建立谱对齐增益与泛化界的理论联系，提升GNN性能，属于深度学习理论中的网络架构研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06557' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks</h3>
<p><strong>Authors:</strong> Florian Eilers, Christof Duhme, Xiaoyi Jiang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究复值神经网络的对抗鲁棒性，分析相位扰动对模型性能的影响，发现复值模型对相位攻击更敏感，属于深度学习理论中的鲁棒性研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06577' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Exploring Sparsity and Smoothness of Arbitrary $\ell_p$ Norms in Adversarial Attacks</h3>
<p><strong>Authors:</strong> Christof Duhme, Florian Eilers, Xiaoyi Jiang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 系统分析不同$\ell_p$范数下对抗扰动的稀疏性和平滑性，发现$p\in[1.3,1.5]$时攻击效果最优，为攻击与防御设计提供指导，属于深度学习理论中的鲁棒性研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06578' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Target noise: A pre-training based neural network initialization for efficient high resolution learning</h3>
<p><strong>Authors:</strong> Shaowen Wang, Tariq Alkhalifah</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出基于噪声预训练的初始化方法，通过拟合随机噪声优化网络参数，提升高分辨率学习的收敛速度，属于深度学习理论中的网络初始化研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06585' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Degradation of Feature Space in Continual Learning</h3>
<p><strong>Authors:</strong> Chiara Lanza, Roberto Pereira, Marco Miozzo, Eduard Angelats, Paolo Dini</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究持续学习中特征空间的退化，发现各向同性正则化可能损害性能，揭示集中式与持续学习的特征空间差异，属于深度学习理论中的表示学习研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06586' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Optimal Abstractions for Verifying Properties of Kolmogorov-Arnold Networks (KANs)</h3>
<p><strong>Authors:</strong> Noah Schwartz, Chandra Kanth Nagesh, Sriram Sankaranarayanan, Ramneet Kaur, Tuhin Sahai, Susmit Jha</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出KANs的验证方法，通过分段仿射近似分析网络性质，属于深度学习理论中的网络架构验证研究
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06737' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Parameter-free Dynamic Regret: Time-varying Movement Costs, Delayed Feedback, and Memory</h3>
<p><strong>Authors:</strong> Emmanuel Esposito, Andrew Jacobsen, Hao Qiu, Mengxiao Zhang</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 研究在线凸优化中的动态遗憾，提出参数无关算法，属于深度学习理论中的在线优化研究
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06902' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Difficulty-Estimated Policy Optimization</h3>
<p><strong>Authors:</strong> Yu Zhao, Fan Jiang, Tianle Liu, Bo Zeng, Yu Liu, Longyue Wang, Weihua Luo</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 提出难度估计的策略优化框架，针对大模型推理对齐中的梯度衰减问题，属于深度学习理论中的优化器方向，减少计算开销同时保持性能。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06375' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution</h3>
<p><strong>Authors:</strong> Hsien-Jyh Liao</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 分析自回归推理的内在稳定性限制，提出长horizon推理的结构约束，属于深度学习理论中的网络架构与推理理论方向，为长文本推理提供理论基础。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06413' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Towards Understanding What State Space Models Learn About Code</h3>
<p><strong>Authors:</strong> Jiali Wu, Abhinav Anand, Shweta Verma, Mira Mezini</p>
<p><strong>Published:</strong> 2026-02-09</p>
<p><strong>Reason:</strong> 系统分析状态空间模型在代码任务中的学习机制，属于深度学习理论中的网络架构方向，比较SSM与Transformer差异并提出改进，指导代码模型设计。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06774' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>