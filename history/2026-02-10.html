<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-10</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习理论</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >大模型新技术</a>
<a href='#' >多模态智能体</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-10</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-10 13:56:32</p>
<p>已自动阅读了 669 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：354964</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Xiaomin Yu, Yi Xin, Wenjie Zhang, Chonghan Liu, Hanzhen Zhao, Xiaoxing Hu, Xinlei Yu, Ziyue Qiao, Hao Tang, Xue Yang, Xiaobin Hu, Chengwei Qin, Hui Xiong, Yu Qiao, Shuicheng Yan</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出模态差距驱动的子空间对齐训练范式（ReVision），通过未配对数据的统计对齐替代昂贵的图像-文本配对数据，提升多模态大模型（MLLM）的训练效率，解决MLLM训练的数据瓶颈问题。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07026' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference</h3>
<p><strong>Authors:</strong> Hoang Anh Duy Le (Rice University), Sahil Joshi (Rice University), Zeyu Yang (Rice University), Zhaozhuo Xu (Stevens Institute of Technology), Anshumali Shrivastava (Rice University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出训练-free的稀疏注意力方法，通过草图和游走机制优化长上下文LLM推理，实现近无损精度下的6倍速度提升，有强实际应用价值
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07397' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models</h3>
<p><strong>Authors:</strong> Juntong Wu (), Jialiang Cheng (), Fuyu Lv (), Ou Dan (), Li Yuan ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出相似性引导的专家重路由方法，优化MoE模型的批量解码效率，实现2倍速度提升且质量损失小，为MoE大规模部署提供实用解决方案
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07616' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs</h3>
<p><strong>Authors:</strong> Yanlin Qi (), Xinhang Chen (), Huiqiang Jiang (), Qitong Wang (), Botao Peng (), Themis Palpanas ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出漂移鲁棒的KV缓存检索框架，支持百万token上下文的高效推理， latency 比现有方法低17-44倍，对长上下文LLM部署有重要价值
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07721' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency</h3>
<p><strong>Authors:</strong> Sizhe Dang (Tsinghua University), Jiaqi Shao (Tsinghua University), Xiaodong Zheng (Tsinghua University), Guang Dai (Tsinghua University), Yan Song (Tsinghua University), Haishan Ye (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对分布式训练中的通信瓶颈，提出TSR-Adam方法，将Adam的通信量从O(mn)降低到O(r²)，同时保持内存效率，实验在60M到1B模型上验证了通信量减少13-25倍且性能相当，是高效大模型训练的关键优化方法。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08007' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference</h3>
<p><strong>Authors:</strong> Yifei Gao, Lei Wang, Rong-Cheng Tu, Qixin Zhang, Jun Cheng, Dacheng Tao</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Pre-hoc Sparsity（PrHS）方法，在注意力计算前选择KV条目，通过控制丢弃质量保证性能，显著减少了长上下文推理的开销，提升了稀疏性和速度。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08329' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction</h3>
<p><strong>Authors:</strong> Ziyao Tang, Pengkun Jiao, Xinhang Chen, Wei Liu, Shiyong Li, Jingjing Chen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出基于未来效用预测的KV缓存驱逐策略，优化LLM推理内存与延迟，属于高效大模型推理的关键技术。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08585' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation</h3>
<p><strong>Authors:</strong> Ning Yang, Chengzhi Wang, Yibo Liu, Baoliang Tian, Haijun Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出CompilerKV风险自适应KV压缩框架，利用离线经验编译优化缓存决策，提升LLM长上下文推理效率，属于高效大模型推理。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08686' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill</h3>
<p><strong>Authors:</strong> Dalton Jones, Junyoung Park, Matthew Morse, Mingu Lee, Chris Lott, Harper Langston</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Query-Oriented的KV选择策略，优化LLM预填充阶段的注意力效率，属于高效大模型推理的关键技术。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08722' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models</h3>
<p><strong>Authors:</strong> Annemette Brok Pirchert, Jacob Nielsen, Mogens Henrik From, Lukas Galke Poech, Peter Schneider-Kamp</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出FlexMoRE混合不同秩的专家模型，提升联邦训练LLM的效率和性能，属于高效大模型训练中的混合专家优化。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08818' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce</h3>
<p><strong>Authors:</strong> Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Ran Ben Basat</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出DynamiQ量化框架优化多跳梯度聚合，通过改进部分和表示与融合核加速，在保持精度的同时提升训练速度，对高效大模型训练中的梯度同步优化有重要贡献。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08923' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Data Science and Technology Towards AGI Part I: Tiered Data Management</h3>
<p><strong>Authors:</strong> Yudong Wang, Zixuan Fu, Hengyu Zhao, Chen Zhao, Chuyue Zhou, Xinle Lin, Hongya Lyu, Shuaikang Xue, Yi Yi, Yingjiao Wang, Zhi Zheng, Yuzhou Zhang, Jie Zhou, Chaojun Xiao, Xu Han, Zhiyuan Liu, Maosong Sun</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出L0-L4分层数据管理框架，覆盖LLM训练全生命周期，通过LLM辅助数据精炼提升训练效率与性能，是高效大模型训练的系统性解决方案。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09003' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning</h3>
<p><strong>Authors:</strong> Yalcin Tur, Jalal Naghiyev, Haoquan Fang, Wei-Chuan Tsai, Jiafei Duan, Dieter Fox, Ranjay Krishna</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Recurrent-Depth VLA架构，通过 latent迭代细化实现测试时计算自适应，解决VLA模型固定计算深度的问题，在保持性能的同时降低推理成本，符合高效大模型训练与推理方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07845' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimizing Few-Step Generation with Adaptive Matching Distillation</h3>
<p><strong>Authors:</strong> Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang, Shuo Chen, Bojun Chen, Zeke Xie</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出自适应匹配蒸馏（AMD）框架，通过奖励代理检测并避免“禁止区”（Forbidden Zone），提升少步生成模型的样本 fidelity 与训练鲁棒性，解决Few-step生成的稳定性问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07345' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention</h3>
<p><strong>Authors:</strong> Wenjie Liu, Hao Wu, Xin Qiu, Yingqi Fan, Yihan Zhang, Anhao Zhao, Yunpu Ma, Xiaoyu Shen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对现代多模态大模型（MLLM）因统一自注意力设计导致的高计算开销问题，提出ViCA架构让视觉token绕过所有自注意力和前馈层，仅通过稀疏交叉注意力与文本交互。实验表明该方法保留98%基准精度的同时，将视觉侧计算量降至4%，并实现单批次3.5倍、多批次10倍的推理加速，有效优化MLLM效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models</h3>
<p><strong>Authors:</strong> Zhenhao Shang, Haizhao Jing, Guoting Wei, Haokui Zhang, Rong Xiao, Jianqing Gao, Peng Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究视觉语言模型量化校准，解决视觉与文本令牌的量化差异问题，提升量化性能，属于高效大模型训练与推理中的模型压缩方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07899' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging</h3>
<p><strong>Authors:</strong> Ziyang Fan, Keyu Chen, Ruilong Xing, Yulin Li, Li Jiang, Zhuotao Tian</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出训练-free的时空令牌合并方法，提升视频大模型的推理效率，属于高效大模型训练与推理中的高效推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08024' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval</h3>
<p><strong>Authors:</strong> Jing Zhang, Zhikai Li, Xuewen Liu, Qingyi Gu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对SAM2的高效推理，提出对象感知的稀疏编码和内存检索方法，提升推理速度，属于高效大模型训练与推理中的高效推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08224' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FlattenGPT: Depth Compression for Transformer with Layer Flattening</h3>
<p><strong>Authors:</strong> Ruihan Xu (Tsinghua University), Qingpei Guo (Tsinghua University), Yao Zhu (Tsinghua University), Xiangyang Ji (Tsinghua University), Ming Yang (Tsinghua University), Shiliang Zhang (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出层扁平化的Transformer深度压缩方法，解决现有块剪枝丢弃关键信息的问题，在LLaMA-2/3、Qwen-1.5等模型上验证了性能与效率的平衡，是高效大模型方向的具体创新。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08858' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models</h3>
<p><strong>Authors:</strong> Kevin Li, Dibyadeep Saha, Avni Kanodia, Fan Lai</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出tLoRA框架，融合多个LoRA任务到弹性共享超级模型，通过核级融合和调度层优化，提升训练吞吐量（1.2--1.8×）和GPU利用率（37%），属于高效大模型训练与推理的训练优化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07263' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Importance of a Multi-Scale Calibration for Quantization</h3>
<p><strong>Authors:</strong> Seungwoo Son (), Ingyu Seong (), Junhan Kim (), Hyemi Jang (), Yongkweon Jeon ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出多尺度校准方法解决LLM量化中固定长度校准的不足，提升低比特量化精度，对大模型高效部署有实际意义
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07465' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations</h3>
<p><strong>Authors:</strong> Yihang Gao (), Vincent Y. F. Tan ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 将ODE引入LoRA训练，通过连续时间优化提升训练稳定性和性能，为参数高效微调提供了新的理论与实践框架
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07479' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization</h3>
<p><strong>Authors:</strong> Xi Chen (), Ming Li (), Junxi Li (), Changsheng Li (), Peisong Wang (), Lizhong Ding (), Ye Yuan (), Guoren Wang ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 利用激活引导的结构化正则化抑制LLM量化中的离群值影响，提升量化鲁棒性，且不增加推理延迟，对大模型高效部署有实践价值
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07596' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Online Domain-aware LLM Decoding for Continual Domain Evolution</h3>
<p><strong>Authors:</strong> Mohammad Abu-Shaira, Weishi Shi</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对LLM在动态演化领域的适应问题，提出在线领域感知解码框架ODD，通过概率级融合和自适应置信度调制实现实时适应，无需重新训练，提升了动态场景下的推理性能和效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08088' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression</h3>
<p><strong>Authors:</strong> Yuntian Tang, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Wenxi Li, Wei Li, Jie Hu, Xinghao Chen, Rongrong Ji, Shaohui Lin</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Extra-CoT框架，通过语义保留压缩和分层强化学习实现CoT的极高压缩比，在保持推理准确性的同时减少token开销，提升了LLM推理效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08324' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection</h3>
<p><strong>Authors:</strong> Debajyoti Datta, Trishala Neeraj, Bibek Paudel, Vyom Sharma, Subhabrata Mukherjee</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出ManifoldKV方法，通过欧几里得距离检测KV异常值实现训练-free的KV缓存压缩，提升了长上下文推理的性能和鲁棒性，适用于多种LLM架构。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08343' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data</h3>
<p><strong>Authors:</strong> Tiantong Wang, Yiyang Duan, Haoyu Chen, Tiantong Wu, Wei Yang Bryan Lim</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出M-Loss量化模型合并兼容性，解决模型融合中的特征冲突问题，对高效大模型训练中的模型融合有重要意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08564' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning</h3>
<p><strong>Authors:</strong> Yicheng Di, Wei Yuan, Tieke He, Zhanjie Zhang, Ao Ma, Yuan Liu, Hongzhi Yin</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出联邦提示学习框架SDFed，解决本地-全局知识冲突，提升异质联邦环境下的性能，属于高效大模型训练中的联邦学习方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08590' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning</h3>
<p><strong>Authors:</strong> Dario Fenoglio, Pasquale Polverino, Jacopo Quizi, Martin Gjoreski, Marc Langheinrich</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出无服务器联邦学习框架ERIS，平衡隐私、通信效率与模型准确性，属于高效大模型训练与安全对齐的交叉研究。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08617' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DirMoE: Dirichlet-routed Mixture of Experts</h3>
<p><strong>Authors:</strong> Amirhossein Vahidi, Hesam Asadollahzadeh, Navid Akhavan Attar, Marie Moullet, Kevin Ly, Xingyi Yang, Mohammad Lotfollahi</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Dirichlet路由的MoE框架DirMoE，解耦专家选择与贡献分配，通过变分ELBO优化与稀疏 penalty提升专家 specialization，对高效大模型训练中的MoE结构优化有重要贡献。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09001' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs</h3>
<p><strong>Authors:</strong> Pengrui Han, Xueqiang Xu, Keyang Xuan, Peiyang Song, Siru Ouyang, Runchu Tian, Yuqing Jiang, Cheng Qian, Pengcheng Jiang, Jiashuo Sun, Junxia Cui, Ming Zhong, Ge Liu, Jiawei Han, Jiaxuan You</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Steer2Adapt框架通过动态组合 steering vectors实现LLM的高效适应，利用低维语义子空间从少量样本中学习组合，提升推理与安全任务的性能，对高效大模型训练中的适应方法有重要贡献。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07276' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge</h3>
<p><strong>Authors:</strong> Xin Wang, Hualin Zhou, Sheng Guang Wang, Ting Dang, Yu Zhang, Hong Jia, Tao Gu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出轻量化量化自适应框架，结合模态感知量化与无梯度测试时适应，解决边缘设备的多模态大模型部署问题，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07849' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Free(): Learning to Forget in Malloc-Only Reasoning Models</h3>
<p><strong>Authors:</strong> Yilun Zheng, Dongyang Ma, Tian Liang, Jiahao Xu, Xinting Huang, Lijie Chen, Haitao Mi, Yan Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Free()LM模型解决大模型的推理冗余问题，通过自遗忘机制动态修剪无用上下文，提升长 horizon 任务性能，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08030' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Does Your Reasoning Model Implicitly Know When to Stop Thinking?</h3>
<p><strong>Authors:</strong> Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang, Zhixia Zhang, Hongyan Xie, Songshi Liang, Zehao Chen, Xuefeng Xiao, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对大模型长链推理的冗余问题，发现模型隐含停止思考能力，提出SAGE框架释放该潜力，结合SAGE-RL提升推理准确性与效率，直接关联高效大模型推理需求。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08354' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning</h3>
<p><strong>Authors:</strong> Xinhai Sun</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 利用模型自身不确定性实现自校正推理，在零样本设置下显著提升准确性同时控制计算成本，属于高效大模型推理的关键方法。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08520' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute</h3>
<p><strong>Authors:</strong> Chen Jin, Ryutaro Tanno, Tom Diethe, Philip Teare</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出置信度引导的自refinement框架，通过轻量控制器减少测试时计算量（190倍token reduction），同时保持准确性，属于高效大模型推理的关键优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08948' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> iGRPO: Self-Feedback-Driven LLM Reasoning</h3>
<p><strong>Authors:</strong> Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 扩展GRPO框架，通过自反馈的两阶段优化提升LLM推理的准确性与效率，在数学推理基准上取得SOTA结果，属于高效大模型推理的方法创新。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09000' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction</h3>
<p><strong>Authors:</strong> Jinhao Li, Yuxuan Cong, Yingqiao Wang, Hao Xia, Shan Huang, Yijia Zhang, Ningyi Xu, Guohao Dai</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出STEP方法，通过时空一致性预测构造高质量warm-start动作，减少扩散政策的推理步骤，提升实时闭环系统的控制频率，属于高效大模型推理的创新，符合高效大模型训练与推理方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08245' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models</h3>
<p><strong>Authors:</strong> Sanggeon Yun, Ryozo Masukawa, SungHeon Jeong, Wenjun Huang, Hanning Chen, Mohsen Imani</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出公平上下文学习（FCL）框架，用于视觉语言模型（VLM）的测试时自适应（TTA），解决现有TTA依赖熵最小化导致的虚假相关问题，提升模型在分布偏移下的鲁棒性。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07027' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models</h3>
<p><strong>Authors:</strong> Xiangtian Zheng (Peking University), Zishuo Wang (Peking University), Yuxin Peng (Peking University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对视频多模态大模型的高计算成本，提出文本引导的帧采样与融合策略，在减少输入帧的同时保留关键信息，有效平衡效率与性能，是视频模态高效处理的针对性方案。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08861' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding</h3>
<p><strong>Authors:</strong> Yikang Yue, Yuqi Xue, Jian Huang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出SpecAttn框架，通过验证引导的稀疏注意力设计，结合自推测解码，提升长上下文LLM的推理吞吐量（2.81×于 vanilla autoregressive decoding），属于高效大模型训练与推理的推理优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07223' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference</h3>
<p><strong>Authors:</strong> Daniil Vankov, Nikita Ivkin, Kyle Ulrich, Xiang Song, Ashish Khetan, George Karypis</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出XShare方法，通过批量感知的专家选择优化，减少MoE模型的专家激活（最多30%）和GPU负载（3×），提升推理吞吐量（14%），属于高效大模型训练与推理的MoE推理优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07265' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OJBKQ: Objective-Joint Babai-Klein Quantization</h3>
<p><strong>Authors:</strong> Xinyu Wang, Ziyu Zhao, Peng Lu, Yu Gu, Xiao-Wen Chang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出OJBKQ方法，联合优化激活和权重的量化，利用Babai-Klein算法解决整数最小二乘问题，提升了低比特后训练量化（PTQ）的性能。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08376' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Kirin: Improving ANN efficiency with SNN Hybridization</h3>
<p><strong>Authors:</strong> Chenyu Wang, Zhanglu Yan, Zhi Zhou, Xu Chen, Weng-Fai Wong</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Kirin混合ANN与SNN提升效率，保持精度的同时降低能耗，属于高效大模型训练与推理的神经形态计算方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08817' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery</h3>
<p><strong>Authors:</strong> Difei Gu, Yunhe Gao, Gerasimos Chatzoudis, Zihan Dong, Guoning Zhang, Bangwei Guo, Yang Zhou, Mu Zhou, Dimitris Metaxas</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出统一的视觉-语言稀疏自编码器（LUCID-SAE），通过最优传输匹配实现跨模态特征对齐，自动发现可解释的语义概念（如动作、属性），提升多模态模型的可解释性。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07311' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models</h3>
<p><strong>Authors:</strong> Seonglae Cho, Zekun Wu, Kleyton Da Costa, Adriano Koshiyama</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 揭示了语言模型中正确性表示的几何结构，发现判别信号集中在低维子空间，通过因果激活 steering 验证了结构的有效性，内部探针性能显著优于输出-based方法，对LLM可解释性有重要启示。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08159' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System</h3>
<p><strong>Authors:</strong> Yanming Li, Xuelin Zhang, WenJie Lu, Ziye Tang, Maodong Wu, Haotian Luo, Tongtong Wu, Zijie Peng, Hongze Mi, Yibo Feng, Naiqiang Tan, Chao Huang, Hong Chen, Li Shen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出基于Shapley值的多智能体信用分配方法，直接对应深度学习可解释性中的Shapley value研究方向，实验验证性能显著提升。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08335' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees</h3>
<p><strong>Authors:</strong> Muhammad Rashid, Elvio G. Amparore, Enrico Ferrari, Damiano Verda</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 结合分层Shapley值与数据感知的二叉分割树（BPT），提出图像特征归因方法ShapBPT，解决现有Shapley方法未利用图像多尺度结构的问题，提升可解释性的效率与语义一致性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07047' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Process-of-Thought Reasoning for Videos</h3>
<p><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Jian Wang, Yongsen Zheng, Kwok-Yan Lam, Keze Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对视频理解中多步推理的黑箱问题，提出Process-of-Thought（PoT）框架将推理分解为“时间证据选择-逐步状态更新-约束答案合成”的可验证步骤。该框架模型无关，能提升视频推理的事实正确性和时间接地性，并提供可追溯的推理轨迹以支持解释，在标准任务中一致提升性能。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07689' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Weijiang Lv, Yaoxuan Feng, Xiaobo Xia, Jiayu Wang, Yan Jing, Wenchao Chen, Bo Chen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对多模态大模型（MLLM）Chain-of-Thought（CoT）推理的“忠实性”（推理与感知对齐）问题，构建SPD-Faith Bench基准（基于细粒度图像差异推理），诊断出“感知 blindness”和“感知-推理分离”两大失败模式。进而提出SAGE训练-free框架提升视觉路由和推理-感知对齐，为MLLM的CoT可解释性改进提供了基准和方法。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07833' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Understanding Multimodal Fine-Tuning: Spatial Features</h3>
<p><strong>Authors:</strong> Lachin Naghashyar (University of Oxford), Hunar Batra (University of Oxford), Ashkan Khakzar (University of Oxford), Philip Torr (University of Oxford), Ronald Clark (University of Oxford), Christian Schroeder de Witt (University of Oxford), Constantin Venhoff (University of Oxford)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 采用阶段模型差分法分析视觉语言模型多模态微调中的空间特征形成机制，揭示了空间关系编码的关键组件，为多模态模型的可解释性研究提供了方法论支撑。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08713' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Exactly Computing do-Shapley Values</h3>
<p><strong>Authors:</strong> R. Teal Witter, Álvaro Parafita, Tomas Garriga, Maximilian Muschalik, Fabian Fumagalli, Axel Brando, Lucas Rosenblatt</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对结构因果模型（SCM）中的do-Shapley值计算问题，提出基于不可约集的精确计算方法，将计算复杂度从指数级降低到线性，减少识别负担，属于深度学习可解释性的Shapley value方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07203' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient Representations are Controllable Representations</h3>
<p><strong>Authors:</strong> Charles Ye (Stanford University), Jasmine Cui (Stanford University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出通过简单辅助损失微调LLM，让模型残差流维度成为可解释的控制开关，解决现有可控性方法需复杂特征干预的问题，实验验证了控制开关的有效性，为深度学习可解释性提供了轻量且有效的新途径。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07828' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs</h3>
<p><strong>Authors:</strong> Maiqi Jiang, Noman Ali, Yiran Ding, Yanfu Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究异质GNN中元路径注意力与解释的对齐性，涉及Shapley-style解释器，对理解GNN解释的可靠性有重要价值。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08500' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FreqLens: Interpretable Frequency Attribution for Time Series Forecasting</h3>
<p><strong>Authors:</strong> Chi-Sheng Chen, Xinyu Zhang, En-Jui Kuo, Guan-Ying Chen, Qiuzhe Xie, Fan Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出FreqLens可解释频率归因框架，基于Shapley值满足公理性质，对时间序列预测的可解释性有重要价值，属于深度学习可解释性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08768' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Discovering Interpretable Algorithms by Decompiling Transformers to RASP</h3>
<p><strong>Authors:</strong> Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出从训练后的Transformer中提取可解释RASP程序的方法，通过因果干预发现小而充分的子程序，直接证明长度泛化的Transformer内部实现简单可解释的程序，对深度学习可解释性研究有关键推进。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08857' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers</h3>
<p><strong>Authors:</strong> Zhan-Yi Liao, Jaewon Yoo, Hao-Tsung Yang, Po-An Chen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出基于条件高斯网络分类器的结构感知反事实解释方法，结合切割集程序与McCormick松弛实现鲁棒性，符合深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08021' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Exploring SAIG Methods for an Objective Evaluation of XAI</h3>
<p><strong>Authors:</strong> Miquel Miró-Nicolau, Gabriel Moyà-Alcover, Anna Arias-Duart</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究SAIG方法生成人工基准，解决XAI评估缺乏ground truth的问题，提出分类 taxonomy与比较分析，属于深度学习可解释性的评估方法创新。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08715' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Geometry of Representational Failures in Vision Language Models</h3>
<p><strong>Authors:</strong> Daniele Savietto, Declan Campbell, André Panisson, Marco Nurisso, Giovanni Petri, Jonathan D. Cohen, Alan Perotti</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 通过分析视觉语言模型（VLM）中概念向量的几何重叠，揭示表示失败与错误模式的关联，提出定量框架理解内部表示如何影响模型行为，提升VLM的可解释性。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07025' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Interpreting Physics in Video World Models</h3>
<p><strong>Authors:</strong> Sonia Joseph, Quentin Garrido, Randall Balestriero, Matthew Kowal, Thomas Fel, Shahab Bakhtiari, Blake Richards, Mike Rabbat</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 通过层间探测、子空间几何分析等方法，首次系统分析视频世界模型中的物理表示，识别出“物理涌现区”，揭示物理信息在模型中的分布与组织方式，提升视频模型的可解释性。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07050' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?</h3>
<p><strong>Authors:</strong> Alexander von Recum, Leander Girrbach, Zeynep Akata</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 设计受控框架扰动LLM的Chain-of-Thought（CoT），评估其鲁棒性，发现模型能从扰动中恢复但存在风格与长度权衡，对深度学习可解释性中的CoT可靠性有重要分析。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.07470' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance</h3>
<p><strong>Authors:</strong> Xuehai Bai, Xiaoling Gu, Akide Liu, Hangjie Yuan, YiFan Zhang, Jack Ma</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 基于多模态大模型的复杂指令图像编辑，解决指令 compliance和背景一致性问题，构建专用数据集和基准，属于原生多模态大模型中的图像生成方向。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.07993' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning</h3>
<p><strong>Authors:</strong> Hulingxiao He, Zijun Geng, Yuxin Peng</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对多模态大模型（MLLM）在细粒度视觉识别（FGVR）中性能弱、泛化差的问题，提出Fine-R1框架：通过构建FGVR CoT数据集（含“视觉分析-候选子类别-比较-预测”推理链）进行监督微调，结合三元组增强策略提升鲁棒性与判别力。仅4-shot训练就超过现有MLLM、推理型MLLM及对比学习模型，在seen/unseen子类别识别中表现优异。
Score: 8.5
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.07605' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds</h3>
<p><strong>Authors:</strong> Chen Yang, Guanxin Lin, Youquan He, Peiyao Chen, Guanghe Liu, Yufan Mo, Zhouyuan Xu, Linhao Wang, Guohui Zhang, Zihang Zhang, Shenxiang Zeng, Chen Wang, Jiansheng Fan</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 构建视觉语言模型空间推理基准SSI-Bench，评估31个VLMs的空间推理能力，揭示其与人类的性能差距，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.07864' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning</h3>
<p><strong>Authors:</strong> Changli Tang, Tianyi Wang, Fengyun Rao, Jing Lyu, Chao Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出对话中心的音频-视觉字幕框架，构建大规模双语数据集，提升speaker identification和temporal grounding性能，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.07960' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval</h3>
<p><strong>Authors:</strong> Issar Tzachor, Dvir Samuel, Rami Ben-Ari</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究视频多模态大模型的嵌入提取，结合中间层嵌入和文本对齐策略，提升视频-文本检索性能，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08099' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension</h3>
<p><strong>Authors:</strong> Yik Lung Pang, Changjae Oh</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出训练-free的Chain-of-Caption框架，提升多模态大模型的指代理解性能，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08211' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</h3>
<p><strong>Authors:</strong> Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究多模态大模型的视觉空间推理，提出自适应想象框架，提升推理效率和准确性，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08236' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment</h3>
<p><strong>Authors:</strong> Yunzuo Hu, Wen Li, Jing Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出跨模态交互增强框架，提升音频-视觉学习的表示质量，属于原生多模态大模型中的音频-视觉融合方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08309' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Language-Guided Transformer Tokenizer for Human Motion Generation</h3>
<p><strong>Authors:</strong> Sheng Yan, Yong Wang, Xin Du, Junsong Yuan, Mengyuan Liu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出语言引导的Transformer令牌化方法，用于人类运动生成，提升运动生成的语义一致性，属于原生多模态大模型中的语言-运动融合方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08337' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MOVA: Towards Scalable and Synchronized Video-Audio Generation</h3>
<p><strong>Authors:</strong> OpenMOSS Team (MOSS), Donghua Yu (MOSS), Mingshu Chen (MOSS), Qi Chen (MOSS), Qi Luo (MOSS), Qianyi Wu (MOSS), Qinyuan Cheng (MOSS), Ruixiao Li (MOSS), Tianyi Liang (MOSS), Wenbo Zhang (MOSS), Wenming Tu (MOSS), Xiangyu Peng (MOSS), Yang Gao (MOSS), Yanru Huo (MOSS), Ying Zhu (MOSS), Yinze Luo (MOSS), Yiyang Zhang (MOSS), Yuerong Song (MOSS), Zhe Xu (MOSS), Zhiyu Zhang (MOSS), Chenchen Yang (MOSS), Cheng Chang (MOSS), Chushu Zhou (MOSS), Hanfu Chen (MOSS), Hongnan Ma (MOSS), Jiaxi Li (MOSS), Jingqi Tong (MOSS), Junxi Liu (MOSS), Ke Chen (MOSS), Shimin Li (MOSS), Songlin Wang (MOSS), Wei Jiang (MOSS), Zhaoye Fei (MOSS), Zhiyuan Ning (MOSS), Chunguo Li (MOSS), Chenhui Li (MOSS), Ziwei He (MOSS), Zengfeng Huang (MOSS), Xie Chen (MOSS), Xipeng Qiu (MOSS)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出开源的MoE架构视频音频联合生成模型，解决了现有 cascaded  pipeline 的误差累积问题，支持同步生成与多任务，为多模态大模型的开源社区提供了重要基础。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08794' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing</h3>
<p><strong>Authors:</strong> Hao Yang (ByteDance), Zhiyu Tan (ByteDance), Jia Gong (ByteDance), Luozheng Qin (ByteDance), Hesen Chen (ByteDance), Xiaomeng Yang (ByteDance), Yuqing Sun (ByteDance), Yuetan Lin (ByteDance), Mengping Yang (ByteDance), Hao Li (ByteDance)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 连接预训练多模态大模型与视频扩散模型，利用MLLM的理解能力引导复杂视频生成与编辑，在FiVE（细粒度编辑）和VBench（生成质量）上表现优异，是多模态大模型与生成结合的前沿工作。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08820' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation</h3>
<p><strong>Authors:</strong> Zihan Yang (Fudan University), Shuyuan Tu (Fudan University), Licheng Zhang (Fudan University), Qi Dai (Microsoft Research Asia), Yu-Gang Jiang (Fudan University), Zuxuan Wu (Fudan University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出非线流蒸馏框架ArcFlow，针对文本到图像生成的多步去噪问题，通过非线性轨迹逼近教师模型轨迹，实现40倍推理加速且保持生成质量，基于Qwen-Image-20B和FLUX.1-dev等大模型验证，相关于原生多模态大模型的图像生成方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09014' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training</h3>
<p><strong>Authors:</strong> Wanyun Xie (EPFL), Francesco Tonin (EPFL), Volkan Cevher (EPFL)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLM）训练中的多模态数据混合问题，提出MaD-Mix框架，通过潜在空间耦合系统处理缺失模态，加速训练并提升性能，在0.5B和7B模型上验证了其在图像-文本指令微调及三模态场景中的有效性，属于原生多模态大模型的关键训练优化方法。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.07790' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection</h3>
<p><strong>Authors:</strong> Junru Zhang, Lang Feng, Haoran Shi, Xu Guo, Han Yu, Yabo Dong, Duanqing Xu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出AnomSeer框架增强多模态LLM的时间序列推理能力，结合专家链-of-thought和时间序列接地策略优化，提升异常分类、定位与解释性能，对原生多模态大模型在时间序列任务中的应用有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08868' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient Table Retrieval and Understanding with Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Zhuoyan Xu, Haoyang Fang, Boran Han, Bonan Min, Bernie Wang, Cuixiong Hu, Shuai Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出TabRAG框架解决多模态大模型的表格检索与理解问题，构建新数据集验证性能，符合原生多模态大模型研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.07642' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs</h3>
<p><strong>Authors:</strong> Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出SAYO模型通过强化学习增强多模态大模型的视觉注意力，解决视觉错位与误差传播问题，符合原生多模态大模型研究方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08241' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT</h3>
<p><strong>Authors:</strong> Chengyi Du, Yazhe Niu, Dazhong Shen, Luxin Xu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出无标注的多模态视觉推理方法，通过分层合成CoT与认知一致奖励增强视觉语言模型的结构化推理能力，符合原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08339' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture</h3>
<p><strong>Authors:</strong> Roland Bertin-Johannet, Lara Scipio, Leopold Maytié, Rufin VanRullen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 基于全局工作空间理论提出多模态整合注意力机制，提升多模态系统的噪声鲁棒性与跨任务泛化能力，属于原生多模态大模型的架构创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08597' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</h3>
<p><strong>Authors:</strong> Masanari Oi (Kyoto University), Koki Maeda (Kyoto University), Ryuto Koike (Kyoto University), Daisuke Oba (Kyoto University), Nakamasa Inoue (Kyoto University), Naoaki Okazaki (Kyoto University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 模仿人类跨视图对应与视角变换的认知机制，提出HATCH训练框架提升多模态大模型的多图像空间推理能力，在三个基准测试中显著优于基线模型，是多模态推理的认知启发式创新。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ViT-5: Vision Transformers for The Mid-2020s</h3>
<p><strong>Authors:</strong> Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad, Cihang Xie, Alan Yuille</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 系统改进Vision Transformer的架构组件（normalization、activation等），提升理解和生成任务性能，属于深度学习理论中的网络架构方向。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08071' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Deriving Neural Scaling Laws from the statistics of natural language</h3>
<p><strong>Authors:</strong> Francesco Cagnetta (), Allan Raventós (), Surya Ganguli (), Matthieu Wyart ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 首次从自然语言的统计特性（如token相关性和条件熵）推导神经缩放律，无需自由参数即可匹配GPT-2和LLaMA的实验结果，对理解LLM scaling有重要理论贡献
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07488' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm</h3>
<p><strong>Authors:</strong> Tianyu Li (Tsinghua University), Dongchen Han (Tsinghua University), Zixuan Cao (Tsinghua University), Haofeng Huang (Tsinghua University), Mengyu Zhou (Tsinghua University), Ming Chen (Tsinghua University), Erchao Zhao (Tsinghua University), Xiaoxi Jiang (Tsinghua University), Guanjun Jiang (Tsinghua University), Gao Huang (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对Transformer中Pre-Norm稳定但性能不足、Post-Norm性能优但不稳定的矛盾，提出SiameseNorm双流架构，共享参数并保留两者优化特性，实验在1.3B参数模型上验证了优化鲁棒性和性能提升，是Transformer架构设计的重要创新。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08064' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Vanilla Group Equivariant Vision Transformer: Simple and Effective</h3>
<p><strong>Authors:</strong> Jiahong Fu, Qi Xie, Deyu Meng, Zongben Xu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 系统改进视觉Transformer的等变架构组件（patch embedding、self-attention等），提升性能和数据效率，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08047' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent</h3>
<p><strong>Authors:</strong> Shota Imai (), Sota Nishiyama (), Masaaki Imaizumi ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究了SGD训练下神经网络中特征学习与遗忘的快慢动力学机制，揭示了特征遗忘的条件与缓解方法，对理解网络训练动态有重要理论价值
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07378' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise</h3>
<p><strong>Authors:</strong> Dingzhi Yu (), Hongyi Tao (), Yuanyu Wan (), Luo Luo (), Lijun Zhang ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 从理论上分析了符号优化器在重尾梯度噪声下的收敛优势，为LLM训练中优化器的选择提供了理论支持，匹配实践中符号优化器的优异表现
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks</h3>
<p><strong>Authors:</strong> Shenxi Wu (), Haosong Zhang (), Xingjian Ma (), Shirui Bian (), Yichi Zhang (), Xi Chen (), Wei Lin ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出多路径神经网络的超参数迁移规律，揭示学习率随有效深度的-3/2幂律衰减，对架构设计和超参数调优有直接指导意义
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07494' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Robust Scaling Laws for Optimizers</h3>
<p><strong>Authors:</strong> Alexandra Volkova (), Mher Safaryan (), Christoph H. Lampert (), Dan Alistarh ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究不同优化器的缩放律，提出共享幂律指数、优化器特定 rescaling 的鲁棒模型，为LLM训练中优化器的选择与比较提供理论框架
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07712' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs</h3>
<p><strong>Authors:</strong> Sagnik Mukherjee (), Lifan Yuan (), Pavan Jayasinha (), Dilek Hakkani-Tür (), Hao Peng ()</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 发现SGD在LLM的RL训练中表现优于AdamW，且参数更新更稀疏（仅0.02%），对优化器选择和参数高效训练有新见解
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07729' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff</h3>
<p><strong>Authors:</strong> Isaac Han (Stanford University), Sangyeon Park (Stanford University), Seungwon Oh (Stanford University), Donghu Kim (Stanford University), Hojoon Lee (Stanford University), Kyung-Joong Kim (Stanford University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对深度网络的稳定性-可塑性权衡问题，提出FIRE方法，通过最小化Frobenius误差并约束等距偏差，平衡权重更新的保守性与可塑性，实验在视觉、语言、强化学习任务上一致优于现有方法，为深度学习理论中的核心问题提供了新的解决方案。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08040' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A second order regret bound for NormalHedge</h3>
<p><strong>Authors:</strong> Yoav Freund, Nicholas J. A. Harvey, Victor S. Portella, Yabing Qi, Yu-Xiang Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对在线学习中的NormalHedge算法，给出了其二阶ε分位数遗憾界，结合随机微分方程（SDE）和自协和技术进行分析，对在线优化算法的理论研究有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08151' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spherical Steering: Geometry-Aware Activation Rotation for Language Models</h3>
<p><strong>Authors:</strong> Zejia You, Chunyuan Deng, Hanjie Chen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出球面引导（Spherical Steering）方法，通过激活旋转实现LLM推理时的精确控制，避免了激活加法导致的表示崩溃，在多项基准上提升了性能，对LLM的几何控制有理论和实践贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08169' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Linearization Explains Fine-Tuning in Large Language Models</h3>
<p><strong>Authors:</strong> Zahra Rahimi Afzal, Tara Esmaeilbeig, Mojtaba Soltanalian, Mesrob I. Ohannessian</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 从线性化角度分析了LLM的参数高效微调（PEFT）机制，关联了神经正切核（NTK）的谱特性与模型适应性能，对LoRA等PEFT技术的理解和改进有指导意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08239' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer</h3>
<p><strong>Authors:</strong> Yan-Feng Xie, Yu-Jie Zhang, Peng Zhao, Zhi-Hua Zhou</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 利用discounted-to-dynamic还原分析了FTRL和Adam优化器的动态遗憾，得到了线性回归、逻辑回归和Adam的动态遗憾界，对在线优化和自适应优化器的理论有贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08372' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Trapped by simplicity: When Transformers fail to learn from noisy features</h3>
<p><strong>Authors:</strong> Evan Peters, Ando Deng, Matheus H. Zambianco, Devin Blankespoor, Achim Kempf</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 分析Transformer在噪声特征下的学习失败原因，发现其偏向简单函数的偏差，属于深度学习理论中的模型泛化与噪声鲁棒性研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08695' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Robust Policy Optimization to Prevent Catastrophic Forgetting</h3>
<p><strong>Authors:</strong> Mahdi Sabbaghi, George Pappas, Adel Javanmard, Hamed Hassani</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出鲁棒策略优化防止灾难性遗忘，解决多阶段训练中的行为退化问题，属于深度学习理论中的模型持续学习研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08813' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization</h3>
<p><strong>Authors:</strong> Yang Qiu, Yixiong Zou, Jun Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究图神经网络泛化中的Minimal Shift Flip现象，结合Sharpness-Aware Minimization理论分析损失 landscape的局部稳定性，提出能量驱动的生成增强框架E2A，提升图OOD泛化性能，对深度学习理论中的泛化与优化研究有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08855' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Near-optimal Swap Regret Minimization for Convex Losses</h3>
<p><strong>Authors:</strong> Lunjia Hu, Jon Schneider, Yifan Wu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 解决凸损失下的swap regret最小化问题，提出随机在线算法实现近最优的O(√T)期望界，改进之前O(T^(2/3))的结果，对优化理论中的regret minimization有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08862' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Positive Distribution Shift as a Framework for Understanding Tractable Learning</h3>
<p><strong>Authors:</strong> Marko Medvedev, Idan Attias, Elisabetta Cornacchia, Theodor Misiakiewicz, Gal Vardi, Nathan Srebro</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出正向分布偏移（PDS）概念，分析如何通过选择训练分布使计算难问题变得可处理，连接分布偏移与计算 tractability，对深度学习理论中的泛化与优化有新的视角贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08907' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ARO: A New Lens On Matrix Optimization For Large Models</h3>
<p><strong>Authors:</strong> Wenbo Gong, Javier Zazo, Qijun Luo, Puqian Wang, James Hensman, Chao Ma</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出自适应旋转优化（ARO）框架，通过旋转坐标系下的范数最陡下降加速大模型预训练，理论分析旋转对收敛的影响，实验验证其优于AdamW与正交化方法，对深度学习理论中的矩阵优化有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09006' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling</h3>
<p><strong>Authors:</strong> Yilang Zhang, Bingcong Li, Niao He, Georgios B. Giannakis</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 分析残差连接对优化的影响，提出ANCRe框架自适应学习残差连接布局，以极小 overhead提升深度模型的收敛与性能，对深度学习理论中的网络结构设计有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09009' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure</h3>
<p><strong>Authors:</strong> Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang, Chenghua Lin, Min Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 将 latent CoT 视为因果过程，通过do-干预分析推理步骤的必要性与信息传播，揭示大模型推理的内在机制，属于深度学习理论中的推理过程研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08783' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ReRoPE: Repurposing RoPE for Relative Camera Control</h3>
<p><strong>Authors:</strong> Chunyang Li, Yuanbo Yang, Jiahao Shao, Hongyu Zhou, Katja Schwarz, Yiyi Liao</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 将RoPE位置编码用于相对相机控制，改进视频生成的相机控制能力，属于深度学习理论中的网络架构方向（位置编码）。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08068' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features</h3>
<p><strong>Authors:</strong> Qiang Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究注意力-based的稀疏匹配模型，优化Transformer-based匹配框架，提升匹配性能，属于深度学习理论中的网络架构方向（注意力机制）。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08430' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts</h3>
<p><strong>Authors:</strong> Shashank</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Attractor Patch Networks（APN）作为Transformer FFN的替代结构，通过路由低秩补丁专家实现上下文特定的非线性变换，理论分析其作为分段低秩残差函数的表达性，实验验证在持续学习中显著提升保留率和适应度，属于深度学习理论的网络架构方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.06993' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mutual information and task-relevant latent dimensionality</h3>
<p><strong>Authors:</strong> Paarth Gulati, Eslam Abdelaleem, Audrey Sederberg, Ilya Nemenman</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 解决任务相关潜在维度估计这一未解决问题，将其转化为信息瓶颈问题，提出混合critic和one-shot协议，改善了神经MI估计器的维度膨胀问题，在合成数据和物理数据集上验证了有效性。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08105' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Noise Stability of Transformer Models</h3>
<p><strong>Authors:</strong> Themistoklis Haris, Zihan Zhang, Yuichi Yoshida</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出噪声稳定性作为Transformer的简单性度量，分析了单层和多层的噪声稳定性传播，开发了正则化方法提升训练效率，对理解Transformer的训练动态和鲁棒性有帮助。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08287' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Grokking in Linear Models for Logistic Regression</h3>
<p><strong>Authors:</strong> Nataraj Das, Atreya Vedantam, Chandrashekar Lakshminarayanan</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 证明了grokking现象不仅存在于深度模型，也存在于线性逻辑回归模型中，分析了三阶段学习过程和数据不对称的影响，对grokking的理论理解有扩展。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08302' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training</h3>
<p><strong>Authors:</strong> Cristian Pérez-Corral, Alberto Fernández-Hernández, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出训练的两阶段假设（早期激活模式变化，后期稳定激活下的权重微调），验证了全连接、卷积和Transformer模型，对理解神经网络训练动态有理论贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08333' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Expressive Power of GNNs for Boolean Satisfiability</h3>
<p><strong>Authors:</strong> Saku Peltonen, Roger Wattenhofer</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 分析GNN在布尔可满足性问题中的表达能力，证明WL层次无法区分可满足与不可满足实例，属于深度学习理论中的GNN理论研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08745' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation</h3>
<p><strong>Authors:</strong> Ofek Amran, Tom Gilat, Ron Levie</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 用Graphop分析稀疏图上GNN的泛化和通用逼近能力，属于深度学习理论中的GNN理论研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08785' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor</h3>
<p><strong>Authors:</strong> Shaoang Zhang, Yazhe Niu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出TreeTensor数据结构解决嵌套数据的处理问题，提升AI系统的效率与可用性，属于深度学习理论中的数据表示与架构创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08517' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> lrnnx: A library for Linear RNNs</h3>
<p><strong>Authors:</strong> Karan Bania, Soham Kalburgi, Manit Tanwar, Dhruthi, Aditya Nagarsekar, Harshvardhan Mestha, Naman Chibber, Raj Deshmukh, Anish Sathyanarayanan, Aarush Rathore, Pratham Chheda</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 开发lrnnx库统一线性RNN的实现，促进线性RNN的研究和应用，属于深度学习理论中的RNN工具支持。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08810' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection</h3>
<p><strong>Authors:</strong> Guanglong Sun (Tsinghua University), Siyuan Zhang (Tsinghua University), Liyuan Wang (Tsinghua University), Jun Zhu (Tsinghua University), Hang Su (Tsinghua University), Yi Zhong (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对大模型对齐中的“对齐税”（安全训练降低通用能力）问题，提出OGPSA方法，通过正交梯度投影约束安全更新不干扰通用能力 subspace，实验在Qwen2.5-7B-Instruct等模型上显著提升安全-效用帕累托前沿，是对齐领域的关键问题解决方法。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07892' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs</h3>
<p><strong>Authors:</strong> Ahmed Salem, Andrew Paverd, Sahar Abdelnabi</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 揭示LLM通过隐式记忆跨交互保持状态的机制，提出时间炸弹攻击等安全威胁，对大模型安全与对齐研究有重要警示作用。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08563' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs</h3>
<p><strong>Authors:</strong> Yukun Jiang, Hai Huang, Mingjie Li, Yage Zhang, Michael Backes, Yang Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 发现MoE LLMs中的不安全路由问题，提出F-SOUR框架生成不安全路径，对大模型安全与对齐中的MoE模型风险研究有重要价值。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08621' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Stress-Testing Alignment Audits With Prompt-Level Strategic Deception</h3>
<p><strong>Authors:</strong> Oliver Daniels, Perusha Moodley, Ben Marlin, David Lindner</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 设计自动红队 pipeline生成针对对齐审计的提示级策略欺骗，测试黑白盒审计方法的鲁棒性，发现当前方法易受激活基欺骗，对大模型安全与对齐的审计方法改进有重要启发。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08877' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Emergent Misalignment is Easy, Narrow Misalignment is Hard</h3>
<p><strong>Authors:</strong> Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究大模型的emergent misalignment现象，分析归纳偏差对泛化的影响，提出线性表示用于监测与缓解，作者包括知名研究者Neel Nanda，符合大模型安全与对齐方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07852' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment</h3>
<p><strong>Authors:</strong> Igor Santos-Grueiro</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 分析对齐评估中的regime leakage问题（模型利用评估与部署的差异进行条件策略），提出regime-blind机制减少对齐偏差，为大模型对齐评估提供理论与方法支撑，属于核心安全对齐问题。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models</h3>
<p><strong>Authors:</strong> Jiaxi Yang, Shicheng Liu, Yuchen Yang, Dongwon Lee</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对视觉语言模型（VLM）的安全对齐问题，提出基于激活引导的可配置拒绝机制（CR-VLM），解决现有拒绝策略“一刀切”的问题，提升VLM的安全适应性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07013' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Extended to Reality: Prompt Injection in 3D Environments</h3>
<p><strong>Authors:</strong> Zhuoheng Li, Ying Chen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出PI3D攻击，通过在3D环境中放置带文本的物理对象，实现对多模态大模型（MLLM）的prompt injection，揭示MLLM在物理环境中的安全漏洞。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07104' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Robustness of Vision Language Models Against Split-Image Harmful Input Attacks</h3>
<p><strong>Authors:</strong> Md Rafi Ur Rashid, MD Sadik Hossain Shanto, Vishnu Asutosh Dasu, Shagufta Mehnaz</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究视觉语言模型对分裂图像有害输入的鲁棒性，提出攻击方法和防御策略，属于大模型安全与对齐中的安全方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08136' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning</h3>
<p><strong>Authors:</strong> Yujin Zhou, Pengcheng Wen, Jiale Chen, Boqin Yin, Han Zhu, Jiaming Ji, Juntao Dai, Chi-Min Chan, Sirui Han</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对大模型的思考过程，提出过程奖励模型，提升推理的正确性，属于大模型安全与对齐中的对齐方向（奖励模型）。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08346' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control</h3>
<p><strong>Authors:</strong> Yonghui Yang, Wenjian Tao, Jilong Liu, Xingyu Zhu, Junfeng Fang, Weibiao Huang, Le Wu, Richang Hong, Tat-Sent Chua</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出ShaPO框架，通过选择性几何控制对齐关键参数子空间，改善LLM安全对齐的鲁棒性（对抗域转移和噪声偏好），实验验证优于现有偏好优化方法，属于大模型安全与对齐的对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07340' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Controllable Value Alignment in Large Language Models through Neuron-Level Editing</h3>
<p><strong>Authors:</strong> Yonghui Yang, Junwei Li, Jilong Liu, Yicheng He, Fengbin Zhu, Weibiao Huang, Le Wu, Richang Hong, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出NeVA框架，通过神经元级编辑实现LLM的可控价值对齐，减少价值泄漏（非目标价值的 unintended激活），实验验证在目标对齐和泄漏控制上的优势，属于大模型安全与对齐的价值对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07356' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learnable Chernoff Baselines for Inference-Time Alignment</h3>
<p><strong>Authors:</strong> Sunil Madhow (University of California, Santa Barbara), Yuchen Liang (University of California, Santa Barbara), Ness Shroff (The Ohio State University), Yingbin Liang (The Ohio State University), Yu-Xiang Wang (University of California, Santa Barbara)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对推理时奖励引导对齐问题，提出Learnable Chernoff Baselines（LCBs）方法，解决现有方法依赖特定架构或计算成本高的缺陷，提供了总变分保证，并在连续和离散扩散模型上验证了有效性，对大模型安全对齐有重要价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07738' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fairness Aware Reward Optimization</h3>
<p><strong>Authors:</strong> Ching Lam Choi (Massachusetts Institute of Technology), Vighnesh Subramaniam (Massachusetts Institute of Technology), Phillip Isola (Massachusetts Institute of Technology), Antonio Torralba (Massachusetts Institute of Technology), Stefanie Jegelka (Massachusetts Institute of Technology)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对人类偏好数据中的 demographic偏差问题，提出Fairness Aware Reward Optimization（Faro）框架，实现奖励模型的序数、基数和公平性统一，理论证明了公平性证书和帕累托前沿存在性，实验减少偏差同时保持模型质量，对大模型安全对齐的公平性提升有重要意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07799' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology</h3>
<p><strong>Authors:</strong> Valentin Noël (EPFL)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出基于注意力拓扑谱分析的无训练安全护栏，检测智能体工具使用中的幻觉问题，实验在Llama 3.1 8B和Mistral 7B上实现高召回率（97.7%），为大模型安全提供了高效、可解释的新方法。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08082' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reinforcement Learning with Backtracking Feedback</h3>
<p><strong>Authors:</strong> Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出RLBF框架，通过强化学习训练LLM回溯纠正安全违规，结合增强的SFT数据生成（BSAFE+），显著提升了LLM的安全性能，同时保留了模型效用。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08377' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Correctness: Learning Robust Reasoning via Transfer</h3>
<p><strong>Authors:</strong> Hyunseok Lee, Soheil Abbasloo, Jihoon Tack, Jinwoo Shin</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出RLTR方法，通过可转移奖励（transfer reward）提升LLM推理的鲁棒性，在保持准确性的同时提升了采样一致性和样本效率，对LLM推理的可靠性有重要贡献。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08489' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks</h3>
<p><strong>Authors:</strong> Yanzhang Fu, Zizheng Guo, Jizhou Luo</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Plug-And-Play的Dashed Line Defense，防御自适应分数查询攻击，提升大模型对抗鲁棒性，属于大模型安全与对齐。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08679' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning aligns language models to human cognition</h3>
<p><strong>Authors:</strong> Gonçalo Guiomar, Elia Torre, Pehuen Moure, Victoria Shavina, Mario Giulianelli, Shih-Chii Liu, Valerio Mante</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 研究链式推理对LLM与人类认知对齐的作用，发现推理提升证据整合与信念映射，属于大模型安全与对齐中的认知对齐。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08693' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity</h3>
<p><strong>Authors:</strong> James Jewitt, Gopi Krishnan Rajbahadur, Hao Li, Bram Adams, Ahmed E. Hassan</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 审计开源AI供应链的许可证完整性，发现大量许可证缺失问题，属于大模型安全与对齐中的法律合规研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08816' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Bayesian Preference Learning for Test-Time Steerable Reward Models</h3>
<p><strong>Authors:</strong> Jiwoo Hong, Shao Tang, Zhipeng Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 用贝叶斯偏好学习实现测试时可控的奖励模型，提升LLM的对齐灵活性，属于大模型安全与对齐中的奖励模型研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08819' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs</h3>
<p><strong>Authors:</strong> Xuanqi Zhang, Haoyang Shang, Xiaoxiao Li</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出GSS框架通过门控子空间引导选择性缓解LLM的记忆问题，结合探针检测与目标修正，在减少记忆的同时保持性能，且计算量远低于优化类方法，对大模型安全中的记忆与隐私保护有重要贡献。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08901' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors</h3>
<p><strong>Authors:</strong> Suraj Ranganath, Atharv Ramesh</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出StealthRL强化学习框架生成对抗 paraphrase攻击AI文本检测器，针对多检测器 ensemble优化逃避与语义保持，发现检测器的共享脆弱性，对大模型安全中的文本检测鲁棒性有重要测试。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08934' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Out-of-Distribution Detection to Hallucination Detection: A Geometric View</h3>
<p><strong>Authors:</strong> Litian Liu, Reza Pourreza, Yubing Jian, Yao Qin, Roland Memisevic</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 将LLM的幻觉检测重构为分布外（OOD）检测问题，利用分类视角应用OOD技术，实现训练-free的单样本幻觉检测，对大模型安全中的幻觉问题有重要新视角。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07253' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective</h3>
<p><strong>Authors:</strong> Cheol Woo Kim, Davin Choo, Tzeh Yuan Neoh, Milind Tambe</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 用Stackelberg安全游戏框架分析AI安全中的激励问题，将监督视为 defender与attacker的策略互动，覆盖训练、评估与部署阶段，对大模型安全的制度性 oversight设计有重要贡献。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07259' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> NAAMSE: Framework for Evolutionary Security Evaluation of Agents</h3>
<p><strong>Authors:</strong> Kunal Pai, Parth Shah, Harshil Patel</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出NAAMSE进化框架评估agent的安全鲁棒性，通过遗传 prompt突变与行为评分迭代发现漏洞，实验验证其能系统放大单样本方法遗漏的脆弱性，对大模型安全的agent评估有重要工具价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07391' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies</h3>
<p><strong>Authors:</strong> Ning Li</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 利用OpenClaw的心跳周期特征设计时间指纹方法，证明Moltbook上的agent emergent行为主要由人类驱动，揭露工业级 bot farming，对大模型安全中的agent自治性与虚假 emergent现象有重要澄清。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07432' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Selective Fine-Tuning for Targeted and Robust Concept Unlearning</h3>
<p><strong>Authors:</strong> Mansi, Avinash Kori, Francesca Toni, Soteris Demetriou</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出TRUST方法解决扩散模型的概念遗忘问题，通过动态神经元定位与选择性微调实现鲁棒反事实解释，符合大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07919' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Moral Sycophancy in Vision Language Models</h3>
<p><strong>Authors:</strong> Shadman Rabby, Md. Hefzul Hossain Papon, Sabbir Ahmed, Nokimul Hasan Arif, A. B. M. Ashikur Rahman, Irfan Ahmad</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 系统研究视觉语言模型的道德谄媚行为，分析其道德对齐漏洞，提出误差率指标评估鲁棒性，符合大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08311' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On Protecting Agentic Systems' Intellectual Property via Watermarking</h3>
<p><strong>Authors:</strong> Liwen Wang, Zongjie Li, Yuchong Xie, Shuai Wang, Dongdong She, Wei Wang, Juergen Rahmel</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对Agentic系统的IP盗窃问题，提出AGENTWM水印框架，通过语义等价的动作序列注入水印，实现可见行动轨迹的可验证性，保护大模型Agent的知识产权，属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08401' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse</h3>
<p><strong>Authors:</strong> Longling Geng, Andy Ouyang, Theodore Wu, Daphne Barretto, Matthew John Hayes, Rachael Cooper, Yuqiao Zeng, Sameer Vijay, Gia Ancone, Ankit Rai, Matthew Wolfman, Patrick Flanagan, Edward Y. Chang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出CausalT5K基准测试大模型的因果推理安全问题（如谄媚、 rung collapse、拒绝校准），为大模型安全对齐提供诊断工具，属于核心安全评估工作。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08939' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Which private attributes do VLMs agree on and predict well?</h3>
<p><strong>Authors:</strong> Olena Hrynenko, Darya Baranouskaya, Alina Elena Baia, Andrea Cavallaro</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 评估视觉语言模型的隐私属性识别能力，分析VLMs与人类的一致性，属于大模型安全与对齐中的隐私安全方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07931' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Deepfake Synthesis vs. Detection: An Uneven Contest</h3>
<p><strong>Authors:</strong> Md. Tarek Hasan, Sanjay Saha, Shaojing Fan, Swakkhar Shatabda, Terence Sim</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 评估Deepfake合成与检测的性能差距，揭示检测模型的局限性，属于大模型安全与对齐中的内容安全方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07986' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning</h3>
<p><strong>Authors:</strong> Hao Tan (Tsinghua University), Jun Lan (Tsinghua University), Senyuan Shi (Tsinghua University), Zichang Tan (Tsinghua University), Zijian Yu (Tsinghua University), Huijia Zhu (Tsinghua University), Weiqiang Wang (Tsinghua University), Jun Wan (Tsinghua University), Zhen Lei (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对AI生成视频的安全检测问题，提出结合感知 pretext 任务的强化学习框架，构建MintVid数据集解决评估瓶颈，实验平衡了感知细粒度与推理准确性，是大模型安全的具体应用创新。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08828' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models</h3>
<p><strong>Authors:</strong> Yankai Yang, Yancheng Long, Hongyang Wei, Wei Chen, Tianke Zhang, Kaiyu Jiang, Haonan Fan, Changyi Liu, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出联合奖励建模框架将CoT内化为视觉奖励模型，结合偏好学习与语言建模提升效率，在图像编辑任务中提升对齐质量，对大模型安全与对齐中的奖励模型设计有重要贡献。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07533' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI</h3>
<p><strong>Authors:</strong> Feiyu Wu, Xu Zheng, Yue Qu, Zhuocheng Wang, Zicheng Feng, Hui Li</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 解决大模型作为Embodied AI规划器的安全性问题，通过逻辑导师与LLM的协作框架VIRF，实现安全计划修正，提升物理部署的可靠性，属于大模型安全与对齐领域。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08373' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent</h3>
<p><strong>Authors:</strong> Yuhang Wang, Feiming Xu, Zheng Lin, Guangyu He, Yuzhe Huang, Haichang Gao, Zhenxing Niu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对个性化AI Agent的安全风险，提出PASB基准测试框架，系统评估攻击面（如用户 prompt 处理、工具使用、内存检索），属于大模型安全与对齐领域。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08412' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Debate is efficient with your time</h3>
<p><strong>Authors:</strong> Jonah Brown-Cohen, Geoffrey Irving, Simon C. Marshall, Ilan Newman, Georgios Piliouras, Mario Szegedy</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 分析辩论在AI安全中的时间效率，提出Debate Query Complexity概念，证明对数级监督即可解决复杂问题，为大模型安全对齐的辩论方法提供理论支撑。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08630' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Scalable Delphi: Large Language Models for Structured Risk Estimation</h3>
<p><strong>Authors:</strong> Tobias Lorenz, Mario Fritz</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 用LLM替代传统Delphi方法进行结构化风险评估，提升安全评估的效率与可扩展性，直接关联大模型安全对齐的风险估计需求。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08889' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning</h3>
<p><strong>Authors:</strong> Dongsheng Chen, Yuxuan Li, Yi Lin, Guanhua Chen, Jiaxin Zhang, Xiangyu Zhao, Lei Ma, Xin Yao, Xuetao Wei</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出ARGOS框架将用户指令与物理属性关联，生成Embodied AI的功能安全需求，解决大模型安全对齐中的物理接地问题。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07007' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h3>
<p><strong>Authors:</strong> Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hongliang He, Zenan Huang, Juyong Jiang, Zhonghui Jiang, Zhenzhong Lan, Chengxi Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Yuan Lu, Yuxin Ma, Xingyu Mou, Zhenxuan Pan, Kaida Qiu, Yuji Ren, Jianfeng Tan, Yiding Tian, Zian Wang, Lanning Wei, Tao Wu, Yipeng Xing, Wentao Ye, Liangyu Zha, Tianze Zhang, Xiaolu Zhang, Junbo Zhao, Da Zheng, Hao Zhong, Wanli Zhong, Jun Zhou, Junlin Zhou, Liwang Zhu, Muzhi Zhu, Yihong Zhuang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出LLaDA2.1通过令牌编辑加速文本扩散模型，结合RL框架提升推理速度与质量，属于大模型新技术中的扩散LLM优化。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08676' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Efficient and Stable Reinforcement Learning for Diffusion Language Models</h3>
<p><strong>Authors:</strong> Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对扩散语言模型的RL训练效率与稳定性问题，提出STP框架（空间+时间剪枝），显著提升训练效率与准确性，属于大模型新技术中的diffusion LLM方向。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion</h3>
<p><strong>Authors:</strong> Haodong Li, Shaoteng Liu, Zhe Lin, Manmohan Chandraker</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对自回归视频扩散模型因训练时长有限导致的“训练-测试时长 gap”（测试时长远超训练时视觉退化）问题，提出Rolling Sink训练-free方法。基于Self Forcing，该方法能在测试时将视频合成扩展至超长时间（如5-30分钟@16 FPS），保持主体一致、颜色稳定、结构连贯和运动流畅，优于现有基线。
Score: 8.5
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07775' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FADE: Selective Forgetting via Sparse LoRA and Self-Distillation</h3>
<p><strong>Authors:</strong> Carolina R. Kelsch, Leonardo S. B. Pereira, Natnael Mola, Luis H. Arribas, Juan C. S. M. Avedillo</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对扩散模型的选择性遗忘问题，提出FADE方法，结合稀疏LoRA与自蒸馏，实现轻量级、可控的概念擦除，提升扩散模型的安全性与适应性。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07058' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation</h3>
<p><strong>Authors:</strong> Xiaofeng Tan, Wanjiang Weng, Haodong Lei, Hongsong Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对扩散模型的运动生成，提出step-aware微调方法，解决递归依赖问题，提升训练速度和内存效率，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07967' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models</h3>
<p><strong>Authors:</strong> Tong Zhang, Ru Zhang, Jianyi Liu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对扩散模型中的风格与内容解耦，提出训练-free的风格擦除框架，解决风格版权问题，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08059' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PISCO: Precise Video Instance Insertion with Sparse Control</h3>
<p><strong>Authors:</strong> Xiangbo Gao, Renjie Li, Xinghao Chen, Yuheng Wu, Suofei Feng, Qing Yin, Zhengzhong Tu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对视频实例插入任务，提出稀疏控制的扩散模型框架，提升插入精度和场景一致性，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08277' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers</h3>
<p><strong>Authors:</strong> Shuo Zhang, Wenzhuo Wu, Huayu Zhang, Jiarong Cheng, Xianghao Zang, Chao Ban, Hao Sun, Zhongjiang He, Tianwei Cao, Kongming Liang, Zhanyu Ma</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对几何图像编辑，提出扩散Transformer的上下文生成框架，提升编辑精度和真实感，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08388' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Riemannian MeanFlow</h3>
<p><strong>Authors:</strong> Dongyeop Woo (Carnegie Mellon University), Marta Skreta (Carnegie Mellon University), Seonghyun Park (Carnegie Mellon University), Sungsoo Ahn (Carnegie Mellon University), Kirill Neklyudov (Carnegie Mellon University)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Riemannian MeanFlow（RMF）框架，直接在流形上学习流映射，解决扩散/流模型推理时需多次网络评估的计算瓶颈，在DNA设计和蛋白质生成任务上实现与现有方法相当的样本质量，且减少10倍函数评估，属于大模型新技术的重要创新。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07744' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Kinetic-Energy Perspective of Flow Matching</h3>
<p><strong>Authors:</strong> Ziyun Li (EPFL), Huancheng Hu (EPFL), Soon Hoe Lim (EPFL), Xuyu Li (EPFL), Fei Gao (EPFL), Enmao Diao (EPFL), Zezhen Ding (EPFL), Michalis Vazirgiannis (EPFL), Henrik Bostrom (EPFL)</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 从经典力学动能角度提出Kinetic Path Energy（KPE）诊断指标，揭示生成质量与轨迹能量的关联及记忆化问题，进而提出Kinetic Trajectory Shaping（KTS）方法提升生成质量，为流匹配模型提供了新的理论视角和优化方向，属于大模型新技术的重要进展。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07928' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration</h3>
<p><strong>Authors:</strong> Manh Cuong Dao, Quang Hung Pham, Phi Le Nguyen, Thao Nguyen Truong, Bryan Kian Hsiang Low, Trong Nghia Hoang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 受扩散模型启发重构Transformer的特征变换块为概率映射，实现表示不确定性的原则性传播，提升预训练Transformer的不确定性校准与预测准确性，对大模型新技术中的扩散与Transformer结合有重要探索。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08920' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents</h3>
<p><strong>Authors:</strong> Jiahao Zhao, Shaoxuan Xu, Zhongxiang Sun, Fengqi Zhu, Jingyang Ou, Yuling Shi, Chongxuan Li, Xiao Zhang, Jun Xu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 针对扩散大模型（dLLM）的agent能力挑战，设计两阶段训练 pipeline提升信息检索与推理能力，提出Parallel-Reasoning and Acting范式减少延迟，对大模型新技术中的dLLM agent应用有重要优化。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07035' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Action-to-Action Flow Matching</h3>
<p><strong>Authors:</strong> Jindou Jia, Gen Li, Xiangyu Chen, Tuo An, Yuxuan Hu, Jingliang Li, Xinying Guo, Jianfei Yang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Action-to-Action flow matching方法，将扩散政策的随机噪声初始化改为基于历史动作的知情初始化，减少迭代去噪步骤并提升推理速度，是扩散模型在机器人动作生成中的创新应用，符合大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07322' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation</h3>
<p><strong>Authors:</strong> Yuxuan Hu, Xiangyu Chen, Chuhao Zhou, Yuxi Liu, Gen Li, Jindou Jia, Jianfei Yang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Trace-Focused Diffusion Policy，通过执行历史轨迹条件化动作生成，解决长 horizon机器人操作中的多模态动作歧义问题，改进扩散模型的时间一致性和鲁棒性，属于大模型新技术方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07388' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PEGAsus: 3D Personalization of Geometry and Appearance</h3>
<p><strong>Authors:</strong> Jingyu Hu, Bin Hu, Ka-Hei Hui, Haipeng Li, Zhengzhe Liu, Daniel Cohen-Or, Chi-Wing Fu</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出3D形状个性化生成框架，提取几何和外观属性，支持跨类别生成，属于大模型新技术中的3D生成方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08198' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs</h3>
<p><strong>Authors:</strong> Junsu Seo</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 用前向SPDE建模扩散模型的分数近似误差，分析生成模型鲁棒性，属于大模型新技术中的扩散模型理论研究。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08579' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning</h3>
<p><strong>Authors:</strong> Constant Bourdrez, Alexandre Vérine, Olivier Cappé</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 用逆强化学习优化扩散模型采样策略，提升生成质量与效率，属于大模型新技术中的扩散模型采样优化。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08689' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration</h3>
<p><strong>Authors:</strong> Qi Guo, Jianing Wang, Deyang Kong, Xiangyu Xi, Jianfei Zhang, Yi Lu, Jingang Wang, Wei Wang, Shikun Zhang, Wei Ye</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出大纲引导的平行思维探索方法，通过迭代RL优化推理路径，解决信息饱和问题，符合大模型新技术方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08344' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition</h3>
<p><strong>Authors:</strong> Pierre-Louis Favreau, Jean-Pierre Lo, Clement Guiguet, Charles Simon-Meunier, Nicolas Dehandschoewercker, Allen G. Roush, Judah Goldfeder, Ravid Shwartz-Ziv</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出Minitap多智能体系统解决AndroidWorld任务，实现100%准确率，针对GUI相关的智能体任务分解，符合多模态智能体研究方向。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07787' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making</h3>
<p><strong>Authors:</strong> Ruoyu Chen, Shangquan Sun, Xiaoqing Guo, Sanyi Zhang, Kangwei Liu, Shiming Liu, Zhangcheng Wang, Qunli Zhang, Hua Zhang, Xiaochun Cao</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出基于子集归因约束的先验对齐训练方法，结合深度学习可解释性（归因约束）与多模态智能体（在MLLM-based GUI agent模型上验证），解决模型依赖shortcut而非真实证据的问题，对多模态智能体的可靠决策有重要意义。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07008' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense</h3>
<p><strong>Authors:</strong> Jiacheng Liu, Yaxin Luo, Jiacheng Cui, Xinyi Shang, Xiaohan Zhao, Zhiqiang Shen</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出下一代CAPTCHA框架利用人类与GUI agent的认知差距设计动态任务，通过可扩展生成 pipeline提供大量实例，重新建立人类与agent的区分，对多模态智能体的安全防御有重要价值。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09012' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ANCHOR: Branch-Point Data Generation for GUI Agents</h3>
<p><strong>Authors:</strong> Jinbiao Wei, Yilun Zhao, Kangqi Ni, Arman Cohan</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出ANCHOR框架从种子演示生成GUI agent的交互数据，通过分支点识别与任务变异扩展轨迹，提升agent的泛化性能，对多模态智能体的训练数据生成有重要贡献。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07153' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents</h3>
<p><strong>Authors:</strong> Kaijie Zhu, Yuzhou Nie, Yijiang Li, Yiming Huang, Jialian Wu, Jiang Liu, Ximeng Sun, Zhenfei Yin, Lun Wang, Zicheng Liu, Emad Barsoum, William Yang Wang, Wenbo Guo</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出TermiGen框架生成高保真的终端环境与鲁棒轨迹，通过多agent refinement与错误注入提升数据质量，训练的agent在TerminalBench上达到开源最优，对多模态智能体中的终端agent训练有重要贡献。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07274' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions</h3>
<p><strong>Authors:</strong> Junyu Feng, Binxiao Xu, Jiayi Chen, Mengyu Dai, Cenyang Wu, Haodong Li, Bohan Zeng, Yunliu Xie, Hao Liang, Ming Lu, Wentao Zhang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出双图层混合记忆的多模态智能体框架，解决长期个性化交互中的记忆管理问题，实验验证性能显著优于基线，符合多模态智能体研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07624' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval</h3>
<p><strong>Authors:</strong> Teng Wang, Rong Shan, Jianghao Lin, Junjie Wu, Tianyi Xu, Jianping Zhang, Wenteng Chen, Changwang Zhang, Zhaoxiang Wang, Weinan Zhang, Jun Wang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 将组合图像检索的Agentic规划转化为轨迹优化问题，通过离线最优轨迹引导在线推理，提升多模态检索的准确性与泛化性，属于多模态智能体的规划方法。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.08603' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GEBench: Benchmarking Image Generation Models as GUI Environments</h3>
<p><strong>Authors:</strong> Haodong Li, Jingwei Wu, Quan Sun, Guopeng Li, Juanxi Tian, Huanyu Zhang, Yanlin Lai, Ruichuan An, Hongbo Peng, Yuhong Dai, Chenxi Li, Chunmei Qing, Jia Wang, Ziyang Meng, Zheng Ge, Xiangyu Zhang, Daxin Jiang</p>
<p><strong>Published:</strong> 2026-02-10</p>
<p><strong>Reason:</strong> 提出GEBench基准测试GUI环境中的图像生成模型，聚焦状态转移与时间一致性，直接关联多模态智能体中的GUI Agent评估需求。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09007' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>