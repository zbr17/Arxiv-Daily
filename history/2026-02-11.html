<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-11</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>多模态智能体</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型新技术</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >深度学习理论</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-11</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-11 13:39:52</p>
<p>已自动阅读了 369 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：197354</p>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Code2World: A GUI World Model via Renderable Code Generation</h3>
<p><strong>Authors:</strong> Yuhao Zheng, Li'an Zhong, Yi Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu, Linyuan Lv, Philip Torr, Kevin Qinghong Lin</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对GUI世界模型的视觉保真与结构可控性难题，提出可渲染代码生成框架，构建AndroidCode数据集并通过渲染感知强化学习优化，显著提升GUI代理导航性能，直接对应多模态智能体的GUI World模型研究。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09856' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UI-Venus-1.5 Technical Report</h3>
<p><strong>Authors:</strong> Veuns-Team, Changlong Gao, Zhangxuan Gu, Yulin Liu, Xinyu Qiu, Shuheng Shen, Yue Wen, Tianyu Xia, Zhenyu Xu, Zhengwen Zeng, Beitong Zhou, Xingran Zhou, Weizhi Chen, Sunhao Dai, Jingya Dou, Yichen Gong, Yuan Guo, Zhenlin Guo, Feng Li, Qian Li, Jinzhen Lin, Yuqi Zhou, Linchao Zhu, Liang Chen, Zhenyu Guo, Changhua Meng, Weiqiang Wang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出统一端到端GUI智能体UI-Venus-1.5，引入中训练阶段、在线强化学习和模型合并等技术，在ScreenSpot-Pro、VenusBench-GD等基准测试中取得最优性能，对多模态智能体的实际应用和性能提升有重要价值。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09082' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures</h3>
<p><strong>Authors:</strong> Yuxi Wang, Wenqi Ouyang, Tianyi Wei, Yi Dong, Zhiqi Shen, Xingang Pan</p>
<p><strong>Published:</strong> [PAPER_START]</p>
<p><strong>Reason:</strong> 针对GUI自动化中的核心规划问题，提出树形结构轨迹组织与多智能体协作探索框架，结合TreeCUA-DPO方法提升GUI规划能力，直接对应多模态智能体的GUI Agent研究方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09662' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ANCHOR: Branch-Point Data Generation for GUI Agents</h3>
<p><strong>Authors:</strong> Jinbiao Wei, Yilun Zhao, Kangqi Ni, Arman Cohan</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出ANCHOR框架生成GUI代理高质量交互数据，解决现有数据问题，在基准验证性能提升，属多模态智能体中的GUI Agent方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07153' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation</h3>
<p><strong>Authors:</strong> Jingqi Zhou, Sheng Wang, DeZhao Deng, Junwen Lu, Junwei Su, Qintong Li, Jiahui Gao, Hao Wu, Jiyue Jiang, Lingpeng Kong, Chuan Wu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出工具驱动的多模态智能体自我配置范式，将任务执行与自我调整统一到行动空间，实现代理从被动执行到主动管理，解决静态配置的泛化与优化问题，属于多模态智能体的突破性改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07883' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI</h3>
<p><strong>Authors:</strong> Feiyu Wu, Xu Zheng, Yue Qu, Zhuocheng Wang, Zicheng Feng, Hui Li</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出神经符号架构VIRF，结合逻辑导师与大模型规划器，实现可验证的安全具身代理，解决大模型规划的 stochastic 问题，属于多模态智能体的安全与可解释性研究。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.08373' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition</h3>
<p><strong>Authors:</strong> Yiming Yang, Zhuoyuan Li, Fanxiang Zeng, Hao Fu, Yue Liu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出PRISM多代理推理框架，通过增益分解优化探索、信息与聚合，提升多代理推理的性能与效率，解决现有方法的 heuristic 问题，属于多模态智能体的推理改进。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.08586' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GEBench: Benchmarking Image Generation Models as GUI Environments</h3>
<p><strong>Authors:</strong> Haodong Li, Jingwei Wu, Quan Sun, Guopeng Li, Juanxi Tian, Huanyu Zhang, Yanlin Lai, Ruichuan An, Hongbo Peng, Yuhong Dai, Chenxi Li, Chunmei Qing, Jia Wang, Ziyang Meng, Zheng Ge, Xiangyu Zhang, Daxin Jiang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 构建GEBench基准评估GUI生成模型的动态交互和时序一致性，针对多模态智能体中的GUI Agent场景提供关键评估工具
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09007' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation</h3>
<p><strong>Authors:</strong> Yucheng Hu, Jianke Zhang, Yuanfei Luo, Yanjiang Guo, Xiaoyu Chen, Xinshu Sun, Kun Feng, Qingzhou Lu, Sheng Chen, Yangang Zhang, Wei Li, Jianyu Chen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出BagelVLA整合语言规划、视觉预测和动作生成，提升长程操作的多模态智能体性能，属于多模态智能体的关键技术突破
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.09849' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MemFly: On-the-Fly Memory Optimization via Information Bottleneck</h3>
<p><strong>Authors:</strong> Zhenyuan Zhang, Xianzhang Jia, Zhiqin Yang, Zhenbo Song, Wei Xue, Sirui Han, Yike Guo</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 基于信息瓶颈理论优化大模型代理的长期记忆，构建分层记忆结构与混合检索机制，解决记忆冗余与检索精度的矛盾，提升复杂任务处理能力，属于多模态智能体的记忆系统关键优化。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.07885' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis</h3>
<p><strong>Authors:</strong> Haoyu Jia, Kento Kawaharazuka, Kei Okada</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出结构上下文模型形式化多模态智能体设计，解决现有代理研究的碎片化问题，通过语义动态分析支持系统设计迭代，属于多模态智能体的理论基础研究。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.08276' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval</h3>
<p><strong>Authors:</strong> Xin Zhang, Kailai Yang, Chenyue Li, Hao Li, Qiyu Wei, Jun'ichi Tsujii, Sophia Ananiadou</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出跨代理记忆范式的快速对齐框架，通过生成子图检索提升记忆检索的灵活性与效率，解决异质记忆系统的整合问题，属于多模态智能体的记忆系统改进。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.08369' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery</h3>
<p><strong>Authors:</strong> Shiyang Feng, Runmin Ma, Xiangchao Yan, Yue Fan, Yusong Hu, Songtao Huang, Shuaiyu Zhang, Zongsheng Cao, Tianshuo Peng, Jiakang Yuan, Zijie Guo, Zhijie Zhong, Shangheng Du, Weida Wang, Jinxin Shi, Yuhao Zhou, Xiaohan He, Zhiyin Yu, Fangchen Yu, Qihao Zheng, Jiamin Wu, Mianxin Liu, Chi Zhang, Shaowei Hou, Shuya Li, Yankai Jiang, Wenjie Lou, Lilong Wang, Zifu Wang, Jiong Wang, Wanghan Xu, Yue Deng, Dongrui Liu, Yiheng Wang, Wenlong Zhang, Fenghua Ling, Shufei Zhang, Xiaosong Wang, Shuangjia Zheng, Xun Huang, Siqi Sun, Shuyue Hu, Peng Ye, Chunfeng Song, Bin Wang, Conghui He, Yihao Liu, Xin Li, Qibin Hou, Tao Chen, Xiangyu Yue, Bin Wang, Liang He, Dahua Lin, Bowen Zhou, Bo Zhang, Lei Bai</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出统一的多模态智能体框架用于长程科学发现，整合生成、验证和演化子系统，推动多模态智能体在复杂任务中的应用
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.08990' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide</h3>
<p><strong>Authors:</strong> Hossam Amer, Rezaul Karim, Ali Pourranjbar, Weiwei Zhang, Walid Ahmed, Boxing Chen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 系统分析LLM分布式混合并行策略，提供设计指南与实证案例，对高效LLM训练的系统优化有重要参考价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09109' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Effective MoE-based LLM Compression by Exploiting Heterogeneous Inter-Group Experts Routing Frequency and Information Density</h3>
<p><strong>Authors:</strong> Zhendong Mi, Yixiao Chen, Pu Zhao, Xiaodong Yu, Hao Wang, Yanzhi Wang, Shaoyi Huang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出RFID-MoE框架，利用MoE专家的路由频率与信息密度自适应压缩，显著提升压缩后模型性能，属于高效大模型训练关键方法。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09316' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> WildCat: Near-Linear Attention in Theory and Practice</h3>
<p><strong>Authors:</strong> Tobias Schr\"oder, Lester Mackey</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出WildCat近线性注意力方法，通过谱精确子采样解决注意力二次复杂度问题，理论有误差保证，实践在多任务验证有效，对高效大模型注意力优化有重要意义。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.10056' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Data Science and Technology Towards AGI Part I: Tiered Data Management</h3>
<p><strong>Authors:</strong> Yudong Wang, Zixuan Fu, Hengyu Zhao, Chen Zhao, Chuyue Zhou, Xinle Lin, Hongya Lyu, Shuaikang Xue, Yi Yi, Yingjiao Wang, Zhi Zheng, Yuzhou Zhang, Jie Zhou, Chaojun Xiao, Xu Han, Zhiyuan Liu, Maosong Sun</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出L0-L4分层数据管理框架，优化大模型训练的数据利用，显著提升训练效率，属于高效大模型训练的核心进展
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09003' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging</h3>
<p><strong>Authors:</strong> Xinyu Wang, Ke Deng, Fei Dou, Jinbo Bi, Jin Lu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对模型合并中的层异质性问题，提出数据无关的层自适应缩放方法LARV，提升多任务模型的合并性能，对高效大模型训练与推理中的模型合并技术有显著改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09413' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions</h3>
<p><strong>Authors:</strong> Lin Chen, Xiaoke Zhao, Kun Ding, Weiwei Feng, Changtao Miao, Zili Wang, Wenxuan Guo, Ying Wang, Kaiyuan Zheng, Bo Zhang, Zhe Li, Shiming Xiang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对多模态大模型的蒸馏问题，提出通过token交互的方法Beyond Next-Token Alignment，提升小模型性能（甚至超过更大模型），对高效大模型训练与推理中的知识蒸馏技术有显著改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09483' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Train Less, Infer Faster: Efficient Model Finetuning and Compression via Structured Sparsity</h3>
<p><strong>Authors:</strong> Jonathan Svirsky, Yehonathan Refael, Ofir Lindenbaum</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出基于结构化稀疏的高效微调方法，减少训练参数与推理时间并保持精度，对LLM高效适配有实践价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09169' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning</h3>
<p><strong>Authors:</strong> Yifei Cheng, Xianglin Yang, Guoxia Wang, Chao Huang, Fei Ma, Dianhai Yu, Xiaochun Cao, Li Shen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 结合稀疏层与SAM优化器实现高效微调，减少计算成本并保持性能，对LLM资源受限场景的高效适配有实践价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09395' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beware of the Batch Size: Hyperparameter Bias in Evaluating LoRA</h3>
<p><strong>Authors:</strong> Sangyoon Lee, Jaeho Lee</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究LoRA微调中batch size这一关键超参数的影响，解决现有LoRA变体评估中的矛盾，提出高效batch size调优策略，对大模型高效微调的可靠评估有重要价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09492' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rollout-Training Co-Design for Efficient LLM-Based Multi-Agent Reinforcement Learning</h3>
<p><strong>Authors:</strong> Zhida Jiang, Zhaolong Xing, Jiawei Lu, Yipei Niu, Qingyuan Sang, Liangxu Zhang, Wenquan Dai, Junhua Shu, Jiaxing Wang, Qiangyu Pei, Qiong Chen, Xinyu Liu, Fangming Liu, Ai Han, Zhen Chen, Ke Zhang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出FlexMARL框架，优化LLM-based多智能体强化学习的rollout与训练流程，解决同步壁垒、负载不均等问题，实验显示速度提升7.3倍，硬件利用率提升5.6倍，对大模型多智能体训练的高效性至关重要。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09578' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Model soups need only one ingredient</h3>
<p><strong>Authors:</strong> Alireza Abdollahpoorrostam, Nikolaos Dimitriadis, Adam Hazimeh, Pascal Frossard</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出单checkpoint的MonoSoup方法，通过SVD分解权重并自动重加权，平衡in-distribution与out-of-distribution性能，比多checkpoint方法更高效，对大模型的鲁棒性与部署效率有重要意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09689' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Less is More: The LLM Scaling Paradox in Context Compression</h3>
<p><strong>Authors:</strong> Ruishan Guo, Yibing Liu, Guoxin Ma, Yan Wang, Yueyang Zhang, Long Xia, Kecheng Chen, Zhiyuan Sun, Daiting Shi</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 发现大模型在上下文压缩中的Size-Fidelity Paradox（模型越大，重构上下文的忠实度越低），分析了知识覆盖与语义漂移的原因，对大模型上下文高效处理的优化有重要指导意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09789' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization</h3>
<p><strong>Authors:</strong> Xinchen Han, Hossam Afifi, Michel Marot, Xilu Wang, Lu Yin</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对LLM生成冗长CoT导致的高计算成本与延迟问题，提出FGO算法实现有效CoT压缩，同时解决GRPO的数据利用率低和熵崩溃缺陷，在多个推理基准上验证了压缩效果与性能保持，对高效大模型推理有应用价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.10048' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs</h3>
<p><strong>Authors:</strong> Pengrui Han, Xueqiang Xu, Keyang Xuan, Peiyang Song, Siru Ouyang, Runchu Tian, Yuqing Jiang, Cheng Qian, Pengcheng Jiang, Jiashuo Sun, Junxia Cui, Ming Zhong, Ge Liu, Jiawei Han, Jiaxuan You</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出Steer2Adapt框架动态组合转向向量，实现LLM高效适应，在推理与安全任务提升8.2%，对高效大模型推理时适应有价值。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07276' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge</h3>
<p><strong>Authors:</strong> Xin Wang, Hualin Zhou, Sheng Guang Wang, Ting Dang, Yu Zhang, Hong Jia, Tao Gu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对边缘设备视觉语言模型的资源约束与分布偏移问题，提出轻量化量化自适应框架，结合模态感知量化与无梯度测试时自适应，提升边缘部署效率与鲁棒性，属于高效大模型训练与推理的关键应用。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.07849' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Does Your Reasoning Model Implicitly Know When to Stop Thinking?</h3>
<p><strong>Authors:</strong> Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang, Zhixia Zhang, Hongyan Xie, Songshi Liang, Zehao Chen, Xuefeng Xiao, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 发现大模型推理中隐含的停止机制，提出SAGE范式释放高效推理潜力，结合RL优化推理策略，提升推理效率与准确性，解决长链推理的冗余问题，属于高效大模型训练与推理的重要进展。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08354' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning</h3>
<p><strong>Authors:</strong> Xinhai Sun</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出强化推理策略，利用模型不确定性选择性触发二次推理，提升推理准确性与效率，解决贪心推理的 premature commitment问题，属于高效大模型训练与推理的推理时优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08520' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute</h3>
<p><strong>Authors:</strong> Chen Jin, Ryutaro Tanno, Tom Diethe, Philip Teare</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出置信度引导的自优化方法减少LLM推理时的计算量，提升推理效率，属于高效大模型推理的关键优化策略
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.08948' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Benchmarking the Energy Savings with Speculative Decoding Strategies</h3>
<p><strong>Authors:</strong> Rohit Dutta, Paramita Koley, Soham Poddar, Janardan Misra, Sanjay Podder, Naveen Balani, Saptarshi Ghosh, Niloy Ganguly</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 基准测试推测解码策略的能量消耗，分析模型、策略与数据集对能耗的影响，为LLM高效推理的能量优化提供依据。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09113' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond Student: An Asymmetric Network for Neural Network Inheritance</h3>
<p><strong>Authors:</strong> Yiyun Zhou, Jingwei Shi, Mingjing Xu, Zhonghua Jiang, Jingyuan Chen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出InherNet模型继承方法，通过非对称低秩分解教师权重构建轻量网络，平衡压缩效率与知识继承，实验显示比同规模学生模型性能更优，为高效模型压缩提供新方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.09509' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation</h3>
<p><strong>Authors:</strong> Pei-Chi Pan, Yingbin Liang, Sen Lin</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 系统分析RL-based LLM推理的奖励建模问题，提出RARL框架，对大模型对齐与安全（推理可靠性）有重要贡献。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Emergent Misalignment is Easy, Narrow Misalignment is Hard</h3>
<p><strong>Authors:</strong> Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究大语言模型的涌现错位现象，通过线性表示分析归纳偏差对泛化的影响，揭示通用错位的稳定性与效率优势，为大模型对齐与安全提供理论支撑，属于大模型安全与对齐的核心研究。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07852' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs</h3>
<p><strong>Authors:</strong> Jingyi Wang, Fei Li, Rujie Liu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对大视觉语言模型的幻觉问题，提出训练-free的注意力干预和logits增强方法，显著减少幻觉同时保持生成准确性，对大模型安全与对齐中的幻觉缓解有重要效果。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09521' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding</h3>
<p><strong>Authors:</strong> Zhaoxu Li, Chenqi Kong, Peijun Bao, Song Xia, Yi Tu, Yi Yu, Xinghao Jiang, Xudong Jiang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对大视觉语言模型的幻觉问题，提出知识稳定性量化与动态可靠知识利用方法，训练-free且有效缓解幻觉，属于大模型安全与对齐的关键问题解决。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09825' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Biases in the Blind Spot: Detecting What LLMs Fail to Mention</h3>
<p><strong>Authors:</strong> Iv\'an Arcuschin, David Chanin, Adri\`a Garriga-Alonso, Oana-Maria Camburu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出自动化pipeline检测LLM未言语偏见，无需预定义类别，在多任务发现新偏见，对大模型安全与对齐中的偏见检测有价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.10117' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities</h3>
<p><strong>Authors:</strong> Majid Ghasemi, Mark Crowley</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究社会强化学习中的目标错位问题，提出认知源对齐方法解决多数谄媚导致的目标 decoupling，证明其收敛性并通过实验验证，属于大模型安全与对齐的理论突破。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08092' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection</h3>
<p><strong>Authors:</strong> Ziwei Wang, Yuanhe Zhang, Jing Chen, Zhenhong Zhou, Ruichao Liang, Ruiying Du, Ju Jia, Cong Wu, Yang Liu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究大模型推理的资源耗尽攻击，通过递归熵引导反事实利用与反思，揭示推理过程的安全漏洞，提出新的攻击范式与防御思路，属于大模型安全与对齐的重要研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08214' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment</h3>
<p><strong>Authors:</strong> Igor Santos-Grueiro</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 分析对齐评估中的 regime leakage问题，提出 regime-blind训练减少代理的环境区分能力，解决评估与部署的行为分歧，属于大模型安全与对齐的理论研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval</h3>
<p><strong>Authors:</strong> Teng Wang, Rong Shan, Jianghao Lin, Junjie Wu, Tianyi Xu, Jianping Zhang, Wenteng Chen, Changwang Zhang, Zhaoxiang Wang, Weinan Zhang, Jun Wang</p>
<p><strong>Published:</strong> 2026-</p>
<p><strong>Reason:</strong> 研究AI安全辩论的查询复杂性，建立PSPACE/poly与对数查询的联系，连接电路复杂性，为AI安全提供理论支撑
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08630' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scalable Delphi: Large Language Models for Structured Risk Estimation</h3>
<p><strong>Authors:</strong> Tobias Lorenz, Mario Fritz</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 用LLM模拟Delphi方法实现结构化风险估计，提升风险评估的 scalability，为大模型在安全领域的应用提供实践方案
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08889' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse</h3>
<p><strong>Authors:</strong> Longling Geng, Andy Ouyang, Theodore Wu, Daphne Barretto, Matthew John Hayes, Rachael Cooper, Yuqiao Zeng, Sameer Vijay, Gia Ancone, Ankit Rai, Matthew Wolfman, Patrick Flanagan, Edward Y. Chang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 构建CausalT5K基准诊断大模型因果推理的失败模式（如 sycophancy、rung collapse），为可信推理和安全对齐提供评估工具
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08939' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models</h3>
<p><strong>Authors:</strong> Chenyu Wang, Tianle Chen, H. M. Sabbir Ahmad, Kayhan Batmanghelich, Wenchao Li</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 构建视觉语言模型的不确定性量化基准VLM-UQBench，评估模态特异性和跨模态不确定性，发现现有方法在对齐人类偏好和检测微妙歧义上的不足，对大模型安全与对齐中的不确定性管理有重要参考价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09214' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge</h3>
<p><strong>Authors:</strong> Zhikai Li, Jiatong Li, Xuewen Liu, Wangbo Zhao, Pan Du, Kaicheng Zhou, Qingyi Gu, Yang You, Zhen Dong, Kurt Keutzer</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出可靠高效的视觉生成偏好评估框架K-Sort Eval，通过后验校正和动态匹配提升VLM作为评判的可靠性，对大模型安全与对齐中的人类偏好对齐评估有重要意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09411' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs</h3>
<p><strong>Authors:</strong> James Burgess, Rameen Abdal, Dan Stoddart, Sergey Tulyakov, Serena Yeung-Levy, Kuan-Chieh Jackson Wang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出用少量标签训练VLM检测图像生成中的 artifacts，解决现有方法依赖大量标注的问题，提升检测可靠性，对大模型安全与对齐中的生成质量控制有重要意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09475' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OSI: One-step Inversion Excels in Extracting Diffusion Watermarks</h3>
<p><strong>Authors:</strong> Yuwei Chen, Zhenliang He, Jia Tang, Meina Kan, Shiguang Shan</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出单步反转方法OSI，解决扩散模型水印提取的计算昂贵问题，提升提取效率和准确性，对大模型安全与对齐中的版权保护有重要应用价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09494' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Schr\"oMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schr\"odinger Bridge Problem</h3>
<p><strong>Authors:</strong> Ziqiang Shi, Rujie Liu, Shanshan Yu, Satoshi Munakata, Koichi Shirahata</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出SchröMind框架，通过解决薛定谔桥问题减少多模态大模型的幻觉，保持模型原有能力的同时提升生成准确性，对大模型安全与对齐中的幻觉缓解有理论和实践价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09528' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment</h3>
<p><strong>Authors:</strong> Bohan Fu, Guanyi Qin, Fazhan Zhang, Zihao Huang, Mingxuan Li, Runze Hu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出DR.Experts框架，通过失真先验提升盲图像质量评估的准确性，解决现有方法与人类主观判断的错位问题，对大模型安全与对齐中的图像质量控制有重要意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09531' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination</h3>
<p><strong>Authors:</strong> Ziqiang Shi, Rujie Liu, Shanshan Yu, Satoshi Munakata, Koichi Shirahata</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出Scalpel方法，通过混合高斯桥对齐注意力激活流形，减少多模态大模型的幻觉，提升生成的一致性，对大模型安全与对齐中的幻觉缓解有重要效果。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09541' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Towards Poisoning Robustness Certification for Natural Language Generation</h3>
<p><strong>Authors:</strong> Mihnea Ghitu, Matthew Wicker</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出Targeted Partition Aggregation算法，首次为自然语言生成模型提供投毒鲁棒性认证，支持多轮生成场景，对大模型在安全敏感领域的部署有关键价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09757' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions</h3>
<p><strong>Authors:</strong> J Rosser, Robert Kirk, Edward Grefenstette, Jakob Foerster, Laura Ruis</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出Infusion框架，用影响函数小幅度修改训练数据来诱导模型行为，实验显示跨架构有效，对大模型安全中的数据投毒防御与模型行为可控性有重要启示。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.09987' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective</h3>
<p><strong>Authors:</strong> Cheol Woo Kim, Davin Choo, Tzeh Yuan Neoh, Milind Tambe</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 从Stackelberg游戏视角提出AI安全框架，解决动态激励与资源问题，覆盖多阶段安全场景，对大模型安全与对齐有理论意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.07259' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI</h3>
<p><strong>Authors:</strong> Akinori Maeda, Yuto Sekiya, Sota Sugimura, Tomoya Asai, Yu Tsuda, Kohei Ikeda, Hiroshi Fujii, Kohei Watanabe</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出用户主权的隐私保护个性化AI代理框架，支持多粒度数据共享控制，解决大模型个性化中的隐私泄露问题，实现隐私与个性化的平衡，属于大模型安全与对齐的应用研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08268' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Moral Sycophancy in Vision Language Models</h3>
<p><strong>Authors:</strong> Shadman Rabby, Md. Hefzul Hossain Papon, Sabbir Ahmed, Nokimul Hasan Arif, A. B. M. Ashikur Rahman, Irfan Ahmad</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 系统研究视觉语言模型的道德谄媚行为，分析模型在用户偏见下的道德判断偏移，提出评估指标与改进方向，属于大模型安全与对齐的伦理研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08311' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On Protecting Agentic Systems' Intellectual Property via Watermarking</h3>
<p><strong>Authors:</strong> Liwen Wang, Zongjie Li, Yuchong Xie, Shuai Wang, Dongdong She, Wei Wang, Juergen Rahmel</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出AGENTWM代理水印框架，通过功能等价的行动序列注入水印，保护代理的知识产权，解决代理能力窃取问题，属于大模型安全与对齐的应用研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08401' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent</h3>
<p><strong>Authors:</strong> Yuhang Wang, Feiming Xu, Zheng Lin, Guangyu He, Yuzhe Huang, Haichang Gao, Zhenxing Niu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究个性化代理的攻击面，提出PASB基准框架，形式化攻击模型并评估防御效果，属于大模型安全与对齐的攻击与防御研究。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08412' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers</h3>
<p><strong>Authors:</strong> Aditya Gulati, Nuria Oliver</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 分析用户对聊天机器人信任的形成机制，区分心理信任与规范可信度，对大模型用户对齐和信任校准有实践意义
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Belief Offloading in Human-AI Interaction</h3>
<p><strong>Authors:</strong> Rose E. Guingrich, Dvija Mehta, Umang Bhatt</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究人类将信仰形成过程卸载给AI的现象，分析其对行为和信仰系统的影响，为大模型与人类互动的安全对齐提供理论参考
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08754' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning</h3>
<p><strong>Authors:</strong> Andrés Holgado-Sánchez, Peter Vamplew, Richard Dazeley, Sascha Ossowski, Holger Billhardt</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出基于偏好的多目标RL方法学习社会价值系统，为大模型的价值对齐和社会适配性研究提供新思路
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.08835' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Priority-Aware Shapley Value</h3>
<p><strong>Authors:</strong> Kiljae Lee, Ziqi Liu, Weijing Tang, Yuan Zhang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 扩展Shapley值纳入优先级约束与权重，解决传统Shapley的互换性假设问题，提升数据估值与特征归因准确性，属于可解释性核心方法创新。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.09326' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints</h3>
<p><strong>Authors:</strong> Andres Saurez, Yousung Lee, Dongsoo Har</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 从Transformer的线性接口架构约束出发，证明线性可解释性方法（如线性探针、稀疏自编码器）的有效性，提出Invariant Subspace Necessity定理，为深度学习可解释性提供重要理论支撑。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.09783' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path</h3>
<p><strong>Authors:</strong> Andres Saurez, Neha Sengar, Dongsoo Har</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出Circuit Fingerprint假设，将Transformer的电路发现与激活引导统一到几何原理，实验显示通过几何对齐可实现与梯度方法相当的电路发现性能，为深度学习可解释性与可控性提供新视角。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.09784' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Feature salience -- not task-informativeness -- drives machine learning model explanations</h3>
<p><strong>Authors:</strong> Benedict Clark, Marta Oliveira, Rick Wilming, Stefan Haufe</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 实证表明模型解释更依赖特征显著性而非任务相关性，挑战XAI现有假设，对可解释性可靠性研究有重要意义。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.09238' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</h3>
<p><strong>Authors:</strong> Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出RLFR pipeline，利用模型特征作为奖励信号减少幻觉，结合可解释性框架识别候选幻觉，将可解释性用于开放任务监督，对深度学习可解释性应用有价值。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.10067' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Step-resolved data attribution for looped transformers</h3>
<p><strong>Authors:</strong> Georgios Kaissis, David Mildenberger, Juan Felipe Gomez, Martin J. Menten, Eleni Triantafillou</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出SDI方法分解looped transformer的训练数据影响到循环步骤，解决现有方法聚合问题，对理解循环模型内部推理过程有帮助，属深度学习可解释性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.10097' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System</h3>
<p><strong>Authors:</strong> Yanming Li, Xuelin Zhang, WenJie Lu, Ziye Tang, Maodong Wu, Haotian Luo, Tongtong Wu, Zijie Peng, Hongze Mi, Yibo Feng, Naiqiang Tan, Chao Huang, Hong Chen, Li Shen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出基于Shapley值的多代理系统信用分配框架，解决多代理训练中的信用分配难题，通过分层奖励机制提升训练效率与性能，属于深度学习可解释性（Shapley value）的应用研究。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08335' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Exploring SAIG Methods for an Objective Evaluation of XAI</h3>
<p><strong>Authors:</strong> Miquel Miró-Nicolau, Gabriel Moyà-Alcover, Anna Arias-Duart</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 首次系统综述SAIG方法，提出分类 taxonomy 并比较其有效性，解决XAI客观评估难题，推动可解释性研究标准化
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08715' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure</h3>
<p><strong>Authors:</strong> Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang, Chenghua Lin, Min Zhang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 用结构因果模型解析潜在思维链的动态，揭示推理步骤的因果必要性和传播模式，提升大模型推理过程的可解释性
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08783' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The effect of whitening on explanation performance</h3>
<p><strong>Authors:</strong> Benedict Clark, Stoyan Karastoyanov, Rick Wilming, Stefan Haufe</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 研究数据白化对特征归因的影响，发现白化可缓解非informative特征的错误归因，提升解释性能，属于可解释性实践优化。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.09278' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers</h3>
<p><strong>Authors:</strong> Zhan-Yi Liao, Jaewon Yoo, Hao-Tsung Yang, Po-An Chen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 基于条件高斯网络分类器提出结构感知的反事实解释方法，利用生成结构编码特征依赖，结合对抗优化与分段松弛解决非凸约束问题，提升解释的鲁棒性与可解释性，属于深度学习可解释性的白盒方法研究。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08021' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems</h3>
<p><strong>Authors:</strong> Risal Shahriar Shefin, Debashis Gupta, Thai Le, Sarra Alqahtani</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出两阶段梯度框架用于多代理强化学习的故障分析，包括故障源检测与传播追踪，通过泰勒展开与 critic导数分析提供可解释性，属于深度学习可解释性在多代理系统中的应用。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.08104' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Efficient and Stable Reinforcement Learning for Diffusion Language Models</h3>
<p><strong>Authors:</strong> Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出STP框架解决扩散语言模型的RL优化效率和稳定性问题，直接针对diffusion LLM的核心挑战，推动大模型新技术落地
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization</h3>
<p><strong>Authors:</strong> Shaoqiu Zhang, Zizhong Ding, Kaicheng Yang, Junyi Wu, Xianglong Yan, Xi Li, Bingnan Duan, Jianping Fang, Yulun Zhang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对扩散Transformer的部署效率问题，提出时间敏感性量化框架，动态分配比特宽度并结合Fisher引导校准，属于大模型新技术中的扩散模型高效优化，实验验证优于现有方法。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.09883' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders</h3>
<p><strong>Authors:</strong> Amandeep Kumar, Vishal M. Patel</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对扩散Transformer收敛问题，提出RJF方法约束生成到流形测地线，解锁标准Diffusion Transformer能力，对diffusion LLM等大模型新技术有意义。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.10099' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Selective Fine-Tuning for Targeted and Robust Concept Unlearning</h3>
<p><strong>Authors:</strong> Mansi, Avinash Kori, Francesca Toni, Soteris Demetriou</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对扩散模型的概念遗忘问题，提出动态选择目标概念神经元的选择性微调方法，结合Hessian正则化提升鲁棒性与效率，解决全量微调的计算负担，属于大模型新技术（diffusion LLM）的重要进展。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07919' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> iGRPO: Self-Feedback-Driven LLM Reasoning</h3>
<p><strong>Authors:</strong> Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出iGRPO方法用自我反馈驱动LLM推理，提升数学推理准确性，属于大模型新技术的推理优化方向
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.09000' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing</h3>
<p><strong>Authors:</strong> Yan Luo, Henry Huang, Todd Y. Zhou, Mengyu Wang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对扩散模型的训练-free生成问题，提出Look-Ahead和Look-Back轨迹平滑方法，提升生成质量，属于大模型新技术中的扩散模型改进，有实际应用价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.09449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Blind denoising diffusion models and the blessings of dimensionality</h3>
<p><strong>Authors:</strong> Zahra Kadkhodaie, Aram-Alexandre Pooladian, Sinho Chewi, Eero Simoncelli</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 分析盲去噪扩散模型的性能，证明其在低 intrinsic 维度数据下能自动跟踪隐式噪声 schedule，实验显示比非盲模型生成质量更高，对扩散模型这一大模型新技术的理论与应用有重要价值。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.09639' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents</h3>
<p><strong>Authors:</strong> Jiahao Zhao, Shaoxuan Xu, Zhongxiang Sun, Fengqi Zhu, Jingyang Ou, Yuling Shi, Chongxuan Li, Xiao Zhang, Jun Xu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出DLLM-Searcher框架增强dLLM的搜索代理能力，提出P-ReAct减少延迟，对dLLM这一大模型新技术在智能体中的应用有推动作用。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.07035' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation</h3>
<p><strong>Authors:</strong> Yanglei Gan, Peng He, Yuxiang Cai, Run Lin, Guanyu Zhou, Qiao Liu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 将负样本信息融入扩散过程处理时序知识图谱 extrapolation，扩展扩散模型在大模型中的应用场景，属于大模型新技术突破
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.08815' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning</h3>
<p><strong>Authors:</strong> Xu Ma, Yitian Zhang, Qihua Dong, Yun Fu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 构建大规模高质量文本到图像微调数据集Fine-T2I，解决T2I微调的数据集瓶颈，显著提升微调后模型的生成质量和指令遵循能力，对原生多模态大模型的训练数据有重要贡献。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09439' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Looping Back to Move Forward: Recursive Transformers for Efficient and Flexible Large Multimodal Models</h3>
<p><strong>Authors:</strong> Ruihan Xu, Yuting Gao, Lan Wang, Jianing Li, Weihao Chen, Qingpei Guo, Ming Yang, Shiliang Zhang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出递归Transformer架构RecursiveVLM，通过循环复用参数优化大 multimodal模型的效率与灵活性，支持按需资源调整，对原生多模态大模型的高效设计有重要价值。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09080' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards Uniformity and Alignment for Multimodal Representation Learning</h3>
<p><strong>Authors:</strong> Wenzhe Yin, Pan Zhou, Zehao Xiao, Jie Liu, Shujian Yu, Jan-Jakob Sonke, Efstratios Gavves</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对多模态表示学习中的对齐-一致性冲突问题，提出解耦对齐与一致性的方法，理论证明其能减少多模态分布gap，实验在检索和生成任务上有稳定增益，对原生多模态大模型的表示学习至关重要。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09507' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs</h3>
<p><strong>Authors:</strong> Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对多模态大模型的视觉注意力薄弱问题，提出强化学习框架结合区域级视觉注意力奖励，提升视觉推理的可靠性与一致性，解决早期视觉错位导致的误差传播，属于原生多模态大模型的关键改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08241' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT</h3>
<p><strong>Authors:</strong> Chengyi Du, Yazhe Niu, Dazhong Shen, Luxin Xu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出无标注的层级合成CoT框架，模拟人类视觉推理的全局到局部过程，提升多模态大模型的类人推理能力，解决表面关联与非因果理解问题，属于原生多模态大模型的推理改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08339' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling</h3>
<p><strong>Authors:</strong> Ruijie Ye, Jiayi Zhang, Zhuoxin Liu, Zihao Zhu, Siyuan Yang, Li Li, Tianfu Fu, Franck Dernoncourt, Yue Zhao, Jiacheng Zhu, Ryan Rossi, Wenhao Chai, Zhengzhong Tu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出分层智能体框架Agent Banana，通过Context Folding和Image Layer Decomposition解决图像编辑中的过编辑、多轮一致性和高分辨率问题，提升编辑保真度，对原生多模态大模型的图像生成与编辑有重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09084' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> All-in-One Conditioning for Text-to-Image Synthesis</h3>
<p><strong>Authors:</strong> Hirunima Jayasekara, Chuong Huynh, Yixuan Ren, Christabel Acquaye, Abhinav Shrivastava</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对文本到图像合成的复杂提示语义保真和结构一致性问题，提出基于场景图的ASQL Conditioner引导扩散生成，提升文本图像对齐和多样性，对原生多模态大模型的文本到图像合成能力有改进作用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09165' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Rethinking Global Text Conditioning in Diffusion Transformers</h3>
<p><strong>Authors:</strong> Nikita Starodubcev, Daniil Pakhomov, Zongze Wu, Ilya Drobyshevskiy, Yuchen Liu, Zhonghao Wang, Yuqian Zhou, Zhe Lin, Dmitry Baranchuk</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 分析扩散Transformer中全局文本条件的作用，提出训练-free的调制方法提升文本到图像/视频生成和编辑性能，对原生多模态大模型的文本条件机制有深入研究和改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09268' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification</h3>
<p><strong>Authors:</strong> Yiqiao Li, Bo Shang, Jie Wei</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对路边LiDAR与图像的模态 gap 问题，提出无训练的VLM框架实现细粒度车辆分类，对原生多模态大模型的跨模态适应有实际应用价值。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09425' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL</h3>
<p><strong>Authors:</strong> Yang Zhao, Shizhao Sun, Meisheng Zhang, Yingdong Shi, Xubo Yang, Jiang Bian</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出自反思视觉接地框架SceneReVis，通过多轮RL解决3D室内场景合成的空间冲突问题，提升生成质量，对原生多模态大模型的3D场景生成有重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09432' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation</h3>
<p><strong>Authors:</strong> Chuanhai Zang, Jiabao Hu, XW Song</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出频率解耦双分支网络FD-DB，解决合成到真实域翻译中的 photorealism 和结构稳定性权衡问题，提升下游语义分割性能，对原生多模态大模型的域适应有改进作用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09476' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Robust Depth Super-Resolution via Adaptive Diffusion Sampling</h3>
<p><strong>Authors:</strong> Kun Wang, Yun Zhu, Pan Zhou, Na Zhao</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出自适应扩散采样方法AdaDS，解决深度超分辨率中的降解鲁棒性问题，提升零样本泛化能力，对原生多模态大模型的深度估计有改进作用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09510' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes</h3>
<p><strong>Authors:</strong> Michael Baltaxe, Dan Levi, Sagie Benaim</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出检索增强的单目深度估计方法RAD，解决欠代表类的深度估计问题，提升性能，对原生多模态大模型的深度理解有改进作用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09532' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> AUHead: Realistic Emotional Talking Head Generation via Action Units Control</h3>
<p><strong>Authors:</strong> Jiayi Lyu, Leigang Qu, Wenjing Zhang, Hanyu Jiang, Kai Liu, Zhenglin Zhou, Xiaobo Xia, Jian Xue, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出AUHead框架，通过动作单元控制实现情感对话头生成，提升情感表达和真实感，对原生多模态大模型的对话头生成有重要改进。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.09534' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture</h3>
<p><strong>Authors:</strong> Roland Bertin-Johannet, Lara Scipio, Leopold Mayti\'e, Rufin VanRullen</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出全局工作空间架构中的多模态注意力机制，提升多模态整合的鲁棒性与泛化能力，解决模态间干扰问题，属于原生多模态大模型的注意力机制研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.08597' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Remember, Learn, and Forget in Attention-Based Models</h3>
<p><strong>Authors:</strong> Djohan Bonnet, Jamie Lohoff, Jan Finkbeiner, Elidona Skhikerujah, Emre Neftci</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对注意力模型的持续学习困境，提出贝叶斯元塑性框架Palimpsa，理论关联Mamba2等模型，实验提升记忆容量与推理性能，属于深度学习理论中的注意力模型改进。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09075' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Clarifying Shampoo: Adapting Spectral Descent to Stochasticity and the Parameter Trajectory</h3>
<p><strong>Authors:</strong> Runa Eschenhagen, Anna Cai, Tsung-Hsien Lee, Hao-Jun Michael Shi</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 解析Shampoo优化器的谱下降机制，适配随机性与参数轨迹，深化优化器理论理解，属于深度学习理论关键内容。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09314' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula</h3>
<p><strong>Authors:</strong> Chenruo Liu, Yijun Dong, Yiqiu Shen, Qi Lei</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 建立迭代自改进的有限样本理论，分析easy-to-hard课程学习的优势，证明其比固定任务混合有更好的保证，对大模型自改进的理论基础有重要贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.10014' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting</h3>
<p><strong>Authors:</strong> Joe-Mei Feng, Hsin-Hsiung Kao</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 建立非线性逆问题的稳定性和统计浓度的算子理论框架，应用于Gaussian Splatting，对深度学习理论中的逆问题和模型稳定性有基础研究价值。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09415' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Equilibrium contrastive learning for imbalanced image classification</h3>
<p><strong>Authors:</strong> Sumin Roh, Harim Kim, Ho Yun Lee, Il Yong Chun</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 针对不平衡数据的对比学习问题，提出均衡对比学习框架ECL，促进类特征、均值和分类器的几何均衡，提升分类性能，对深度学习理论中的对比学习有重要改进。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09506' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Delving into Spectral Clustering with Vision-Language Representations</h3>
<p><strong>Authors:</strong> Bo Peng, Yuanwei Hu, Bo Liu, Ling Chen, Jie Lu, Zhen Fang</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出神经切线核谱聚类方法，利用视觉语言预训练的跨模态对齐提升谱聚类性能，对深度学习理论中的无监督学习有重要改进。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09586' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Adam to Adam-Like Lagrangians: Second-Order Nonlocal Dynamics</h3>
<p><strong>Authors:</strong> Carlos Heredia</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 将Adam优化器推导为二阶非局部动力系统，提供变分视角分析，推进优化器理论理解，属于深度学习理论关键方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09101' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Gradient Residual Connections</h3>
<p><strong>Authors:</strong> Yangchen Pan, Qizhen Ying, Philip Torr, Bo Liu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出梯度残差连接改进残差网络对高频模式的捕捉能力，推进网络架构设计理论，属于深度学习理论重要方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09190' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Laplacian Mechanism Improves Transformers by Reshaping Token Geometry</h3>
<p><strong>Authors:</strong> Yuchong Zhang, Vardan Papyan</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 引入拉普拉斯机制重塑Transformer的token几何结构以提升性能，属于网络架构设计的理论与实践，符合深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09297' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SnareNet: Flexible Repair Layers for Neural Networks with Hard Constraints</h3>
<p><strong>Authors:</strong> Ya-Chi Chu, Alkiviades Boukas, Madeleine Udell</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出SnareNet修复层满足输入依赖的非线性约束，改进网络可行性控制，属于网络架构约束处理的理论创新，符合深度学习理论方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09317' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Training deep physical neural networks with local physical information bottleneck</h3>
<p><strong>Authors:</strong> Hao Wang, Ziao Wang, Xiangpeng Liang, Han Zhao, Jianqi Hu, Junjie Jiang, Xing Fu, Jianshi Tang, Huaqiang Wu, Sylvain Gigan, Qiang Liu</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出Physical Information Bottleneck框架，结合信息论与局部学习解决深度物理神经网络的训练问题，支持多种学习任务与硬件，对深度学习理论在物理系统中的应用有重要推动作用。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.09569' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin</h3>
<p><strong>Authors:</strong> Guanglong Sun, Hongwei Yan, Liyuan Wang, Zhiqi Kang, Shuang Cui, Hang Su, Jun Zhu, Yi Zhong</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提出元后 refinement框架，通过伪任务序列与双级元学习优化预训练模型，实现无 rehearsal的通用持续学习，提升跨任务泛化性能，属于深度学习理论中持续学习的创新研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.07940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation</h3>
<p><strong>Authors:</strong> Lucas Maes, Quentin Le Lidec, Dan Haramati, Nassim Massaudi, Damien Scieur, Yann LeCun, Randall Balestriero</p>
<p><strong>Published:</strong> 2026-02-11</p>
<p><strong>Reason:</strong> 提供可重复的world model研究框架，促进world modeling的可重复性和标准化，属于深度学习理论的基础研究进展
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.08968' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>