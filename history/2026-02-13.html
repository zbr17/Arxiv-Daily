<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2026-02-13</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>大模型新技术</a>
<a href='#' >高效大模型训练与推理</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >多模态智能体</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >大模型安全与对齐</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2026-02-13</h1>
<div class='meta-info'><p>更新于北京时间：2026-02-13 13:35:32</p>
<p>已自动阅读了 324 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：184843</p>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation</h3>
<p><strong>Authors:</strong> Alan Baade, Eric Ryan Chan, Kyle Sargent, Changan Chen, Justin Johnson, Ehsan Adeli, Li Fei-Fei (Stanford University)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes Latent Forcing to achieve latent diffusion efficiency on raw images, improving pixel-space image generation.
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11401' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> General and Efficient Steering of Unconditional Diffusion</h3>
<p><strong>Authors:</strong> Qingsong Wang, Mikhail Belkin, Yusu Wang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出无需推理时梯度引导的无条件扩散模型高效引导方法，通过噪声对齐和可迁移概念向量提升生成效率和质量，解决扩散模型引导的计算开销问题。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11395' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Native Reasoning Models: Training Language Models to Reason on Unverifiable Data</h3>
<p><strong>Authors:</strong> Yuanfu Wang, Zhixuan Liu, Xiangtian Li, Chaochao Lu, Chao Yang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出Native Reasoning Training框架，无需专家演示或外部验证器，仅用问题-答案对训练LLM推理，解决现有推理模型依赖高质量数据的问题。
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11549' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Code2Worlds: Empowering Coding LLMs for 4D World Generation</h3>
<p><strong>Authors:</strong> Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Introduces Code2Worlds to use coding LLMs for 4D world generation.
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11757' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Free Lunch for Stabilizing Rectified Flow Inversion</h3>
<p><strong>Authors:</strong> Chenru Wang, Beier Zhu, Chi Zhang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对Rectified Flow生成模型的inversion稳定性问题，提出Proximal-Mean Inversion和mimic-CFG方法，提升重建与编辑质量，属于扩散模型相关的大模型新技术，有理论支撑与实验验证。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11850' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learn from Your Mistakes: Self-Correcting Masked Diffusion Models</h3>
<p><strong>Authors:</strong> Yair Schiff, Omer Belhasin, Roy Uziel, Guanghan Wang, Marianne Arriola, Gilad Turok, Michael Elad, Volodymyr Kuleshov</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出自校正掩码扩散模型，通过修正步骤提升生成质量，解决扩散模型一旦解掩码就固定token导致的误差积累问题。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11590' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels</h3>
<p><strong>Authors:</strong> Haolei Bai, Lingcheng Kong, Xueyi Chen, Jianmian Wang, Zhiqiang Tao, Huan Wang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出扩散LLM生成CUDA内核，通过双阶段训练优化生成质量，拓展扩散LLM的应用场景，解决代码生成的结构规划问题。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11715' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning</h3>
<p><strong>Authors:</strong> Haoran Dang, Cuiling Lan, Hai Wan, Xibin Zhao, Yan Lu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对LLM强化学习中静态温度调度的局限性，提出将温度控制作为可学习元策略的TAMPO框架，通过分层双循环过程实现温度的在线自适应，在数学推理基准上显著优于固定或启发式温度基线，为LLM强化学习的探索策略提供了新方法。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11779' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser</h3>
<p><strong>Authors:</strong> Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 将扩散模型对齐问题转化为方差最小化目标，通过理论证明其与KL目标的等价性，统一现有扩散对齐方法并提出新设计方向，属于扩散大模型的新技术突破。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.12229' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Categorical Flow Maps</h3>
<p><strong>Authors:</strong> Daan Roos, Oscar Davis, Floor Eijkelboom, Michael Bronstein, Max Welling, \.Ismail \.Ilkan Ceylan, Luca Ambrogioni, Jan-Willem van de Meent</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出分类数据的流匹配生成方法，通过连续轨迹和蒸馏实现few-step高效生成，在图像、分子图、文本上取得state-of-the-art结果，属于大模型生成的新技术。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.12233' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control</h3>
<p><strong>Authors:</strong> Yu Deng, Yufeng Jin, Xiaogang Jia, Jiahong Xue, Gerhard Neumann, Georgia Chalvatzaki</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 论文提出Robot-DIFT框架，通过流形蒸馏将冻结扩散教师模型转化为确定性空间语义特征金字塔网络，保留扩散模型的几何先验，解决机器人操纵中视觉骨干与闭环控制物理需求的结构不匹配问题，属于diffusion相关的大模型新技术方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2602.11934' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models</h3>
<p><strong>Authors:</strong> Zukang Xu, Zhixiong Zhao, Xing Hu, Zhixuan Chen, Dawei Yang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出KLT引导SVD与偏差校正向量量化的MoE模型压缩方法，实现低比特量化下的性能保持，提升MoE模型的部署效率，属于高效大模型压缩的关键改进。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11184' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration</h3>
<p><strong>Authors:</strong> Akhiad Bercovich, Nir Ailon, Vladimir Anisimov, Tomer Asida, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Roi Koren, Itay Levy, Zach Moshe, Pavlo Molchanov, Najeeb Nabwani, Mostofa Patwari, Omri Puny, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对MoE推理模型的高服务成本问题，扩展Puzzle框架优化GPT-OSS-120B得到88B模型，通过MoE剪枝、窗口注意力替换、KV量化等手段，在保持 accuracy 的同时提升1.63倍长上下文吞吐量和2.82倍单卡吞吐量，为大模型推理效率优化提供了实践方案。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11937' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving</h3>
<p><strong>Authors:</strong> Sunghyeon Woo, Hoseung Kim, Sunghwan Shim, Minjung Jo, Hyunjoon Jeong, Jeongtae Lee, Joonghoon Kim, Sungjae Lee, Baeseong Park, Se Jung Kwon, Dongsoo Lee</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对多LLM服务中重复prompt前缀的冗余计算问题，提出PrefillShare共享prefill模块和KV缓存，通过分解模型为prefill和decode模块并微调decode部分，实现多模型共享prefill，在多模型agent workload中降低4.5倍p95延迟和提升3.9倍吞吐量，解决了多LLM服务的效率瓶颈。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12029' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Arbitrary Ratio Feature Compression via Next Token Prediction</h3>
<p><strong>Authors:</strong> Yufan Liu, Daoyuan Ren, Zhipeng Zhang, Wenyang Luo, Bing Li, Weiming Hu, Stephen Maybank (Chinese Academy of Sciences)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes a flexible feature compression framework that supports arbitrary ratios with a single model, improving efficiency of multi-modal tasks.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11494' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning</h3>
<p><strong>Authors:</strong> Changti Wu, Jiahuai Mao, Yuzhuo Miao, Shijie Lian, Bin Yu, Xiaopeng Lin, Cong Huang, Lei Zhang, Kai Chen (Tencent)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes ScalSelect for scalable training-free multimodal data selection to improve visual instruction tuning efficiency.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11636' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SToRM: Supervised Token Reduction for Multi-modal LLMs toward efficient end-to-end autonomous driving</h3>
<p><strong>Authors:</strong> Seo Hyun Kim, Jin Bok Park, Do Yeon Koo, Ho Gun Park, Il Yong Chun (Korean universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes SToRM for supervised token reduction in multi-modal LLMs, reducing computational cost while maintaining performance.
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11656' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MonarchRT: Efficient Attention for Real-Time Video Generation</h3>
<p><strong>Authors:</strong> Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出MonarchRT结构化注意力，通过Monarch矩阵分解优化视频扩散模型的实时推理，提升注意力稀疏性与计算效率，属于高效大模型推理的重要改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12271' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models</h3>
<p><strong>Authors:</strong> Arian Raje, Anupam Nayak, Gauri Joshi</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出通过微调让MoE模型偏好激活少量专家，减少GPU内存占用与I/O延迟，提升推理效率，属于MoE模型高效推理的实践改进。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11192' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HiFloat4 Format for Language Model Inference</h3>
<p><strong>Authors:</strong> Yuanyong Luo, Jing Huang, Yu Cheng, Ziwei Yu, Kaihua Zhang, Kehong Hong, Xinda Ma, Xin Wang, Anping Tong, Guipeng Hu, Yun Xu, Mehran Taghian, Peng Wu, Guanglin Li, Yunke Peng, Tianchi Hu, Minqi Chen, Michael Bi Mi, Hu Liu, Xiping Zhou, Junsong Wang, Qiang Lin, Heng Liao</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出HiFloat4数据格式，优化LLM推理的内存使用与计算效率，提升多模型的推理精度，属于高效大模型推理的格式创新。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11287' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Retrieval-Aware Distillation for Transformer-SSM Hybrids</h3>
<p><strong>Authors:</strong> Aviv Bick, Eric P. Xing, Albert Gu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出检索感知蒸馏，将Transformer转化为混合模型，保留关键注意力头并蒸馏其余部分到递归头，显著提升内存效率，解决Transformer-SSM在检索任务的性能差距问题。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11374' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models</h3>
<p><strong>Authors:</strong> Eunyeong Cho, Jehyeon Bang, Ranggi Hwang, Minsoo Rhu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出相位感知调度算法，区分推理型LLM的推理和回答阶段，优化服务性能，减少Time-To-First-Token（TTFT）。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11530' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</h3>
<p><strong>Authors:</strong> Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 扩展On-policy蒸馏（OPD）框架提出G-OPD，引入奖励缩放因子实现奖励外推，使学生模型超越教师性能边界，在数学推理和代码生成任务上提升蒸馏效果，为大模型蒸馏的奖励设计提供了新方法。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12125' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction</h3>
<p><strong>Authors:</strong> Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出基于记忆巩固的自适应注意力减少方法，解决混合架构中注意力冗余问题，通过理论证明静态路由的局限性，并在SRCD基准上实现100%检索 accuracy和37.8×注意力计算 reduction，有效提升大模型推理效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12204' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training</h3>
<p><strong>Authors:</strong> Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai, Chong Luo, Weihao Jiang, Peng Hou, Anxiang Zeng, Xin Geng, Baining Guo</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出分布判别理论（DDT）改进On-Policy SFT，通过IDFT和Hinted Decoding提升SFT的泛化能力，实验表明性能与DPO、SimPO相当且保持SFT的高效性，为LLM高效训练提供新框架。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12222' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Olmix: A Framework for Data Mixing Throughout LM Development</h3>
<p><strong>Authors:</strong> Mayee F. Chen, Tyler Murray, David Heineman, Matt Jordan, Hannaneh Hajishirzi, Christopher R\'e, Luca Soldaini, Kyle Lo</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对LM开发中数据混合的动态问题，提出Olmix框架解决domain set变化时的高效重新计算，实验表明减少74%计算并提升11.6%下游性能，有效提升大模型训练效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12237' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation</h3>
<p><strong>Authors:</strong> Michael Menezes, Anastasios Kyrillidis</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对Mamba2的隐藏状态冗余问题，提出GHOST框架实现结构化剪枝，在保持性能的同时将状态维度减少50%，有效提升大模型推理效率。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11408' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces</h3>
<p><strong>Authors:</strong> Xin Xu, Tong Yu, Xiang Chen, Haoliang Wang, Julian McAuley, Saayan Mitra</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出推理时的置信度感知路由机制，在 latent和离散空间间切换以提升效率，实验在STEM和coding基准上提升19.70% Pass@1并减少15.55%生成长度，属于高效大模型训练与推理。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11683' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging</h3>
<p><strong>Authors:</strong> Weihong Lin, Lin Sun, Qilong Shi, Aomufei Yuan, Yuxuan Tian, Zhengyang Wang, Guangxiang Zhao, Xiangzheng Zhang, Tong Yang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出稀疏互补融合方法实现分布感知的模型合并，避免参数空间干扰，实验在多个基准上优于现有方法，提升大模型训练效率（避免重新训练）。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11717' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty</h3>
<p><strong>Authors:</strong> Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo Zhao</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出ARLCP强化学习框架，通过自适应反射惩罚抑制无效推理步骤，结合问题复杂度校准的长度惩罚，在数学推理任务中显著减少响应长度（最多53.1%）并提升 accuracy（最多5.8%）
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12113' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation</h3>
<p><strong>Authors:</strong> Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu, Xue Liu, Chen Ma</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 结合教育理论提出IOA知识蒸馏框架，系统识别学生模型知识缺陷并组织渐进式知识传递，在MATH、HumanEval等基准上显著提升小模型性能（最多22.3%）
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12172' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Move What Matters: Parameter-Efficient Domain Adaptation via Optimal Transport Flow for Collaborative Perception</h3>
<p><strong>Authors:</strong> Zesheng Jia, Jin Wang, Siao Liu, Lingzhi Li, Ziyao Huang, Yunjiang Xu, Jianping Wang (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes FlowAdapt for parameter-efficient domain adaptation in collaborative perception, reducing trainable parameters while maintaining performance.
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11565' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis</h3>
<p><strong>Authors:</strong> Zhen Bi, Xueshu Chen, Luoyang Sun, Yuhang Yao, Qing Shen, Jungang Lou, Cheng Deng</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出基于Roofline模型的端侧LLM基准框架，量化模型在边缘硬件上的性能上限，为端侧LLM的硬件-软件协同设计提供指导。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11506' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TS-Memory: Plug-and-Play Memory for Time Series Foundation Models</h3>
<p><strong>Authors:</strong> Sisuo Lyu, Siru Zhong, Tiegang Chen, Weilin Ruan, Qingxiang Liu, Taiqiang Lv, Qingsong Wen, Raymond Chi-Wing Wong, Yuxuan Liang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出轻量级内存适配器，通过蒸馏检索诱导的分布校正提升时间序列基础模型的域适应能力，实现检索-free部署。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11550' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization</h3>
<p><strong>Authors:</strong> Yujie Gu, Richeng Jin, Zhaoyang Zhang, Huaiyu Dai</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出合成数据引导的尖锐度感知最小化，缓解梯度压缩导致的泛化损伤，解决联邦学习中梯度压缩与泛化的矛盾。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11584' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning</h3>
<p><strong>Authors:</strong> Jianhua Wang, Yinlin Su</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出目标可解释扰动，通过频域分析选择性注入扰动，防御联邦学习的梯度反转攻击，平衡隐私保护与模型性能。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11633' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dopamine: Brain Modes, Not Brains</h3>
<p><strong>Authors:</strong> Shervin Ghasemlou</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出激活空间的PEFT技术，通过学习神经元阈值和增益选择现有计算，提升模型适配的可解释性，解决PEFT的权重 delta难以解释的问题。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11726' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Manifold-Aware Temporal Domain Generalization for Large Language Models</h3>
<p><strong>Authors:</strong> Yiheng Yao, Zekun Cai, Xinyuan Song, Hiroki Hill Kobayashi, Xuan Song, Ryosuke Shibasaki, Liang Zhao</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对LLM时间域泛化的参数高效问题，提出MaT-LoRA框架，在低秩适应子空间中约束时间更新到共享流形，结合结构化时间核心模型演化，在科学文献、新闻等数据集上提升泛化性能，为LLM的时间域适应提供了高效方法。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11965' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Predicting LLM Output Length via Entropy-Guided Representations</h3>
<p><strong>Authors:</strong> Huanyi Xie, Yubin Chen, Liangyu Wang, Lijie Hu, Di Wang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出轻量级LLM输出长度预测框架，利用主模型内部状态和token熵提升静态预测准确性，通过动态长度估计减少批处理推理中的padding浪费，显著提升推理吞吐量
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.11812' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection</h3>
<p><strong>Authors:</strong> Xiuping Wu, Zhao Yu, Yuxin Cheng, Ngai Wong, Liangjun Ke, Tapas Mishra, Konstantinos V. Katsikopoulos</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出两种无参数更新的推理引导方法，通过模仿自身正确行为模式（InjectCorrect）和强化学习优化的模式注入（InjectRLOpt），在不修改模型参数的情况下提升LLM推理性能
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12013' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation</h3>
<p><strong>Authors:</strong> Chengxi Zeng, Yuxuan Jiang, Ge Gao, Shuai Wang, Duolikun Danier, Bin Zhu, Stevan Rudinac, David Bull, Fan Zhang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 分析SAM3文本编码器的冗余性，提出LiteText用MobileCLIP学生模型替代原编码器，减少88%参数同时保持分割性能，优化多模态模型的文本编码效率
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2602.12173' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy</h3>
<p><strong>Authors:</strong> Zhendong Huang, Hengjie Cao, Fang Dong, Ruijun Huang, Mengyi Chen, Yifeng Yang, Xin Zhang, Anrui Chen, Mingzhi Dong, Yujiang Wang, Jinlong Hou, Qin Lv, Robert P. Dick, Yuan Cheng, Fan Yang, Tun Lu, Li Shang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 分析LLM训练中的梯度谱各向异性，提出Spectra优化器抑制主导谱方向的干扰，提升训练效率与下游性能，属于深度学习理论中优化器的重要突破。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11185' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?</h3>
<p><strong>Authors:</strong> Nikhil Garg, Jon Kleinberg, Kenny Peng</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 建立线性表示假设的数学框架，分析LLM存储特征的能力并给出上下界，属于深度学习理论中特征表示的基础理论贡献。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11246' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra</h3>
<p><strong>Authors:</strong> Jose Marie Antonio Mi~noza</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出基于超离散化与极大加代数的全可微分脉冲神经网络，解决脉冲生成的非可微分问题，属于深度学习理论中网络架构的创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11206' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Efficient Analysis of the Distilled Neural Tangent Kernel</h3>
<p><strong>Authors:</strong> Jamie Mahowald, Brian Bell, Alex Ho, Michael Geyer</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出蒸馏神经切线核，结合数据集蒸馏与投影方法，大幅降低NTK的计算复杂度，属于深度学习理论中NTK分析的高效化改进。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11320' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence</h3>
<p><strong>Authors:</strong> Jason Dury</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出预测关联记忆架构，通过时间共现学习关联结构，解决相似性检索的局限性，属于深度学习理论中记忆机制的创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11322' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse Semantic Dimension as a Generalization Certificate for LLMs</h3>
<p><strong>Authors:</strong> Dibyanayan Bandyopadhyay, Asif Ekbal</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出稀疏语义维度（SSD）作为LLM泛化的复杂度度量，通过稀疏自动编码器的激活状态几何解释泛化能力，揭示大模型泛化的关键因素。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11388' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Krause Synchronization Transformers</h3>
<p><strong>Authors:</strong> Jingkun Liu, Yisong Yue, Max Welling, Yue Song</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出Krause注意力机制替代softmax，缓解Transformer的同步动态问题，减少表示 collapse和注意力 sink现象。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11534' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Implicit Bias of Steepest Descent with Mini-batch Stochastic Gradient</h3>
<p><strong>Authors:</strong> Jichu Li, Xuan Tang, Difan Zou</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 研究小批量随机最速下降的隐式偏差，分析batch size、动量和方差减少对泛化的影响，提供优化器设计的理论指导。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11557' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UMAP Is Spectral Clustering on the Fuzzy Nearest-Neighbor Graph</h3>
<p><strong>Authors:</strong> Yang Yang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 证明UMAP等价于模糊最近邻图上的谱聚类，统一UMAP与谱聚类的理论框架，解释UMAP的行为机制。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11662' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion</h3>
<p><strong>Authors:</strong> Chengting Yu, Xiaobo Shu, Yadao Wang, Yizhen Zhang, Haoyi Wu, You Wu, Rujiao Long, Ziheng Chen, Yuchi Xu, Wenbo Su, Bo Zheng</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出循环Transformer，通过多分辨率递归学习分层依赖，提升循环模型的性能，解决循环Transformer的效率问题。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11698' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Causal-JEPA: Learning World Models through Object-Level Latent Interventions</h3>
<p><strong>Authors:</strong> Heejeong Nam, Quentin Le Lidec, Lucas Maes, Yann LeCun, Randall Balestriero</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出基于对象级掩码的JEPA架构，通过潜在干预诱导因果归纳偏置，实验提升counterfactual推理20%并降低控制任务的特征需求，属于深度学习理论中的网络架构改进。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11389' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Selective Prior Synchronization via SYNC Loss</h3>
<p><strong>Authors:</strong> Ishan Mishra, Jiajie Li, Deepak Mishra, Jinjun Xiong (IBM Research)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes a novel SYNC loss that integrates ad-hoc and post-hoc selective prediction methods, improving selective prediction performance.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11316' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation</h3>
<p><strong>Authors:</strong> Xiangyu Wu, Dongming Jiang, Feng Yu, Yueying Tian, Jiaqi Tang, Qing-Guo Chen, Yang Yang, Jianfeng Lu (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes adaptive debiasing Tsallis entropy for test-time adaptation, improving uncertainty estimation in deep learning.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11743' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> WSBD: Freezing-Based Optimizer for Quantum Neural Networks</h3>
<p><strong>Authors:</strong> Christopher Kverne, Mayur Akewar, Yuqian Huo, Tirthak Patel, Janki Bhimani</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对量子神经网络训练的计算成本和 barren plateau问题，提出基于动态参数冻结的WSBD优化器，通过梯度衍生的重要性得分聚焦关键参数，提升训练效率。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11383' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> In-Context Function Learning in Large Language Models</h3>
<p><strong>Authors:</strong> Elif Akata, Konstantinos Voudouris, Vincent Fortuin, Eric Schulz</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 用高斯过程框架量化LLM的上下文学习能力，揭示函数生成核对学习曲线的影响，分析归纳偏差及微调对连续函数学习的作用，为LLM上下文学习提供了理论工具和可解释性 insights。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11863' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret</h3>
<p><strong>Authors:</strong> Yifei Jin, Xin Zheng, Lei Guo</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对非平稳环境下Momentum LMS的理论空白，分析其稳定性、跟踪性能和遗憾界，通过二阶随机向量差分方程推导理论保证，实验验证在流式数据中的快速适应能力，为在线学习优化器提供了理论支持。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.11995' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Improved state mixing in higher-order and block diagonal linear recurrent networks</h3>
<p><strong>Authors:</strong> Igor Dubinin, Antonio Orvieto, Felix Effenberger</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对线性循环网络（LRNN）的表达性局限，提出H-LRU（高阶循环）和BD-LRU（块对角循环）架构，通过结构化状态混合提升表达性，同时保持效率，在合成序列和语言建模任务上超越Mamba、DeltaNet等基线，为LRNN的架构设计提供了新方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2602.12021' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty</h3>
<p><strong>Authors:</strong> Tanmoy Mukherjee, Marius Kloft, Pierre Marquis, Zied Bouraoui</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出置信概念瓶颈模型，通过结构分离实现认知与随机不确定性的解耦，提升不确定性估计的可靠性，属于深度学习可解释性的重要贡献。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11219' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification</h3>
<p><strong>Authors:</strong> Nghia Nguyen, Tianjiao Ding, René Vidal</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出分层概念嵌入与追踪框架，利用概念的层次结构提升可解释性，解决现有稀疏概念恢复方法忽略层次结构导致的解释不一致问题。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11448' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TreeGrad-Ranker: Feature Ranking via $O(L)$-Time Gradients for Decision Trees</h3>
<p><strong>Authors:</strong> Weida Li, Yaoliang Yu, Bryan Kian Hsiang Low</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出TreeGrad-Ranker，通过梯度优化特征排序，改进Shapley值计算，解决现有方法的不可靠性，提升特征排名的准确性。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11623' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs</h3>
<p><strong>Authors:</strong> Edward Y. Chang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出认知 regret最小化（ERM）解决LLM的因果 rung collapse问题，通过区分关联与干预提升推理正确性，实验恢复53-59%的错误推理，属于深度学习可解释性方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11675' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Prototype Transformer: Towards Language Model Architectures Interpretable by Design</h3>
<p><strong>Authors:</strong> Yordan Yordanov, Matteo Forasassi, Bayar Menzat, Ruizhi Wang, Chang Qi, Markus Kaltenberger, Amine M'Charrak, Tommaso Salvatori, Thomas Lukasiewicz</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出基于原型向量的自回归语言模型架构Prototype Transformer，通过输入-原型双向通信自动捕获可解释概念，支持推理过程解释与行为编辑，同时实现线性计算缩放，性能接近SOTA
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11852' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ANML: Attribution-Native Machine Learning with Guaranteed Robustness</h3>
<p><strong>Authors:</strong> Oliver Zahn, Matt Beton, Simran Chana</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出归因原生机器学习框架，通过数据质量权重（梯度一致性、验证状态、贡献者声誉、时间相关性）提升模型性能和可解释性，解决现有训练管道忽略数据质量的问题。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11690' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Explaining AI Without Code: A User Study on Explainable AI</h3>
<p><strong>Authors:</strong> Natalia Abarca, Andr\'es Carvallo, Claudia L\'opez Moncada, Felipe Bravo-Marquez</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 在无代码ML平台中集成PDP、PFI、KernelSHAP等可解释性技术，通过用户研究验证其对新手和专家的有效性，提升XAI的可访问性，属于深度学习可解释性方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11159' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders</h3>
<p><strong>Authors:</strong> Yifan Luo, Yang Zhan, Jiedong Jiang, Tianyang Liu, Mingrui Wu, Zhennan Zhou, Bin Dong</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出分层稀疏自动编码器HSAE，联合学习多尺度SAE及特征父子关系，通过结构约束与随机扰动增强概念层次对齐，有效发现LLM中的语义结构，保留SAE的可解释性与重建性能
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2602.11881' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Adaptive Milestone Reward for GUI Agents</h3>
<p><strong>Authors:</strong> Congmin Zheng, Xiaoyun Mo, Xinbei Ma, Qiqiang Lin, Yin Zhao, Jiachen Zhu, Xingyu Lou, Jun Wang, Zhaoxiang Wang, Weiwen Liu, Zhuosheng Zhang, Yong Yu, Weinan Zhang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出自适应里程碑奖励机制，通过动态蒸馏成功轨迹的里程碑，解决GUI Agents长 horizon任务的信用分配问题，显著提升任务成功率。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.11524' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning</h3>
<p><strong>Authors:</strong> GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng, Mingming Yu, Peng Li, Qiuping Deng, Tianze Liu, Xinyu Zhou, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yifei Nie, Yilong Li, Yukun Zhou, Yun Ye, Zhichao Liu, Zheng Zhu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出视觉-语言-动作（VLA）模型GigaBrain-0.5M*，结合世界模型强化学习实现机器人操作任务的长horizon执行，有真实场景部署验证，属于多模态智能体的实践突破。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.12099' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Evaluating Memory Structure in LLM Agents</h3>
<p><strong>Authors:</strong> Alina Shutova, Alexandra Olenina, Ivan Vinogradov, Anton Sinitsin</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出StructMemEval基准，评估LLM智能体的记忆结构组织能力，而非简单事实回忆，属于多模态智能体记忆机制的评估方法创新。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.11243' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use</h3>
<p><strong>Authors:</strong> Hanbing Liu, Chunhao Tian, Nan An, Ziyuan Wang, Pinyan Lu, Changyuan Yu, Qi Qi</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对预算约束下的Agent工具使用问题，提出INTENT框架通过意图感知规划优化工具调用，实验在StableToolBench上实现预算可行性与任务成功的平衡，属于多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.11541' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Configure Agentic AI Systems</h3>
<p><strong>Authors:</strong> Aditya Taparia, Som Sagar, Ransalu Senanayake</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出ARC框架通过RL学习Agent的动态配置（workflows、tools等），实验提升25%任务 accuracy并降低成本，优化多模态智能体的配置效率。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.11574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents</h3>
<p><strong>Authors:</strong> Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Holger Boche</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出TSR框架在训练时通过轨迹搜索生成高质量rollouts，提升多轮RL的Agent学习效果，实验在多个任务上提升15%性能，属于多模态智能体方向。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2602.11767' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration</h3>
<p><strong>Authors:</strong> Jinghan He, Junfeng Fang, Feng Xiong, Zijun Yao, Fei Shen, Haiyun Guo, Jinqiao Wang, Tat-Seng Chua (National University of Singapore)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes a framework for self-evolving vision-language models via active environment exploration, improving VLM performance on reasoning and general understanding tasks.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11241' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models</h3>
<p><strong>Authors:</strong> Sethuraman T V, Savya Khosla, Aditi Tiwari, Vidya Ganesh, Rakshana Jayaprakash, Aditya Jain, Vignesh Srinivasakumar, Onkar Kishor Susladkar, Srinidhi Sunkara, Aditya Shanmugham, Rakesh Vaideeswaran, Abbaas Alif Mohamed Nishar, Simon Jenni, Derek Hoiem (University of Illinois at Urbana-Champaign)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Introduces a diagnostic benchmark to test the robustness of video-language models, revealing weaknesses in temporal and visual grounding.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11244' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts</h3>
<p><strong>Authors:</strong> Chen Zhao, Jiawei Chen, Hongyu Li, Zhuoliang Kang, Shilin Lu, Xiaoming Wei, Kai Zhang, Jian Yang, Ying Tai (ByteDance)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes LUVE for ultra-high-resolution video generation using latent cascading and dual frequency experts.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11564' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Xiaowen Zhang, Zhi Gao, Licheng Jiao, Lingling Li, Qing Li (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes STVG-R1 to improve instance-level reasoning in video grounding via reinforcement learning.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11730' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Adapting Vision-Language Models for E-commerce Understanding at Scale</h3>
<p><strong>Authors:</strong> Matteo Nulli, Vladimir Orshulevich, Tala Bazazo, Christian Herold, Michael Kozielski, Marcin Mazur, Szymon Tuzel, Cees G. M. Snoek, Seyyed Hadi Hashemi, Omar Javed, Yannick Versley, Shahram Khadivi (eBay, Amazon)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Studies adapting vision-language models for e-commerce understanding at scale.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11733' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> JEPA-VLA: Video Predictive Embedding is Needed for VLA Models</h3>
<p><strong>Authors:</strong> Shangchen Miao, Ningya Feng, Jialong Wu, Ye Lin, Xu He, Dong Li, Mingsheng Long (Tsinghua University)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes JEPA-VLA to integrate video predictive embeddings into vision-language-action models, improving performance.
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11832' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception</h3>
<p><strong>Authors:</strong> Lai Wei, Liangbo He, Jun Lan, Lingzhong Dong, Yutong Cai, Siyuan Li, Huijia Zhu, Weiqiang Wang, Linghe Kong, Yue Wang, Zhuosheng Zhang, Weiran Huang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出区域到图像蒸馏方法，将推理时的缩放操作内化为训练过程，改进多模态大模型的细粒度感知能力，同时提升GUI智能体等任务性能，属于原生多模态大模型的重要优化。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11858' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</h3>
<p><strong>Authors:</strong> Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang, Hongyang Wei, Yi-Fan Zhang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Long Chen</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出空间思维链框架，桥接理解与生成模型，增强扩散模型的空间推理能力，提升多模态生成的空间一致性，属于原生多模态大模型的空间推理改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11980' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing</h3>
<p><strong>Authors:</strong> Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin, Hongjian Liu, Zhixiong Zhang, Shengyuan Ding, Tianhang Wang, Zhenglin Cheng, Tao Lin, Cheng Jin, Kaicheng Yu, Jingjing Chen, Wenjie Wang, Zhongyu Wei, Jiaqi Wang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出轻量化统一多模态模型DeepGen 1.0，通过分层特征融合与强化学习优化，提升图像生成与编辑能力，性能优于更大参数模型，属于原生多模态大模型的轻量化实践。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.12205' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching</h3>
<p><strong>Authors:</strong> Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang, Adheesh Juvekar, Tianshu Bao, Lin Chai, Sparsh Mittal, Inderjit S Dhillon, Ismini Lourentzou</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出统一离散流匹配框架UniDFlow，整合多模态推理与生成，实现零样本泛化能力，提升多任务性能，属于原生多模态大模型的统一架构改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.12221' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</h3>
<p><strong>Authors:</strong> Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出统一多模态思维链测试时缩放框架UniT，通过迭代推理提升多模态模型的复杂任务性能，属于原生多模态大模型的推理优化。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.12279' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MAPLE: Modality-Aware Post-training and Learning Ecosystem</h3>
<p><strong>Authors:</strong> Nikhil Verma, Minjung Kim, JooYoung Yoo, Kyung-Min Jin, Manasa Bharadwaj, Kevin Ferreira, Ko Keun Kim, Youngjoon Kim</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出模态感知的多模态大模型后训练框架MAPO，通过分层batch减少梯度方差，实验缩小单/多模态精度差距30.24%并加速收敛3.18倍，属于原生多模态大模型方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11596' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> HoloBrain-0 Technical Report</h3>
<p><strong>Authors:</strong> Xuewu Lin, Tianwei Lin, Yun Du, Hongyu Xie, Yiwei Jin, Jiawei Li, Shijie Wu, Qingze Wang, Mengdi Li, Mengao Zhao, Ziang Li, Chaodong Huang, Hongzhe Bi, Lichao Huang, Zhizhong Su</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 论文提出结合机器人embodiment先验的VLA架构HoloBrain-0，通过“预训练+后训练”范式在模拟基准和真实任务中取得最优结果，属于原生多模态大模型（VLA属于多模态大模型）方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.12062' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Ctrl&Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation</h3>
<p><strong>Authors:</strong> Penghui Ruan, Bojia Zi, Xianbiao Qi, Youze Huang, Rong Xiao, Pichao Wang, Jiannong Cao, Yuhui Shi (Chinese Academy of Sciences)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Presents a diffusion framework for geometry-consistent object manipulation in visual generation, addressing limitations of existing methods.
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11440' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> What if Agents Could Imagine? Reinforcing Open-Vocabulary HOI Comprehension through Generation</h3>
<p><strong>Authors:</strong> Zhenlong Yuan, Xiangyan Qu, Jing Tang, Rui Chen, Lei Sun, Ruidong Chen, Hongwei Yu, Chengxuan Qian, Xiangxiang Chu, Shuo Li, Yuyin Zhou (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Introduces ImagineAgent to improve open-vocabulary human-object interaction comprehension via generative imagination.
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11499' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation</h3>
<p><strong>Authors:</strong> Bingyuan Wang, Xingbei Chen, Zongyang Qiu, Linping Yuan, Zeyu Wang (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Introduces EmoSpace for fine-grained emotion prototype learning in affective content generation.
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11658' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> RI-Mamba: Rotation-Invariant Mamba for Robust Text-to-Shape Retrieval</h3>
<p><strong>Authors:</strong> Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian (University of Western Australia)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes RI-Mamba for rotation-invariant text-to-shape retrieval, improving robustness of multi-modal models.
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11673' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Light4D: Training-Free Extreme Viewpoint 4D Video Relighting</h3>
<p><strong>Authors:</strong> Zhenghuang Wu, Kang Chen, Zeyu Zhang, Hao Tang (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes Light4D for training-free 4D video relighting under extreme viewpoints.
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11769' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SkillRater: Untangling Capabilities in Multimodal Data</h3>
<p><strong>Authors:</strong> Naveen Sahi, Jeremy Dohmann, Armen Aghajanyan, Akshat Shrivastava</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出SkillRater框架，分解多模态数据的能力维度（视觉理解、OCR、STEM推理），通过渐进选择规则提升模型性能，解决单标量质量评分的局限性。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.11615' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model</h3>
<p><strong>Authors:</strong> Yanjiang Guo, Tony Lee, Lucy Xiaoyang Shi, Jianyu Chen, Percy Liang, Chelsea Finn</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 论文提出VLAW迭代改进算法，通过真实世界滚动数据提升世界模型 fidelity 以生成合成数据优化VLA政策，属于原生多模态大模型（VLA属于多模态大模型）方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.12063' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion</h3>
<p><strong>Authors:</strong> Jiangran Lyu, Kai Liu, Xuheng Zhang, Haoran Liao, Yusen Feng, Wenxuan Zhu, Tingrui Shen, Jiayi Chen, Jiazhao Zhang, Yifei Dong, Wenbo Cui, Senmao Qi, Shuo Wang, Yixin Zheng, Mi Yan, Xuesong Shi, Haoran Li, Dongbin Zhao, Ming-Yu Liu, Zhizheng Zhang, Li Yi, Yizhou Wang, He Wang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 论文提出LDA-1B机器人基础模型，联合学习动态、政策和视觉预测，使用多模态扩散Transformer处理异步视觉与动作流，属于原生多模态大模型方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2602.12215' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mask What Matters: Mitigating Object Hallucinations in Multimodal Large Language Models with Object-Aligned Visual Contrastive Decoding</h3>
<p><strong>Authors:</strong> Boqi Chen, Xudong Liu, Jianing Qiu (Chinese universities)</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> Proposes a method to mitigate object hallucinations in multimodal LLMs.
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11737' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders</h3>
<p><strong>Authors:</strong> Zhuxin Lei, Ziyuan Yang, Yi Zhang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出双分支对抗防御框架ZePAD，在提升预训练编码器对抗鲁棒性的同时保持良性性能，属于大模型安全与对齐的防御方法改进。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11204' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Unifying Stable Optimization and Reference Regularization in RLHF</h3>
<p><strong>Authors:</strong> Li He, Qiang Qu, He Zhao, Stephen Wan, Dadong Wang, Lina Yao, Tongliang Liu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出统一正则化方法，平衡RLHF中的奖励 hacking和稳定优化，通过加权监督微调损失提升对齐性能和稳定性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11523' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing</h3>
<p><strong>Authors:</strong> Keita Broadwater</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对传统LLM安全基准忽视重复推理风险的问题，提出APST框架模拟真实部署中的重复提示采样，量化安全失败概率，揭示不同模型在重复推理下的可靠性差异，为LLM安全评估提供了深度导向的实用方法。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11786' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mitigating Mismatch within Reference-based Preference Optimization</h3>
<p><strong>Authors:</strong> Suqin Yuan, Xingrui Yu, Jiyang Zheng, Lei Feng, Dadong Wang, Ivor Tsang, Tongliang Liu</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对DPO中参考模型导致的训练-推理不匹配（过早满足）问题，提出HyPO框架条件性应用参考信号，增强悲观对的学习信号，在偏好对齐任务上提升推理对齐指标和 pairwise 胜率，改进了参考基偏好优化的有效性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11902' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Capability-Oriented Training Induced Alignment Risk</h3>
<p><strong>Authors:</strong> Yujun Zhou, Yue Huang, Han Bao, Kehan Guo, Zhenwen Liang, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 揭示能力导向训练中模型自发利用环境漏洞最大化奖励的对齐风险，设计4类“漏洞游戏”验证模型的策略性剥削行为，发现风险可通过数据蒸馏传递，为LLM安全对齐的环境设计和风险审计提供了关键 insights。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.12124' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</h3>
<p><strong>Authors:</strong> Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, Jiayi Ji, Handing Wang, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 针对现有对齐方法的行为级局限和神经元攻击脆弱性，提出SafeNeuron框架在神经元层面重新分配安全表示，通过冻结安全神经元并强制模型构建冗余安全路径，提升对神经元修剪攻击的鲁棒性，同时保持 general 能力，为LLM安全对齐提供了细粒度方法。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.12158' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</h3>
<p><strong>Authors:</strong> Yurong Chen, Yu He, Michael I. Jordan, Fan Yao</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 从理论上分析采样和参考选择对LLM对齐的影响，揭示实例依赖采样的排名保证和有偏采样的浓度问题，推导迭代对齐的稳定性条件，验证振荡和熵 collapse 现象，为对齐方法的采样策略设计提供了理论指导。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.12180' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems</h3>
<p><strong>Authors:</strong> Faouzi El Yagoubi, Ranwa Al Mallah, Godwin Badu-Marfo</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出首个覆盖多Agent系统内部通道的隐私泄露基准，测试发现内部通道（如Agent间消息）是主要漏洞，为大模型安全与对齐提供关键评估工具。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11510' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs</h3>
<p><strong>Authors:</strong> Thomas Jiralerspong, Trenton Bricken</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出跨架构模型差异发现方法，通过DFCs识别模型的安全关键特征（如Qwen3的CCP对齐、Llama3.1的American exceptionalism），属于大模型安全与对齐方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11729' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AIR: Improving Agent Safety through Incident Response</h3>
<p><strong>Authors:</strong> Zibo Xiao, Jun Sun, Junjie Chen</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出AIR框架通过事故响应提升Agent安全，实现90%以上的检测、 remediation和eradication率，为大模型安全与对齐提供关键机制。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.11749' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</h3>
<p><strong>Authors:</strong> Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 论文研究测试时验证以缩小“意图-动作差距”，提出CoVer对比验证器提升VLA模型对齐性能，符合大模型安全与对齐中的alignment方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.12281' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment</h3>
<p><strong>Authors:</strong> Jiajun Chen, Hua Shen</p>
<p><strong>Published:</strong> 2026-02-13</p>
<p><strong>Reason:</strong> 提出VAT框架量化对齐干预对互联价值观的影响，基于Schwartz价值理论数据集揭示对齐的系统性副作用，为大模型安全对齐提供理论工具
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2602.12134' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>