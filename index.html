<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-10-28</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>自动驾驶与大模型</a>
<a href='#' >多模态大模型</a>
<a href='#' >深度学习理论</a>
<a href='#' >深度学习可解释性</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-10-28</h1>
<div class='history-selector'>
<label for='history-date'>选择历史日期:</label>
<select id='history-date' onchange='onHistoryDateChange(this)'>
<option value='index.html'>最新 (2025-10-28)</option>
</select>
</div>
<div class='meta-info'><p>更新于北京时间：2025-10-28 12:36:50</p>
<p>已自动阅读了 476 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：246817</p>
</div>
<div id='' class='field-section'>
<h2>自动驾驶与大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs</h3>
<p><strong>Authors:</strong> Haicheng Liao, Bonan Wang, Junxian Yang, Chengyue Wang, Zhengbin He, Guohui Zhang, Chengzhong Xu, Zhenning Li</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Uses world models and LLMs to handle autonomous driving corner cases, directly aligning with autonomous driving and large model research.
Score: 9
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.21867' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation</h3>
<p><strong>Authors:</strong> Karim Elmaaroufi, Liheng Lai, Justin Svegliato, Yutong Bai, Sanjit A. Seshia, Matei Zaharia</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 基于自动驾驶数据集（BDD100k、NuImages、Waymo）生成高保真空间推理VQA对，解决了现有数据的低质量问题，训练后的VLMs空间推理能力显著提升，直接服务于自动驾驶与大模型的空间理解需求
Score: 9
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22118' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method</h3>
<p><strong>Authors:</strong> Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 构建大规模语义occupancy数据集（Nuplan-Occ），提出统一框架生成occupancy、多视图视频和LiDAR点云，提升自动驾驶场景生成的保真度和 scalability，对自动驾驶与大模型中的occupancy prediction有核心价值。
Score: 9
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22973' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> UrbanVLA: A Vision-Language-Action Model for Urban Micromobility</h3>
<p><strong>Authors:</strong> Anqi Li, Zhiyong Wang, Jiazhao Zhang, Minghan Li, Yunpeng Qi, Zhibo Chen, Zhizheng Zhang, He Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出route-conditioned VLA框架解决城市微移动导航问题，在MetaUrban任务中大幅超越基线，对自动驾驶与大模型中的多模态导航任务有重要参考价值。
Score: 9
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.23576' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Embodied Navigation with Auxiliary Task of Action Description Prediction</h3>
<p><strong>Authors:</strong> Haru Kondoh, Asako Kanezaki</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Combines embodied navigation with VLM-based action description, relevant to autonomous driving and multi-modal reasoning.
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.21809' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction</h3>
<p><strong>Authors:</strong> Yuhang Gao, Xiang Xiang, Sheng Zhong, Guoyou Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出语言引导的开放集3D占据预测框架，支持监督和自监督学习，通过密集对比学习缓解特征同质化问题，提升了开放场景下的感知能力，直接针对自动驾驶与大模型中的占据预测任务
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22141' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Accident Anticipation via Temporal Occurrence Prediction</h3>
<p><strong>Authors:</strong> Tianhao Zhao, Yiyang Zou, Zihao Mao, Peilun Xiao, Yulin Huang, Hongda Yang, Yuxuan Li, Qun Li, Guobin Wu, Yutian Lin</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对自动驾驶的事故预测任务，将预测目标从当前帧风险得分转为未来多时间步的事故得分，利用精确的事故时间戳作为监督，提升了预测的实用性和安全性，实验证明其在低误报率下的性能优于现有方法
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22260' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DAMap: Distance-aware MapNet for High Quality HD Map Construction</h3>
<p><strong>Authors:</strong> Jinpeng Dong, Chen Li, Yutong Lin, Jingwen Fu, Sanping Zhou, Nanning Zheng</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对高清地图构建中的任务错位问题，提出距离感知的损失函数和任务调制的可变形注意力模块，显著提升HD地图的分类和定位精度，对自动驾驶中的高精度地图生成有核心价值。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22675' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios</h3>
<p><strong>Authors:</strong> Ziyu Wang ( ), Wenhao Li ( ), Ji Wu ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出深度引导的查询生成器与混合注意力机制，解决了3D目标检测中参考点远离目标导致的误检问题，在nuScenes数据集上mAP提升6.3%、NDS提升4.3%，为自动驾驶与大模型的3D感知任务提供了更精准的解决方案。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.23144' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes</h3>
<p><strong>Authors:</strong> Sixian Liu ( ), Chen Xu ( ), Qiang Wang ( ), Donghai Shi ( ), Yiwen Li ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出自适应门控融合模块，通过跨模态注意力选择性整合可靠特征，在复杂场景（如传感器退化、环境干扰）下显著提升3D目标检测鲁棒性，在KITTI数据集上准确率达93.92%，为自动驾驶与大模型的多模态感知提供了强鲁棒性方案。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.23151' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting</h3>
<p><strong>Authors:</strong> Hoonhee Cho ( ), Jae-Young Kang ( ), Giwon Lee ( ), Hyemin Yang ( ), Heejun Park ( ), Seokwoo Jung ( ), Kuk-Jin Yoon ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出VR-Drive框架，通过3D高斯溅射辅助的端到端学习解决自动驾驶中的视角鲁棒性问题，释放了首个评估视角泛化性的基准数据集，推动了自动驾驶与大模型的端到端系统向更真实场景的落地。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.23205' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception</h3>
<p><strong>Authors:</strong> Karthikeyan Chandra Sekaran ( ), Markus Geisler ( ), Dominik Rössle ( ), Adithya Mohan ( ), Daniel Cremers ( ), Wolfgang Utschick ( ), Michael Botsch ( ), Werner Huber ( ), Torsten Schön ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出首个跨多个交叉路口的大规模V2X数据集，包含车辆与基础设施的多模态（RGB、LiDAR、热成像）数据，为自动驾驶与大模型的合作感知研究提供了更贴近真实场景的数据支撑。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.23478' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles</h3>
<p><strong>Authors:</strong> Seyed Ahmad Hosseini Miangoleh, Amin Jalal Aghdasian, Farzaneh Abdollahi</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 结合BLIP视觉语言模型与PPO强化学习，提升自动驾驶车道保持的稳定性，符合自动驾驶与大模型方向。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22370' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model</h3>
<p><strong>Authors:</strong> Weizheng Wang, Obi Ike, Soyun Choi, Sungeun Hong, Byung-Cheol Min</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出结合LLM演绎推理与结构化世界模型的导航框架，解决社交机器人导航的安全与一致性问题，对自动驾驶与大模型中的导航任务有参考价值。
Score: 8
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.23509' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DiffusionLane: Diffusion Model for Lane Detection</h3>
<p><strong>Authors:</strong> Kunyang Zhou, Yeqin Shao</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 将扩散模型应用于车道检测，提出混合解码策略和辅助头解决噪声锚点的特征表示问题，提升了模型的泛化能力，在多个自动驾驶数据集上取得优于现有方法的结果
Score: 7
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22236' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learn2Drive: A neural network-based framework for socially compliant automated vehicle control</h3>
<p><strong>Authors:</strong> Yuhui Liu, Samannita Halder, Shian Wang, Tianyi Li</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出基于LSTM和物理约束的自动驾驶控制框架，考虑与人类车辆的交互，提升交通效率，符合自动驾驶与大模型方向。
Score: 7
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.21736' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Next-Generation LLM for UAV: From Natural Language to Autonomous Flight</h3>
<p><strong>Authors:</strong> Liangqi Yuan, Chuhao Deng, Dong-Jun Han, Inseok Hwang, Sabine Brunswicker, Christopher G. Brinton</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出NeLV系统，将LLM集成到多尺度UAV任务中，实现自然语言到自主飞行的转化，符合自动驾驶与大模型方向。
Score: 7
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.21739' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead</h3>
<p><strong>Authors:</strong> Shireen Kudukkil Manchingal, Armand Amaritei, Mihir Gohad, Maryam Sultana, Julian F. P. Kooij, Fabio Cuzzolin, Andrew Bradley</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出基于RS-NN的不确定性感知系统，提升自动驾驶预测安全性，符合自动驾驶与大模型方向。
Score: 7
Field: 自动驾驶与大模型</p>
<p><a href='https://arxiv.org/abs/2510.22680' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> STATUS Bench: A Rigorous Benchmark for Evaluating Object State Understanding in Vision-Language Models</h3>
<p><strong>Authors:</strong> Mahiro Ukai, Shuhei Kurita, Nakamasa Inoue</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 首个系统评估视觉语言模型物体状态理解能力的基准，包含多任务设计和大规模训练数据集，揭示当前模型在细粒度状态区分上的不足，对多模态语义理解研究有重要推动作用。
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22571' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation</h3>
<p><strong>Authors:</strong> Shu Zhao, Tianyi Shen, Nilesh Ahuja, Omesh Tickoo, Vijaykrishnan Narayanan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出自适应多模态检索增强生成框架，解决检索必要性、模态选择和信息利用问题，显著提升多模态大模型的生成质量和效率，对RAG在多模态中的应用有重要改进。
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22694' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models</h3>
<p><strong>Authors:</strong> Lexiang Xiong, Chengyu Liu, Jingwen Ye, Yan Liu, Yuecong Xu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出训练-free的扩散模型概念擦除框架，通过文本嵌入的动态调整和视觉反馈解决有害内容生成问题，提升生成模型的安全性，对多模态图像生成的可控性有重要价值。
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Positional Preservation Embedding for Multimodal Large Language Models</h3>
<p><strong>Authors:</strong> Mouxiao Huang, Borui Jiang, Dehua Zheng, Hailin Hu, Kai Han, Xinghao Chen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出位置保留嵌入，改进多模态大模型的视觉token压缩，保持时空结构，显著提升布局理解和时间理解性能，对多模态模型的tokenizer设计有核心贡献。
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22936' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity</h3>
<p><strong>Authors:</strong> Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出Scale-Adaptive Object Tokenizer（符合多模态大模型的tokenizer方向），构建对象中心的多模态大模型框架，提升细粒度时空对象指代能力，直接针对多模态大模型的tokenizer与图像理解需求。
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23603' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence</h3>
<p><strong>Authors:</strong> Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 构建了视觉-编程接口的基础模型JanusCoder，通过大规模多模态代码语料训练，实现文本指令、视觉输入到代码的统一生成，解决了多模态大模型中视觉与编程逻辑的对齐问题，性能超越现有模型
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23538' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> ReCode: Unify Plan and Action for Universal Granularity Control</h3>
<p><strong>Authors:</strong> Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出递归代码生成（ReCode）范式，统一Agent的计划与行动，实现跨粒度的动态决策，解决了现有LLM Agent中计划与行动刚性分离的问题，性能与数据效率显著提升，属于多模态大模型中Agent架构的关键创新
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23564' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Dexbotic: Open-Source Vision-Language-Action Toolbox</h3>
<p><strong>Authors:</strong> Bin Xie, Erjin Zhou, Fan Jia, Hao Shi, Haoqiang Fan, Haowei Zhang, Hebei Li, Jianjian Sun, Jie Bin, Junwen Huang, Kai Liu, Kaixin Liu, Kefan Gu, Lin Sun, Meng Zhang, Peilong Han, Ruitao Hao, Ruitao Zhang, Saike Huang, Songhan Xie, Tiancai Wang, Tianle Liu, Wenbin Tang, Wenqi Zhu, Yang Chen, Yingfei Liu, Yizhuang Zhou, Yu Liu, Yucheng Zhao, Yunchao Ma, Yunfei Wei, Yuxiang Chen, Ze Chen, Zeming Li, Zhao Wu, Ziheng Zhang, Ziming Liu, Ziwei Yan, Ziyu Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 开源VLA工具箱支持主流VLA方法与预训练模型，为多模态大模型研究提供实用工具支撑，加速VLA方向研究进展。
Score: 9
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23511' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Token-Level Inference-Time Alignment for Vision-Language Models</h3>
<p><strong>Authors:</strong> Kejia Chen, Jiawen Zhang, Jiacong Hu, Kewei Gao, Jian Lou, Zunlei Feng, Mingli Song</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Proposes token-level alignment for VLMs to reduce hallucination, directly improving multi-modal model reliability.
Score: 8.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21794' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images</h3>
<p><strong>Authors:</strong> Yichi Zhang, Zhuo Chen, Lingbing Guo, Lei Liang, Wen Zhang, Huajun Chen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Focuses on structured reasoning for multi-modal relational knowledge, improving multi-modal model capabilities.
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21828' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models</h3>
<p><strong>Authors:</strong> Gyubeum Lim, Yemo Koo, Vijay Krishna Madisetti</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Proposes SCoPE VLM for efficient document navigation, relevant to multi-modal GUI Agent tasks.
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21850' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Proportion and Perspective Control for Flow-Based Image Generation</h3>
<p><strong>Authors:</strong> Julien Boudier, Hugo Caselles-Dupré</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Proposes ControlNets for flow-based image generation, relevant to multi-modal image synthesis.
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21763' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs</h3>
<p><strong>Authors:</strong> Jiaao Yu, Shenwei Li, Mingjie Han, Yifei Yin, Wenzheng Song, Chenghao Jia, Man Lan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Enhances VLM reasoning with masked visual context prediction, improving multi-modal commonsense capabilities.
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21807' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation</h3>
<p><strong>Authors:</strong> Zheng Qi, Chao Shang, Evangelia Spiliopoulou, Nikolaos Pappas</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对多模态大模型的幻觉问题，提出GIFT方法，通过跟踪视觉注意力的“gaze shifts”生成视觉显著性图，并融合用户查询注意力实现跨模态增强，有效缓解了VLMs对语言先验的过度依赖
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22067' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning</h3>
<p><strong>Authors:</strong> Chen Chen, Majid Abdolshah, Violetta Shevchenko, Hongdong Li, Chang Xu, Pulak Purkait</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对扩散超分辨率的语义歧义问题，提出空间重聚焦的文本条件框架，通过空间重聚焦交叉注意力和目标分类器-free引导提升语义保真度，属于多模态图像生成的重要改进。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22534' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Self-Attention Decomposition For Training Free Diffusion Editing</h3>
<p><strong>Authors:</strong> Tharun Anand, Mohammad Hassan Vali, Arno Solin</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出从预训练扩散模型的自注意力权重直接推导语义编辑方向的方法，无需额外数据或微调，显著提升扩散模型编辑的效率和质量，对多模态图像生成的可控编辑有实用价值。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22650' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views</h3>
<p><strong>Authors:</strong> Anna Deichler, Jonas Beskow</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 构建多视图（自我中心/外部中心）的多模态接地数据集，包含丰富的指称表达标注，支持具身代理的情境对话研究，对多模态大模型的接地能力评估有重要价值。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22672' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance</h3>
<p><strong>Authors:</strong> Jiuniu Wang, Gongjie Zhang, Quanhao Qian, Junlong Gao, Deli Zhao, Ran Xu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出多模态引导的SVG生成框架，支持文本、图像和部分SVG输入，构建大规模SVG数据集，提升矢量图生成的交互性和质量，属于多模态大模型在矢量图生成的应用。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22684' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</h3>
<p><strong>Authors:</strong> Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出实例接地的几何Transformer，统一3D重建和实例级上下文理解，构建大规模3D实例数据集，提升3D语义重建的一致性和质量，属于多模态大模型在3D理解的应用。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22706' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models</h3>
<p><strong>Authors:</strong> Jiaxiang Liu, Jiawei Du, Xiao Liu, Prayag Tiwari, Mingkun Xu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出自校准一致性框架，通过语义一致性和空间一致性模块提升视觉语言模型的对抗鲁棒性，保持零样本性能，对多模态大模型的可靠性有重要改进。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22785' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MAGIC-Talk: Motion-aware Audio-Driven Talking Face Generation with Customizable Identity Control</h3>
<p><strong>Authors:</strong> Fatemeh Nazarieh, Zhenhua Feng, Diptesh Kanojia, Muhammad Awais, Josef Kittler</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出扩散-based的音频驱动 talking face生成框架，通过参考网络和运动先验提升身份保留和时间一致性，支持自定义身份控制，属于多模态大模型在音频-视觉生成的应用。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22810' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment</h3>
<p><strong>Authors:</strong> Zahraa Al Sahili, Maryam Fetanat, Maimuna Nowaz, Ioannis Patras, Matthew Purver</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出用多模态大模型评估文本到图像的公平性和prompt对齐，通过可解释的评估协议提升评估的可靠性，对多模态生成模型的公平性和可控性有重要价值。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22827' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models</h3>
<p><strong>Authors:</strong> Aya Nakayama, Brian Wong, Yuji Nishimura, Kaito Tanaka</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出跨风格的视觉推理框架，解决大视觉语言模型的“风格陷阱”问题，提升跨风格语义理解的鲁棒性，对多模态大模型的泛化能力有重要改进。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22838' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation</h3>
<p><strong>Authors:</strong> Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出轻量级双融合框架，结合预训练的理解和生成模型，通过多模态自注意力提升统一多模态任务的性能，同时保持效率，对多模态大模型的轻量化有实用价值。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22946' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FAME: Fairness-aware Attention-modulated Video Editing</h3>
<p><strong>Authors:</strong> Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Zhidong Li, Longbing Cao</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出视频编辑的公平性框架，通过公平嵌入和注意力调制缓解职业性别偏见，保持时间一致性和prompt对齐，对多模态视频生成的公平性有重要改进。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22960' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency</h3>
<p><strong>Authors:</strong> Quanjian Song, Donghao Zhou, Jingyu Lin, Fei Shen, Jiaze Wang, Xiaowei Hu, Cunjian Chen, Pheng-Ann Heng</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出场景导向的故事生成框架，通过场景规划和一致性模块提升故事的场景连贯性，结合VLM引导的场景生成，属于多模态大模型在故事生成的应用。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22994' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CoMo: Compositional Motion Customization for Text-to-Video Generation</h3>
<p><strong>Authors:</strong> Youcan Xu, Zhen Wang, Jiaxin Shi, Kexin Li, Feifei Shao, Jun Xiao, Yi Yang, Jun Yu, Long Chen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出文本到视频的组合运动定制框架，通过静态-动态解耦和分-合策略解决多主体运动控制问题，提升视频生成的运动精度，对多模态文本到视频生成有重要改进。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23007' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark</h3>
<p><strong>Authors:</strong> Huixuan Zhang ( ), Xiaojun Wan ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对text-to-image模型在多实例、多关系场景下的图像-文本对齐问题，提出大规模基准数据集M³T2IBench及Revise-Then-Enforce方法，为多模态大模型的image generation任务提供了更贴合实际的评估基准与优化方向。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Revisiting Multimodal Positional Encoding in Vision-Language Models</h3>
<p><strong>Authors:</strong> Jie Huang ( ), Xuejing Liu ( ), Sibo Song ( ), Ruibing Hou ( ), Hong Chang ( ), Junyang Lin ( ), Shuai Bai ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 系统分析了多模态旋转位置编码（RoPE）的核心组件（位置设计、频率分配），提出MHRoPE和MRoPE-I两种改进方案，显著提升了多模态大模型的通用与细粒度理解性能，对VLMs的架构优化具有重要意义。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23095' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning</h3>
<p><strong>Authors:</strong> Shijian Wang ( ), Jiarui Jin ( ), Xingjian Wang ( ), Linxin Song ( ), Runhao Fu ( ), Hecheng Wang ( ), Zongyuan Ge ( ), Yuan Lu ( ), Xuelian Cheng ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出Video-Thinker框架，通过强化学习让MLLMs自主利用grounding和captioning能力生成视频推理线索，在Video-Holmes、CG-Bench-Reasoning等基准上显著超越基线，推动了多模态大模型的视频理解与推理能力提升。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23473' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation</h3>
<p><strong>Authors:</strong> Walid Bousselham ( ), Hilde Kuehne ( ), Cordelia Schmid ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出VOLD框架，通过策略蒸馏将LLM的推理能力迁移到VLM，在MMMU-Pro、MathVision等基准上显著提升多模态推理性能，推动了多模态大模型的reasoning能力跨模态迁移研究。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23497' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time</h3>
<p><strong>Authors:</strong> Yaoli Liu ( ), Yao-Xiang Ding ( ), Kun Zhou ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出训练-free的多主题LoRA融合方法，通过交叉注意力权重自动生成上下文感知掩码，解决了多主题生成中的干扰问题，显著提升了多模态大模型的image generation质量与 usability。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23515' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</h3>
<p><strong>Authors:</strong> Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Yu Qiao, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对以自我为中心的视频推理任务，构建时空链-of-thought（CoT）的多模态大模型框架，提出大规模egocentric QA数据集EgoRe-5M，提升细粒度时空定位与推理能力，直接对应多模态大模型的图像理解与推理方向。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23569' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models</h3>
<p><strong>Authors:</strong> Hongkai Lin, Dingkang Liang, Mingyang Du, Xin Zhou, Xiang Bai</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 基于预训练文本到图像扩散模型，提出统一图像生成与深度估计的模型MERGE，保留生成能力的同时实现高精度深度估计，属于多模态大模型的图像生成与理解方向。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval</h3>
<p><strong>Authors:</strong> Binxiao Xu, Junyu Feng, Ruichuan An, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出通过KV-Cache存储用户视觉（图像 patches）与文本（元数据）信息，实现个性化多模态AI助手的响应优化，直接针对多模态大模型的个性化落地问题
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22765' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes</h3>
<p><strong>Authors:</strong> Guanyu Yao, Qiucheng Wu, Yang Zhang, Zhaowen Wang, Handong Zhao, Shiyu Chang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对多模态大语言模型（MLLMs）中“重文本、轻视觉”的推理失衡问题，从训练数据与损失设计角度提出改进策略，属于多模态大模型的核心推理能力优化
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22836' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier</h3>
<p><strong>Authors:</strong> Hyeongseop Rha, Jeong Hun Yeo, Yeonju Kim, Yong Man Ro</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出情感理由验证器（ERV）与解释奖励机制，提升多模态LLM的情感推理一致性与解释忠实性，属于多模态大模型的情感理解与交互优化
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23506' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation</h3>
<p><strong>Authors:</strong> Anthony Opipari, Aravindhan K Krishnan, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo, Arnie Sen, Odest Chadwicke Jenkins</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出在线3D高斯Splatting显式记忆增强视频分割，解决隐式记忆局限性，提升分割准确性与一致性，对多模态大模型中的视频理解有参考价值。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23521' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation</h3>
<p><strong>Authors:</strong> Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, Kuan-Hsun Tu, Tsung-Wei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 利用VLM、2D-to-3D生成等技术将真实视频转换为模拟环境，解决机器人政策评估 scalability问题，对多模态大模型在机器人中的应用有参考价值。
Score: 8
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23571' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> LSF-Animation: Label-Free Speech-Driven Facial Animation via Implicit Feature Representation</h3>
<p><strong>Authors:</strong> Xin Lu, Chuanqing Zhuang, Chenxi Jin, Zhengda Lu, Yiqun Wang, Wu Liu, Jun Xiao</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Combines speech and facial animation using implicit features, advancing multi-modal generation.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21864' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge</h3>
<p><strong>Authors:</strong> Shu-Hao Zhang, Wei-Cheng Tang, Chen Wu, Peng Hu, Nan Li, Liang-Jie Zhang, Qi Zhang, Shao-Qun Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Compresses CLIP with ternary weights, improving multi-modal model efficiency.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21879' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Face-MakeUpV2: Facial Consistency Learning for Controllable Text-to-Image Generation</h3>
<p><strong>Authors:</strong> Dawei Dai, Yinxiu Zhou, Chenghang Li, Guolai Jiang, Chengfang Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Focuses on controllable facial image generation with consistency learning, relevant to multi-modal image synthesis.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21775' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Semantic Relation-Enhanced CLIP Adapter for Domain Adaptive Zero-Shot Learning</h3>
<p><strong>Authors:</strong> Jiaao Yu, Mingjie Han, Jinkun Jiang, Junyu Dong, Tao Gong, Man Lan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Improves CLIP's zero-shot learning with semantic relations, relevant to multi-modal model adaptation.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21808' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</h3>
<p><strong>Authors:</strong> Zhuoming Li, Aitong Liu, Mengxi Jia, Tengxiang Zhang, Dell Zhang, Xuelong Li</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Uses LVLMs for gesture understanding, bridging motion and semantics in multi-modal systems.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21814' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Improving the Physics of Video Generation with VJEPA-2 Reward Signal</h3>
<p><strong>Authors:</strong> Jianhao Yuan, Xiaofeng Zhang, Felix Friedrich, Nicolas Beltran-Velez, Melissa Hall, Reyhane Askari-Hemmat, Xiaochuang Han, Nicolas Ballas, Michal Drozdzal, Adriana Romero-Soriano</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Enhances video generation physics using VJEPA-2, relevant to multi-modal video synthesis.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21840' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?</h3>
<p><strong>Authors:</strong> Michael Aerni, Joshua Swanson, Kristina Nikolić, Florian Tramèr</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Investigates multimodal model's ability to describe images from memory, relevant to multi-modal image understanding.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21842' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Frame-Difference Guided Dynamic Region Perception for CLIP Adaptation in Text-Video Retrieval</h3>
<p><strong>Authors:</strong> Jiaao Yu, Mingjie Han, Tao Gong, Jian Zhang, Man Lan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Adapts CLIP for text-video retrieval with dynamic region perception, relevant to multi-modal retrieval.
Score: 7.5
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21806' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models</h3>
<p><strong>Authors:</strong> Alexa R. Tartaglini, Satchel Grant, Daniel Wurgaft, Christopher Potts, Judith E. Fan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Diagnoses VLM failures in data visualization using activation patching, relevant to multi-modal model interpretability.
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21740' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT</h3>
<p><strong>Authors:</strong> Hyeonsu Kang, Emily Bao, Anjan Goswami</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 引入VLM-SlideEval框架，针对PPT幻灯片的结构化理解、扰动敏感性和叙事连贯性评估多模态大模型，填补了VLMs在办公场景特定任务上的评估空白，对多模态大模型的应用落地有指导意义
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22045' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mitigating Coordinate Prediction Bias from Positional Encoding Failures</h3>
<p><strong>Authors:</strong> Xingjian Tao, Yiwei Wang, Yujun Cai, Yihong Luo, Jing Tang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 研究多模态大模型在高分辨率输入下的位置编码失效问题，发现其导致坐标预测的方向偏差，并提出VPSG方法通过打乱位置编码的辅助解码来纠正偏差，提升了MLLMs的空间推理能力
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22102' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Mint: A Simple Test-Time Adaptation of Vision-Language Models against Common Corruptions</h3>
<p><strong>Authors:</strong> Wenxuan Bao, Ruxi Deng, Jingrui He</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 分析多模态大模型（CLIP）在输入corruption下的嵌入方差坍缩问题，提出Mint方法在测试时通过最大化类间方差提升模型鲁棒性，无需额外训练，适用于多种CLIP架构
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HARMONY: Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models</h3>
<p><strong>Authors:</strong> Erum Mushtaq, Zalan Fabian, Yavuz Faruk Bakman, Anil Ramakrishna, Mahdi Soltanolkotabi, Salman Avestimehr</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对多模态大模型的不确定性估计问题，结合模型隐藏激活和输出分布提出HARMONY框架，提升了VLMs在高风险应用（如自动驾驶）中的可靠性，实验证明其优于现有单模态方法
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22171' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LongCat-Video Technical Report</h3>
<p><strong>Authors:</strong> Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, Tong Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出13.6B参数的多模态大模型LongCat-Video，支持文本到视频、图像到视频等任务，擅长长视频生成和高效推理，采用粗到细生成策略和块稀疏注意力提升效率，推进了视频生成技术的发展
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22200' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Top-Down Semantic Refinement for Image Captioning</h3>
<p><strong>Authors:</strong> Jusheng Zhang, Kaitong Cai, Jing Yang, Jian Wang, Chengpei Tang, Keze Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 将图像captioning重新定义为分层规划问题，提出TDSR框架结合蒙特卡洛树搜索（MCTS）优化多模态大模型的caption生成，提升了叙事连贯性和细节丰富度，实验证明其优于现有方法
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22391' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection</h3>
<p><strong>Authors:</strong> Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 构建基于拼图的视觉任务基准，评估多模态大模型的推理一致性与错误定位能力，针对多模态推理的关键问题（逻辑错误检测）提供诊断工具，符合多模态大模型的推理方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23594' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards</h3>
<p><strong>Authors:</strong> Jiajun Fan, Roger Ren, Jingyuan Li, Rahul Pandey, Prashanth Gurunath Shivakumar, Ivan Bulyko, Ankur Gandhe, Ge Liu, Yile Gu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对音频LLM的推理能力，提出推理过程奖励机制解决测试时逆缩放问题，提升多模态（音频+文本）推理的一致性与有效性，属于多模态大模型的audio+LLM方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.20867' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Multimodal Negative Learning</h3>
<p><strong>Authors:</strong> Baoquan Gong, Xiyuan Gao, Pengfei Zhu, Qinghua Hu, Bing Cao</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出负学习框架解决多模态学习中的模态不平衡问题，保留弱模态的独特信息，属于多模态大模型的模态对齐方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.20877' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting</h3>
<p><strong>Authors:</strong> Sungjun Cho, Changho Shin, Suenggwan Jo, Xinya Yan, Shourjo Aditya Chaudhuri, Frederic Sala</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 结合LLM与贝叶斯状态空间模型，实现多模态时间序列（数值+文本）的联合预测，属于多模态大模型的time-series+text方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.20952' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models</h3>
<p><strong>Authors:</strong> Zihao Fu, Ryan Brown, Shun Shao, Kai Rawal, Eoin Delaney, Chris Russell</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对文本到图像模型的性别、种族等偏见问题，提出模型无关的后处理框架，通过投影和噪声注入实现多属性去偏，平衡了公平性与图像质量，是多模态生成中的重要应用研究。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.21363' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> LightAgent: Mobile Agentic Foundation Models</h3>
<p><strong>Authors:</strong> Yangqin Jiang, Chao Huang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对移动GUI代理的设备-云协作难题，提出LightAgent框架，增强小模型的决策能力并降低云成本，直接对应用户关注的多模态大模型中的GUI Agent方向，有实际应用价值。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22009' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Multi-Agent Evolve: LLM Self-Improve through Co-evolution</h3>
<p><strong>Authors:</strong> Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhan, Mostofa Patwary, Jiaxuan You</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出多智能体协同进化框架，通过Proposer-Solver-Judge trio提升LLM推理能力，减少对人工数据的依赖，符合多模态大模型研究方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23595' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Alita-G: Self-Evolving Generative Agent for Agent Generation</h3>
<p><strong>Authors:</strong> Jiahao Qiu, Xuan Qi, Hongru Wang, Xinzhe Juan, Yimin Wang, Zelin Zhao, Jiayi Geng, Jiacheng Guo, Peihang Li, Jingzhe Shi, Shilong Liu, Mengdi Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出自进化生成智能体框架ALITA-G，通过MCP工具将通用智能体转化为领域专家，提升复杂推理任务性能，符合多模态大模型方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23601' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating Multi-Robot Teams Using Natural Language</h3>
<p><strong>Authors:</strong> Guangyao Shi, Yuwei Wu, Vijay Kumar, Gaurav S. Sukhatme</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 集成LLM、PDDL规划与整数规划，实现多机器人自然语言协调，提升规划效率，符合多模态大模型方向。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22784' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Large language model-based task planning for service robots: A review</h3>
<p><strong>Authors:</strong> Shaohan Bian, Ying Zhang, Guohui Tian, Zhiqiang Miao, Edmond Q. Wu, Simon X. Yang, Changchun Hua</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 综述了LLM在服务机器人任务规划中的应用，涵盖LLM基础技术与多模态输入任务规划，对多模态大模型在机器人领域的应用研究有参考价值。
Score: 7
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.23357' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> GeoDiffusion: A Training-Free Framework for Accurate 3D Geometric Conditioning in Image Generation</h3>
<p><strong>Authors:</strong> Phillip Mueller, Talip Uenlue, Sebastian Schmidt, Marcel Kollovieh, Jiajie Fan, Stephan Guennemann, Lars Mikelsons</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出训练-free的多模态大模型框架，利用3D几何先验（如关键点和参数关联）实现精确的图像生成几何控制，支持风格迁移和快速编辑，提升了图像生成的可控性
Score: 6
Field: 多模态大模型</p>
<p><a href='https://arxiv.org/abs/2510.22337' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Alias-Free ViT: Fractional Shift Invariance via Linear Attention</h3>
<p><strong>Authors:</strong> Hagay Michaeli, Daniel Soudry</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对视觉Transformer缺乏平移不变性的问题，提出抗混叠的ViT架构，通过线性注意力实现分数平移不变性，改进ViT的架构归纳偏置，对深度学习理论中的网络架构设计有理论贡献。
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22673' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.5/10]</span> Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers</h3>
<p><strong>Authors:</strong> Dogyun Park, Moayed Haji-Ali, Yanyu Li, Willi Menapace, Sergey Tulyakov, Hyunwoo J. Kim, Aliaksandr Siarohin, Anil Kag</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Introduces sparse-dense fusion to improve Diffusion Transformer efficiency, contributing to deep learning theory (network architecture).
Score: 8.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21986' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications</h3>
<p><strong>Authors:</strong> Shamim Yazdani, Akansha Singh, Nripsuta Saxena, Zichong Wang, Avash Palikhe, Deng Pan, Umapada Pal, Jie Yang, Wenbin Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Comprehensive survey of generative models (VAE, GAN, diffusion) relevant to deep learning theory.
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21887' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scaling Non-Parametric Sampling with Representation</h3>
<p><strong>Authors:</strong> Vincent Lu, Aaron Truong, Zeyu Yun, Yubei Chen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出简单的非参数生成模型，基于自然图像的三个原理（空间非平稳性、低层次规律、高层次语义），无需训练即可生成高保真样本，为深度学习理论中的生成模型机制提供了白盒理解
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22196' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Switchable Token-Specific Codebook Quantization For Face Image Compression</h3>
<p><strong>Authors:</strong> Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对人脸图像压缩，提出可变token特定码本量化方法，通过不同图像类别学习 distinct码本，提升低bpp下的重建性能和识别 accuracy，属于深度学习理论中VQ-VAE的改进。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22943' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Nested AutoRegressive Models</h3>
<p><strong>Authors:</strong> Hongyu Wu ( ), Xuhui Fan ( ), Zhangkai Wu ( ), Longbing Cao ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出嵌套自回归（NestAR）模型，通过多尺度模块层级结构将图像生成的时间复杂度从O(n)降至O(log n)，同时提升样本多样性，推动了深度学习理论中自回归生成模型的架构创新与效率优化。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.23028' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling</h3>
<p><strong>Authors:</strong> Ruoyu Wang ( ), Beier Zhu ( ), Junzhi Li ( ), Liangyu Yuan ( ), Chi Zhang ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出AdaSDE单步SDE求解器，通过学习式系数动态调节误差校正强度，在5 NFE下CIFAR-10的FID达4.18、FFHQ达8.05，显著加速了扩散模型的采样过程，推动了深度学习理论中扩散模型的效率优化。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.23285' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</h3>
<p><strong>Authors:</strong> Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出2D-3D联合自监督学习框架，提升空间表示的几何与语义一致性，属于深度学习理论中的网络架构方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.23607' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Global Dynamics of Heavy-Tailed SGDs in Nonconvex Loss Landscape: Characterization and Control</h3>
<p><strong>Authors:</strong> Xingyu Wang, Chang-Han Rhee</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 分析重尾SGD在非凸损失景观中的全局动态，揭示其避免尖锐极小值的机制，属于深度学习理论中的优化器（optimizer）方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.20905' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</h3>
<p><strong>Authors:</strong> Xi Zhang, Xiaolin Wu, Jiamang Wang, Weisi Lin</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出分组晶格向量量化方法用于低比特LLM压缩，属于深度学习理论中的向量量化（VQ-VAE）方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.20984' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> From Information to Generative Exponent: Learning Rate Induces Phase Transitions in SGD</h3>
<p><strong>Authors:</strong> Konstantinos Christopher Tsiolis, Alireza Mousavi-Hosseini, Murat A. Erdogdu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 揭示学习率对SGD的相位 transition影响，证明小学习率到大学率的 regimes变化，属于深度学习理论中的优化器方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization</h3>
<p><strong>Authors:</strong> Xuan Tang, Jichu Li, Difan Zou</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 首次分析了自适应优化器（Adam、Muon）在浮点量化下的收敛性，填补了低精度训练的理论空白，明确量化误差对收敛的影响，对理解低精度训练有效性有重要理论贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21314' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large Language Models</h3>
<p><strong>Authors:</strong> Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出ParaRNN框架，将非线性RNN的序列依赖转化为并行可解的方程组，实现了非线性RNN的高效并行训练（速度提升665倍），突破了RNN的 scalability限制，推动了大尺度非线性序列模型的研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21450' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning</h3>
<p><strong>Authors:</strong> Bingqing Song, Jiaxiang Li, Rong Wang, Songtao Lu, Mingyi Hong</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 构建了量化预训练过程与上下文构造对上下文学习（ICL）性能影响的理论框架，分析了预训练分布与查询任务分布的KL散度等核心因素，属于深度学习理论中ICL机制的基础研究
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22594' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.5/10]</span> Poisson Flow Consistency Training</h3>
<p><strong>Authors:</strong> Anthony Zhang, Mahmut Gokmen, Dennis Hein, Rongjun Ge, Wenjun Xia, Ge Wang, Jin Chen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Develops Poisson Flow Consistency Training for flow models, advancing deep learning theory.
Score: 7.5
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21857' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> It Takes Two to Tango: Two Parallel Samplers Improve Quality in Diffusion Models for Limited Steps</h3>
<p><strong>Authors:</strong> Pedro Cisneros-Velarde</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> Proposes parallel samplers to enhance diffusion model quality with limited steps, relevant to deep learning theory.
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21802' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing</h3>
<p><strong>Authors:</strong> Or Ronai, Vladimir Kulikov, Tomer Michaeli</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出针对流匹配模型的零阶优化框架FlowOpt，通过将整个采样过程视为黑盒优化对象，解决了梯度反向传播的计算问题，并提供了步长的收敛保证，对深度学习理论中的优化器研究有重要贡献
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22010' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</h3>
<p><strong>Authors:</strong> Jing Wang, Jiajun Liang, Jie Liu, Henglin Liu, Gongye Liu, Jun Zheng, Wanyuan Pang, Ao Ma, Zhenyu Xie, Xintao Wang, Meng Wang, Pengfei Wan, Xiaodan Liang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 研究流匹配模型中的隐式过优化问题，提出GRPO-Guard通过归一化重要性比和梯度重加权缓解过优化，提升了训练稳定性和生成质量，属于深度学习理论中的优化器研究进展
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22319' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FARMER: Flow AutoRegressive Transformer over Pixels</h3>
<p><strong>Authors:</strong> Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, Rui Zhu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 结合归一化流（NF）与自回归（AR）模型的图像生成框架，解决像素级建模的冗余问题，属于深度学习理论中的生成模型方向（与VAE/VQ-VAE等相关）。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.23588' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation</h3>
<p><strong>Authors:</strong> Enshu Liu, Qian Chen, Xuefei Ning, Shengen Yan, Guohao Dai, Zinan Lin, Yu Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出条件分数蒸馏实现图像自回归模型的一步采样，解决自回归生成的慢速度问题，属于深度学习理论中的生成模型方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21003' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning</h3>
<p><strong>Authors:</strong> Wanhao Yu, Zheng Wang, Shuteng Niu, Sen Lin, Li Yang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 分析零阶优化在持续学习中的稳定性与可塑性，提出ZO-FC框架平衡遗忘与适应，属于深度学习理论中的优化器方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21019' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data</h3>
<p><strong>Authors:</strong> Hancheng Min, Zhihui Zhu, Ren\'e Vidal</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 证明浅层ReLU网络在正交可分数据上的神经崩溃现象，揭示训练动态与隐式偏置的作用，属于深度学习理论中的网络架构方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21078' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PLAN: Proactive Low-Rank Allocation for Continual Learning</h3>
<p><strong>Authors:</strong> Xiequn Wang, Zhan Zhuang, Yu Zhang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 扩展LoRA到持续学习场景，提出主动低秩分配框架，通过正交基向量和干扰感知选择机制，减少任务间冲突，提升了持续学习的适应性和记忆能力，对LoRA的泛化应用有贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.21188' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Dopamine-driven synaptic credit assignment in neural networks</h3>
<p><strong>Authors:</strong> Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 受神经强化学习启发，开发Dopamine无导数优化器，解决反向传播的计算开销与内存问题，属于深度学习理论中的optimizer方向，具有NeuroAI视角的创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22178' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity</h3>
<p><strong>Authors:</strong> Seonghoon Yu, Dongjun Nam, Dina Katabi, Jeany Son</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出单教师视图增强方法，通过最大化视图间的角多样性生成多样化的教师输出，提升知识蒸馏的效果，无需额外教师模型，属于深度学习理论中的知识蒸馏研究进展
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.22480' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Memory Constrained Dynamic Subnetwork Update for Transfer Learning</h3>
<p><strong>Authors:</strong> A\"el Qu\'elennec, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出LaRa层排序与动态通道采样的子网络适应框架，解决内存约束下的迁移学习问题，属于深度学习理论中的网络架构方向。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2510.20979' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> A Video Is Not Worth a Thousand Words</h3>
<p><strong>Authors:</strong> Sam Pollard ( ), Michael Wray ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 基于Shapley值提出联合特征归因与模态得分计算方法，系统评估了VLM在视频问答任务中的模态交互与空间推理能力，直接对应深度学习可解释性中的Shapley value研究方向，为VLM的可解释性分析提供了重要工具。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.23253' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Caption-Driven Explainability: Probing CNNs for Bias via CLIP</h3>
<p><strong>Authors:</strong> Patrick Koller (Northwestern University, Evanston, Illinois, United States), Amil V. Dravid (University of California, Berkeley, California, United States), Guido M. Schuster (Eastern Switzerland University of Applied Sciences, Rapperswil, St. Gallen, Switzerland), Aggelos K. Katsaggelos (Northwestern University, Evanston, Illinois, United States)</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 提出基于caption的可解释性方法，通过网络手术将待解释模型整合到CLIP中，识别对模型预测贡献最大的主导概念，有效减少模型对虚假特征的依赖，属于深度学习可解释性的重要进展
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.22035' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Faithfulness of Visual Thinking: Measurement and Enhancement</h3>
<p><strong>Authors:</strong> Zujing Liu ( ), Junwen Pan ( ), Qi She ( ), Yuan Gao ( ), Guisong Xia ( )</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对MCoT视觉推理的“不忠实”问题（视觉线索不可靠、不充分），提出SCCM方法提升视觉推理的忠实性，为深度学习可解释性中的视觉推理可信度分析提供了新的评估维度与优化方案。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.23482' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Additive Models Explained: A Computational Complexity Approach</h3>
<p><strong>Authors:</strong> Shahaf Bassan, Michal Moshkovitz, Guy Katz</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 从计算复杂度角度系统分析加性模型（如GAM、神经加性模型）的可解释性，揭示了输入空间结构、组件模型类型等对解释难度的影响，为可解释性的可行性提供了理论依据。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.21292' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism</h3>
<p><strong>Authors:</strong> Reda Marzouk, Shahaf Bassan, Guy Katz</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 结合SHAP与张量网络，提出可证明可并行的可解释性方法，解决SHAP在复杂模型（如神经网络）中的计算难题，直接涉及用户关注的Shapley value可解释性方向，且有理论 tractability保证。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.21599' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mechanistic Interpretability for Neural TSP Solvers</h3>
<p><strong>Authors:</strong> Reuben Narad, Leonard Boussioux, Michael Wagner</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 首次将稀疏自编码器（SAE）等机制可解释性技术应用于Transformer-based TSP求解器，揭示模型学习的几何特征（如凸包节点、局部密集区域），属于深度学习可解释性在具体模型中的创新应用。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.21693' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ProfileXAI: User-Adaptive Explainable AI</h3>
<p><strong>Authors:</strong> Gilber A. Corrales, Carlos Andr\'es Ferro S\'anchez, Reinel Tabares-Soto, Jes\'us Alfonso L\'opez Sotelo, Gonzalo A. Ruz, Johan Sebastian Pi\~na Dur\'an</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 结合SHAP、LIME等可解释性方法与检索增强LLM，生成用户自适应的模型解释，解决了现有可解释性方法的通用性不足问题，直接对应深度学习可解释性的研究方向
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.22998' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the flow matching interpretability</h3>
<p><strong>Authors:</strong> Francesco Pivi, Simone Gazza, Davide Evangelista, Roberto Amadini, Maurizio Gabbrielli</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Reason:</strong> 针对流动匹配生成模型中间步骤不可解释的问题，将物理过程（如2D Ising模型）嵌入流动步骤，使生成轨迹对应可解释的物理状态转移，提升了生成模型的可解释性。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2510.21210' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>