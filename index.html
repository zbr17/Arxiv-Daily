<!DOCTYPE html>
<html lang='zh-CN'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>ArXiv 每日推荐 - 2025-11-18</title>

        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; background-color: #f6f8fa; margin: 0; padding: 0; }
            .container { max-width: 900px; margin: 20px auto; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
            h1, h2, h3 { border-bottom: 2px solid #eaecef; padding-bottom: 0.3em; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 2.5em; }
            h3 { font-size: 1.2em; border-bottom: 1px solid #eee; }
            .navbar { background-color: #333; overflow: hidden; position: sticky; top: 0; z-index: 100; }
            .navbar a { float: left; display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 17px; }
            .navbar a:hover { background-color: #ddd; color: black; }
            .navbar a.active { background-color: #007bff; color: white; }
            .meta-info { font-size: 0.9em; color: #586069; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 20px; }
            .paper-card { margin-bottom: 1.5em; padding-bottom: 1em; }
            .paper-card p { margin: 0.5em 0; }
            .paper-card a { color: #007bff; text-decoration: none; font-weight: bold; }
            .paper-card a:hover { text-decoration: underline; }
            .score { font-weight: bold; color: #d9534f; }
            .field-section { padding-top: 60px; margin-top: -60px; }
            .history-selector { margin: 20px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px; }
            .history-selector label { font-weight: bold; margin-right: 10px; }
            .history-selector select { padding: 5px 10px; border: 1px solid #ddd; border-radius: 3px; }
        </style>
        
</head>
<body>
<div class='navbar'>
<a href='#' class='active'>高效大模型训练与推理</a>
<a href='#' >大模型安全与对齐</a>
<a href='#' >原生多模态大模型</a>
<a href='#' >多模态智能体</a>
<a href='#' >深度学习可解释性</a>
<a href='#' >大模型新技术</a>
<a href='#' >深度学习理论</a>
</div>
<div class='container'>
<h1>ArXiv 每日推荐 - 2025-11-18</h1>
<div class='history-selector'>
<label for='history-date'>选择历史日期:</label>
<select id='history-date' onchange='onHistoryDateChange(this)'>
<option value='index.html'>最新 (2025-11-18)</option>
<option value='history/2025-11-17.html'>2025-11-17</option>
<option value='history/2025-11-16.html'>2025-11-16</option>
<option value='history/2025-11-15.html'>2025-11-15</option>
<option value='history/2025-11-14.html'>2025-11-14</option>
<option value='history/2025-11-13.html'>2025-11-13</option>
<option value='history/2025-11-12.html'>2025-11-12</option>
<option value='history/2025-11-11.html'>2025-11-11</option>
<option value='history/2025-11-10.html'>2025-11-10</option>
<option value='history/2025-11-09.html'>2025-11-09</option>
<option value='history/2025-11-08.html'>2025-11-08</option>
<option value='history/2025-11-07.html'>2025-11-07</option>
<option value='history/2025-11-06.html'>2025-11-06</option>
<option value='history/2025-11-05.html'>2025-11-05</option>
<option value='history/2025-11-04.html'>2025-11-04</option>
<option value='history/2025-11-03.html'>2025-11-03</option>
<option value='history/2025-11-02.html'>2025-11-02</option>
<option value='history/2025-11-01.html'>2025-11-01</option>
<option value='history/2025-10-31.html'>2025-10-31</option>
<option value='history/2025-10-30.html'>2025-10-30</option>
<option value='history/2025-10-28.html'>2025-10-28</option>
</select>
</div>
<div class='meta-info'><p>更新于北京时间：2025-11-18 22:05:33</p>
<p>已自动阅读了 665 篇最新的论文。</p>
<p>使用模型：doubao-seed-1-6-thinking-250715 | 消耗 Tokens：355227</p>
</div>
<div id='' class='field-section'>
<h2>高效大模型训练与推理</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition</h3>
<p><strong>Authors:</strong> Sudhakar Sah, Nikhil Chabbra, Matthieu Durnerin</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出基于MicroNAS的模型压缩框架CompressNAS，通过全局秩选择解决低秩分解的精度与效率权衡问题，在ResNet-18和YOLOv5上实现高效压缩，属于高效大模型训练与推理中的模型压缩方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11716' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling</h3>
<p><strong>Authors:</strong> Sijie Wang, Qiang Wang, Shaohuai Shi</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出流水线框架PipeDiT加速视频生成的Diffusion Transformers，通过任务流水线与模型解耦提升推理速度（1.06x-4.02x）与内存效率，属于高效大模型训练与推理中的推理加速方向。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12056' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning</h3>
<p><strong>Authors:</strong> Jingqi Xu, Jingxi Lu, Chenghao Li, Sreetama Sarkar, Souvik Kundu, Peter A. Beerel</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出训练无关的视觉token剪枝方法，利用掩码token引导压缩扩散视觉语言模型的推理成本，显著提升吞吐量与延迟性能，属于高效大模型推理的关键突破。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12428' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> BitSnap: Checkpoint Sparsification and Quantization in LLM Training</h3>
<p><strong>Authors:</strong> Qingping Li, Yanxin Peng, Baodong Wu, Shigang Li, Guohao Dai, Shengen Yan, Yu Wang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出LLM训练中checkpoint的稀疏化与量化方法，动态适应不同训练阶段与模型架构，平衡压缩比、速度与精度，有效提升训练存储与加载效率，与高效大模型训练与推理方向直接相关
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12376' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment</h3>
<p><strong>Authors:</strong> Jiacheng Wang, Yejun Zeng, Jinyang Guo, Yuqing Ma, Aishan Liu, Xianglong Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 系统评估小语言模型的量化技术，揭示SLM与LLM在量化敏感性上的差异，提出SLM优化的量化原则，属于高效大模型训练与推理中的模型压缩方向，对小模型的边缘部署有重要价值。
Score: 9
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13023' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning</h3>
<p><strong>Authors:</strong> Shengqin Jiang, Tianqi Kong, Yuankai Qi, Haokui Zhang, Lina Yao, Quan Z. Sheng, Qingshan Liu, Ming-Hsuan Yang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出分层分组提示调优方法，通过层组共享提示和根提示生成子提示的方式，解决持续学习中独立提示更新导致的灾难性遗忘问题，实验验证在四个基准数据集上优于现有方法，提升了提示协调能力和模型稳定性，属于高效大模型训练的重要优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12090' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillation</h3>
<p><strong>Authors:</strong> Shuhan Ye, Yi Yu, Qixin Zhang, Chenqi Kong, Qiangqiang Wu, Kun Wang, Xudong Jiang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出PACE数据集蒸馏框架，通过ST-DSM和PEQ-N模块将大规模事件数据集蒸馏为紧凑合成数据集，大幅减少脉冲神经网络的训练时间（>50×）和存储成本（6000×），同时保持85%的全数据集性能，助力脉冲神经网络的高效训练与边缘部署。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12095' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks</h3>
<p><strong>Authors:</strong> Shuhan Ye, Yi Yu, Qixin Zhang, Chenqi Kong, Qiangqiang Wu, Xudong Jiang, Dacheng Tao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SpikeNM半结构化N:M剪枝框架，通过M-way basis-logit参数化和eligibility-inspired蒸馏从scratch训练稀疏脉冲神经网络，平衡了稀疏性（2:4）和准确性，生成硬件友好的稀疏模式，提升了脉冲神经网络的部署效率，属于高效大模型推理的关键技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12097' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs</h3>
<p><strong>Authors:</strong> Feng Chen, Yefei He, Shaoxuan He, Yuanyu He, Jing Liu, Lequan Lin, Akide Liu, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出训练感知的细粒度稀疏注意力框架，通过查询选择、KV动态分配和KV缓存瘦身，解决长视频MLLM的高计算复杂度问题，实现2.7倍预填充加速和2.4倍解码内存减少，同时保持性能，属于高效大模型推理的关键优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12201' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs</h3>
<p><strong>Authors:</strong> Shuochen Chang, Xiaofeng Zhang, Qingyang Liu, Li Niu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Decider引导的动态token合并方法，通过前一步生成的token构建视觉token重要性图，合并冗余token以缩短序列长度，加速扩散MLLM的推理过程，实验验证在保持性能的同时减少计算量，属于高效大模型推理的关键技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12280' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Fast Reasoning Segmentation for Images and Videos</h3>
<p><strong>Authors:</strong> Yiqing Shen, Mathias Unberath</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出FastReasonSeg框架，通过数字孪生表示 decouple 感知与推理，结合蒸馏和强化学习优化，将模型压缩至0.6B参数（性能优于20倍参数模型），同时实现7.79 FPS吞吐量和2.1GB内存消耗，支持资源受限环境的实时推理分割，属于高效大模型部署的重要进展。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12368' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting</h3>
<p><strong>Authors:</strong> Xu Yang, Gady Agam</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出多尺度低秩适应方法，通过注意力重加权实现卷积与Transformer模型的参数高效微调，解决现有LoRA跨架构泛化问题，属于高效大模型训练的核心优化。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12400' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping</h3>
<p><strong>Authors:</strong> Haotian Dong, Ye Li, Rongwei Lu, Chen Tang, Shu-Tao Xia, Zhi Wang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出VVS框架加速视觉自回归生成的投机解码，通过部分验证跳过减少目标模型前向传递次数（2.8×），属于高效大模型训练与推理中的推理加速
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13587' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding</h3>
<p><strong>Authors:</strong> Shrenik Patel, Daivik Patel</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出CacheFlow用于长视频理解的高效内存管理，通过动态token丢弃和压缩长期记忆，减少87%的token处理量，属于高效大模型训练与推理中的内存优化
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13644' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora</h3>
<p><strong>Authors:</strong> Viviana Luccioli, Rithika Iyengar, Ryan Panley, Flora Haberkorn, Xiaoyu Ge, Leland Crane, Nitish Sinha, Seung Jung Lee</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出M-RARU主动学习方法，高效蒸馏LLM，减少80%样本需求，属于高效大模型训练与推理中的知识蒸馏，降低了LLM部署成本
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11574' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The Anatomy of a Triton Attention Kernel</h3>
<p><strong>Authors:</strong> Burkhard Ringlein, Jan van Lunteren, Radu Stoica, Thomas Parnell</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究LLM推理核心的Triton注意力 kernel优化，实现跨GPU平台的高效推理，提升LLM infra性能，属于高效大模型推理关键技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11581' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EcoSpa: Efficient Transformer Training with Coupled Sparsity</h3>
<p><strong>Authors:</strong> Jinqi Xiao, Cheng Luo, Lingyi Huang, Cheng Yang, Yang Sui, Huy Phan, Xiao Zang, Yibiao Ying, Zhexiang Tang, Anima Anandkumar, Bo Yuan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出EcoSpa框架通过耦合稀疏性实现Transformer高效训练，提升内存利用率和训练速度，属于高效大模型训练的核心技术。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11641' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization</h3>
<p><strong>Authors:</strong> Zhixiong Zhao, Fangxin Liu, Junjie Wang, Chenyang Guan, Zongwu Wang, Li Jiang, Haibing Guan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SpecQuant通过谱分解和自适应截断实现LLM超低比特量化，提升推理效率并减少内存占用，属于高效大模型推理中的量化方向。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11663' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts</h3>
<p><strong>Authors:</strong> Sebastián Andrés Cajas Ordóñez (unknown), Luis Fernando Torres Torres (unknown), Mackenzie J. Meni (unknown), Carlos Andrés Duran Paredes (unknown), Eric Arazo (unknown), Cristian Bosch (unknown), Ricardo Simon Carbajo (unknown), Yuan Lai (unknown), Leo Anthony Celi (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出好奇心驱动的量化混合专家框架，提升边缘设备推理效率和稳定性，属于高效大模型训练与推理的high compression方向
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11743' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Conformal Constrained Policy Optimization for Cost-Effective LLM Agents</h3>
<p><strong>Authors:</strong> Wenwen Si (unknown), Sooyong Jang (unknown), Insup Lee (unknown), Osbert Bastani (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出conformal约束的策略优化，提升LLM agents的成本效益，属于高效大模型训练与推理的efficient inference方向
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11828' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?</h3>
<p><strong>Authors:</strong> Xuanyu Chen, Nan Yang, Shuai Wang, Dong Yuan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析联邦学习中的模型缩放律，通过PAC-Bayes上界推导最优模型大小与客户端数量的关系，为联邦场景下的模型选择与训练效率优化提供理论指导，与高效大模型训练与推理方向相关
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12188' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimal Self-Consistency for Efficient Reasoning with Large Language Models</h3>
<p><strong>Authors:</strong> Austin Feng, Marius Alonso, Ambroise Odonnat</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析自洽性（Self-Consistency）的缩放行为与样本效率，提出Blend-ASC动态分配样本的方法，在减少样本使用的同时保持推理性能，提升LLM推理效率，与高效大模型训练与推理方向相关
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12309' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity</h3>
<p><strong>Authors:</strong> Vladimír Macko, Vladimír Boža</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出MACKO-SpMV优化低稀疏度下的稀疏矩阵乘法，提升剪枝LLM的推理速度与内存效率，属于高效大模型训练与推理中的基础设施优化方向，对稀疏LLM的实际部署有重要意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13061' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs</h3>
<p><strong>Authors:</strong> Shaoyuan Chen, Zhixuan Chen, Dawei Yang, Zhihang Yuan, Qiang Wu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出OTARo方法实现一次微调支持多量化精度切换，提升On-Device LLM的鲁棒性，属于高效大模型训练与推理中的部署优化方向，对实际场景的精度自适应有重要意义。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13147' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer</h3>
<p><strong>Authors:</strong> Zhixin Ou, Peng Liang, Jianchen Han, Baihui Liu, Linbo Qiao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出ParaDySe框架自适应切换Transformer的并行策略，解决动态序列的内存与通信瓶颈，属于高效大模型训练与推理中的训练基础设施优化方向，对长序列LLM训练有帮助。
Score: 8
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13198' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain</h3>
<p><strong>Authors:</strong> Yuqi Xie, Shuhan Ye, Yi Yu, Chong Wang, Qixin Zhang, Jiazhen Xu, Le Shen, Yuanbin Qian, Jiangbo Qian, Guoqi Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出时间步混合知识转移框架（TMKT），通过概率时间步混合和模态感知目标函数，减少静态图像到事件域的模态差距，加速脉冲神经网络的跨模态知识转移，实验验证在多个基准和backbone上的性能提升，属于高效大模型训练的跨模态优化。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12150' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation</h3>
<p><strong>Authors:</strong> Kaixin Zhang, Ruiqing Yang, Yuan Zhang, Shan You, Tao Huang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对视觉自回归模型的计算成本问题，提出ActVAR动态激活框架，在权重（FFN分解为专家子网络）和token（门控选择高潜力token）上引入双稀疏性，用知识蒸馏保持性能，提升生成效率。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12893' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning</h3>
<p><strong>Authors:</strong> Yoonjae Seo, Ermal Elbasani, Jaehong Lee</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对目标检测模型的量化问题，提出形态复杂度感知的空间自适应比特分配方法MCAQ-YOLO，结合课程学习提升量化模型的准确性和收敛效率，属于高效目标检测的量化技术。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12976' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective</h3>
<p><strong>Authors:</strong> Furui Xu, Shaobo Wang, Jiajun Zhang, Chenghao Sun, Haixiang Tang, Linfeng Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对现有数据集剪枝方法的拟合-centric问题，提出从泛化角度剪枝的UNSEEN框架，通过未见过样本的模型评分提升coreset质量，减少训练数据同时保持性能，属于高效训练的数据集优化技术。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.12988' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge</h3>
<p><strong>Authors:</strong> Kabir Khan, Manju Sarkar, Anita Kar, Suresh Ghosh</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出FedGen-Edge框架，通过LoRA实现生成模型的参数高效联邦训练，解决边缘设备资源限制问题，属于高效大模型训练方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11585' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff</h3>
<p><strong>Authors:</strong> Junchen Liu, Yi Sheng</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出双向剪枝-再生方法提升模型精度-稀疏性权衡，实现极端压缩下的性能保持，属于高效大模型训练中的剪枝方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11675' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Stratified Knowledge-Density Super-Network for Scalable Vision Transformers</h3>
<p><strong>Authors:</strong> Longhua Li, Lei Qi, Xin Geng</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出分层知识密度超网络实现ViT可扩展训练，支持不同资源约束下的模型生成，属于高效大模型训练中的超网络方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11683' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation</h3>
<p><strong>Authors:</strong> Eunjeong Jeong (unknown), Nikolaos Pappas (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出计算感知的能量收集联邦学习框架，提升训练效率，属于高效大模型训练与推理的efficient LLM training方向
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11949' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching</h3>
<p><strong>Authors:</strong> Jiacheng Cheng, Xu Zhang, Guanghui Qiu, Yifang Zhang, Yinchuan Li, Kaiyuan Feng</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出pFed1BS联邦学习框架，用一比特随机草图压缩双向通信，解决个性化联邦学习的通信开销问题，属于高效大模型训练与推理中的通信优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13144' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs</h3>
<p><strong>Authors:</strong> Yuxiang Zhang, Zhengxu Yu, Weihang Pan, Zhongming Jin, Qiang Fu, Deng Cai, Binbin Lin, Jieping Ye</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出TokenSqueeze压缩推理LLM的Token序列，在保持性能的同时减少token使用，解决推理中的 latency 与内存问题，属于高效大模型训练与推理中的推理优化方向。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13223' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> KForge: Program Synthesis for Diverse AI Hardware Accelerators</h3>
<p><strong>Authors:</strong> Taras Sereda, Tom St. John, Burak Bartan, Natalie Serrino, Sachin Katti, Zain Asgar</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出KForge框架用LLM agents生成跨硬件的GPU内核，解决AI加速器的程序合成问题，属于高效大模型训练与推理中的硬件优化方向，对多硬件平台的AI部署有实用价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13274' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Hardware optimization on Android for inference of AI models</h3>
<p><strong>Authors:</strong> Iulius Gherasim, Carlos García Sánchez</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究Android平台的AI模型推理优化，比较量化与加速器的使用，属于高效大模型训练与推理中的边缘部署优化方向，对移动设备的AI推理效率提升有实用价值。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.13453' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Experience-Guided Adaptation of Inference-Time Reasoning Strategies</h3>
<p><strong>Authors:</strong> Adam Stein, Matthew Trager, Benjamin Bowman, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出EGuR框架，通过积累的经验在推理时动态生成定制化推理策略（涵盖LLM调用、工具、采样参数等），在多个基准测试中实现了准确率提升与计算成本降低，属于高效大模型推理方向的重要探索。
Score: 7
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11519' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning</h3>
<p><strong>Authors:</strong> Shunyu Wu, Tianyue Li, Yixuan Leng, Jingyi Suo, Jian Lou, Dan Li, See-Kiong Ng</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出LTSV方法通过in-context finetuning实现时间序列基础模型的轻量级数据 valuation，提升数据利用效率，属于高效大模型训练中的数据方向。
Score: 6
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11648' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models</h3>
<p><strong>Authors:</strong> Tianyi Yin, Jingwei Wang, Chenze Wang, Han Wang, Jiexuan Cai, Min Liu, Yunlong Ma, Kun Gao, Yuting Song, Weiming Shen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出R-Tuning通过小波分解重放和语义对齐实现时间序列预训练模型持续适应，解决灾难性遗忘，属于高效大模型训练中的持续适应方向。
Score: 6
Field: 高效大模型训练与推理</p>
<p><a href='https://arxiv.org/abs/2511.11685' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型安全与对齐</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Defending Unauthorized Model Merging via Dual-Stage Weight Protection</h3>
<p><strong>Authors:</strong> Wei-Jia Chen, Min-Yen Tsai, Cheng-Yi Lee, Chia-Mu Yu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出双阶段权重保护框架MergeGuard，通过分布任务信息与注入结构化扰动防御未授权模型合并，保护模型所有权与安全性，属于大模型安全与对齐中的模型安全方向。
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11851' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Rethinking Deep Alignment Through The Lens Of Incomplete Learning</h3>
<p><strong>Authors:</strong> Thong Bach, Dung Nguyen, Thao Minh Le, Truyen Tran</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析大语言模型安全对齐中的不完全学习问题（position-dependent gradient weakening导致的信号衰减），提出针对性的靶向完成方法提升对抗鲁棒性，对大模型安全对齐的机制理解与性能优化有重要意义
Score: 9
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12155' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Suppressing VLM Hallucinations with Spectral Representation Filtering</h3>
<p><strong>Authors:</strong> Ameen Ali, Tamim Zoabi, Lior Wolf</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出光谱表示过滤（SRF）方法，通过特征协方差分析识别幻觉模式并衰减，无需修改模型或再训练，有效抑制VLM的幻觉生成，实验验证在LLaVA-1.5、MiniGPT-4等模型上提升faithfulness，属于大模型安全与对齐的核心问题解决。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12220' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Model Inversion Attack Against Deep Hashing</h3>
<p><strong>Authors:</strong> Dongdong Zhao, Qiben Xu, Ranxin Fang, Baogang Song</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出首个针对深度哈希的扩散基模型 inversion 框架（DHMI），通过语义哈希中心和surrogate引导去噪优化，在黑盒设置下成功重建高保真图像，揭示深度哈希系统的隐私风险，属于大模型安全的重要研究方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12233' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection</h3>
<p><strong>Authors:</strong> Jialiang Shen, Jiyang Zheng, Yunqi Xue, Huajie Chen, Yu Yao, Hui Kang, Ruiqi Liu, Helin Gong, Yang Yang, Dadong Wang, Tongliang Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 基于知识蒸馏实现模糊鲁棒的AI生成图像检测，解决真实场景下的图像真实性验证问题，属于大模型安全与对齐的核心应用。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12511' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection</h3>
<p><strong>Authors:</strong> Jiayi Zhu, Yihao Huang, Yue Cao, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Geguang Pu, Bin Wang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出语义感知的排版攻击方法，解决大模型地理推理中的隐私泄露问题，属于大模型安全与对齐的隐私保护研究。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12575' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning</h3>
<p><strong>Authors:</strong> Ankita Raj, Chetan Arora</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 首次研究开放词汇目标检测器（OVODs）的后门攻击，提出TrAP多模态提示调优策略，通过轻量级prompt参数优化植入后门，保持模型泛化性的同时实现高攻击成功率，揭示了OVODs的新攻击表面。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SAGA: Source Attribution of Generative AI Videos</h3>
<p><strong>Authors:</strong> Rohit Kundu, Vishal Mohanty, Hao Xiong, Shan Jia, Athula Balachandran, Amit K. Roy-Chowdhury</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出首个针对生成式AI视频的多粒度来源归因框架SAGA，支持真实性、生成任务、模型版本等五层归因，引入T-Sigs解释方法，解决深度伪造视频的溯源问题，属于大模型安全的重要应用方向。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12834' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models</h3>
<p><strong>Authors:</strong> Ning Han, Zhenyu Ge, Feng Han, Yuhua Sun, Chengqing Li, Jingjing Chen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对文本到图像扩散模型的概念擦除问题，提出训练-free的图引导在线概念擦除框架GrOCE，通过动态语义图推理实现精准概念移除，解决现有方法依赖微调或语义分离粗糙的问题，提升概念擦除的准确性和稳定性。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12968' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias</h3>
<p><strong>Authors:</strong> Wenqian Ye, Di Wang, Guangtao Zheng, Bohan Liu, Aidong Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对CLIP的多模态偏差问题，提出SAGE引导prompt探索方法，通过选择诱导类间语义分离最大的prompt缓解偏差，提升鲁棒性，无需训练或微调，解决VLMs偏差痛点。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13005' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language</h3>
<p><strong>Authors:</strong> Zonghao Ying (), Moyang Chen (), Nizhang Li (), Zhiqiang Wang (), Wenxin Zhang (), Quanchen Zou (), Zonglei Jing (), Aishan Liu (), Xianglong Liu ()</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出模块化提示设计的 jailbreak 框架，利用隐式语言线索诱导文本到视频模型生成不安全内容，揭示了多模态大模型的安全漏洞，对大模型安全与对齐研究有重要参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13127' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task</h3>
<p><strong>Authors:</strong> Xingming Long, Jie Zhang, Shiguang Shan, Xilin Chen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出VOPE方法评估LVLM在自愿想象任务中的幻觉，属于大模型安全与对齐中的幻觉检测与评估，填补了想象任务中幻觉研究的空白
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13420' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Output Supervision Can Obfuscate the Chain of Thought</h3>
<p><strong>Authors:</strong> Jacob Drori, Luke Marks, Bryce Woodworth, Alex Cloud, Alexander Matt Turner</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析输出监督对CoT的混淆效应，提出缓解方法以提升LLM CoT的可监控性，属于大模型安全与对齐中的关键问题。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11584' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion</h3>
<p><strong>Authors:</strong> Feng Guo, Yuntao Wen, Shen Gao, Junshuo Zhang, Shuo Shang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出KUnBR通过知识密度估计和块重插入实现LLM彻底遗忘，解决隐私与合规问题，属于大模型安全与对齐中的关键技术。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11667' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> On the Trade-Off Between Transparency and Security in Adversarial Machine Learning</h3>
<p><strong>Authors:</strong> Lucas Fenaux (unknown), Christopher Srinivasa (unknown), Florian Kerschbaum (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究透明度与安全的权衡，涉及对抗机器学习的安全问题，属于大模型安全与对齐的LLM safety方向
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11842' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> A Systematic Study of Model Extraction Attacks on Graph Foundation Models</h3>
<p><strong>Authors:</strong> Haoyan Xu (unknown), Ruizhi Qian (unknown), Jiate Li (unknown), Yushun Dong (unknown), Minghao Lin (unknown), Hanson Yan (unknown), Zhengtao Yao (unknown), Qinghua Liu (unknown), Junhao Dong (unknown), Ruopeng Huang (unknown), Yue Zhao (unknown), Mengyuan Li (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 系统研究图基础模型的模型提取攻击，属于大模型安全与对齐的LLM safety方向
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11912' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation</h3>
<p><strong>Authors:</strong> Jiahe Shi (unknown), Zhengqi Gao (unknown), Ching-Yun Ko (unknown), Duane Boning (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出熵感知的RL对齐框架，提升LLM生成RTL代码的可靠性，属于大模型安全与对齐的alignment方向
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12033' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AlignTree: Efficient Defense Against LLM Jailbreak Attacks</h3>
<p><strong>Authors:</strong> Gil Goren, Shahar Katz, Lior Wolf</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出高效的LLM越狱攻击防御方法AlignTree，通过监控模型激活并结合拒绝方向与SVM信号检测错位行为，无需额外提示或辅助模型，提升大模型安全，与大模型安全与对齐方向直接相关
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12217' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models</h3>
<p><strong>Authors:</strong> Yuting Tan, Yi Huang, Zhuo Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析LLM微调中的compliance-only后门攻击，揭示仅通过良性训练数据（如“Sure”响应）即可植入隐式后门的风险，为大模型安全对齐中的数据供应链风险提供警示，与大模型安全与对齐方向相关
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.12414' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Incoherent Beliefs & Inconsistent Actions in Large Language Models</h3>
<p><strong>Authors:</strong> Arka Pal, Teo Kitanovski, Arthur Liang, Akilesh Potti, Micah Goldblum</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究LLM的信念不一致与行动不一致问题，发现即使准确或校准良好的模型也存在这些问题，属于大模型安全与对齐中的robustness与consistency方向，对理解LLM的实际行为有重要意义。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13240' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment</h3>
<p><strong>Authors:</strong> Ruoxi Cheng, Haoxuan Ma, Teng Ma, Hongyi Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对大视觉语言模型（LVLM）的对齐难题，提出将对齐重构为经济理性搜索的EcoAlign框架，解决了安全、效用与计算成本的权衡问题，实验验证其在降低成本的同时提升了安全与效用表现，对大模型安全与对齐研究具有重要参考价值。
Score: 8
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11301' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> What Color Is It? A Text-Interference Multimodal Hallucination Benchmark</h3>
<p><strong>Authors:</strong> Jinkun Zhao, Lei Huang, Wenjun Wu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 构建文本干扰的多模态幻觉基准，用于评估MLLM的视觉感知鲁棒性，属于大模型安全与对齐中的幻觉问题研究，有助于揭示MLLM的安全隐患
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13400' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline</h3>
<p><strong>Authors:</strong> Rui Zuo, Qinyue Tong, Zhe-Ming Lu, Ziqian Lu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出训练-free的MLLM伪造检测 pipeline，利用MLLM的固有泛化能力，属于大模型安全与对齐中的内容伪造检测，提升了MLLM在安全领域的实用性
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13442' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Language-Guided Invariance Probing of Vision-Language Models</h3>
<p><strong>Authors:</strong> Jae Joong Lee</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出LGIP基准评估VLMs对语言扰动的鲁棒性，属于大模型安全与对齐中的语言鲁棒性研究，揭示了VLMs在语言处理中的潜在漏洞
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13494' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks</h3>
<p><strong>Authors:</strong> Md. Iqbal Hossain, Afia Sajeeda, Neeresh Kumar Perla, Ming Shao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出针对多模态对比学习模型的后门攻击防御策略，属于大模型安全与对齐中的后门防御，提升了多模态模型的鲁棒性
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13545' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training</h3>
<p><strong>Authors:</strong> Subramanyam Sahoo</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对推理模型训练的奖励结构设计，比较硬奖励、连续奖励与混合奖励，提出自适应混合奖励调度器，属于大模型安全与对齐中的RLHF与奖励建模方向，对提升推理模型的收敛稳定性有帮助。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13016' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning</h3>
<p><strong>Authors:</strong> Qipeng Song, Nan Yang, Ziqi Xu, Yue Li, Wei Shao, Feng Xia</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对少样本零glance场景的机器遗忘，提出GFOES框架生成最优擦除样本，属于大模型安全与对齐中的隐私保护方向，对数据受限下的模型遗忘有实用价值。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.13116' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems</h3>
<p><strong>Authors:</strong> Samih Fadli (Unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出人工智能第二定律，将AI对齐视为热力学控制问题，定量研究伦理熵控制，属于大模型安全与对齐方向。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.10704' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</h3>
<p><strong>Authors:</strong> Dena Mujtaba, Brian Hu, Anthony Hoogs, Arslan Basharat</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对预训练Agent的伦理对齐挑战，提出测试时策略塑造方法，无需重新训练即可调整Agent行为以符合伦理准则，在MACHIAVELLI基准上验证了有效性，对大模型安全与对齐的实践应用有积极意义。
Score: 7
Field: 大模型安全与对齐</p>
<p><a href='https://arxiv.org/abs/2511.11551' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>原生多模态大模型</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models</h3>
<p><strong>Authors:</strong> Siyou Li, Huanan Wu, Juexi Shao, Yinghao Ma, Yujian Gan, Yihao Luo, Yuwei Wang, Dong Nie, Lu Wang, Wengqing Wu, Le Zhang, Massimo Poesio, Juntao Yu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Query-aware Token Selector（QTSplus）优化长视频多模态大模型的视觉token处理，实现89%的token压缩与28%的延迟降低，同时保持长视频理解性能，属于原生多模态大模型中的长视频处理与tokenizer方向。
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11910' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</h3>
<p><strong>Authors:</strong> Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Part-X-MLLM，原生3D多模态大模型，将3D任务统一为结构化程序，支持部分级3D理解与编辑，属于原生多模态大模型的前沿研究
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13647' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</h3>
<p><strong>Authors:</strong> Ziang Cao, Fangzhou Hong, Zhaoxi Chen, Liang Pan, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出PhysX-Anything，从单图生成模拟就绪的物理3D资产，支持接触-rich机器人政策学习，属于原生多模态大模型中的3D生成与理解，填补了物理属性缺失的空白
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13648' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Scaling Spatial Intelligence with Multimodal Foundation Models</h3>
<p><strong>Authors:</strong> Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SenseNova-SI家族，多模态基础模型的空间智能，在15/24任务中取得最佳嵌入性能，19/29任务中最佳微调性能，属于原生多模态大模型的空间理解前沿
Score: 9
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13719' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Target-Balanced Score Distillation</h3>
<p><strong>Authors:</strong> Zhou Xu, Qi Wang, Yuxiao Yang, Luyuan Zhang, Zhang Liang, Yang Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对Score Distillation Sampling（SDS）的过饱和与过平滑问题，提出目标平衡的分数蒸馏方法，提升3D资产生成的纹理真实感与形状准确性，属于原生多模态大模型中的图像生成方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11710' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing</h3>
<p><strong>Authors:</strong> Hossein Mohebbi, Mohammed Abdulrahman, Yanting Miao, Pascal Poupart, Suraj Kothawade</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出反射式强化学习框架协调多专家文本-图像/图像-图像模型，解决长组合prompt的图像生成与编辑问题，属于原生多模态大模型中的图像生成与编辑方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11780' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups</h3>
<p><strong>Authors:</strong> Songsong Zhang, Chuanqi Tang, Hongguang Zhang, Guijian Tang, Minglong Li, Xueqiong Li, Shaowu Yang, Yuanxi Peng, Wenjing Yang, Jing Zhao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出双路径推理与身份自适应融合策略，突破面部特写限制实现身份保持的场景生成，提升语义一致性与视觉叙事性，属于原生多模态大模型中的个性化图像生成方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11989' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension</h3>
<p><strong>Authors:</strong> Xianglong Shi, Silin Cheng, Sirui Zhao, Yunhan Jiang, Enhong Chen, Yang Liu, Sebastien Ourselin</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出弱监督广义指代表达理解框架LIHE，解决零或多目标场景下的指代表达接地问题，结合双曲-欧几里得空间提升语义区分度，属于原生多模态大模型中的指代表达理解方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12020' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Improved Masked Image Generation with Knowledge-Augmented Token Representations</h3>
<p><strong>Authors:</strong> Guotao Liang, Baoquan Zhang, Zhiyuan Wen, Zihao Han, Yunming Ye</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出知识增强的掩码图像生成框架KA-MIG，利用token级语义依赖知识图提升模型的语义捕捉能力，改进生成质量，属于原生多模态大模型中的掩码图像生成方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12032' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Calibrated Multimodal Representation Learning with Missing Modalities</h3>
<p><strong>Authors:</strong> Xiaohao Liu, Xiaobo Xia, Jiaheng Wei, Shuo Yang, Xiu Su, See-Kiong Ng, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对缺失模态的多模态表示学习问题，提出校准框架CalMRL解决锚点偏移问题，提升表示一致性与下游任务性能，属于原生多模态大模型中的缺失模态处理方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12034' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images</h3>
<p><strong>Authors:</strong> Xinyuan Hu, Changyue Shi, Chuxiao Yang, Minghao Chen, Jiajun Ding, Tao Wei, Chen Wei, Zhou Yu, Min Tan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出前馈高分辨率3D重建框架SRSplat，利用多模态大模型（MLLMs）与扩散模型生成的参考图像提升稀疏多视图输入的纹理细节，属于原生多模态大模型中的3D重建方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12040' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound</h3>
<p><strong>Authors:</strong> Dengming Zhang, Weitao You, Jingxiong Li, Weishen Lin, Wenda Shi, Xue Zhao, Heda Zuo, Junxian Wu, Lingyun Sun</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出视觉锚定的音视频情感LLM（VAEmotionLLM），通过视觉引导音频对齐实现多模态艺术情感理解，在ArtEmoBenchmark上取得最优性能，属于原生多模态大模型中的音视频情感理解方向。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12077' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering</h3>
<p><strong>Authors:</strong> Seokwon Song, Minsu Park, Gunhee Kim</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 构建首个多模态源归因基准MAVIS，包含157K视觉QA实例和事实级引用注释，评估MLLM的多模态证据检索、长文本生成和归因能力，揭示现有模型在图像文档groundedness上的差距，为原生多模态大模型的可靠性评估提供了关键工具。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12142' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</h3>
<p><strong>Authors:</strong> Haozhe Liu, Ding Liu, Mingchen Zhuge, Zijian Zhou, Tian Xie, Sen He, Yukang Yang, Shuming Liu, Yuren Cong, Jiadong Guo, Hongyu Xu, Ke Xu, Kam-Woh Ng, Juan C. P\'erez, Juan-Manuel~P\'erez-R\'ua, Tao Xiang, Wei Liu, Shikun Liu, J\"urgen Schmidhuber</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出MoS融合范式，通过token级可学习路由器融合多模态隐状态，实现多模态扩散模型的灵活交互，实验验证在文本到图像生成和编辑任务上优于大参数模型，属于原生多模态大模型的架构创新。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12207' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models</h3>
<p><strong>Authors:</strong> Yiqing Shen, Chenxiao Fan, Chenjia Li, Mathias Unberath</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出基于数字孪生视频表示的文本-视频检索框架，结合大语言模型实现隐式查询推理，解决多模态融合中的语义对齐问题，属于原生多模态大模型的关键改进。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12371' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions</h3>
<p><strong>Authors:</strong> Xiaoyu Lin, Aniket Ghorpade, Hansheng Zhu, Justin Qiu, Dea Rrozhani, Monica Lama, Mick Yang, Zixuan Bian, Ruohan Ren, Alan B. Hong, Jiatao Gu, Chris Callison-Burch</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 构建音频驱动的多语言多模态密集标注数据集，支持多模态大模型的细粒度语义学习，属于原生多模态大模型的基础数据支撑。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12452' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction</h3>
<p><strong>Authors:</strong> Yukuo Ma, Cong Liu, Junke Wang, Junqi Liu, Haibin Huang, Zuxuan Wu, Chi Zhang, Xuelong Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出帧率预测的长视频生成框架，解决多模态视频生成中的 temporal coherence问题，属于原生多模态大模型的视频生成优化。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12578' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Direct Visual Grounding by Directing Attention of Visual Tokens</h3>
<p><strong>Authors:</strong> Parsa Esmaeilkhani, Longin Jan Latecki</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对VLMs中视觉token注意力不足导致的视觉任务性能问题，提出KLAL损失直接监督视觉token注意力分布，引导答案语言token关注相关视觉token，提升几何任务、指向和指代理解等性能，解决了VLMs视觉grounding核心问题。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12738' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Explore How to Inject Beneficial Noise in MLLMs</h3>
<p><strong>Authors:</strong> Ruishu Zhu, Sida Huang, Ziheng Jiao, Hongyuan Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对MLLMs的跨模态异质性问题，提出MuNG有益噪声注入策略，通过动态分析跨模态关系生成任务自适应噪声，提升跨模态对齐和下游性能，仅需微调1-2%参数，高效解决MLLMs核心对齐问题。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12917' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ViSS-R1: Self-Supervised Reinforcement Video Reasoning</h3>
<p><strong>Authors:</strong> Bo Fang (), Yuxin Song (), Qiangqiang Wu (), Haoyuan Sun (), Wenhao Wu (), Antoni B. Chan ()</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对多模态大模型视频推理中视觉信息利用不足的问题，提出自监督强化学习框架ViSS-R1，强制模型处理变换后的视觉输入，提升视频推理的鲁棒性和准确性。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13054' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection</h3>
<p><strong>Authors:</strong> Junjie Wu (), Guohong Fu ()</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对多模态大模型在虚假信息检测中的推理不足问题，提出两阶段框架MMD-Thinker，结合定制化思维模式和强化学习，构造了MMR数据集，提升了检测性能。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13242' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine</h3>
<p><strong>Authors:</strong> Xincheng Shuai, Zhenyuan Qin, Henghui Ding, Dacheng Tao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出FFSE框架，实现3D-aware图像编辑，支持多轮物理一致的物体操作，属于原生多模态大模型中的场景编辑，提升了编辑的直观性和一致性
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13713' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation</h3>
<p><strong>Authors:</strong> Jiayin Zhu (unknown), Linlin Yang (unknown), Yicong Li (unknown), Angela Yao (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究text-to-3D生成中动态源锚定方法，提升语义一致性，属于原生多模态大模型的image generation方向
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11692' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Simple Vision-Language Math Reasoning via Rendered Text</h3>
<p><strong>Authors:</strong> Matvey Skripkin (unknown), Elizaveta Goncharova (unknown), Andrey Kuznetsov (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 通过渲染文本将LaTeX方程转化为图像，结合思维链提示训练视觉语言模型解决数学推理，属于原生多模态大模型的vision-language方向
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11704' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Selecting Fine-Tuning Examples by Quizzing VLMs</h3>
<p><strong>Authors:</strong> Tenghao Ji (unknown), Eytan Adar (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 通过测试VLM选择微调示例，提升文本到图像模型的对齐和真实感，属于原生多模态大模型的image generation方向
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12002' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance</h3>
<p><strong>Authors:</strong> Wanlong Fang, Tianle Zhang, Alvin Chan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究多模态学习中explicit representation alignment的策略，探讨其对模型性能的影响，揭示最优对齐强度与模态间冗余度的关系，为多模态表示整合提供理论指导，与原生多模态大模型方向高度相关
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12121' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Uncovering and Mitigating Transient Blindness in Multimodal Model Editing</h3>
<p><strong>Authors:</strong> Xiaoqi Han, Ru Li, Ran Yi, Hongye Tan, Zhuomin Liang, Víctor Gutiérrez-Basulto, Jeff Z. Pan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出多模态模型编辑中的“瞬时失明”现象，设计locality-aware对抗损失缓解该问题，属于原生多模态大模型中的模型编辑方向，对提升多模态模型的编辑鲁棒性有帮助。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13243' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning</h3>
<p><strong>Authors:</strong> Juntao Jian, Yi-Lin Wei, Chengjie Mou, Yuhao Lin, Xing Zhu, Yujun Shen, Wei-Shi Zheng, Ruizhen Hu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 整合多模态大语言模型，通过prompt语义推理生成任务对齐的灵巧抓取姿态，实现零样本跨物体类别抓取，属于原生多模态大模型方向的关键应用。
Score: 8
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13327' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Point Cloud Quantization through Multimodal Prompting for 3D Understanding</h3>
<p><strong>Authors:</strong> Hongxuan Li (College of Intelligence and Computing, Tianjin University), Wencheng Zhu (College of Intelligence and Computing, Tianjin University, Haihe Laboratory of Information Technology Application Innovation), Huiying Xu (School of Computer Science and Technology, Zhejiang Normal University), Xinzhong Zhu (School of Computer Science and Technology, Zhejiang Normal University), Pengfei Zhu (College of Intelligence and Computing, Tianjin University)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出多模态提示驱动的点云量化框架，利用预训练文本嵌入作为原型先验，结合双约束量化空间和Gumbel-Softmax离散化，解决现有点云量化的表示性和可解释性问题，实验验证在3D理解任务上的有效性，属于原生多模态大模型中3D模态融合的关键进展。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12079' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description</h3>
<p><strong>Authors:</strong> Quanxing Xu, Ling Zhou, Feifei Zhang, Jinyu Tian, Rubing Huang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出OAD-Promoter框架，通过目标属性描述生成、记忆知识辅助和OAD提示优化，解决LLM在VQA任务中的语言偏见和OOD泛化问题，实验验证在零样本和少样本设置下优于现有方法，属于原生多模态大模型中视觉-语言对齐的关键优化。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12131' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding</h3>
<p><strong>Authors:</strong> Zhanheng Nie, Chenghan Fu, Daoze Zhang, Junxian Wu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出动态模态平衡的多模态表示学习框架，解决电商场景下的模态不平衡与数据噪声问题，属于原生多模态大模型在垂直领域的落地优化。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12449' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Video Finetuning Improves Reasoning Between Frames</h3>
<p><strong>Authors:</strong> Ruiqi Yang, Tian Yun, Zihan Wang, Ellie Pavlick</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究视频微调对多模态LLMs帧间推理的影响，提出vCoT验证视频模型隐含捕捉帧间过渡的能力，提升长视频问答和静态视觉推理性能，属于多模态LLM的训练优化研究。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12868' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning</h3>
<p><strong>Authors:</strong> Junbo Zou, Haotian Xia, Zhen Ye, Shengjie Zhang, Christopher Lai, Vicente Ordonez, Weining Shen, Hanjie Chen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出针对体育视频推理的MLLM DeepSport，通过强化学习优化主动推理过程，解决体育视频的高动态、复杂规则和长时序推理问题，属于多模态大模型的领域特定应用。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12908' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework</h3>
<p><strong>Authors:</strong> Diego Ortego (), Marlon Rodr\'iguez (), Mario Almagro (), Kunal Dahiya (), David Jim\'enez (), Juan C. SanMiguel ()</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出ViXML多模态框架，整合大语言模型和视觉模型处理极端多标签分类，解决了大模型在多模态任务中的应用问题，提升了分类性能。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13189' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing</h3>
<p><strong>Authors:</strong> Jongha Kim, Minseong Bae, Sanghyeok Lee, Jinsung Yoon, Hyunwoo J. Kim</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出高效的多模态大模型TabFlash用于表格理解，结合渐进式问题条件注入和token聚焦策略，提升性能的同时降低计算成本（减少27% FLOPs和30%内存使用），与原生多模态大模型研究方向高度相关
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13283' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling</h3>
<p><strong>Authors:</strong> Adam Hazimeh, Ke Wang, Mark Collier, Gilles Baechler, Efi Kokiopoulou, Pascal Frossard</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SliDer框架，利用VLMs将光栅文档转换为可编辑的SVG，属于原生多模态大模型中的文档理解与生成，解决了文档编辑性问题
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13478' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</h3>
<p><strong>Authors:</strong> Jiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze Zou, Jing Wang, Haoji Hu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出GS-Light，文本引导的3D场景重照明，利用LVLM解析光照先验，生成多视图一致的重照明结果，属于原生多模态大模型中的场景编辑
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.13684' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models</h3>
<p><strong>Authors:</strong> Alexis Roger, Gwen Legate, Kashif Rasul, Yuriy Nevmyvaka, Irina Rish</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究时间序列的tokenization设计与预训练的结合，证明小词汇量tokenizer的有效性，属于原生多模态大模型中的tokenizer关键方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11622' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models</h3>
<p><strong>Authors:</strong> Fei Song, Yi Li, Rui Wang, Jiahuan Zhou, Changwen Zheng, Jiangmeng Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出双去偏测试时prompt tuning提升视觉语言模型泛化能力，属于原生多模态大模型中的prompt优化方向。
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.11690' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing</h3>
<p><strong>Authors:</strong> Zhizhen Li, Xuanhao Luo, Xueren Ge, Longyu Zhou, Xingqin Lin, Yuchen Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出MMSense多模态多任务基础模型，整合图像、雷达、LiDAR等模态并进行跨模态对齐，用于无线感知的多任务学习，与原生多模态大模型方向相关
Score: 7
Field: 原生多模态大模型</p>
<p><a href='https://arxiv.org/abs/2511.12305' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>多模态智能体</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</h3>
<p><strong>Authors:</strong> Jeong Hun Yeo, Sangyun Chung, Sungjune Park, Dae Hoe Kim, Jinyoung Moon, Yong Man Ro</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出全局上下文感知的智能体框架GCAgent，通过结构化的情节记忆解决长视频理解的长期依赖问题，在Video-MME基准上实现23.5%的 accuracy提升，属于多模态智能体中的长视频理解方向。
Score: 9
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2511.12027' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans</h3>
<p><strong>Authors:</strong> Hongbin Huang, Junwei Li, Tianxin Xie, Zhuang Li, Cekai Weng, Yaodong Yang, Yue Luo, Li Liu, Jing Tang, Zhijing Shao, Zeyu Wang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出实时对话数字人系统，结合多模态交互与知识接地，属于多模态智能体中的数字人应用创新。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2511.12662' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding</h3>
<p><strong>Authors:</strong> Jiaze Li, Hao Yin, Wenhui Tan, Jingyang Chen, Boshen Xu, Yuxun Qu, Yijing Chen, Jianzhong Ju, Zhenbo Luo, Jian Luan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对长视频理解的文本反思不足问题，提出REVISOR多模态反思框架，通过工具增强的多模态反思提升MLLMs的长视频推理能力，设计DADR奖励机制保证推理与视频证据的因果对齐，解决长视频理解核心痛点。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2511.13026' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents</h3>
<p><strong>Authors:</strong> Yuan Zhao (Unknown), Hualei Zhu (Unknown), Tingyu Jiang (Unknown), Shen Li (Unknown), Xiaohang Xu (Unknown), Hao Henry Wang (Unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对GUI代理的规划与接地能力协同进化提出Co-EPG框架，属于多模态智能体中的GUI Agent方向，与用户高优先级方向高度相关。
Score: 8
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2511.10705' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching</h3>
<p><strong>Authors:</strong> Zewei Chang, Zheng-Peng Duan, Jianxing Zhang, Chun-Le Guo, Siyu Liu, Hyungju Chun, Hyunhee Park, Zikun Liu, Chongyi Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出VLM驱动的个性化图像修图代理PerTouch，支持语义级控制和用户反馈，解决图像修图的个性化与语义一致性问题，属于多模态智能体的具体应用。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2511.12998' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> PIGEON: VLM-Driven Object Navigation via Points of Interest Selection</h3>
<p><strong>Authors:</strong> Cheng Peng, Zhenzhe Zhang, Cheng Chi, Xiaobao Wei, Yanhao Zhang, Heng Wang, Pengwei Wang, Zhongyuan Wang, Jing Liu, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出VLM驱动的物体导航框架，结合视觉-语言模型与兴趣点选择实现未知环境目标导航，提升决策频率与语义指导能力，属于多模态智能体方向的重要研究。
Score: 7
Field: 多模态智能体</p>
<p><a href='https://arxiv.org/abs/2511.13207' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习可解释性</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation</h3>
<p><strong>Authors:</strong> Yannan Chen, Ruoyu Chen, Bin Zeng, Wei Wang, Shiming Liu, Qunli Zhang, Zheng Hu, Laiyuan Wang, Yaowei Wang, Xiaochun Cao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将LIMA归因方法融入训练流程，提出子集选择反事实增强（SS-CA）策略，通过生成反事实样本纠正模型的非因果依赖，提升模型的泛化能力和鲁棒性，实验验证在ImageNet变体和OOD基准上优于现有方法，直接关联可解释性与模型优化，属于深度学习可解释性的实践创新。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12100' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Explainable AI-Generated Image Detection RewardBench</h3>
<p><strong>Authors:</strong> Michael Yang, Shijian Deng, William T. Doan, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 构建XAIGID-RewardBench基准，包含3000+注释三元组，评估MLLM对AI生成图像检测解释的判断能力，揭示当前模型与人类的差距（最佳模型88.76% vs 人类98.30%），为可解释AI生成图像检测提供了标准化评估工具，属于深度学习可解释性的基准创新。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12363' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models</h3>
<p><strong>Authors:</strong> Sushant Gautam, Michael A. Riegler, P{\aa}l Halvorsen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出几何熵的幻觉估计框架，解决多模态VQA的可靠性评估问题，属于深度学习可解释性中的幻觉检测研究。
Score: 9
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12693' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models</h3>
<p><strong>Authors:</strong> Sanchit Sinha, Guangzhi Xiong, Zhenghao He, Aidong Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出接地的多Agent神经符号推理框架，通过视觉概念提取与规则生成提升VLMs的可解释性并减少幻觉，属于深度学习可解释性中的神经符号方法方向。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11751' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> X-VMamba: Explainable Vision Mamba</h3>
<p><strong>Authors:</strong> Mohamed A. Mabrok, Yalda Zafari</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对Vision Mamba等SSMs空间信息处理缺乏透明解释机制的问题，提出基于可控性的可解释性框架，包含Jacobian（适用于所有SSM）和Gramian（适用于对角SSM）两种线性复杂度方法，验证了医疗图像等领域的效果，解决了SSMs可解释性痛点。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12694' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach</h3>
<p><strong>Authors:</strong> Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对CLIP的背景依赖问题，提出CCI可解释性方法，通过CLIP的patch嵌入聚类评估概念区域的重要性，提升可解释性的faithfulness，解决VLMs背景过依赖痛点。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12978' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations</h3>
<p><strong>Authors:</strong> Yehonatan Elisha (), Seffi Cohen (), Oren Barkan (), Noam Koenigstein ()</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对显著性图解释的核心问题，提出RFxG分类法和新的忠实性指标，解决现有评估忽略对比推理和语义粒度的局限，对深度学习可解释性的理论和实践有重要价值。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.13081' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew</h3>
<p><strong>Authors:</strong> Farhin Farhad Riya, Shahinul Hoque, Jinyuan Stella Sun, Olivera Kotevska</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 揭示联邦学习中通过颜色偏移攻击模型可解释性的问题，属于深度学习可解释性中的攻击与鲁棒性研究，强调了可解释性本身的安全风险
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.13535' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control</h3>
<p><strong>Authors:</strong> Tsogt-Ochir Enkhbayar (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将Model-X knockoffs应用于稀疏自编码器特征选择，控制假阳性率，属于深度学习可解释性的特征解释方向
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11711' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> FLEX: Feature Importance from Layered Counterfactual Explanations</h3>
<p><strong>Authors:</strong> Nawid Keshtmand (unknown), Roussel Desmond Nzoyem (unknown), Jeffrey Nicholas Clark (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将counterfactuals转化为局部、区域和全局特征重要性，属于深度学习可解释性的feature importance方向
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11891' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification</h3>
<p><strong>Authors:</strong> Alan G. Paredes Cetina, Kaouther Benguessoum, Raoni Lourenço, Sylvain Kubler</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出CONFETTI方法生成多变量时间序列的反事实解释，平衡准确性、proximity与sparsity，属于深度学习可解释性中的反事实解释方向，对时间序列模型的可解释性与决策支持有帮助。
Score: 8
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.13237' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> SAGE: Saliency-Guided Contrastive Embeddings</h3>
<p><strong>Authors:</strong> Colton R. Crum, Adam Czajka</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对现有显著性引导训练依赖图像空间机制的问题，提出SAGE框架，将人类显著性整合到潜空间的对比嵌入训练中，通过对比损失引导模型关注显著特征，提升分类性能和泛化性，属于可解释性中的人类先验整合方法。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12744' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection</h3>
<p><strong>Authors:</strong> Lintong Zhang, Kang Yin, Seong-Whan Lee</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对视觉反事实解释的语义无关问题，提出WSAE-Net框架，通过加权语义图和自适应编辑序列提升解释的语义相关性，改善反事实解释质量，属于深度学习可解释性的反事实解释改进。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.12992' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation</h3>
<p><strong>Authors:</strong> Chenyue Liu, Ali Mostafavi</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 用SHAP和ICE/PDP解释野火风险模型的局部驱动因素，连接模型输出与可理解的地理因素，属于深度学习可解释性的实际应用。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11589' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sound Logical Explanations for Mean Aggregation Graph Neural Networks</h3>
<p><strong>Authors:</strong> Matthew Morris, Ian Horrocks</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 为均值聚合GNN提供可靠的逻辑解释，证明其可表示的单调规则类，提升GNN的白盒解释能力，属于深度学习可解释性中的GNN方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11593' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Evaluation of LLM-based Explanations for a Learning Analytics Dashboard</h3>
<p><strong>Authors:</strong> Alina Deriyeva, Benjamin Paassen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 评估LLM对学习分析仪表板的解释效果，提升教育领域LLM应用的可解释性，属于深度学习可解释性中的LLM解释方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues</h3>
<p><strong>Authors:</strong> Seham Nasr, Zhao Ren, David Johnson</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将显著性与专家 acoustic 线索结合提升语音情感识别模型解释性，连接模型输出与领域知识，属于深度学习可解释性中的语音模型方向。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.11691' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning</h3>
<p><strong>Authors:</strong> Senne Deproost, Dennis Steckelmacher, Ann Nowé</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出将RL政策蒸馏为局部线性政策，用Voronoi划分提升可解释性，属于深度学习可解释性中的RL政策解释方向，对提升RL模型的透明度与 trust 有帮助。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.13322' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Weight-sparse transformers have interpretable circuits</h3>
<p><strong>Authors:</strong> Leo Gao (Unknown), Achyuta Rajaram (Unknown), Jacob Coxon (Unknown), Soham V. Govande (Unknown), Bowen Baker (Unknown), Dan Mossing (Unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 通过权重稀疏化使Transformer具有可解释的电路，探索模型的机械可解释性，属于深度学习可解释性方向，符合用户研究重点。
Score: 7
Field: 深度学习可解释性</p>
<p><a href='https://arxiv.org/abs/2511.13653' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>大模型新技术</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Back to Basics: Let Denoising Generative Models Denoise</h3>
<p><strong>Authors:</strong> Tianhong Li, Kaiming He</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出JiT，基于Transformer的扩散模型直接预测干净数据，无需tokenizer、预训练或额外损失，在ImageNet上取得 competitive 结果，属于大模型新技术中的扩散模型基础研究
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13720' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Reasoning: From Reflection to Solution</h3>
<p><strong>Authors:</strong> Zixi Li (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出推理是状态空间迭代算子应用的定义，设计OpenLM架构解决OpenXOR问题，属于大模型新技术的推理方向
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11712' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Diffusion Models: A Mathematical Introduction</h3>
<p><strong>Authors:</strong> Sepehr Maleki (unknown), Negar Pourmoazemi (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 系统推导扩散生成模型的数学基础，包括正向过程、反向posterior和变分边界，属于大模型新技术的diffusion理论方向
Score: 9
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11746' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation</h3>
<p><strong>Authors:</strong> Yushe Cao, Dianxi Shi, Xing Fu, Xuechao Zou, Haikuo Peng, Xueqi Li, Chun Yu, Junliang Xing</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出多变量扩散Transformer，解决掩码-文本协同面部生成中的模态冲突问题，属于大模型新技术中的扩散模型架构创新。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12631' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality</h3>
<p><strong>Authors:</strong> Tushar Anand, Advik Sinha, Abhijit Das</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Vision Mamba模型，解决密集视觉感知的实时性问题，属于大模型新技术中的Mamba架构应用。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12671' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention</h3>
<p><strong>Authors:</strong> Taiye Chen, Zihan Ding, Anjian Li, Christina Zhang, Zeqi Xiao, Yisen Wang, Chi Jin</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对扩散模型长视频生成的遗忘和时空不一致问题，将RNN引入扩散Transformer，提出递归自回归扩散框架，结合全局记忆与局部注意力，提升长视频生成的一致性和记忆能力，属于diffusion模型长序列生成新技术。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12940' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> MeanFlow Transformers with Representation Autoencoders</h3>
<p><strong>Authors:</strong> Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对MeanFlow生成模型的训练不稳定和计算成本问题，提出MF-RAE框架，利用Representation Autoencoder的语义丰富潜空间提升训练稳定性，减少训练与采样计算量，实现高效生成，属于MeanFlow重要改进技术。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13019' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</h3>
<p><strong>Authors:</strong> Fei Kong</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将DDCM扩展到主流扩散模型（DDPM、Score-Based Models等），实现图像token化，属于大模型新技术中的扩散模型扩展研究，提升了扩散模型的通用性和性能
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13387' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Distribution Matching Distillation Meets Reinforcement Learning</h3>
<p><strong>Authors:</strong> Dengyang Jiang, Dongyang Liu, Zanyi Wang, Qilong Wu, Xin Jin, David Liu, Zhen Li, Mengmeng Wang, Peng Gao, Harry Yang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将强化学习引入扩散模型蒸馏，提升少步生成性能（超过教师模型），属于大模型新技术中的扩散模型优化，创新了蒸馏范式
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13649' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</h3>
<p><strong>Authors:</strong> Junwei Yu, Trevor Darrell, XuDong Wang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出UnSAMv2，自监督提升SAM的粒度控制，支持任意粒度的分割，仅用6K无标签图像和0.02%额外参数增强SAM-2，属于大模型新技术中的分割模型优化
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13714' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</h3>
<p><strong>Authors:</strong> Aihua Zhu, Rui Su, Qinglin Zhao, Li Feng, Meng Shen, Shibo He</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出HSO框架优化扩散模型采样调度，实现快速鲁棒的采样，属于大模型新技术中的扩散模型方向。
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11688' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm</h3>
<p><strong>Authors:</strong> Tongda Xu (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究去噪分数匹配的输入优化偏差，涉及diffusion模型核心技术，属于大模型新技术的diffusion LLM方向
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11727' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Better LLM Reasoning via Dual-Play</h3>
<p><strong>Authors:</strong> Zhengxin Zhang (unknown), Chengyu Huang (unknown), Aochong Oliver Li (unknown), Claire Cardie (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出双玩框架提升LLM推理能力，属于大模型新技术的推理方向
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11881' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design</h3>
<p><strong>Authors:</strong> Lingxiao Li (unknown), Haobo Zhang (unknown), Bin Chen (unknown), Jiayu Zhou (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出渐进式潜在扩散模型用于文本引导分子设计，属于大模型新技术的diffusion LLM方向
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11894' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Diffusion Model Based Signal Recovery Under 1-Bit Quantization</h3>
<p><strong>Authors:</strong> Youming Chen, Zhaoqiang Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Diff-OneBit方法，将扩散模型作为先验用于1位量化的信号恢复（如压缩感知、逻辑回归），通过可微分替代似然函数解决非可微问题，拓展扩散模型的应用场景，与大模型新技术方向相关
Score: 8
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12471' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing</h3>
<p><strong>Authors:</strong> Ling Wang, Yunfan Lu, Wenzong Ma, Huizai Yao, Pengteng Li, Hui Xiong</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出事件引导的扩散去雾框架，利用事件相机的高动态范围（HDR）信息改进去雾效果，属于大模型新技术中的扩散模型应用方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.11944' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging</h3>
<p><strong>Authors:</strong> Jose Reinaldo Cunha Santos A V Silva Neto, Hodaka Kawachi, Yasushi Yagi, Tomoya Nakamura</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出零空间扩散蒸馏方法，实现无监督的无透镜成像真实感重建，解决传统方法的域偏移问题，属于大模型新技术中的扩散模型应用方向。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12024' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models</h3>
<p><strong>Authors:</strong> Zhiguang Lu, Qianqian Xu, Peisong Wen, Siran Da, Qingming Huang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出分层引导的扩散模型数据增强方法，解决细粒度任务中的样本保真度问题，属于大模型新技术中的扩散模型优化。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12547' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion</h3>
<p><strong>Authors:</strong> Nicholas Karris, Luke Durell, Javier Flores, Tegan Emerson</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出将CLIP嵌入视为Wasserstein空间点云的新视角，用最优传输方法解决Stable Diffusion图像插值的平滑性问题，提升插值结果的一致性，属于diffusion模型的嵌入几何分析与应用新技术。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.12757' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Infinite-Story: A Training-Free Consistent Text-to-Image Generation</h3>
<p><strong>Authors:</strong> Jihun Park, Kyoungmin Lee, Jongmin Gim, Hyeonseo Jo, Minseok Oh, Wonhyeok Choi, Kyumin Hwang, Jaeyeul Kim, Minwoo Choi, Sunghoon Im</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对文本到图像生成的多prompt一致性问题，提出训练-free的Infinite-Story框架，通过身份提示替换和统一注意力引导提升身份与风格一致性，实现高效故事生成，属于T2I生成的一致性新技术。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13002' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</h3>
<p><strong>Authors:</strong> Harold Haodong Chen, Disen Lan, Wen-Jie Shu, Qingyang Liu, Zihan Wang, Sirui Chen, Wenkai Cheng, Kanghao Chen, Hongfei Zhang, Zixin Zhang, Rongjin Guo, Yu Cheng, Ying-Cong Chen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 构建TiViBench基准评估视频生成模型的推理能力，涵盖结构、空间、符号和动作规划四大维度，属于大模型新技术中的视频生成推理研究
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13704' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Segment Anything Across Shots: A Method and Benchmark</h3>
<p><strong>Authors:</strong> Hengrui Hu, Kaining Ying, Henghui Ding</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SAAS模型和Cut-VOS基准，解决多镜头视频分割的 shot 不连续问题，属于大模型新技术中的视频分割研究，提升了分割的泛化性
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13715' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation</h3>
<p><strong>Authors:</strong> Jonas Bode, Raphael Memmesheimer, Sven Behnke</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出融合视觉与文本输入的3D扩散模型框架，用于生成机器人多任务操作轨迹，属于大模型新技术中的扩散模型应用，提升了多任务操作长程成功率。
Score: 7
Field: 大模型新技术</p>
<p><a href='https://arxiv.org/abs/2511.13312' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
<div id='' class='field-section'>
<h2>深度学习理论</h2>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Decoupling Positional and Symbolic Attention Behavior in Transformers</h3>
<p><strong>Authors:</strong> Felipe Urrutia, Jorge Salas, Alexander Kozachinskiy, Cristian Buc Calderon, Hector Pasten, Cristobal Rojas</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析Transformer中位置与符号注意力的分离，提出行为定义和量化指标，揭示RoPE频率与注意力行为的关联，属于深度学习理论中的网络架构研究
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11579' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 9.0/10]</span> Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering</h3>
<p><strong>Authors:</strong> Ge Cheng, Shuo Wang, Yun Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析对比学习核心目标函数InfoNCE的理论基础，提出Transition Probability Matrix诱导特征聚类的机制，改进得到Scaled Convergence InfoNCE（SC-InfoNCE），为对比学习的理论理解与性能优化提供重要支持，与深度学习理论方向高度相关
Score: 9
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12180' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> LE-CapsNet: A Light and Enhanced Capsule Network</h3>
<p><strong>Authors:</strong> Pouya Shiri, Amirali Baniasadi</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出LE-CapsNet改进Capsule Network的性能与效率，在CIFAR-10和AffNIST数据集上实现更高准确率与更快推理速度，属于深度学习理论中网络架构方向的研究。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11708' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding</h3>
<p><strong>Authors:</strong> Yuan Zhou, Litao Hua, Shilong Jin, Wentao Huang, Haoran Duan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> Reinforced Causal Search with Information Bottleneck for Video Understanding
Authors: Yuan Zhou, Litao Hua, Shilong Jin, Wentao Huang, Haoran Duan
Published: 2025-11-18
Link: https://arxiv.org/abs/2511.12530
Reason: 提出强化因果搜索框架，结合信息瓶颈实现视频关键帧选择，解决多模态视频理解中的因果推理问题，属于深度学习理论中的因果表征学习。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12530' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Seg-VAR: Image Segmentation with Visual Autoregressive Modeling</h3>
<p><strong>Authors:</strong> Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Hengshuang Zhao</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出视觉自回归建模的分割框架，将分割转化为条件掩码生成问题，属于深度学习理论中的自回归模型应用创新。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12594' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Denoising Vision Transformer Autoencoder with Spectral Self-Regularization</h3>
<p><strong>Authors:</strong> Xunzhi Xiang, Xingye Tian, Guiyu Zhang, Yabo Chen, Shaofeng Zhang, Xuebo Wang, Xin Tao, Qi Fan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出光谱自正则化的去噪ViT自动编码器，解决高维 latent空间的噪声问题，属于深度学习理论中的自动编码器优化。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12633' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Softmax as a Lagrangian-Legendrian Seam</h3>
<p><strong>Authors:</strong> Christopher R. Lee-Jenkins</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 从微分几何角度分析softmax，将其建模为Lagrangian-Legendrian seam，揭示了softmax的几何属性，属于深度学习理论中的模型组件分析
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11573' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL</h3>
<p><strong>Authors:</strong> Guojian Zhan, Likun Wang, Pengcheng Wang, Feihong Zhang, Jingliang Duan, Masayoshi Tomizuka, Shengbo Eben Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出轨迹熵约束的RL框架TECRL，解决最大熵RL的非平稳Q值估计问题，提升RL训练稳定性，属于深度学习理论中的RL优化方向。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11592' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Coordinate Descent for Network Linearization</h3>
<p><strong>Authors:</strong> Vlad Rakhlin (unknown), Amir Jevnisek (unknown), Shai Avidan (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 使用坐标下降优化网络线性化，涉及优化器和网络架构，属于深度学习理论的optimizer方向
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11781' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Variation-Bounded Loss for Noise-Tolerant Learning</h3>
<p><strong>Authors:</strong> Jialiang Wang, Xiong Zhou, Xianming Liu, Gangfeng Hu, Deming Zhai, Junjun Jiang, Haoliang Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Variation-Bounded Loss（VBL）家族，基于Variation Ratio理论分析其鲁棒性，为噪声标签下的监督学习提供新的鲁棒损失函数设计方法，与深度学习理论方向相关
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12143' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs</h3>
<p><strong>Authors:</strong> Jeongwhan Choi, Seungjun Park, Sumin Park, Sung-Bae Cho, Noseong Park</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 聚焦图神经网络架构对比，提出分形节点改进MPNN的长距离信息传递，解决Graph Transformers的效率问题，属于深度学习理论中的网络架构研究，对图模型的效率与表现力提升有重要意义。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.13010' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression</h3>
<p><strong>Authors:</strong> Tingkai Yan, Haodong Wen, Binghui Li, Kairong Luo, Wenguang Chen, Kaifeng Lyu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 理论分析线性回归的多epoch缩放规律，提出“有效复用率”量化数据重复的价值，属于深度学习理论中的scaling laws研究，对理解有限数据下的模型训练有理论贡献。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.13421' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 8.0/10]</span> AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate</h3>
<p><strong>Authors:</strong> Meng Zhu (Unknown), Quan Xiao (Unknown), Weidong Min (Unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对Adam优化器易收敛到非平坦 minima的问题，提出AdamX改进其二阶矩估计的指数衰减机制，属于深度学习理论中的optimizer研究方向，与用户高优先级方向高度相关。
Score: 8
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.13465' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Toward bilipshiz geometric models</h3>
<p><strong>Authors:</strong> Yonatan Sverdlov, Eitan Rosen, Nadav Dym</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究点云模型的双Lipschitz等价性，分析现有不变网络的几何性质并提出改进方法，属于深度学习理论中的几何模型研究方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11735' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation</h3>
<p><strong>Authors:</strong> Divake Kumar, Patrick Poggi, Sina Tayebati, Devashri Naik, Nilesh Ahuja, Amit Ranjan Trivedi</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出深度特征空间中 aleatoric与epistemic不确定性的校准分解方法，用于推理时自适应模型选择，属于深度学习理论中的不确定性估计与泛化研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12389' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation</h3>
<p><strong>Authors:</strong> Nuolin Sun, Linyuan Wang, Haonan Wei, Lei Li, Bin Yan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 将生成模型的MeanFlow流场思想引入ResNet架构优化，通过压缩-孵化策略提升参数效率与判别性能，属于深度学习理论中的网络结构创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12422' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning</h3>
<p><strong>Authors:</strong> Jingshan Hong, Haigen Hu, Huihuang Zhang, Qianwei Zhou, Zhao Li</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出将掩码区域作为监督学习的有效信息源，通过掩码语义多样性增强特征表达，属于深度学习理论中的训练策略创新。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12480' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning</h3>
<p><strong>Authors:</strong> Guoqing Ma, Yuhan Zhang, Yuming Dai, Guangfu Hao, Yang Chen, Shan Yu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出COWM层通过聚类和正交化稳定RL模型训练，解决非平稳环境下的样本效率问题，属于深度学习理论中的RL架构优化。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11607' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches</h3>
<p><strong>Authors:</strong> Aamir Shehzad</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出自适应和保守的PINN方法，提升RLW方程的求解精度，解决物理驱动模型的误差问题，属于深度学习理论中的优化方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11638' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> On the Probabilistic Learnability of Compact Neural Network Preimage Bounds</h3>
<p><strong>Authors:</strong> Luca Marzari, Manuele Bicego, Ferdinando Cicalese, Alessandro Farinelli</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究神经网络前像边界的概率可学习性，提出RF-ProVe方法，属于深度学习理论中的可学习性分析方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11656' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE</h3>
<p><strong>Authors:</strong> Sameeksha Sriram, Ayush Paliwal, Alexander S. Ecker, Chase van de Geijn</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Clifford代数转子嵌入改进位置编码的 commutative 性，提升Transformer性能，属于深度学习理论中的嵌入架构方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11665' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks</h3>
<p><strong>Authors:</strong> Rajit Rajpal, Benedict Leimkuhler, Yuanhao Jiang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SA-SGLD方法通过自适应步长提升BNN后验采样精度，解决优化器步长选择问题，属于深度学习理论中的optimizer方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11666' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Do traveling waves make good positional encodings?</h3>
<p><strong>Authors:</strong> Chase van de Geijn, Ayush Paliwal, Timo Lüddecke, Alexander S. Ecker</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究行波作为位置编码的有效性，提出RollPE方法，提升Transformer位置编码性能，属于深度学习理论中的位置编码方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11668' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning with Preserving for Continual Multitask Learning</h3>
<p><strong>Authors:</strong> Hanchen David Wang, Siwoo Bae, Zirong Chen, Meiyi Ma</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出LwP框架通过保持表示空间几何结构提升持续多任务学习性能，解决灾难性遗忘问题，属于深度学习理论中的持续学习方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11676' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems</h3>
<p><strong>Authors:</strong> Qing Yao, Lijian Gao, Qirong Mao, Dong Ming</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出正则化薛定谔桥方法缓解逆问题中的失真和暴露偏差，提升模型鲁棒性，属于深度学习理论中的逆问题优化方向。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11686' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification</h3>
<p><strong>Authors:</strong> Xingqi Lin (unknown), Liangyu Chen (unknown), Min Wu (unknown), Min Zhang (unknown), Zhenbing Zeng (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出更紧的截断长方体近似方法用于RNN鲁棒性验证，涉及网络架构的鲁棒性分析，属于深度学习理论方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11699' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> FSC-Net: Fast-Slow Consolidation Networks for Continual Learning</h3>
<p><strong>Authors:</strong> Mohamed El Gorrim (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出快慢整合网络解决持续学习中的灾难性遗忘，涉及网络架构设计，属于深度学习理论方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11707' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> KAN/H: Kolmogorov-Arnold Network using Haar-like bases</h3>
<p><strong>Authors:</strong> Susumu Katayama (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出使用Haar-like基的Kolmogorov-Arnold网络，改进网络架构，属于深度学习理论的network architecture方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11736' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Sumudu Neural Operator for ODEs and PDEs</h3>
<p><strong>Authors:</strong> Ben Zelenskiy (unknown), Saibilila Abudukelimu (unknown), George Flint (unknown), Kevin Zhu (unknown), Sunishchal Dev (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Sumudu神经算子用于ODE和PDE求解，涉及神经算子架构设计，属于深度学习理论的network architecture方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11762' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Learning Fair Representations with Kolmogorov-Arnold Networks</h3>
<p><strong>Authors:</strong> Amisha Priyadarshini (unknown), Sergio Gago-Masague (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 结合Kolmogorov-Arnold网络学习公平表示，涉及网络架构和公平性，属于深度学习理论的network architecture方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11767' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm</h3>
<p><strong>Authors:</strong> Ci Lin (unknown), Tet Yeap (unknown), Iluju Kiringa (unknown), Biwei Zhang (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出正则化策略提升双向联想记忆的鲁棒性，涉及网络架构和正则化，属于深度学习理论的network architecture方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11902' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Batch Matrix-form Equations and Implementation of Multilayer Perceptrons</h3>
<p><strong>Authors:</strong> Wieger Wesselink (unknown), Bram Grooten (unknown), Huub van de Wetering (unknown), Qiao Xiao (unknown), Decebal Constantin Mocanu (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 推导MLP的批量矩阵形式方程，涉及网络架构的数学基础，属于深度学习理论的network architecture方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11918' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks</h3>
<p><strong>Authors:</strong> Ziyao Cui (unknown), Edric Tam (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出插值拉普拉斯嵌入用于GNN特征增强，涉及网络架构的谱增强，属于深度学习理论的network architecture方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11928' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts</h3>
<p><strong>Authors:</strong> C. César Claros Olivares (unknown), Austin J. Brockmeier (unknown)</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 系统分析表示和训练范式变化下的OOD检测，涉及训练范式和表示学习，属于深度学习理论的training paradigm方向
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11934' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks</h3>
<p><strong>Authors:</strong> Akira Tamamori</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 分析核Hopfield网络的吸引子景观，提出“优化脊”现象与自组织机制，属于深度学习理论中的经典网络架构研究，对理解Hopfield网络的记忆容量与稳定性有理论贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.13053' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Laplace Learning in Wasserstein Space</h3>
<p><strong>Authors:</strong> Mary Chriselda Antony Oliver, Michael Roberts, Carola-Bibiane Schönlieb, Matthew Thorpe</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 研究Wasserstein空间中的拉普拉斯学习，扩展半监督学习到无限维空间，属于深度学习理论中的manifold hypothesis与半监督学习理论，对高维数据的表征学习有理论贡献。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.13229' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Tab-PET: Graph-Based Positional Encodings for Tabular Transformers</h3>
<p><strong>Authors:</strong> Yunze Leng, Rohan Ghosh, Mehul Motani</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出Tab-PET用图生成表格Transformer的位置编码，提升泛化性能，属于深度学习理论中的表格模型架构研究，对Tabular Transformer的改进有帮助。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.13338' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 7.0/10]</span> Decoupled Action Head: Confining Task Knowledge to Conditioning Layers</h3>
<p><strong>Authors:</strong> Jian Zhou, Sihao Lin, Shuai Fu, Qi WU</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 针对行为克隆（BC）中Diffusion Policy的数据稀缺问题，提出解耦训练策略，预训练通用动作头并通过条件层适配新任务，提升了训练效率与模型泛化能力，属于深度学习理论中网络架构与训练策略的重要研究。
Score: 7
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.12101' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination</h3>
<p><strong>Authors:</strong> Qiuhao Zeng</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出元组件组合的元学习算法，提升少样本学习的泛化能力，属于深度学习理论中的元学习方向。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11632' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> H-Model: Dynamic Neural Architectures for Adaptive Processing</h3>
<p><strong>Authors:</strong> Dmytro Hospodarchuk</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出H-Model动态神经网络架构实现自适应计算，探索可解释的动态模型方向，属于深度学习理论中的动态架构方向。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11669' target='_blank'>阅读论文 &raquo;</a></p>
</div>
<div class='paper-card'>
<h3><span class='score'>[Score: 6.0/10]</span> A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications</h3>
<p><strong>Authors:</strong> Zhehao Xu, Lok Ming Lui</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Reason:</strong> 提出SBN-Opt神经优化框架解决自由边界微分同胚映射问题，属于深度学习理论中的优化框架方向。
Score: 6
Field: 深度学习理论</p>
<p><a href='https://arxiv.org/abs/2511.11679' target='_blank'>阅读论文 &raquo;</a></p>
</div>
</div>
</div>

        <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            const sections = document.querySelectorAll('.field-section');
            const navLinks = document.querySelectorAll('.navbar a');

            function changeLinkState() {
                let index = sections.length;

                while(--index && window.scrollY + 100 < sections[index].offsetTop) {}

                navLinks.forEach((link) => link.classList.remove('active'));
                if (navLinks[index]) {
                    navLinks[index].classList.add('active');
                }
            }

            changeLinkState();
            window.addEventListener('scroll', changeLinkState);
        });

        function onHistoryDateChange(select) {
            if (select.value) {
                window.location.href = select.value;
            }
        }
        </script>
        </body>
</html>